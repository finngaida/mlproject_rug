Lmod has detected the following error: These module(s) exist but cannot be
loaded as requested: "foss/2018a"

   Try: "module spider foss/2018a" to see how to load the module(s).




Collecting numpy==1.14
  Using cached https://files.pythonhosted.org/packages/0d/8a/e0223a40f980e0442a2045dcf79e4a8a90339593525599a0add318da2428/numpy-1.14.0-cp27-cp27mu-manylinux1_x86_64.whl
Installing collected packages: numpy
Successfully installed numpy-1.11.1
You are using pip version 8.1.2, however version 18.1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
Requirement already satisfied (use --upgrade to upgrade): tensorflow-gpu in /home/s3801128/.local/lib/python2.7/site-packages
Requirement already satisfied (use --upgrade to upgrade): protobuf>=3.4.0 in /apps/sandybridge/software/protobuf-python/3.4.0-foss-2016a-Python-2.7.12/lib/python2.7/site-packages/protobuf-3.4.0-py2.7.egg (from tensorflow-gpu)
Requirement already satisfied (use --upgrade to upgrade): enum34>=1.1.6 in /apps/sandybridge/software/Python/2.7.12-foss-2016a/lib/python2.7/site-packages/enum34-1.1.6-py2.7.egg (from tensorflow-gpu)
Requirement already satisfied (use --upgrade to upgrade): wheel in /home/s3801128/.local/lib/python2.7/site-packages (from tensorflow-gpu)
Requirement already satisfied (use --upgrade to upgrade): astor>=0.6.0 in /home/s3801128/.local/lib/python2.7/site-packages (from tensorflow-gpu)
Requirement already satisfied (use --upgrade to upgrade): backports.weakref>=1.0rc1 in /apps/sandybridge/software/tensorflow/1.3.1-foss-2016a-Python-2.7.12-CUDA-8.0.61/lib/python2.7/site-packages (from tensorflow-gpu)
Requirement already satisfied (use --upgrade to upgrade): mock>=2.0.0 in /home/s3801128/.local/lib/python2.7/site-packages (from tensorflow-gpu)
Requirement already satisfied (use --upgrade to upgrade): termcolor>=1.1.0 in /home/s3801128/.local/lib/python2.7/site-packages (from tensorflow-gpu)
Requirement already satisfied (use --upgrade to upgrade): gast>=0.2.0 in /home/s3801128/.local/lib/python2.7/site-packages (from tensorflow-gpu)
Requirement already satisfied (use --upgrade to upgrade): tensorboard<1.8.0,>=1.7.0 in /home/s3801128/.local/lib/python2.7/site-packages (from tensorflow-gpu)
Requirement already satisfied (use --upgrade to upgrade): absl-py>=0.1.6 in /home/s3801128/.local/lib/python2.7/site-packages (from tensorflow-gpu)
Requirement already satisfied (use --upgrade to upgrade): grpcio>=1.8.6 in /home/s3801128/.local/lib/python2.7/site-packages (from tensorflow-gpu)
Requirement already satisfied (use --upgrade to upgrade): six>=1.10.0 in /apps/sandybridge/software/Python/2.7.12-foss-2016a/lib/python2.7/site-packages/six-1.10.0-py2.7.egg (from tensorflow-gpu)
Collecting numpy>=1.13.3 (from tensorflow-gpu)
  Using cached https://files.pythonhosted.org/packages/9f/85/163127d3fb0573deb9eca947cfc73aa3618eaaf8656501460574471d114a/numpy-1.16.0-cp27-cp27mu-manylinux1_x86_64.whl
Requirement already satisfied (use --upgrade to upgrade): setuptools in /apps/sandybridge/software/Python/2.7.12-foss-2016a/lib/python2.7/site-packages/setuptools-23.1.0-py2.7.egg (from protobuf>=3.4.0->tensorflow-gpu)
Requirement already satisfied (use --upgrade to upgrade): funcsigs>=1; python_version < "3.3" in /apps/sandybridge/software/Python/2.7.12-foss-2016a/lib/python2.7/site-packages/funcsigs-1.0.2-py2.7.egg (from mock>=2.0.0->tensorflow-gpu)
Requirement already satisfied (use --upgrade to upgrade): pbr>=0.11 in /home/s3801128/.local/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow-gpu)
Requirement already satisfied (use --upgrade to upgrade): futures>=3.1.1; python_version < "3" in /home/s3801128/.local/lib/python2.7/site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow-gpu)
Requirement already satisfied (use --upgrade to upgrade): bleach==1.5.0 in /apps/sandybridge/software/tensorflow/1.3.1-foss-2016a-Python-2.7.12-CUDA-8.0.61/lib/python2.7/site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow-gpu)
Requirement already satisfied (use --upgrade to upgrade): html5lib==0.9999999 in /apps/sandybridge/software/tensorflow/1.3.1-foss-2016a-Python-2.7.12-CUDA-8.0.61/lib/python2.7/site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow-gpu)
Requirement already satisfied (use --upgrade to upgrade): werkzeug>=0.11.10 in /apps/sandybridge/software/tensorflow/1.3.1-foss-2016a-Python-2.7.12-CUDA-8.0.61/lib/python2.7/site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow-gpu)
Requirement already satisfied (use --upgrade to upgrade): markdown>=2.6.8 in /apps/sandybridge/software/tensorflow/1.3.1-foss-2016a-Python-2.7.12-CUDA-8.0.61/lib/python2.7/site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow-gpu)
Installing collected packages: numpy
Successfully installed numpy-1.11.1
You are using pip version 8.1.2, however version 18.1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
Collecting Cython==0.28
  Using cached https://files.pythonhosted.org/packages/f7/a1/0ecf3ace2e97eaf63f93931be38361763fff0118aca6b70119dde9867e14/Cython-0.28-cp27-cp27mu-manylinux1_x86_64.whl
Installing collected packages: Cython
Successfully installed Cython-0.24
You are using pip version 8.1.2, however version 18.1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
Requirement already satisfied (use --upgrade to upgrade): configparser in /home/s3801128/.local/lib/python2.7/site-packages
You are using pip version 8.1.2, however version 18.1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
Traceback (most recent call last):
  File "setup.py", line 66, in <module>
    setup_requires = ["cython>=0.28", "numpy>=1.14.0"]
  File "/software/software/Python/2.7.12-foss-2016a/lib/python2.7/distutils/core.py", line 111, in setup
    _setup_distribution = dist = klass(attrs)
  File "build/bdist.linux-x86_64/egg/setuptools/dist.py", line 269, in __init__
  File "build/bdist.linux-x86_64/egg/setuptools/dist.py", line 313, in fetch_build_eggs
  File "build/bdist.linux-x86_64/egg/pkg_resources/__init__.py", line 826, in resolve
  File "build/bdist.linux-x86_64/egg/pkg_resources/__init__.py", line 1085, in best_match
  File "build/bdist.linux-x86_64/egg/pkg_resources/__init__.py", line 695, in find
pkg_resources.VersionConflict: (Cython 0.24 (/apps/sandybridge/software/Python/2.7.12-foss-2016a/lib/python2.7/site-packages/Cython-0.24-py2.7-linux-x86_64.egg), Requirement.parse('cython>=0.28'))
Using TensorFlow backend.
keras_retinanet/bin/train.py:353: UserWarning: Using experimental backbone vgg16. Only resnet50 has been properly tested.
  warnings.warn('Using experimental backbone {}. Only resnet50 has been properly tested.'.format(parsed_args.backbone))
2019-01-19 16:56:20.745377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: Tesla K40m
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:82:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2019-01-19 16:56:20.745477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
2019-01-19 16:56:20.745499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
2019-01-19 16:56:20.745524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:82:00.0)
Creating model, this may take a second...
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, None, None, 3 0                                            
__________________________________________________________________________________________________
block1_conv1 (Conv2D)           (None, None, None, 6 1792        input_1[0][0]                    
__________________________________________________________________________________________________
block1_conv2 (Conv2D)           (None, None, None, 6 36928       block1_conv1[0][0]               
__________________________________________________________________________________________________
block1_pool (MaxPooling2D)      (None, None, None, 6 0           block1_conv2[0][0]               
__________________________________________________________________________________________________
block2_conv1 (Conv2D)           (None, None, None, 1 73856       block1_pool[0][0]                
__________________________________________________________________________________________________
block2_conv2 (Conv2D)           (None, None, None, 1 147584      block2_conv1[0][0]               
__________________________________________________________________________________________________
block2_pool (MaxPooling2D)      (None, None, None, 1 0           block2_conv2[0][0]               
__________________________________________________________________________________________________
block3_conv1 (Conv2D)           (None, None, None, 2 295168      block2_pool[0][0]                
__________________________________________________________________________________________________
block3_conv2 (Conv2D)           (None, None, None, 2 590080      block3_conv1[0][0]               
__________________________________________________________________________________________________
block3_conv3 (Conv2D)           (None, None, None, 2 590080      block3_conv2[0][0]               
__________________________________________________________________________________________________
block3_pool (MaxPooling2D)      (None, None, None, 2 0           block3_conv3[0][0]               
__________________________________________________________________________________________________
block4_conv1 (Conv2D)           (None, None, None, 5 1180160     block3_pool[0][0]                
__________________________________________________________________________________________________
block4_conv2 (Conv2D)           (None, None, None, 5 2359808     block4_conv1[0][0]               
__________________________________________________________________________________________________
block4_conv3 (Conv2D)           (None, None, None, 5 2359808     block4_conv2[0][0]               
__________________________________________________________________________________________________
block4_pool (MaxPooling2D)      (None, None, None, 5 0           block4_conv3[0][0]               
__________________________________________________________________________________________________
block5_conv1 (Conv2D)           (None, None, None, 5 2359808     block4_pool[0][0]                
__________________________________________________________________________________________________
block5_conv2 (Conv2D)           (None, None, None, 5 2359808     block5_conv1[0][0]               
__________________________________________________________________________________________________
block5_conv3 (Conv2D)           (None, None, None, 5 2359808     block5_conv2[0][0]               
__________________________________________________________________________________________________
block5_pool (MaxPooling2D)      (None, None, None, 5 0           block5_conv3[0][0]               
__________________________________________________________________________________________________
C5_reduced (Conv2D)             (None, None, None, 2 131328      block5_pool[0][0]                
__________________________________________________________________________________________________
P5_upsampled (UpsampleLike)     (None, None, None, 2 0           C5_reduced[0][0]                 
                                                                 block4_pool[0][0]                
__________________________________________________________________________________________________
C4_reduced (Conv2D)             (None, None, None, 2 131328      block4_pool[0][0]                
__________________________________________________________________________________________________
P4_merged (Add)                 (None, None, None, 2 0           P5_upsampled[0][0]               
                                                                 C4_reduced[0][0]                 
__________________________________________________________________________________________________
P4_upsampled (UpsampleLike)     (None, None, None, 2 0           P4_merged[0][0]                  
                                                                 block3_pool[0][0]                
__________________________________________________________________________________________________
C3_reduced (Conv2D)             (None, None, None, 2 65792       block3_pool[0][0]                
__________________________________________________________________________________________________
P6 (Conv2D)                     (None, None, None, 2 1179904     block5_pool[0][0]                
__________________________________________________________________________________________________
P3_merged (Add)                 (None, None, None, 2 0           P4_upsampled[0][0]               
                                                                 C3_reduced[0][0]                 
__________________________________________________________________________________________________
C6_relu (Activation)            (None, None, None, 2 0           P6[0][0]                         
__________________________________________________________________________________________________
P3 (Conv2D)                     (None, None, None, 2 590080      P3_merged[0][0]                  
__________________________________________________________________________________________________
P4 (Conv2D)                     (None, None, None, 2 590080      P4_merged[0][0]                  
__________________________________________________________________________________________________
P5 (Conv2D)                     (None, None, None, 2 590080      C5_reduced[0][0]                 
__________________________________________________________________________________________________
P7 (Conv2D)                     (None, None, None, 2 590080      C6_relu[0][0]                    
__________________________________________________________________________________________________
regression_submodel (Model)     (None, None, 4)      2443300     P3[0][0]                         
                                                                 P4[0][0]                         
                                                                 P5[0][0]                         
                                                                 P6[0][0]                         
                                                                 P7[0][0]                         
__________________________________________________________________________________________________
classification_submodel (Model) (None, None, 2)      2401810     P3[0][0]                         
                                                                 P4[0][0]                         
                                                                 P5[0][0]                         
                                                                 P6[0][0]                         
                                                                 P7[0][0]                         
__________________________________________________________________________________________________
regression (Concatenate)        (None, None, 4)      0           regression_submodel[1][0]        
                                                                 regression_submodel[2][0]        
                                                                 regression_submodel[3][0]        
                                                                 regression_submodel[4][0]        
                                                                 regression_submodel[5][0]        
__________________________________________________________________________________________________
classification (Concatenate)    (None, None, 2)      0           classification_submodel[1][0]    
                                                                 classification_submodel[2][0]    
                                                                 classification_submodel[3][0]    
                                                                 classification_submodel[4][0]    
                                                                 classification_submodel[5][0]    
==================================================================================================
Total params: 23,428,470
Trainable params: 8,713,782
Non-trainable params: 14,714,688
__________________________________________________________________________________________________
None
Epoch 1/50

    1/10000 [..............................] - ETA: 19:15:09 - loss: 0.2412 - regression_loss: 0.2173 - classification_loss: 0.0239
    2/10000 [..............................] - ETA: 10:51:41 - loss: 0.2300 - regression_loss: 0.2064 - classification_loss: 0.0236
    3/10000 [..............................] - ETA: 8:03:48 - loss: 0.2323 - regression_loss: 0.2033 - classification_loss: 0.0290 
    4/10000 [..............................] - ETA: 6:39:43 - loss: 0.2304 - regression_loss: 0.2052 - classification_loss: 0.0251
    5/10000 [..............................] - ETA: 5:49:19 - loss: 0.2832 - regression_loss: 0.2418 - classification_loss: 0.0415
    6/10000 [..............................] - ETA: 5:15:37 - loss: 0.3042 - regression_loss: 0.2553 - classification_loss: 0.0489
    7/10000 [..............................] - ETA: 4:51:35 - loss: 0.3658 - regression_loss: 0.3164 - classification_loss: 0.0494
    8/10000 [..............................] - ETA: 4:33:38 - loss: 0.4024 - regression_loss: 0.3552 - classification_loss: 0.0472
    9/10000 [..............................] - ETA: 4:19:40 - loss: 0.3818 - regression_loss: 0.3338 - classification_loss: 0.0480
   10/10000 [..............................] - ETA: 4:08:26 - loss: 0.4098 - regression_loss: 0.3618 - classification_loss: 0.0480
   11/10000 [..............................] - ETA: 3:59:21 - loss: 0.4254 - regression_loss: 0.3775 - classification_loss: 0.0478
   12/10000 [..............................] - ETA: 3:51:40 - loss: 0.3899 - regression_loss: 0.3461 - classification_loss: 0.0439
   13/10000 [..............................] - ETA: 3:45:06 - loss: 0.3717 - regression_loss: 0.3287 - classification_loss: 0.0430
   14/10000 [..............................] - ETA: 3:39:30 - loss: 0.3569 - regression_loss: 0.3154 - classification_loss: 0.0414
   15/10000 [..............................] - ETA: 3:34:42 - loss: 0.3518 - regression_loss: 0.3120 - classification_loss: 0.0397
   16/10000 [..............................] - ETA: 3:30:28 - loss: 0.3397 - regression_loss: 0.3018 - classification_loss: 0.0379
   17/10000 [..............................] - ETA: 3:26:44 - loss: 0.3521 - regression_loss: 0.3130 - classification_loss: 0.0391
   18/10000 [..............................] - ETA: 3:23:27 - loss: 0.3527 - regression_loss: 0.3136 - classification_loss: 0.0391
   19/10000 [..............................] - ETA: 3:20:30 - loss: 0.3562 - regression_loss: 0.3165 - classification_loss: 0.0397
   20/10000 [..............................] - ETA: 3:17:50 - loss: 0.3625 - regression_loss: 0.3211 - classification_loss: 0.0414
   21/10000 [..............................] - ETA: 3:15:24 - loss: 0.3452 - regression_loss: 0.3058 - classification_loss: 0.0395
   22/10000 [..............................] - ETA: 3:13:14 - loss: 0.3406 - regression_loss: 0.3026 - classification_loss: 0.0380
   23/10000 [..............................] - ETA: 3:11:16 - loss: 0.3387 - regression_loss: 0.3003 - classification_loss: 0.0384
   24/10000 [..............................] - ETA: 3:09:28 - loss: 0.3482 - regression_loss: 0.3103 - classification_loss: 0.0379
   25/10000 [..............................] - ETA: 3:07:46 - loss: 0.3607 - regression_loss: 0.3241 - classification_loss: 0.0366
   26/10000 [..............................] - ETA: 3:06:13 - loss: 0.3524 - regression_loss: 0.3166 - classification_loss: 0.0358
   27/10000 [..............................] - ETA: 3:04:46 - loss: 0.3703 - regression_loss: 0.3355 - classification_loss: 0.0348
   28/10000 [..............................] - ETA: 3:03:27 - loss: 0.3571 - regression_loss: 0.3235 - classification_loss: 0.0335
   29/10000 [..............................] - ETA: 3:02:11 - loss: 0.3573 - regression_loss: 0.3235 - classification_loss: 0.0338
   30/10000 [..............................] - ETA: 3:00:59 - loss: 0.3454 - regression_loss: 0.3127 - classification_loss: 0.0327
   31/10000 [..............................] - ETA: 2:59:53 - loss: 0.3487 - regression_loss: 0.3157 - classification_loss: 0.0330
   32/10000 [..............................] - ETA: 2:58:51 - loss: 0.3447 - regression_loss: 0.3125 - classification_loss: 0.0322
   33/10000 [..............................] - ETA: 2:57:53 - loss: 0.3462 - regression_loss: 0.3139 - classification_loss: 0.0323
   34/10000 [..............................] - ETA: 2:56:58 - loss: 0.3495 - regression_loss: 0.3171 - classification_loss: 0.0324
   35/10000 [..............................] - ETA: 2:56:06 - loss: 0.3452 - regression_loss: 0.3135 - classification_loss: 0.0317
   36/10000 [..............................] - ETA: 2:55:18 - loss: 0.3467 - regression_loss: 0.3158 - classification_loss: 0.0309
   37/10000 [..............................] - ETA: 2:54:31 - loss: 0.3500 - regression_loss: 0.3192 - classification_loss: 0.0308
   38/10000 [..............................] - ETA: 2:53:47 - loss: 0.3555 - regression_loss: 0.3252 - classification_loss: 0.0302
   39/10000 [..............................] - ETA: 2:53:04 - loss: 0.3599 - regression_loss: 0.3296 - classification_loss: 0.0303
   40/10000 [..............................] - ETA: 2:52:24 - loss: 0.3628 - regression_loss: 0.3323 - classification_loss: 0.0305
   41/10000 [..............................] - ETA: 2:51:46 - loss: 0.3651 - regression_loss: 0.3341 - classification_loss: 0.0309
   42/10000 [..............................] - ETA: 2:51:09 - loss: 0.3688 - regression_loss: 0.3384 - classification_loss: 0.0304
   43/10000 [..............................] - ETA: 2:50:35 - loss: 0.3692 - regression_loss: 0.3389 - classification_loss: 0.0303
   44/10000 [..............................] - ETA: 2:50:02 - loss: 0.3683 - regression_loss: 0.3383 - classification_loss: 0.0301
   45/10000 [..............................] - ETA: 2:49:31 - loss: 0.3708 - regression_loss: 0.3407 - classification_loss: 0.0301
   46/10000 [..............................] - ETA: 2:49:01 - loss: 0.3699 - regression_loss: 0.3404 - classification_loss: 0.0296
   47/10000 [..............................] - ETA: 2:48:33 - loss: 0.3678 - regression_loss: 0.3382 - classification_loss: 0.0297
   48/10000 [..............................] - ETA: 2:48:05 - loss: 0.3602 - regression_loss: 0.3311 - classification_loss: 0.0291
   49/10000 [..............................] - ETA: 2:47:39 - loss: 0.3611 - regression_loss: 0.3300 - classification_loss: 0.0310
   50/10000 [..............................] - ETA: 2:47:14 - loss: 0.3563 - regression_loss: 0.3258 - classification_loss: 0.0305
   51/10000 [..............................] - ETA: 2:46:49 - loss: 0.3652 - regression_loss: 0.3345 - classification_loss: 0.0307
   52/10000 [..............................] - ETA: 2:46:25 - loss: 0.3679 - regression_loss: 0.3371 - classification_loss: 0.0309
   53/10000 [..............................] - ETA: 2:46:02 - loss: 0.3676 - regression_loss: 0.3370 - classification_loss: 0.0306
   54/10000 [..............................] - ETA: 2:45:40 - loss: 0.3659 - regression_loss: 0.3355 - classification_loss: 0.0305
   55/10000 [..............................] - ETA: 2:45:18 - loss: 0.3676 - regression_loss: 0.3377 - classification_loss: 0.0299
   56/10000 [..............................] - ETA: 2:44:57 - loss: 0.3696 - regression_loss: 0.3397 - classification_loss: 0.0299
   57/10000 [..............................] - ETA: 2:44:37 - loss: 0.3664 - regression_loss: 0.3369 - classification_loss: 0.0295
   58/10000 [..............................] - ETA: 2:44:18 - loss: 0.3718 - regression_loss: 0.3419 - classification_loss: 0.0298
   59/10000 [..............................] - ETA: 2:43:59 - loss: 0.3680 - regression_loss: 0.3385 - classification_loss: 0.0295
   60/10000 [..............................] - ETA: 2:43:42 - loss: 0.3650 - regression_loss: 0.3356 - classification_loss: 0.0295
   61/10000 [..............................] - ETA: 2:43:25 - loss: 0.3620 - regression_loss: 0.3327 - classification_loss: 0.0293
   62/10000 [..............................] - ETA: 2:43:08 - loss: 0.3572 - regression_loss: 0.3283 - classification_loss: 0.0289
   63/10000 [..............................] - ETA: 2:42:52 - loss: 0.3515 - regression_loss: 0.3231 - classification_loss: 0.0285
   64/10000 [..............................] - ETA: 2:42:35 - loss: 0.3529 - regression_loss: 0.3241 - classification_loss: 0.0288
   65/10000 [..............................] - ETA: 2:42:21 - loss: 0.3537 - regression_loss: 0.3252 - classification_loss: 0.0285
   66/10000 [..............................] - ETA: 2:42:05 - loss: 0.3616 - regression_loss: 0.3324 - classification_loss: 0.0292
   67/10000 [..............................] - ETA: 2:41:51 - loss: 0.3603 - regression_loss: 0.3315 - classification_loss: 0.0288
   68/10000 [..............................] - ETA: 2:41:37 - loss: 0.3550 - regression_loss: 0.3266 - classification_loss: 0.0284
   69/10000 [..............................] - ETA: 2:41:23 - loss: 0.3555 - regression_loss: 0.3274 - classification_loss: 0.0282
   70/10000 [..............................] - ETA: 2:41:10 - loss: 0.3592 - regression_loss: 0.3309 - classification_loss: 0.0283
   71/10000 [..............................] - ETA: 2:40:57 - loss: 0.3585 - regression_loss: 0.3303 - classification_loss: 0.0282
   72/10000 [..............................] - ETA: 2:40:44 - loss: 0.3618 - regression_loss: 0.3321 - classification_loss: 0.0297
   73/10000 [..............................] - ETA: 2:40:31 - loss: 0.3619 - regression_loss: 0.3323 - classification_loss: 0.0295
   74/10000 [..............................] - ETA: 2:40:20 - loss: 0.3597 - regression_loss: 0.3303 - classification_loss: 0.0293
   75/10000 [..............................] - ETA: 2:40:08 - loss: 0.3604 - regression_loss: 0.3308 - classification_loss: 0.0295
   76/10000 [..............................] - ETA: 2:39:57 - loss: 0.3630 - regression_loss: 0.3331 - classification_loss: 0.0299
   77/10000 [..............................] - ETA: 2:39:45 - loss: 0.3614 - regression_loss: 0.3315 - classification_loss: 0.0298
   78/10000 [..............................] - ETA: 2:39:34 - loss: 0.3615 - regression_loss: 0.3318 - classification_loss: 0.0297
   79/10000 [..............................] - ETA: 2:39:24 - loss: 0.3601 - regression_loss: 0.3306 - classification_loss: 0.0295
   80/10000 [..............................] - ETA: 2:39:13 - loss: 0.3601 - regression_loss: 0.3302 - classification_loss: 0.0299
   81/10000 [..............................] - ETA: 2:39:03 - loss: 0.3619 - regression_loss: 0.3319 - classification_loss: 0.0301
   82/10000 [..............................] - ETA: 2:38:53 - loss: 0.3633 - regression_loss: 0.3332 - classification_loss: 0.0301
   83/10000 [..............................] - ETA: 2:38:43 - loss: 0.3589 - regression_loss: 0.3292 - classification_loss: 0.0297
   84/10000 [..............................] - ETA: 2:38:33 - loss: 0.3582 - regression_loss: 0.3285 - classification_loss: 0.0297
   85/10000 [..............................] - ETA: 2:38:24 - loss: 0.3578 - regression_loss: 0.3282 - classification_loss: 0.0296
   86/10000 [..............................] - ETA: 2:38:15 - loss: 0.3565 - regression_loss: 0.3260 - classification_loss: 0.0305
   87/10000 [..............................] - ETA: 2:38:06 - loss: 0.3545 - regression_loss: 0.3242 - classification_loss: 0.0302
   88/10000 [..............................] - ETA: 2:37:57 - loss: 0.3570 - regression_loss: 0.3262 - classification_loss: 0.0308
   89/10000 [..............................] - ETA: 2:37:49 - loss: 0.3556 - regression_loss: 0.3247 - classification_loss: 0.0309
   90/10000 [..............................] - ETA: 2:37:41 - loss: 0.3535 - regression_loss: 0.3226 - classification_loss: 0.0309
   91/10000 [..............................] - ETA: 2:37:33 - loss: 0.3577 - regression_loss: 0.3264 - classification_loss: 0.0314
   92/10000 [..............................] - ETA: 2:37:24 - loss: 0.3559 - regression_loss: 0.3248 - classification_loss: 0.0311
   93/10000 [..............................] - ETA: 2:37:16 - loss: 0.3564 - regression_loss: 0.3251 - classification_loss: 0.0313
   94/10000 [..............................] - ETA: 2:37:09 - loss: 0.3540 - regression_loss: 0.3230 - classification_loss: 0.0310
   95/10000 [..............................] - ETA: 2:37:01 - loss: 0.3539 - regression_loss: 0.3231 - classification_loss: 0.0308
   96/10000 [..............................] - ETA: 2:36:54 - loss: 0.3558 - regression_loss: 0.3244 - classification_loss: 0.0314
   97/10000 [..............................] - ETA: 2:36:46 - loss: 0.3552 - regression_loss: 0.3238 - classification_loss: 0.0314
   98/10000 [..............................] - ETA: 2:36:39 - loss: 0.3654 - regression_loss: 0.3315 - classification_loss: 0.0339
   99/10000 [..............................] - ETA: 2:36:32 - loss: 0.3646 - regression_loss: 0.3308 - classification_loss: 0.0338
  100/10000 [..............................] - ETA: 2:36:25 - loss: 0.3610 - regression_loss: 0.3275 - classification_loss: 0.0335
  101/10000 [..............................] - ETA: 2:36:18 - loss: 0.3603 - regression_loss: 0.3268 - classification_loss: 0.0335
  102/10000 [..............................] - ETA: 2:36:11 - loss: 0.3587 - regression_loss: 0.3254 - classification_loss: 0.0333
  103/10000 [..............................] - ETA: 2:36:04 - loss: 0.3575 - regression_loss: 0.3242 - classification_loss: 0.0333
  104/10000 [..............................] - ETA: 2:35:58 - loss: 0.3567 - regression_loss: 0.3235 - classification_loss: 0.0332
  105/10000 [..............................] - ETA: 2:35:51 - loss: 0.3555 - regression_loss: 0.3225 - classification_loss: 0.0331
  106/10000 [..............................] - ETA: 2:35:45 - loss: 0.3559 - regression_loss: 0.3227 - classification_loss: 0.0332
  107/10000 [..............................] - ETA: 2:35:39 - loss: 0.3546 - regression_loss: 0.3216 - classification_loss: 0.0330
  108/10000 [..............................] - ETA: 2:35:32 - loss: 0.3566 - regression_loss: 0.3238 - classification_loss: 0.0328
  109/10000 [..............................] - ETA: 2:35:26 - loss: 0.3562 - regression_loss: 0.3234 - classification_loss: 0.0328
  110/10000 [..............................] - ETA: 2:35:20 - loss: 0.3566 - regression_loss: 0.3241 - classification_loss: 0.0325
  111/10000 [..............................] - ETA: 2:35:14 - loss: 0.3601 - regression_loss: 0.3277 - classification_loss: 0.0324
  112/10000 [..............................] - ETA: 2:35:08 - loss: 0.3580 - regression_loss: 0.3258 - classification_loss: 0.0321
  113/10000 [..............................] - ETA: 2:35:02 - loss: 0.3548 - regression_loss: 0.3229 - classification_loss: 0.0319
  114/10000 [..............................] - ETA: 2:34:57 - loss: 0.3517 - regression_loss: 0.3201 - classification_loss: 0.0316
  115/10000 [..............................] - ETA: 2:34:52 - loss: 0.3539 - regression_loss: 0.3223 - classification_loss: 0.0316
  116/10000 [..............................] - ETA: 2:34:46 - loss: 0.3569 - regression_loss: 0.3248 - classification_loss: 0.0320
  117/10000 [..............................] - ETA: 2:34:41 - loss: 0.3581 - regression_loss: 0.3258 - classification_loss: 0.0323
  118/10000 [..............................] - ETA: 2:34:35 - loss: 0.3574 - regression_loss: 0.3251 - classification_loss: 0.0324
  119/10000 [..............................] - ETA: 2:34:30 - loss: 0.3562 - regression_loss: 0.3239 - classification_loss: 0.0323
  120/10000 [..............................] - ETA: 2:34:25 - loss: 0.3552 - regression_loss: 0.3228 - classification_loss: 0.0324
  121/10000 [..............................] - ETA: 2:34:20 - loss: 0.3533 - regression_loss: 0.3210 - classification_loss: 0.0323
  122/10000 [..............................] - ETA: 2:34:15 - loss: 0.3569 - regression_loss: 0.3207 - classification_loss: 0.0362
  123/10000 [..............................] - ETA: 2:34:10 - loss: 0.3573 - regression_loss: 0.3213 - classification_loss: 0.0360
  124/10000 [..............................] - ETA: 2:34:05 - loss: 0.3569 - regression_loss: 0.3209 - classification_loss: 0.0360
  125/10000 [..............................] - ETA: 2:34:00 - loss: 0.3563 - regression_loss: 0.3206 - classification_loss: 0.0357
  126/10000 [..............................] - ETA: 2:33:56 - loss: 0.3571 - regression_loss: 0.3214 - classification_loss: 0.0356
  127/10000 [..............................] - ETA: 2:33:51 - loss: 0.3596 - regression_loss: 0.3241 - classification_loss: 0.0355
  128/10000 [..............................] - ETA: 2:33:46 - loss: 0.3568 - regression_loss: 0.3216 - classification_loss: 0.0352
  129/10000 [..............................] - ETA: 2:33:42 - loss: 0.3605 - regression_loss: 0.3250 - classification_loss: 0.0355
  130/10000 [..............................] - ETA: 2:33:37 - loss: 0.3597 - regression_loss: 0.3243 - classification_loss: 0.0354
  131/10000 [..............................] - ETA: 2:33:33 - loss: 0.3593 - regression_loss: 0.3240 - classification_loss: 0.0353
  132/10000 [..............................] - ETA: 2:33:29 - loss: 0.3586 - regression_loss: 0.3233 - classification_loss: 0.0352
  133/10000 [..............................] - ETA: 2:33:25 - loss: 0.3623 - regression_loss: 0.3267 - classification_loss: 0.0356
  134/10000 [..............................] - ETA: 2:33:20 - loss: 0.3616 - regression_loss: 0.3261 - classification_loss: 0.0355
  135/10000 [..............................] - ETA: 2:33:16 - loss: 0.3607 - regression_loss: 0.3253 - classification_loss: 0.0354
  136/10000 [..............................] - ETA: 2:33:12 - loss: 0.3591 - regression_loss: 0.3240 - classification_loss: 0.0352
  137/10000 [..............................] - ETA: 2:33:08 - loss: 0.3576 - regression_loss: 0.3225 - classification_loss: 0.0350
  138/10000 [..............................] - ETA: 2:33:04 - loss: 0.3573 - regression_loss: 0.3225 - classification_loss: 0.0349
  139/10000 [..............................] - ETA: 2:33:00 - loss: 0.3577 - regression_loss: 0.3227 - classification_loss: 0.0350
  140/10000 [..............................] - ETA: 2:32:57 - loss: 0.3566 - regression_loss: 0.3218 - classification_loss: 0.0348
  141/10000 [..............................] - ETA: 2:32:52 - loss: 0.3563 - regression_loss: 0.3215 - classification_loss: 0.0348
  142/10000 [..............................] - ETA: 2:32:48 - loss: 0.3555 - regression_loss: 0.3209 - classification_loss: 0.0346
  143/10000 [..............................] - ETA: 2:32:45 - loss: 0.3558 - regression_loss: 0.3212 - classification_loss: 0.0346
  144/10000 [..............................] - ETA: 2:32:41 - loss: 0.3557 - regression_loss: 0.3212 - classification_loss: 0.0344
  145/10000 [..............................] - ETA: 2:32:38 - loss: 0.3565 - regression_loss: 0.3220 - classification_loss: 0.0344
  146/10000 [..............................] - ETA: 2:32:34 - loss: 0.3559 - regression_loss: 0.3217 - classification_loss: 0.0342
  147/10000 [..............................] - ETA: 2:32:30 - loss: 0.3549 - regression_loss: 0.3209 - classification_loss: 0.0340
  148/10000 [..............................] - ETA: 2:32:27 - loss: 0.3541 - regression_loss: 0.3201 - classification_loss: 0.0339
  149/10000 [..............................] - ETA: 2:32:23 - loss: 0.3540 - regression_loss: 0.3201 - classification_loss: 0.0339
  150/10000 [..............................] - ETA: 2:32:19 - loss: 0.3548 - regression_loss: 0.3208 - classification_loss: 0.0340
  151/10000 [..............................] - ETA: 2:32:16 - loss: 0.3537 - regression_loss: 0.3199 - classification_loss: 0.0338
  152/10000 [..............................] - ETA: 2:32:12 - loss: 0.3543 - regression_loss: 0.3206 - classification_loss: 0.0337
  153/10000 [..............................] - ETA: 2:32:09 - loss: 0.3519 - regression_loss: 0.3185 - classification_loss: 0.0335
  154/10000 [..............................] - ETA: 2:32:06 - loss: 0.3516 - regression_loss: 0.3182 - classification_loss: 0.0334
  155/10000 [..............................] - ETA: 2:32:02 - loss: 0.3511 - regression_loss: 0.3176 - classification_loss: 0.0335
  156/10000 [..............................] - ETA: 2:31:59 - loss: 0.3509 - regression_loss: 0.3175 - classification_loss: 0.0334
  157/10000 [..............................] - ETA: 2:31:55 - loss: 0.3500 - regression_loss: 0.3167 - classification_loss: 0.0333
  158/10000 [..............................] - ETA: 2:31:52 - loss: 0.3497 - regression_loss: 0.3165 - classification_loss: 0.0332
  159/10000 [..............................] - ETA: 2:31:49 - loss: 0.3475 - regression_loss: 0.3145 - classification_loss: 0.0330
  160/10000 [..............................] - ETA: 2:31:46 - loss: 0.3476 - regression_loss: 0.3146 - classification_loss: 0.0330
  161/10000 [..............................] - ETA: 2:31:42 - loss: 0.3463 - regression_loss: 0.3134 - classification_loss: 0.0329
  162/10000 [..............................] - ETA: 2:31:39 - loss: 0.3453 - regression_loss: 0.3125 - classification_loss: 0.0328
  163/10000 [..............................] - ETA: 2:31:36 - loss: 0.3496 - regression_loss: 0.3158 - classification_loss: 0.0338
  164/10000 [..............................] - ETA: 2:31:33 - loss: 0.3486 - regression_loss: 0.3149 - classification_loss: 0.0338
  165/10000 [..............................] - ETA: 2:31:31 - loss: 0.3558 - regression_loss: 0.3206 - classification_loss: 0.0351
  166/10000 [..............................] - ETA: 2:31:27 - loss: 0.3635 - regression_loss: 0.3261 - classification_loss: 0.0375
  167/10000 [..............................] - ETA: 2:31:25 - loss: 0.3627 - regression_loss: 0.3254 - classification_loss: 0.0373
  168/10000 [..............................] - ETA: 2:31:22 - loss: 0.3662 - regression_loss: 0.3286 - classification_loss: 0.0376
  169/10000 [..............................] - ETA: 2:31:18 - loss: 0.3652 - regression_loss: 0.3277 - classification_loss: 0.0374
  170/10000 [..............................] - ETA: 2:31:15 - loss: 0.3649 - regression_loss: 0.3274 - classification_loss: 0.0375
  171/10000 [..............................] - ETA: 2:31:12 - loss: 0.3640 - regression_loss: 0.3265 - classification_loss: 0.0375
  172/10000 [..............................] - ETA: 2:31:10 - loss: 0.3619 - regression_loss: 0.3246 - classification_loss: 0.0373
  173/10000 [..............................] - ETA: 2:31:07 - loss: 0.3637 - regression_loss: 0.3261 - classification_loss: 0.0375
  174/10000 [..............................] - ETA: 2:31:04 - loss: 0.3628 - regression_loss: 0.3253 - classification_loss: 0.0375
  175/10000 [..............................] - ETA: 2:31:01 - loss: 0.3642 - regression_loss: 0.3264 - classification_loss: 0.0378
  176/10000 [..............................] - ETA: 2:30:58 - loss: 0.3634 - regression_loss: 0.3255 - classification_loss: 0.0379
  177/10000 [..............................] - ETA: 2:30:55 - loss: 0.3633 - regression_loss: 0.3252 - classification_loss: 0.0381
  178/10000 [..............................] - ETA: 2:30:53 - loss: 0.3625 - regression_loss: 0.3246 - classification_loss: 0.0380
  179/10000 [..............................] - ETA: 2:30:50 - loss: 0.3626 - regression_loss: 0.3247 - classification_loss: 0.0379
  180/10000 [..............................] - ETA: 2:30:47 - loss: 0.3664 - regression_loss: 0.3284 - classification_loss: 0.0380
  181/10000 [..............................] - ETA: 2:30:44 - loss: 0.3683 - regression_loss: 0.3295 - classification_loss: 0.0388
  182/10000 [..............................] - ETA: 2:30:42 - loss: 0.3678 - regression_loss: 0.3292 - classification_loss: 0.0386
  183/10000 [..............................] - ETA: 2:30:39 - loss: 0.3687 - regression_loss: 0.3298 - classification_loss: 0.0389
  184/10000 [..............................] - ETA: 2:30:36 - loss: 0.3684 - regression_loss: 0.3297 - classification_loss: 0.0387
  185/10000 [..............................] - ETA: 2:30:34 - loss: 0.3678 - regression_loss: 0.3292 - classification_loss: 0.0387
  186/10000 [..............................] - ETA: 2:30:31 - loss: 0.3674 - regression_loss: 0.3288 - classification_loss: 0.0385
  187/10000 [..............................] - ETA: 2:30:28 - loss: 0.3678 - regression_loss: 0.3292 - classification_loss: 0.0386
  188/10000 [..............................] - ETA: 2:30:26 - loss: 0.3684 - regression_loss: 0.3297 - classification_loss: 0.0387
  189/10000 [..............................] - ETA: 2:30:23 - loss: 0.3665 - regression_loss: 0.3280 - classification_loss: 0.0385
  190/10000 [..............................] - ETA: 2:30:20 - loss: 0.3663 - regression_loss: 0.3278 - classification_loss: 0.0385
  191/10000 [..............................] - ETA: 2:30:18 - loss: 0.3661 - regression_loss: 0.3276 - classification_loss: 0.0385
  192/10000 [..............................] - ETA: 2:30:15 - loss: 0.3664 - regression_loss: 0.3279 - classification_loss: 0.0385
  193/10000 [..............................] - ETA: 2:30:13 - loss: 0.3657 - regression_loss: 0.3274 - classification_loss: 0.0384
  194/10000 [..............................] - ETA: 2:30:11 - loss: 0.3659 - regression_loss: 0.3276 - classification_loss: 0.0383
  195/10000 [..............................] - ETA: 2:30:08 - loss: 0.3654 - regression_loss: 0.3271 - classification_loss: 0.0383
  196/10000 [..............................] - ETA: 2:30:06 - loss: 0.3653 - regression_loss: 0.3270 - classification_loss: 0.0383
  197/10000 [..............................] - ETA: 2:30:03 - loss: 0.3651 - regression_loss: 0.3269 - classification_loss: 0.0382
  198/10000 [..............................] - ETA: 2:30:01 - loss: 0.3657 - regression_loss: 0.3275 - classification_loss: 0.0382
  199/10000 [..............................] - ETA: 2:29:58 - loss: 0.3656 - regression_loss: 0.3273 - classification_loss: 0.0383
  200/10000 [..............................] - ETA: 2:29:56 - loss: 0.3655 - regression_loss: 0.3273 - classification_loss: 0.0382
  201/10000 [..............................] - ETA: 2:29:53 - loss: 0.3651 - regression_loss: 0.3270 - classification_loss: 0.0381
  202/10000 [..............................] - ETA: 2:29:51 - loss: 0.3654 - regression_loss: 0.3273 - classification_loss: 0.0381
  203/10000 [..............................] - ETA: 2:29:49 - loss: 0.3649 - regression_loss: 0.3268 - classification_loss: 0.0381
  204/10000 [..............................] - ETA: 2:29:47 - loss: 0.3673 - regression_loss: 0.3291 - classification_loss: 0.0382
  205/10000 [..............................] - ETA: 2:29:44 - loss: 0.3673 - regression_loss: 0.3293 - classification_loss: 0.0380
  206/10000 [..............................] - ETA: 2:29:42 - loss: 0.3655 - regression_loss: 0.3277 - classification_loss: 0.0379
  207/10000 [..............................] - ETA: 2:29:39 - loss: 0.3650 - regression_loss: 0.3271 - classification_loss: 0.0378
  208/10000 [..............................] - ETA: 2:29:37 - loss: 0.3645 - regression_loss: 0.3268 - classification_loss: 0.0378
  209/10000 [..............................] - ETA: 2:29:35 - loss: 0.3658 - regression_loss: 0.3276 - classification_loss: 0.0382
  210/10000 [..............................] - ETA: 2:29:33 - loss: 0.3670 - regression_loss: 0.3287 - classification_loss: 0.0382
  211/10000 [..............................] - ETA: 2:29:30 - loss: 0.3688 - regression_loss: 0.3306 - classification_loss: 0.0382
  212/10000 [..............................] - ETA: 2:29:28 - loss: 0.3686 - regression_loss: 0.3303 - classification_loss: 0.0383
  213/10000 [..............................] - ETA: 2:29:26 - loss: 0.3690 - regression_loss: 0.3307 - classification_loss: 0.0383
  214/10000 [..............................] - ETA: 2:29:24 - loss: 0.3689 - regression_loss: 0.3307 - classification_loss: 0.0382
  215/10000 [..............................] - ETA: 2:29:21 - loss: 0.3691 - regression_loss: 0.3311 - classification_loss: 0.0381
  216/10000 [..............................] - ETA: 2:29:19 - loss: 0.3699 - regression_loss: 0.3318 - classification_loss: 0.0381
  217/10000 [..............................] - ETA: 2:29:17 - loss: 0.3697 - regression_loss: 0.3318 - classification_loss: 0.0379
  218/10000 [..............................] - ETA: 2:29:15 - loss: 0.3694 - regression_loss: 0.3316 - classification_loss: 0.0378
  219/10000 [..............................] - ETA: 2:29:13 - loss: 0.3684 - regression_loss: 0.3306 - classification_loss: 0.0378
  220/10000 [..............................] - ETA: 2:29:11 - loss: 0.3694 - regression_loss: 0.3314 - classification_loss: 0.0380
  221/10000 [..............................] - ETA: 2:29:08 - loss: 0.3690 - regression_loss: 0.3311 - classification_loss: 0.0379
  222/10000 [..............................] - ETA: 2:29:06 - loss: 0.3682 - regression_loss: 0.3304 - classification_loss: 0.0378
  223/10000 [..............................] - ETA: 2:29:04 - loss: 0.3693 - regression_loss: 0.3313 - classification_loss: 0.0381
  224/10000 [..............................] - ETA: 2:29:02 - loss: 0.3696 - regression_loss: 0.3314 - classification_loss: 0.0381
  225/10000 [..............................] - ETA: 2:29:00 - loss: 0.3688 - regression_loss: 0.3307 - classification_loss: 0.0380
  226/10000 [..............................] - ETA: 2:28:58 - loss: 0.3714 - regression_loss: 0.3332 - classification_loss: 0.0383
  227/10000 [..............................] - ETA: 2:28:56 - loss: 0.3705 - regression_loss: 0.3323 - classification_loss: 0.0383
  228/10000 [..............................] - ETA: 2:28:53 - loss: 0.3694 - regression_loss: 0.3312 - classification_loss: 0.0381
  229/10000 [..............................] - ETA: 2:28:51 - loss: 0.3687 - regression_loss: 0.3306 - classification_loss: 0.0381
  230/10000 [..............................] - ETA: 2:28:49 - loss: 0.3693 - regression_loss: 0.3312 - classification_loss: 0.0381
  231/10000 [..............................] - ETA: 2:28:47 - loss: 0.3695 - regression_loss: 0.3313 - classification_loss: 0.0382
  232/10000 [..............................] - ETA: 2:28:45 - loss: 0.3695 - regression_loss: 0.3314 - classification_loss: 0.0381
  233/10000 [..............................] - ETA: 2:28:44 - loss: 0.3679 - regression_loss: 0.3300 - classification_loss: 0.0379
  234/10000 [..............................] - ETA: 2:28:42 - loss: 0.3684 - regression_loss: 0.3306 - classification_loss: 0.0378
  235/10000 [..............................] - ETA: 2:28:40 - loss: 0.3684 - regression_loss: 0.3308 - classification_loss: 0.0377
  236/10000 [..............................] - ETA: 2:28:38 - loss: 0.3678 - regression_loss: 0.3302 - classification_loss: 0.0375
  237/10000 [..............................] - ETA: 2:28:36 - loss: 0.3710 - regression_loss: 0.3330 - classification_loss: 0.0380
  238/10000 [..............................] - ETA: 2:28:34 - loss: 0.3705 - regression_loss: 0.3324 - classification_loss: 0.0381
  239/10000 [..............................] - ETA: 2:28:32 - loss: 0.3700 - regression_loss: 0.3321 - classification_loss: 0.0379
  240/10000 [..............................] - ETA: 2:28:30 - loss: 0.3705 - regression_loss: 0.3326 - classification_loss: 0.0379
  241/10000 [..............................] - ETA: 2:28:28 - loss: 0.3706 - regression_loss: 0.3326 - classification_loss: 0.0380
  242/10000 [..............................] - ETA: 2:28:26 - loss: 0.3712 - regression_loss: 0.3333 - classification_loss: 0.0379
  243/10000 [..............................] - ETA: 2:28:25 - loss: 0.3697 - regression_loss: 0.3319 - classification_loss: 0.0378
  244/10000 [..............................] - ETA: 2:28:23 - loss: 0.3695 - regression_loss: 0.3318 - classification_loss: 0.0377
  245/10000 [..............................] - ETA: 2:28:21 - loss: 0.3703 - regression_loss: 0.3326 - classification_loss: 0.0376
  246/10000 [..............................] - ETA: 2:28:19 - loss: 0.3708 - regression_loss: 0.3333 - classification_loss: 0.0375
  247/10000 [..............................] - ETA: 2:28:17 - loss: 0.3705 - regression_loss: 0.3330 - classification_loss: 0.0376
  248/10000 [..............................] - ETA: 2:28:15 - loss: 0.3700 - regression_loss: 0.3325 - classification_loss: 0.0375
  249/10000 [..............................] - ETA: 2:28:13 - loss: 0.3702 - regression_loss: 0.3327 - classification_loss: 0.0375
  250/10000 [..............................] - ETA: 2:28:11 - loss: 0.3705 - regression_loss: 0.3329 - classification_loss: 0.0375
  251/10000 [..............................] - ETA: 2:28:10 - loss: 0.3705 - regression_loss: 0.3330 - classification_loss: 0.0375
  252/10000 [..............................] - ETA: 2:28:08 - loss: 0.3707 - regression_loss: 0.3333 - classification_loss: 0.0374
  253/10000 [..............................] - ETA: 2:28:06 - loss: 0.3693 - regression_loss: 0.3320 - classification_loss: 0.0373
  254/10000 [..............................] - ETA: 2:28:04 - loss: 0.3696 - regression_loss: 0.3322 - classification_loss: 0.0374
  255/10000 [..............................] - ETA: 2:28:02 - loss: 0.3738 - regression_loss: 0.3352 - classification_loss: 0.0387
  256/10000 [..............................] - ETA: 2:28:01 - loss: 0.3752 - regression_loss: 0.3363 - classification_loss: 0.0389
  257/10000 [..............................] - ETA: 2:27:59 - loss: 0.3751 - regression_loss: 0.3362 - classification_loss: 0.0389
  258/10000 [..............................] - ETA: 2:27:57 - loss: 0.3764 - regression_loss: 0.3375 - classification_loss: 0.0389
  259/10000 [..............................] - ETA: 2:27:55 - loss: 0.3754 - regression_loss: 0.3366 - classification_loss: 0.0388
  260/10000 [..............................] - ETA: 2:27:53 - loss: 0.3758 - regression_loss: 0.3371 - classification_loss: 0.0387
  261/10000 [..............................] - ETA: 2:27:52 - loss: 0.3763 - regression_loss: 0.3375 - classification_loss: 0.0388
  262/10000 [..............................] - ETA: 2:27:50 - loss: 0.3822 - regression_loss: 0.3415 - classification_loss: 0.0407
  263/10000 [..............................] - ETA: 2:27:48 - loss: 0.3850 - regression_loss: 0.3442 - classification_loss: 0.0408
  264/10000 [..............................] - ETA: 2:27:46 - loss: 0.3860 - regression_loss: 0.3451 - classification_loss: 0.0409
  265/10000 [..............................] - ETA: 2:27:45 - loss: 0.3858 - regression_loss: 0.3449 - classification_loss: 0.0409
  266/10000 [..............................] - ETA: 2:27:43 - loss: 0.3862 - regression_loss: 0.3453 - classification_loss: 0.0408
  267/10000 [..............................] - ETA: 2:27:41 - loss: 0.3869 - regression_loss: 0.3461 - classification_loss: 0.0408
  268/10000 [..............................] - ETA: 2:27:40 - loss: 0.3878 - regression_loss: 0.3470 - classification_loss: 0.0408
  269/10000 [..............................] - ETA: 2:27:38 - loss: 0.3880 - regression_loss: 0.3472 - classification_loss: 0.0408
  270/10000 [..............................] - ETA: 2:27:37 - loss: 0.3877 - regression_loss: 0.3469 - classification_loss: 0.0408
  271/10000 [..............................] - ETA: 2:27:35 - loss: 0.3878 - regression_loss: 0.3472 - classification_loss: 0.0406
  272/10000 [..............................] - ETA: 2:27:33 - loss: 0.3864 - regression_loss: 0.3460 - classification_loss: 0.0405
  273/10000 [..............................] - ETA: 2:27:31 - loss: 0.3862 - regression_loss: 0.3456 - classification_loss: 0.0405
  274/10000 [..............................] - ETA: 2:27:30 - loss: 0.3856 - regression_loss: 0.3449 - classification_loss: 0.0407
  275/10000 [..............................] - ETA: 2:27:28 - loss: 0.3865 - regression_loss: 0.3458 - classification_loss: 0.0407
  276/10000 [..............................] - ETA: 2:27:26 - loss: 0.3864 - regression_loss: 0.3457 - classification_loss: 0.0407
  277/10000 [..............................] - ETA: 2:27:25 - loss: 0.3875 - regression_loss: 0.3466 - classification_loss: 0.0409
  278/10000 [..............................] - ETA: 2:27:23 - loss: 0.3874 - regression_loss: 0.3466 - classification_loss: 0.0408
  279/10000 [..............................] - ETA: 2:27:21 - loss: 0.3864 - regression_loss: 0.3457 - classification_loss: 0.0407
  280/10000 [..............................] - ETA: 2:27:20 - loss: 0.3895 - regression_loss: 0.3483 - classification_loss: 0.0412
  281/10000 [..............................] - ETA: 2:27:18 - loss: 0.3897 - regression_loss: 0.3485 - classification_loss: 0.0411
  282/10000 [..............................] - ETA: 2:27:17 - loss: 0.3897 - regression_loss: 0.3485 - classification_loss: 0.0412
  283/10000 [..............................] - ETA: 2:27:15 - loss: 0.3897 - regression_loss: 0.3485 - classification_loss: 0.0411
  284/10000 [..............................] - ETA: 2:27:13 - loss: 0.3883 - regression_loss: 0.3473 - classification_loss: 0.0410
  285/10000 [..............................] - ETA: 2:27:12 - loss: 0.3887 - regression_loss: 0.3477 - classification_loss: 0.0410
  286/10000 [..............................] - ETA: 2:27:10 - loss: 0.3888 - regression_loss: 0.3478 - classification_loss: 0.0410
  287/10000 [..............................] - ETA: 2:27:08 - loss: 0.3892 - regression_loss: 0.3482 - classification_loss: 0.0410
  288/10000 [..............................] - ETA: 2:27:07 - loss: 0.3904 - regression_loss: 0.3492 - classification_loss: 0.0412
  289/10000 [..............................] - ETA: 2:27:05 - loss: 0.3900 - regression_loss: 0.3489 - classification_loss: 0.0411
  290/10000 [..............................] - ETA: 2:27:04 - loss: 0.3906 - regression_loss: 0.3495 - classification_loss: 0.0411
  291/10000 [..............................] - ETA: 2:27:02 - loss: 0.3900 - regression_loss: 0.3490 - classification_loss: 0.0410
  292/10000 [..............................] - ETA: 2:27:00 - loss: 0.3898 - regression_loss: 0.3488 - classification_loss: 0.0410
  293/10000 [..............................] - ETA: 2:26:59 - loss: 0.3897 - regression_loss: 0.3487 - classification_loss: 0.0409
  294/10000 [..............................] - ETA: 2:26:57 - loss: 0.3905 - regression_loss: 0.3492 - classification_loss: 0.0412
  295/10000 [..............................] - ETA: 2:26:55 - loss: 0.3902 - regression_loss: 0.3491 - classification_loss: 0.0412
  296/10000 [..............................] - ETA: 2:26:54 - loss: 0.3901 - regression_loss: 0.3490 - classification_loss: 0.0411
  297/10000 [..............................] - ETA: 2:26:52 - loss: 0.3895 - regression_loss: 0.3485 - classification_loss: 0.0410
  298/10000 [..............................] - ETA: 2:26:51 - loss: 0.3904 - regression_loss: 0.3490 - classification_loss: 0.0414
  299/10000 [..............................] - ETA: 2:26:49 - loss: 0.3899 - regression_loss: 0.3486 - classification_loss: 0.0413
  300/10000 [..............................] - ETA: 2:26:48 - loss: 0.3890 - regression_loss: 0.3479 - classification_loss: 0.0412
  301/10000 [..............................] - ETA: 2:26:46 - loss: 0.3888 - regression_loss: 0.3476 - classification_loss: 0.0412
  302/10000 [..............................] - ETA: 2:26:44 - loss: 0.3882 - regression_loss: 0.3471 - classification_loss: 0.0411
  303/10000 [..............................] - ETA: 2:26:43 - loss: 0.3876 - regression_loss: 0.3467 - classification_loss: 0.0410
  304/10000 [..............................] - ETA: 2:26:41 - loss: 0.3870 - regression_loss: 0.3461 - classification_loss: 0.0409
  305/10000 [..............................] - ETA: 2:26:39 - loss: 0.3886 - regression_loss: 0.3469 - classification_loss: 0.0416
  306/10000 [..............................] - ETA: 2:26:38 - loss: 0.3887 - regression_loss: 0.3472 - classification_loss: 0.0415
  307/10000 [..............................] - ETA: 2:26:36 - loss: 0.3890 - regression_loss: 0.3476 - classification_loss: 0.0415
  308/10000 [..............................] - ETA: 2:26:35 - loss: 0.3892 - regression_loss: 0.3477 - classification_loss: 0.0415
  309/10000 [..............................] - ETA: 2:26:33 - loss: 0.3894 - regression_loss: 0.3479 - classification_loss: 0.0416
  310/10000 [..............................] - ETA: 2:26:32 - loss: 0.3887 - regression_loss: 0.3473 - classification_loss: 0.0415
  311/10000 [..............................] - ETA: 2:26:30 - loss: 0.3883 - regression_loss: 0.3469 - classification_loss: 0.0414
  312/10000 [..............................] - ETA: 2:26:29 - loss: 0.3876 - regression_loss: 0.3464 - classification_loss: 0.0413
  313/10000 [..............................] - ETA: 2:26:27 - loss: 0.3879 - regression_loss: 0.3465 - classification_loss: 0.0415
  314/10000 [..............................] - ETA: 2:26:26 - loss: 0.3880 - regression_loss: 0.3465 - classification_loss: 0.0415
  315/10000 [..............................] - ETA: 2:26:24 - loss: 0.3876 - regression_loss: 0.3462 - classification_loss: 0.0414
  316/10000 [..............................] - ETA: 2:26:23 - loss: 0.3872 - regression_loss: 0.3459 - classification_loss: 0.0413
  317/10000 [..............................] - ETA: 2:26:21 - loss: 0.3860 - regression_loss: 0.3448 - classification_loss: 0.0412
  318/10000 [..............................] - ETA: 2:26:20 - loss: 0.3880 - regression_loss: 0.3467 - classification_loss: 0.0412
  319/10000 [..............................] - ETA: 2:26:19 - loss: 0.3874 - regression_loss: 0.3463 - classification_loss: 0.0412
  320/10000 [..............................] - ETA: 2:26:17 - loss: 0.3870 - regression_loss: 0.3459 - classification_loss: 0.0411
  321/10000 [..............................] - ETA: 2:26:16 - loss: 0.3868 - regression_loss: 0.3457 - classification_loss: 0.0411
  322/10000 [..............................] - ETA: 2:26:14 - loss: 0.3871 - regression_loss: 0.3461 - classification_loss: 0.0410
  323/10000 [..............................] - ETA: 2:26:13 - loss: 0.3871 - regression_loss: 0.3463 - classification_loss: 0.0409
  324/10000 [..............................] - ETA: 2:26:11 - loss: 0.3865 - regression_loss: 0.3457 - classification_loss: 0.0408
  325/10000 [..............................] - ETA: 2:26:10 - loss: 0.3865 - regression_loss: 0.3456 - classification_loss: 0.0408
  326/10000 [..............................] - ETA: 2:26:08 - loss: 0.3885 - regression_loss: 0.3477 - classification_loss: 0.0408
  327/10000 [..............................] - ETA: 2:26:07 - loss: 0.3889 - regression_loss: 0.3481 - classification_loss: 0.0408
  328/10000 [..............................] - ETA: 2:26:05 - loss: 0.3887 - regression_loss: 0.3479 - classification_loss: 0.0408
  329/10000 [..............................] - ETA: 2:26:04 - loss: 0.3886 - regression_loss: 0.3479 - classification_loss: 0.0407
  330/10000 [..............................] - ETA: 2:26:02 - loss: 0.3894 - regression_loss: 0.3486 - classification_loss: 0.0408
  331/10000 [..............................] - ETA: 2:26:01 - loss: 0.3893 - regression_loss: 0.3485 - classification_loss: 0.0407
  332/10000 [..............................] - ETA: 2:26:00 - loss: 0.3888 - regression_loss: 0.3482 - classification_loss: 0.0407
  333/10000 [..............................] - ETA: 2:25:58 - loss: 0.3892 - regression_loss: 0.3486 - classification_loss: 0.0406
  334/10000 [>.............................] - ETA: 2:25:57 - loss: 0.3893 - regression_loss: 0.3487 - classification_loss: 0.0406
  335/10000 [>.............................] - ETA: 2:25:56 - loss: 0.3891 - regression_loss: 0.3484 - classification_loss: 0.0406
  336/10000 [>.............................] - ETA: 2:25:54 - loss: 0.3887 - regression_loss: 0.3481 - classification_loss: 0.0405
  337/10000 [>.............................] - ETA: 2:25:53 - loss: 0.3875 - regression_loss: 0.3471 - classification_loss: 0.0404
  338/10000 [>.............................] - ETA: 2:25:51 - loss: 0.3871 - regression_loss: 0.3467 - classification_loss: 0.0404
  339/10000 [>.............................] - ETA: 2:25:50 - loss: 0.3867 - regression_loss: 0.3462 - classification_loss: 0.0404
  340/10000 [>.............................] - ETA: 2:25:48 - loss: 0.3867 - regression_loss: 0.3462 - classification_loss: 0.0404
  341/10000 [>.............................] - ETA: 2:25:47 - loss: 0.3864 - regression_loss: 0.3460 - classification_loss: 0.0404
  342/10000 [>.............................] - ETA: 2:25:46 - loss: 0.3856 - regression_loss: 0.3453 - classification_loss: 0.0403
  343/10000 [>.............................] - ETA: 2:25:44 - loss: 0.3861 - regression_loss: 0.3456 - classification_loss: 0.0404
  344/10000 [>.............................] - ETA: 2:25:43 - loss: 0.3864 - regression_loss: 0.3461 - classification_loss: 0.0403
  345/10000 [>.............................] - ETA: 2:25:41 - loss: 0.3862 - regression_loss: 0.3459 - classification_loss: 0.0403
  346/10000 [>.............................] - ETA: 2:25:40 - loss: 0.3859 - regression_loss: 0.3457 - classification_loss: 0.0402
  347/10000 [>.............................] - ETA: 2:25:39 - loss: 0.3859 - regression_loss: 0.3457 - classification_loss: 0.0402
  348/10000 [>.............................] - ETA: 2:25:37 - loss: 0.3858 - regression_loss: 0.3456 - classification_loss: 0.0402
  349/10000 [>.............................] - ETA: 2:25:36 - loss: 0.3851 - regression_loss: 0.3450 - classification_loss: 0.0401
  350/10000 [>.............................] - ETA: 2:25:34 - loss: 0.3848 - regression_loss: 0.3447 - classification_loss: 0.0400
  351/10000 [>.............................] - ETA: 2:25:33 - loss: 0.3851 - regression_loss: 0.3450 - classification_loss: 0.0401
  352/10000 [>.............................] - ETA: 2:25:32 - loss: 0.3871 - regression_loss: 0.3470 - classification_loss: 0.0401
  353/10000 [>.............................] - ETA: 2:25:30 - loss: 0.3880 - regression_loss: 0.3479 - classification_loss: 0.0401
  354/10000 [>.............................] - ETA: 2:25:29 - loss: 0.3960 - regression_loss: 0.3528 - classification_loss: 0.0431
  355/10000 [>.............................] - ETA: 2:25:28 - loss: 0.3956 - regression_loss: 0.3526 - classification_loss: 0.0430
  356/10000 [>.............................] - ETA: 2:25:26 - loss: 0.3951 - regression_loss: 0.3521 - classification_loss: 0.0430
  357/10000 [>.............................] - ETA: 2:25:25 - loss: 0.3950 - regression_loss: 0.3520 - classification_loss: 0.0431
  358/10000 [>.............................] - ETA: 2:25:24 - loss: 0.3948 - regression_loss: 0.3518 - classification_loss: 0.0430
  359/10000 [>.............................] - ETA: 2:25:22 - loss: 0.3944 - regression_loss: 0.3513 - classification_loss: 0.0431
  360/10000 [>.............................] - ETA: 2:25:21 - loss: 0.3970 - regression_loss: 0.3535 - classification_loss: 0.0435
  361/10000 [>.............................] - ETA: 2:25:20 - loss: 0.3964 - regression_loss: 0.3530 - classification_loss: 0.0434
  362/10000 [>.............................] - ETA: 2:25:18 - loss: 0.3957 - regression_loss: 0.3524 - classification_loss: 0.0433
  363/10000 [>.............................] - ETA: 2:25:17 - loss: 0.3962 - regression_loss: 0.3528 - classification_loss: 0.0434
  364/10000 [>.............................] - ETA: 2:25:15 - loss: 0.3961 - regression_loss: 0.3527 - classification_loss: 0.0434
  365/10000 [>.............................] - ETA: 2:25:14 - loss: 0.3963 - regression_loss: 0.3528 - classification_loss: 0.0435
  366/10000 [>.............................] - ETA: 2:25:13 - loss: 0.3964 - regression_loss: 0.3528 - classification_loss: 0.0436
  367/10000 [>.............................] - ETA: 2:25:12 - loss: 0.3965 - regression_loss: 0.3527 - classification_loss: 0.0437
  368/10000 [>.............................] - ETA: 2:25:10 - loss: 0.3959 - regression_loss: 0.3523 - classification_loss: 0.0437
  369/10000 [>.............................] - ETA: 2:25:09 - loss: 0.3956 - regression_loss: 0.3520 - classification_loss: 0.0436
  370/10000 [>.............................] - ETA: 2:25:08 - loss: 0.3957 - regression_loss: 0.3522 - classification_loss: 0.0435
  371/10000 [>.............................] - ETA: 2:25:06 - loss: 0.3953 - regression_loss: 0.3518 - classification_loss: 0.0434
  372/10000 [>.............................] - ETA: 2:25:05 - loss: 0.3947 - regression_loss: 0.3513 - classification_loss: 0.0434
  373/10000 [>.............................] - ETA: 2:25:04 - loss: 0.3960 - regression_loss: 0.3524 - classification_loss: 0.0436
  374/10000 [>.............................] - ETA: 2:25:02 - loss: 0.3957 - regression_loss: 0.3520 - classification_loss: 0.0437
  375/10000 [>.............................] - ETA: 2:25:01 - loss: 0.3953 - regression_loss: 0.3516 - classification_loss: 0.0437
  376/10000 [>.............................] - ETA: 2:25:00 - loss: 0.3992 - regression_loss: 0.3539 - classification_loss: 0.0454
  377/10000 [>.............................] - ETA: 2:24:59 - loss: 0.3982 - regression_loss: 0.3529 - classification_loss: 0.0452
  378/10000 [>.............................] - ETA: 2:24:57 - loss: 0.3978 - regression_loss: 0.3526 - classification_loss: 0.0452
  379/10000 [>.............................] - ETA: 2:24:56 - loss: 0.3977 - regression_loss: 0.3526 - classification_loss: 0.0451
  380/10000 [>.............................] - ETA: 2:24:54 - loss: 0.3990 - regression_loss: 0.3538 - classification_loss: 0.0452
  381/10000 [>.............................] - ETA: 2:24:53 - loss: 0.3997 - regression_loss: 0.3544 - classification_loss: 0.0454
  382/10000 [>.............................] - ETA: 2:24:52 - loss: 0.3991 - regression_loss: 0.3538 - classification_loss: 0.0453
  383/10000 [>.............................] - ETA: 2:24:51 - loss: 0.3994 - regression_loss: 0.3541 - classification_loss: 0.0453
  384/10000 [>.............................] - ETA: 2:24:49 - loss: 0.4000 - regression_loss: 0.3545 - classification_loss: 0.0454
  385/10000 [>.............................] - ETA: 2:24:48 - loss: 0.3994 - regression_loss: 0.3541 - classification_loss: 0.0454
  386/10000 [>.............................] - ETA: 2:24:47 - loss: 0.3984 - regression_loss: 0.3531 - classification_loss: 0.0453
  387/10000 [>.............................] - ETA: 2:24:45 - loss: 0.3974 - regression_loss: 0.3522 - classification_loss: 0.0451
  388/10000 [>.............................] - ETA: 2:24:44 - loss: 0.3972 - regression_loss: 0.3521 - classification_loss: 0.0451
  389/10000 [>.............................] - ETA: 2:24:43 - loss: 0.3970 - regression_loss: 0.3520 - classification_loss: 0.0450
  390/10000 [>.............................] - ETA: 2:24:41 - loss: 0.3969 - regression_loss: 0.3519 - classification_loss: 0.0450
  391/10000 [>.............................] - ETA: 2:24:40 - loss: 0.3967 - regression_loss: 0.3518 - classification_loss: 0.0449
  392/10000 [>.............................] - ETA: 2:24:39 - loss: 0.3987 - regression_loss: 0.3536 - classification_loss: 0.0451
  393/10000 [>.............................] - ETA: 2:24:38 - loss: 0.3991 - regression_loss: 0.3540 - classification_loss: 0.0451
  394/10000 [>.............................] - ETA: 2:24:36 - loss: 0.3984 - regression_loss: 0.3533 - classification_loss: 0.0451
  395/10000 [>.............................] - ETA: 2:24:35 - loss: 0.3980 - regression_loss: 0.3530 - classification_loss: 0.0450
  396/10000 [>.............................] - ETA: 2:24:34 - loss: 0.3976 - regression_loss: 0.3527 - classification_loss: 0.0450
  397/10000 [>.............................] - ETA: 2:24:32 - loss: 0.3970 - regression_loss: 0.3520 - classification_loss: 0.0449
  398/10000 [>.............................] - ETA: 2:24:31 - loss: 0.3969 - regression_loss: 0.3520 - classification_loss: 0.0449
  399/10000 [>.............................] - ETA: 2:24:30 - loss: 0.3967 - regression_loss: 0.3518 - classification_loss: 0.0450
  400/10000 [>.............................] - ETA: 2:24:28 - loss: 0.3968 - regression_loss: 0.3518 - classification_loss: 0.0450
  401/10000 [>.............................] - ETA: 2:24:27 - loss: 0.3983 - regression_loss: 0.3531 - classification_loss: 0.0452
  402/10000 [>.............................] - ETA: 2:24:26 - loss: 0.3987 - regression_loss: 0.3534 - classification_loss: 0.0453
  403/10000 [>.............................] - ETA: 2:24:25 - loss: 0.3980 - regression_loss: 0.3528 - classification_loss: 0.0452
  404/10000 [>.............................] - ETA: 2:24:23 - loss: 0.3987 - regression_loss: 0.3535 - classification_loss: 0.0452
  405/10000 [>.............................] - ETA: 2:24:22 - loss: 0.3985 - regression_loss: 0.3533 - classification_loss: 0.0452
  406/10000 [>.............................] - ETA: 2:24:21 - loss: 0.3979 - regression_loss: 0.3528 - classification_loss: 0.0451
  407/10000 [>.............................] - ETA: 2:24:20 - loss: 0.3981 - regression_loss: 0.3529 - classification_loss: 0.0452
  408/10000 [>.............................] - ETA: 2:24:18 - loss: 0.3978 - regression_loss: 0.3525 - classification_loss: 0.0452
  409/10000 [>.............................] - ETA: 2:24:17 - loss: 0.3974 - regression_loss: 0.3521 - classification_loss: 0.0453
  410/10000 [>.............................] - ETA: 2:24:16 - loss: 0.3973 - regression_loss: 0.3520 - classification_loss: 0.0453
  411/10000 [>.............................] - ETA: 2:24:15 - loss: 0.3980 - regression_loss: 0.3528 - classification_loss: 0.0453
  412/10000 [>.............................] - ETA: 2:24:14 - loss: 0.3984 - regression_loss: 0.3529 - classification_loss: 0.0455
  413/10000 [>.............................] - ETA: 2:24:12 - loss: 0.3981 - regression_loss: 0.3525 - classification_loss: 0.0455
  414/10000 [>.............................] - ETA: 2:24:11 - loss: 0.4020 - regression_loss: 0.3557 - classification_loss: 0.0462
  415/10000 [>.............................] - ETA: 2:24:10 - loss: 0.4012 - regression_loss: 0.3550 - classification_loss: 0.0462
  416/10000 [>.............................] - ETA: 2:24:09 - loss: 0.4009 - regression_loss: 0.3549 - classification_loss: 0.0461
  417/10000 [>.............................] - ETA: 2:24:07 - loss: 0.4000 - regression_loss: 0.3540 - classification_loss: 0.0459
  418/10000 [>.............................] - ETA: 2:24:06 - loss: 0.4001 - regression_loss: 0.3542 - classification_loss: 0.0458
  419/10000 [>.............................] - ETA: 2:24:05 - loss: 0.4008 - regression_loss: 0.3545 - classification_loss: 0.0462
  420/10000 [>.............................] - ETA: 2:24:04 - loss: 0.4007 - regression_loss: 0.3546 - classification_loss: 0.0462
  421/10000 [>.............................] - ETA: 2:24:02 - loss: 0.4012 - regression_loss: 0.3551 - classification_loss: 0.0461
  422/10000 [>.............................] - ETA: 2:24:01 - loss: 0.4009 - regression_loss: 0.3548 - classification_loss: 0.0460
  423/10000 [>.............................] - ETA: 2:24:00 - loss: 0.4008 - regression_loss: 0.3549 - classification_loss: 0.0460
  424/10000 [>.............................] - ETA: 2:23:59 - loss: 0.4007 - regression_loss: 0.3548 - classification_loss: 0.0459
  425/10000 [>.............................] - ETA: 2:23:58 - loss: 0.4009 - regression_loss: 0.3551 - classification_loss: 0.0459
  426/10000 [>.............................] - ETA: 2:23:56 - loss: 0.4000 - regression_loss: 0.3542 - classification_loss: 0.0458
  427/10000 [>.............................] - ETA: 2:23:55 - loss: 0.3996 - regression_loss: 0.3539 - classification_loss: 0.0457
  428/10000 [>.............................] - ETA: 2:23:54 - loss: 0.3995 - regression_loss: 0.3539 - classification_loss: 0.0457
  429/10000 [>.............................] - ETA: 2:23:53 - loss: 0.4003 - regression_loss: 0.3542 - classification_loss: 0.0461
  430/10000 [>.............................] - ETA: 2:23:52 - loss: 0.4002 - regression_loss: 0.3540 - classification_loss: 0.0462
  431/10000 [>.............................] - ETA: 2:23:51 - loss: 0.4001 - regression_loss: 0.3539 - classification_loss: 0.0462
  432/10000 [>.............................] - ETA: 2:23:49 - loss: 0.4000 - regression_loss: 0.3538 - classification_loss: 0.0462
  433/10000 [>.............................] - ETA: 2:23:48 - loss: 0.3997 - regression_loss: 0.3536 - classification_loss: 0.0461
  434/10000 [>.............................] - ETA: 2:23:47 - loss: 0.3994 - regression_loss: 0.3533 - classification_loss: 0.0461
  435/10000 [>.............................] - ETA: 2:23:46 - loss: 0.3994 - regression_loss: 0.3533 - classification_loss: 0.0461
  436/10000 [>.............................] - ETA: 2:23:44 - loss: 0.3994 - regression_loss: 0.3533 - classification_loss: 0.0461
  437/10000 [>.............................] - ETA: 2:23:43 - loss: 0.3996 - regression_loss: 0.3534 - classification_loss: 0.0461
  438/10000 [>.............................] - ETA: 2:23:42 - loss: 0.3991 - regression_loss: 0.3530 - classification_loss: 0.0460
  439/10000 [>.............................] - ETA: 2:23:41 - loss: 0.3986 - regression_loss: 0.3526 - classification_loss: 0.0460
  440/10000 [>.............................] - ETA: 2:23:40 - loss: 0.3984 - regression_loss: 0.3525 - classification_loss: 0.0459
  441/10000 [>.............................] - ETA: 2:23:38 - loss: 0.3979 - regression_loss: 0.3521 - classification_loss: 0.0458
  442/10000 [>.............................] - ETA: 2:23:37 - loss: 0.3980 - regression_loss: 0.3523 - classification_loss: 0.0457
  443/10000 [>.............................] - ETA: 2:23:36 - loss: 0.3971 - regression_loss: 0.3515 - classification_loss: 0.0456
  444/10000 [>.............................] - ETA: 2:23:35 - loss: 0.3971 - regression_loss: 0.3514 - classification_loss: 0.0457
  445/10000 [>.............................] - ETA: 2:23:34 - loss: 0.3973 - regression_loss: 0.3517 - classification_loss: 0.0456
  446/10000 [>.............................] - ETA: 2:23:33 - loss: 0.3984 - regression_loss: 0.3526 - classification_loss: 0.0458
  447/10000 [>.............................] - ETA: 2:23:31 - loss: 0.3983 - regression_loss: 0.3526 - classification_loss: 0.0457
  448/10000 [>.............................] - ETA: 2:23:30 - loss: 0.3980 - regression_loss: 0.3524 - classification_loss: 0.0456
  449/10000 [>.............................] - ETA: 2:23:29 - loss: 0.3976 - regression_loss: 0.3520 - classification_loss: 0.0456
  450/10000 [>.............................] - ETA: 2:23:28 - loss: 0.3972 - regression_loss: 0.3517 - classification_loss: 0.0455
  451/10000 [>.............................] - ETA: 2:23:27 - loss: 0.3997 - regression_loss: 0.3540 - classification_loss: 0.0457
  452/10000 [>.............................] - ETA: 2:23:25 - loss: 0.3997 - regression_loss: 0.3541 - classification_loss: 0.0456
  453/10000 [>.............................] - ETA: 2:23:24 - loss: 0.3988 - regression_loss: 0.3533 - classification_loss: 0.0455
  454/10000 [>.............................] - ETA: 2:23:23 - loss: 0.3986 - regression_loss: 0.3531 - classification_loss: 0.0455
  455/10000 [>.............................] - ETA: 2:23:22 - loss: 0.3983 - regression_loss: 0.3529 - classification_loss: 0.0454
  456/10000 [>.............................] - ETA: 2:23:21 - loss: 0.3983 - regression_loss: 0.3529 - classification_loss: 0.0454
  457/10000 [>.............................] - ETA: 2:23:19 - loss: 0.3980 - regression_loss: 0.3526 - classification_loss: 0.0454
  458/10000 [>.............................] - ETA: 2:23:18 - loss: 0.3993 - regression_loss: 0.3537 - classification_loss: 0.0455
  459/10000 [>.............................] - ETA: 2:23:17 - loss: 0.4024 - regression_loss: 0.3566 - classification_loss: 0.0458
  460/10000 [>.............................] - ETA: 2:23:16 - loss: 0.4019 - regression_loss: 0.3562 - classification_loss: 0.0457
  461/10000 [>.............................] - ETA: 2:23:15 - loss: 0.4012 - regression_loss: 0.3556 - classification_loss: 0.0456
  462/10000 [>.............................] - ETA: 2:23:14 - loss: 0.4012 - regression_loss: 0.3557 - classification_loss: 0.0456
  463/10000 [>.............................] - ETA: 2:23:13 - loss: 0.4007 - regression_loss: 0.3552 - classification_loss: 0.0455
  464/10000 [>.............................] - ETA: 2:23:11 - loss: 0.4002 - regression_loss: 0.3547 - classification_loss: 0.0455
  465/10000 [>.............................] - ETA: 2:23:10 - loss: 0.4003 - regression_loss: 0.3548 - classification_loss: 0.0456
  466/10000 [>.............................] - ETA: 2:23:09 - loss: 0.4015 - regression_loss: 0.3558 - classification_loss: 0.0457
  467/10000 [>.............................] - ETA: 2:23:08 - loss: 0.4017 - regression_loss: 0.3561 - classification_loss: 0.0456
  468/10000 [>.............................] - ETA: 2:23:07 - loss: 0.4014 - regression_loss: 0.3558 - classification_loss: 0.0456
  469/10000 [>.............................] - ETA: 2:23:06 - loss: 0.4013 - regression_loss: 0.3557 - classification_loss: 0.0456
  470/10000 [>.............................] - ETA: 2:23:05 - loss: 0.4009 - regression_loss: 0.3553 - classification_loss: 0.0455
  471/10000 [>.............................] - ETA: 2:23:03 - loss: 0.4010 - regression_loss: 0.3555 - classification_loss: 0.0455
  472/10000 [>.............................] - ETA: 2:23:02 - loss: 0.4008 - regression_loss: 0.3554 - classification_loss: 0.0454
  473/10000 [>.............................] - ETA: 2:23:01 - loss: 0.4008 - regression_loss: 0.3554 - classification_loss: 0.0453
  474/10000 [>.............................] - ETA: 2:23:00 - loss: 0.4008 - regression_loss: 0.3555 - classification_loss: 0.0452
  475/10000 [>.............................] - ETA: 2:22:59 - loss: 0.4001 - regression_loss: 0.3550 - classification_loss: 0.0451
  476/10000 [>.............................] - ETA: 2:22:58 - loss: 0.3996 - regression_loss: 0.3545 - classification_loss: 0.0451
  477/10000 [>.............................] - ETA: 2:22:56 - loss: 0.3988 - regression_loss: 0.3538 - classification_loss: 0.0450
  478/10000 [>.............................] - ETA: 2:22:55 - loss: 0.3986 - regression_loss: 0.3536 - classification_loss: 0.0449
  479/10000 [>.............................] - ETA: 2:22:54 - loss: 0.3988 - regression_loss: 0.3540 - classification_loss: 0.0449
  480/10000 [>.............................] - ETA: 2:22:53 - loss: 0.3985 - regression_loss: 0.3537 - classification_loss: 0.0448
  481/10000 [>.............................] - ETA: 2:22:52 - loss: 0.3979 - regression_loss: 0.3531 - classification_loss: 0.0447
  482/10000 [>.............................] - ETA: 2:22:51 - loss: 0.3981 - regression_loss: 0.3533 - classification_loss: 0.0447
  483/10000 [>.............................] - ETA: 2:22:50 - loss: 0.3978 - regression_loss: 0.3531 - classification_loss: 0.0447
  484/10000 [>.............................] - ETA: 2:22:48 - loss: 0.3975 - regression_loss: 0.3529 - classification_loss: 0.0446
  485/10000 [>.............................] - ETA: 2:22:47 - loss: 0.3975 - regression_loss: 0.3529 - classification_loss: 0.0446
  486/10000 [>.............................] - ETA: 2:22:46 - loss: 0.3976 - regression_loss: 0.3530 - classification_loss: 0.0446
  487/10000 [>.............................] - ETA: 2:22:45 - loss: 0.3976 - regression_loss: 0.3531 - classification_loss: 0.0445
  488/10000 [>.............................] - ETA: 2:22:44 - loss: 0.3975 - regression_loss: 0.3530 - classification_loss: 0.0445
  489/10000 [>.............................] - ETA: 2:22:43 - loss: 0.4023 - regression_loss: 0.3523 - classification_loss: 0.0500
  490/10000 [>.............................] - ETA: 2:22:42 - loss: 0.4020 - regression_loss: 0.3521 - classification_loss: 0.0499
  491/10000 [>.............................] - ETA: 2:22:40 - loss: 0.4017 - regression_loss: 0.3518 - classification_loss: 0.0499
  492/10000 [>.............................] - ETA: 2:22:39 - loss: 0.4011 - regression_loss: 0.3514 - classification_loss: 0.0498
  493/10000 [>.............................] - ETA: 2:22:38 - loss: 0.4011 - regression_loss: 0.3514 - classification_loss: 0.0497
  494/10000 [>.............................] - ETA: 2:22:37 - loss: 0.4017 - regression_loss: 0.3518 - classification_loss: 0.0498
  495/10000 [>.............................] - ETA: 2:22:36 - loss: 0.4012 - regression_loss: 0.3515 - classification_loss: 0.0497
  496/10000 [>.............................] - ETA: 2:22:35 - loss: 0.4007 - regression_loss: 0.3510 - classification_loss: 0.0497
  497/10000 [>.............................] - ETA: 2:22:34 - loss: 0.4014 - regression_loss: 0.3516 - classification_loss: 0.0498
  498/10000 [>.............................] - ETA: 2:22:33 - loss: 0.4010 - regression_loss: 0.3512 - classification_loss: 0.0497
  499/10000 [>.............................] - ETA: 2:22:31 - loss: 0.4007 - regression_loss: 0.3510 - classification_loss: 0.0497
  500/10000 [>.............................] - ETA: 2:22:30 - loss: 0.4014 - regression_loss: 0.3516 - classification_loss: 0.0498
  501/10000 [>.............................] - ETA: 2:22:29 - loss: 0.4015 - regression_loss: 0.3517 - classification_loss: 0.0498
  502/10000 [>.............................] - ETA: 2:22:28 - loss: 0.4016 - regression_loss: 0.3517 - classification_loss: 0.0498
  503/10000 [>.............................] - ETA: 2:22:27 - loss: 0.4015 - regression_loss: 0.3516 - classification_loss: 0.0498
  504/10000 [>.............................] - ETA: 2:22:26 - loss: 0.4010 - regression_loss: 0.3513 - classification_loss: 0.0498
  505/10000 [>.............................] - ETA: 2:22:25 - loss: 0.4007 - regression_loss: 0.3509 - classification_loss: 0.0498
  506/10000 [>.............................] - ETA: 2:22:23 - loss: 0.4002 - regression_loss: 0.3505 - classification_loss: 0.0497
  507/10000 [>.............................] - ETA: 2:22:22 - loss: 0.4013 - regression_loss: 0.3515 - classification_loss: 0.0497
  508/10000 [>.............................] - ETA: 2:22:21 - loss: 0.4009 - regression_loss: 0.3512 - classification_loss: 0.0497
  509/10000 [>.............................] - ETA: 2:22:20 - loss: 0.4006 - regression_loss: 0.3510 - classification_loss: 0.0496
  510/10000 [>.............................] - ETA: 2:22:19 - loss: 0.4002 - regression_loss: 0.3506 - classification_loss: 0.0496
  511/10000 [>.............................] - ETA: 2:22:18 - loss: 0.4007 - regression_loss: 0.3512 - classification_loss: 0.0495
  512/10000 [>.............................] - ETA: 2:22:17 - loss: 0.4009 - regression_loss: 0.3514 - classification_loss: 0.0495
  513/10000 [>.............................] - ETA: 2:22:16 - loss: 0.4004 - regression_loss: 0.3509 - classification_loss: 0.0495
  514/10000 [>.............................] - ETA: 2:22:15 - loss: 0.4001 - regression_loss: 0.3507 - classification_loss: 0.0494
  515/10000 [>.............................] - ETA: 2:22:14 - loss: 0.4006 - regression_loss: 0.3511 - classification_loss: 0.0495
  516/10000 [>.............................] - ETA: 2:22:12 - loss: 0.4000 - regression_loss: 0.3506 - classification_loss: 0.0494
  517/10000 [>.............................] - ETA: 2:22:11 - loss: 0.4000 - regression_loss: 0.3506 - classification_loss: 0.0494
  518/10000 [>.............................] - ETA: 2:22:10 - loss: 0.4000 - regression_loss: 0.3506 - classification_loss: 0.0493
  519/10000 [>.............................] - ETA: 2:22:09 - loss: 0.3997 - regression_loss: 0.3503 - classification_loss: 0.0494
  520/10000 [>.............................] - ETA: 2:22:08 - loss: 0.3996 - regression_loss: 0.3503 - classification_loss: 0.0493
  521/10000 [>.............................] - ETA: 2:22:07 - loss: 0.3997 - regression_loss: 0.3504 - classification_loss: 0.0494
  522/10000 [>.............................] - ETA: 2:22:06 - loss: 0.3998 - regression_loss: 0.3505 - classification_loss: 0.0493
  523/10000 [>.............................] - ETA: 2:22:04 - loss: 0.3994 - regression_loss: 0.3502 - classification_loss: 0.0492
  524/10000 [>.............................] - ETA: 2:22:03 - loss: 0.4016 - regression_loss: 0.3515 - classification_loss: 0.0500
  525/10000 [>.............................] - ETA: 2:22:02 - loss: 0.4014 - regression_loss: 0.3515 - classification_loss: 0.0500
  526/10000 [>.............................] - ETA: 2:22:01 - loss: 0.4015 - regression_loss: 0.3516 - classification_loss: 0.0499
  527/10000 [>.............................] - ETA: 2:22:00 - loss: 0.4032 - regression_loss: 0.3530 - classification_loss: 0.0503
  528/10000 [>.............................] - ETA: 2:21:59 - loss: 0.4030 - regression_loss: 0.3528 - classification_loss: 0.0503
  529/10000 [>.............................] - ETA: 2:21:58 - loss: 0.4039 - regression_loss: 0.3537 - classification_loss: 0.0502
  530/10000 [>.............................] - ETA: 2:21:57 - loss: 0.4040 - regression_loss: 0.3538 - classification_loss: 0.0502
  531/10000 [>.............................] - ETA: 2:21:56 - loss: 0.4042 - regression_loss: 0.3540 - classification_loss: 0.0501
  532/10000 [>.............................] - ETA: 2:21:55 - loss: 0.4039 - regression_loss: 0.3538 - classification_loss: 0.0501
  533/10000 [>.............................] - ETA: 2:21:54 - loss: 0.4036 - regression_loss: 0.3536 - classification_loss: 0.0500
  534/10000 [>.............................] - ETA: 2:21:52 - loss: 0.4034 - regression_loss: 0.3535 - classification_loss: 0.0499
  535/10000 [>.............................] - ETA: 2:21:51 - loss: 0.4026 - regression_loss: 0.3528 - classification_loss: 0.0498
  536/10000 [>.............................] - ETA: 2:21:50 - loss: 0.4024 - regression_loss: 0.3526 - classification_loss: 0.0498
  537/10000 [>.............................] - ETA: 2:21:49 - loss: 0.4030 - regression_loss: 0.3532 - classification_loss: 0.0498
  538/10000 [>.............................] - ETA: 2:21:48 - loss: 0.4032 - regression_loss: 0.3533 - classification_loss: 0.0498
  539/10000 [>.............................] - ETA: 2:21:47 - loss: 0.4032 - regression_loss: 0.3534 - classification_loss: 0.0498
  540/10000 [>.............................] - ETA: 2:21:46 - loss: 0.4029 - regression_loss: 0.3531 - classification_loss: 0.0497
  541/10000 [>.............................] - ETA: 2:21:45 - loss: 0.4029 - regression_loss: 0.3533 - classification_loss: 0.0497
  542/10000 [>.............................] - ETA: 2:21:44 - loss: 0.4047 - regression_loss: 0.3549 - classification_loss: 0.0498
  543/10000 [>.............................] - ETA: 2:21:43 - loss: 0.4044 - regression_loss: 0.3547 - classification_loss: 0.0497
  544/10000 [>.............................] - ETA: 2:21:42 - loss: 0.4041 - regression_loss: 0.3544 - classification_loss: 0.0496
  545/10000 [>.............................] - ETA: 2:21:40 - loss: 0.4037 - regression_loss: 0.3542 - classification_loss: 0.0496
  546/10000 [>.............................] - ETA: 2:21:39 - loss: 0.4038 - regression_loss: 0.3543 - classification_loss: 0.0495
  547/10000 [>.............................] - ETA: 2:21:38 - loss: 0.4037 - regression_loss: 0.3542 - classification_loss: 0.0495
  548/10000 [>.............................] - ETA: 2:21:37 - loss: 0.4038 - regression_loss: 0.3543 - classification_loss: 0.0495
  549/10000 [>.............................] - ETA: 2:21:36 - loss: 0.4035 - regression_loss: 0.3541 - classification_loss: 0.0494
  550/10000 [>.............................] - ETA: 2:21:35 - loss: 0.4034 - regression_loss: 0.3540 - classification_loss: 0.0494
  551/10000 [>.............................] - ETA: 2:21:34 - loss: 0.4032 - regression_loss: 0.3538 - classification_loss: 0.0493
  552/10000 [>.............................] - ETA: 2:21:33 - loss: 0.4039 - regression_loss: 0.3545 - classification_loss: 0.0494
  553/10000 [>.............................] - ETA: 2:21:32 - loss: 0.4044 - regression_loss: 0.3549 - classification_loss: 0.0495
  554/10000 [>.............................] - ETA: 2:21:40 - loss: 0.4042 - regression_loss: 0.3548 - classification_loss: 0.0494
  555/10000 [>.............................] - ETA: 2:21:38 - loss: 0.4038 - regression_loss: 0.3544 - classification_loss: 0.0494
  556/10000 [>.............................] - ETA: 2:21:37 - loss: 0.4037 - regression_loss: 0.3544 - classification_loss: 0.0493
  557/10000 [>.............................] - ETA: 2:21:36 - loss: 0.4033 - regression_loss: 0.3540 - classification_loss: 0.0493
  558/10000 [>.............................] - ETA: 2:21:35 - loss: 0.4031 - regression_loss: 0.3539 - classification_loss: 0.0492
  559/10000 [>.............................] - ETA: 2:21:34 - loss: 0.4027 - regression_loss: 0.3536 - classification_loss: 0.0492
  560/10000 [>.............................] - ETA: 2:21:33 - loss: 0.4027 - regression_loss: 0.3536 - classification_loss: 0.0491
  561/10000 [>.............................] - ETA: 2:21:32 - loss: 0.4023 - regression_loss: 0.3533 - classification_loss: 0.0491
  562/10000 [>.............................] - ETA: 2:21:31 - loss: 0.4026 - regression_loss: 0.3536 - classification_loss: 0.0490
  563/10000 [>.............................] - ETA: 2:21:30 - loss: 0.4024 - regression_loss: 0.3534 - classification_loss: 0.0490
  564/10000 [>.............................] - ETA: 2:21:29 - loss: 0.4022 - regression_loss: 0.3532 - classification_loss: 0.0490
  565/10000 [>.............................] - ETA: 2:21:27 - loss: 0.4015 - regression_loss: 0.3526 - classification_loss: 0.0489
  566/10000 [>.............................] - ETA: 2:21:26 - loss: 0.4011 - regression_loss: 0.3522 - classification_loss: 0.0489
  567/10000 [>.............................] - ETA: 2:21:25 - loss: 0.4006 - regression_loss: 0.3518 - classification_loss: 0.0488
  568/10000 [>.............................] - ETA: 2:21:24 - loss: 0.4011 - regression_loss: 0.3523 - classification_loss: 0.0489
  569/10000 [>.............................] - ETA: 2:21:23 - loss: 0.4014 - regression_loss: 0.3525 - classification_loss: 0.0489
  570/10000 [>.............................] - ETA: 2:21:22 - loss: 0.4015 - regression_loss: 0.3527 - classification_loss: 0.0488
  571/10000 [>.............................] - ETA: 2:21:21 - loss: 0.4016 - regression_loss: 0.3527 - classification_loss: 0.0489
  572/10000 [>.............................] - ETA: 2:21:20 - loss: 0.4012 - regression_loss: 0.3524 - classification_loss: 0.0488
  573/10000 [>.............................] - ETA: 2:21:19 - loss: 0.4009 - regression_loss: 0.3521 - classification_loss: 0.0488
  574/10000 [>.............................] - ETA: 2:21:18 - loss: 0.4022 - regression_loss: 0.3533 - classification_loss: 0.0489
  575/10000 [>.............................] - ETA: 2:21:17 - loss: 0.4029 - regression_loss: 0.3540 - classification_loss: 0.0489
  576/10000 [>.............................] - ETA: 2:21:16 - loss: 0.4036 - regression_loss: 0.3547 - classification_loss: 0.0489
  577/10000 [>.............................] - ETA: 2:21:15 - loss: 0.4037 - regression_loss: 0.3548 - classification_loss: 0.0489
  578/10000 [>.............................] - ETA: 2:21:13 - loss: 0.4034 - regression_loss: 0.3546 - classification_loss: 0.0489
  579/10000 [>.............................] - ETA: 2:21:12 - loss: 0.4030 - regression_loss: 0.3542 - classification_loss: 0.0488
  580/10000 [>.............................] - ETA: 2:21:11 - loss: 0.4035 - regression_loss: 0.3546 - classification_loss: 0.0488
  581/10000 [>.............................] - ETA: 2:21:10 - loss: 0.4035 - regression_loss: 0.3547 - classification_loss: 0.0488
  582/10000 [>.............................] - ETA: 2:21:09 - loss: 0.4030 - regression_loss: 0.3542 - classification_loss: 0.0487
  583/10000 [>.............................] - ETA: 2:21:08 - loss: 0.4026 - regression_loss: 0.3539 - classification_loss: 0.0487
  584/10000 [>.............................] - ETA: 2:21:07 - loss: 0.4024 - regression_loss: 0.3537 - classification_loss: 0.0487
  585/10000 [>.............................] - ETA: 2:21:06 - loss: 0.4018 - regression_loss: 0.3532 - classification_loss: 0.0486
  586/10000 [>.............................] - ETA: 2:21:05 - loss: 0.4016 - regression_loss: 0.3530 - classification_loss: 0.0485
  587/10000 [>.............................] - ETA: 2:21:04 - loss: 0.4017 - regression_loss: 0.3531 - classification_loss: 0.0485
  588/10000 [>.............................] - ETA: 2:21:03 - loss: 0.4014 - regression_loss: 0.3529 - classification_loss: 0.0485
  589/10000 [>.............................] - ETA: 2:21:01 - loss: 0.4015 - regression_loss: 0.3531 - classification_loss: 0.0484
  590/10000 [>.............................] - ETA: 2:21:00 - loss: 0.4013 - regression_loss: 0.3529 - classification_loss: 0.0484
  591/10000 [>.............................] - ETA: 2:20:59 - loss: 0.4010 - regression_loss: 0.3526 - classification_loss: 0.0484
  592/10000 [>.............................] - ETA: 2:20:58 - loss: 0.4019 - regression_loss: 0.3535 - classification_loss: 0.0483
  593/10000 [>.............................] - ETA: 2:20:57 - loss: 0.4021 - regression_loss: 0.3538 - classification_loss: 0.0483
  594/10000 [>.............................] - ETA: 2:20:56 - loss: 0.4020 - regression_loss: 0.3537 - classification_loss: 0.0483
  595/10000 [>.............................] - ETA: 2:20:55 - loss: 0.4020 - regression_loss: 0.3537 - classification_loss: 0.0483
  596/10000 [>.............................] - ETA: 2:20:54 - loss: 0.4025 - regression_loss: 0.3541 - classification_loss: 0.0483
  597/10000 [>.............................] - ETA: 2:20:53 - loss: 0.4025 - regression_loss: 0.3542 - classification_loss: 0.0483
  598/10000 [>.............................] - ETA: 2:20:52 - loss: 0.4022 - regression_loss: 0.3540 - classification_loss: 0.0482
  599/10000 [>.............................] - ETA: 2:20:51 - loss: 0.4021 - regression_loss: 0.3538 - classification_loss: 0.0483
  600/10000 [>.............................] - ETA: 2:20:50 - loss: 0.4014 - regression_loss: 0.3532 - classification_loss: 0.0482
  601/10000 [>.............................] - ETA: 2:20:49 - loss: 0.4011 - regression_loss: 0.3529 - classification_loss: 0.0482
  602/10000 [>.............................] - ETA: 2:20:47 - loss: 0.4005 - regression_loss: 0.3523 - classification_loss: 0.0481
  603/10000 [>.............................] - ETA: 2:20:46 - loss: 0.4002 - regression_loss: 0.3522 - classification_loss: 0.0481
  604/10000 [>.............................] - ETA: 2:20:45 - loss: 0.4000 - regression_loss: 0.3520 - classification_loss: 0.0480
  605/10000 [>.............................] - ETA: 2:20:44 - loss: 0.3998 - regression_loss: 0.3518 - classification_loss: 0.0480
  606/10000 [>.............................] - ETA: 2:20:43 - loss: 0.3997 - regression_loss: 0.3518 - classification_loss: 0.0479
  607/10000 [>.............................] - ETA: 2:20:42 - loss: 0.3993 - regression_loss: 0.3515 - classification_loss: 0.0479
  608/10000 [>.............................] - ETA: 2:20:41 - loss: 0.3997 - regression_loss: 0.3518 - classification_loss: 0.0479
  609/10000 [>.............................] - ETA: 2:20:40 - loss: 0.4006 - regression_loss: 0.3528 - classification_loss: 0.0478
  610/10000 [>.............................] - ETA: 2:20:39 - loss: 0.4012 - regression_loss: 0.3534 - classification_loss: 0.0479
  611/10000 [>.............................] - ETA: 2:20:38 - loss: 0.4013 - regression_loss: 0.3535 - classification_loss: 0.0478
  612/10000 [>.............................] - ETA: 2:20:37 - loss: 0.4012 - regression_loss: 0.3534 - classification_loss: 0.0478
  613/10000 [>.............................] - ETA: 2:20:36 - loss: 0.4021 - regression_loss: 0.3543 - classification_loss: 0.0478
  614/10000 [>.............................] - ETA: 2:20:35 - loss: 0.4018 - regression_loss: 0.3540 - classification_loss: 0.0478
  615/10000 [>.............................] - ETA: 2:20:34 - loss: 0.4016 - regression_loss: 0.3538 - classification_loss: 0.0477
  616/10000 [>.............................] - ETA: 2:20:33 - loss: 0.4014 - regression_loss: 0.3538 - classification_loss: 0.0477
  617/10000 [>.............................] - ETA: 2:20:31 - loss: 0.4013 - regression_loss: 0.3536 - classification_loss: 0.0477
  618/10000 [>.............................] - ETA: 2:20:30 - loss: 0.4010 - regression_loss: 0.3534 - classification_loss: 0.0476
  619/10000 [>.............................] - ETA: 2:20:29 - loss: 0.4008 - regression_loss: 0.3532 - classification_loss: 0.0476
  620/10000 [>.............................] - ETA: 2:20:28 - loss: 0.4006 - regression_loss: 0.3530 - classification_loss: 0.0476
  621/10000 [>.............................] - ETA: 2:20:27 - loss: 0.4002 - regression_loss: 0.3527 - classification_loss: 0.0475
  622/10000 [>.............................] - ETA: 2:20:26 - loss: 0.4002 - regression_loss: 0.3527 - classification_loss: 0.0475
  623/10000 [>.............................] - ETA: 2:20:25 - loss: 0.3996 - regression_loss: 0.3521 - classification_loss: 0.0475
  624/10000 [>.............................] - ETA: 2:20:24 - loss: 0.3999 - regression_loss: 0.3525 - classification_loss: 0.0475
  625/10000 [>.............................] - ETA: 2:20:23 - loss: 0.3997 - regression_loss: 0.3523 - classification_loss: 0.0474
  626/10000 [>.............................] - ETA: 2:20:22 - loss: 0.3994 - regression_loss: 0.3520 - classification_loss: 0.0474
  627/10000 [>.............................] - ETA: 2:20:21 - loss: 0.3992 - regression_loss: 0.3518 - classification_loss: 0.0474
  628/10000 [>.............................] - ETA: 2:20:20 - loss: 0.3989 - regression_loss: 0.3515 - classification_loss: 0.0474
  629/10000 [>.............................] - ETA: 2:20:19 - loss: 0.3988 - regression_loss: 0.3514 - classification_loss: 0.0474
  630/10000 [>.............................] - ETA: 2:20:18 - loss: 0.3986 - regression_loss: 0.3513 - classification_loss: 0.0474
  631/10000 [>.............................] - ETA: 2:20:17 - loss: 0.3986 - regression_loss: 0.3513 - classification_loss: 0.0473
  632/10000 [>.............................] - ETA: 2:20:16 - loss: 0.3982 - regression_loss: 0.3510 - classification_loss: 0.0473
  633/10000 [>.............................] - ETA: 2:20:15 - loss: 0.3984 - regression_loss: 0.3511 - classification_loss: 0.0473
  634/10000 [>.............................] - ETA: 2:20:14 - loss: 0.3981 - regression_loss: 0.3508 - classification_loss: 0.0473
  635/10000 [>.............................] - ETA: 2:20:13 - loss: 0.3980 - regression_loss: 0.3508 - classification_loss: 0.0472
  636/10000 [>.............................] - ETA: 2:20:12 - loss: 0.3976 - regression_loss: 0.3504 - classification_loss: 0.0472
  637/10000 [>.............................] - ETA: 2:20:11 - loss: 0.3980 - regression_loss: 0.3508 - classification_loss: 0.0473
  638/10000 [>.............................] - ETA: 2:20:10 - loss: 0.3982 - regression_loss: 0.3510 - classification_loss: 0.0472
  639/10000 [>.............................] - ETA: 2:20:09 - loss: 0.3982 - regression_loss: 0.3510 - classification_loss: 0.0472
  640/10000 [>.............................] - ETA: 2:20:07 - loss: 0.3984 - regression_loss: 0.3512 - classification_loss: 0.0472
  641/10000 [>.............................] - ETA: 2:20:07 - loss: 0.3982 - regression_loss: 0.3510 - classification_loss: 0.0472
  642/10000 [>.............................] - ETA: 2:20:06 - loss: 0.3979 - regression_loss: 0.3507 - classification_loss: 0.0471
  643/10000 [>.............................] - ETA: 2:20:04 - loss: 0.3985 - regression_loss: 0.3513 - classification_loss: 0.0472
  644/10000 [>.............................] - ETA: 2:20:03 - loss: 0.3984 - regression_loss: 0.3513 - classification_loss: 0.0471
  645/10000 [>.............................] - ETA: 2:20:02 - loss: 0.3978 - regression_loss: 0.3508 - classification_loss: 0.0470
  646/10000 [>.............................] - ETA: 2:20:01 - loss: 0.3978 - regression_loss: 0.3507 - classification_loss: 0.0470
  647/10000 [>.............................] - ETA: 2:20:00 - loss: 0.3981 - regression_loss: 0.3511 - classification_loss: 0.0470
  648/10000 [>.............................] - ETA: 2:19:59 - loss: 0.3979 - regression_loss: 0.3509 - classification_loss: 0.0470
  649/10000 [>.............................] - ETA: 2:19:58 - loss: 0.3979 - regression_loss: 0.3510 - classification_loss: 0.0470
  650/10000 [>.............................] - ETA: 2:19:57 - loss: 0.3979 - regression_loss: 0.3510 - classification_loss: 0.0469
  651/10000 [>.............................] - ETA: 2:19:56 - loss: 0.3975 - regression_loss: 0.3506 - classification_loss: 0.0469
  652/10000 [>.............................] - ETA: 2:19:55 - loss: 0.3982 - regression_loss: 0.3514 - classification_loss: 0.0468
  653/10000 [>.............................] - ETA: 2:19:54 - loss: 0.3979 - regression_loss: 0.3511 - classification_loss: 0.0468
  654/10000 [>.............................] - ETA: 2:19:53 - loss: 0.3985 - regression_loss: 0.3517 - classification_loss: 0.0468
  655/10000 [>.............................] - ETA: 2:19:52 - loss: 0.3995 - regression_loss: 0.3525 - classification_loss: 0.0470
  656/10000 [>.............................] - ETA: 2:19:51 - loss: 0.3993 - regression_loss: 0.3524 - classification_loss: 0.0469
  657/10000 [>.............................] - ETA: 2:19:50 - loss: 0.3997 - regression_loss: 0.3527 - classification_loss: 0.0470
  658/10000 [>.............................] - ETA: 2:19:48 - loss: 0.3993 - regression_loss: 0.3523 - classification_loss: 0.0470
  659/10000 [>.............................] - ETA: 2:19:48 - loss: 0.3990 - regression_loss: 0.3521 - classification_loss: 0.0470
  660/10000 [>.............................] - ETA: 2:19:46 - loss: 0.3990 - regression_loss: 0.3521 - classification_loss: 0.0470
  661/10000 [>.............................] - ETA: 2:19:45 - loss: 0.3995 - regression_loss: 0.3525 - classification_loss: 0.0470
  662/10000 [>.............................] - ETA: 2:19:44 - loss: 0.3994 - regression_loss: 0.3523 - classification_loss: 0.0471
  663/10000 [>.............................] - ETA: 2:19:43 - loss: 0.3993 - regression_loss: 0.3523 - classification_loss: 0.0470
  664/10000 [>.............................] - ETA: 2:19:42 - loss: 0.3989 - regression_loss: 0.3520 - classification_loss: 0.0470
  665/10000 [>.............................] - ETA: 2:19:41 - loss: 0.3986 - regression_loss: 0.3517 - classification_loss: 0.0469
  666/10000 [>.............................] - ETA: 2:19:40 - loss: 0.3990 - regression_loss: 0.3520 - classification_loss: 0.0470
  667/10000 [=>............................] - ETA: 2:19:39 - loss: 0.3986 - regression_loss: 0.3516 - classification_loss: 0.0470
  668/10000 [=>............................] - ETA: 2:19:38 - loss: 0.3986 - regression_loss: 0.3517 - classification_loss: 0.0469
  669/10000 [=>............................] - ETA: 2:19:37 - loss: 0.3980 - regression_loss: 0.3511 - classification_loss: 0.0469
  670/10000 [=>............................] - ETA: 2:19:36 - loss: 0.3974 - regression_loss: 0.3506 - classification_loss: 0.0468
  671/10000 [=>............................] - ETA: 2:19:35 - loss: 0.3971 - regression_loss: 0.3503 - classification_loss: 0.0468
  672/10000 [=>............................] - ETA: 2:19:34 - loss: 0.3971 - regression_loss: 0.3503 - classification_loss: 0.0468
  673/10000 [=>............................] - ETA: 2:19:33 - loss: 0.3983 - regression_loss: 0.3514 - classification_loss: 0.0469
  674/10000 [=>............................] - ETA: 2:19:32 - loss: 0.3982 - regression_loss: 0.3513 - classification_loss: 0.0469
  675/10000 [=>............................] - ETA: 2:19:31 - loss: 0.3989 - regression_loss: 0.3519 - classification_loss: 0.0470
  676/10000 [=>............................] - ETA: 2:19:30 - loss: 0.3988 - regression_loss: 0.3519 - classification_loss: 0.0469
  677/10000 [=>............................] - ETA: 2:19:29 - loss: 0.3985 - regression_loss: 0.3516 - classification_loss: 0.0469
  678/10000 [=>............................] - ETA: 2:19:28 - loss: 0.3987 - regression_loss: 0.3519 - classification_loss: 0.0469
  679/10000 [=>............................] - ETA: 2:19:27 - loss: 0.3986 - regression_loss: 0.3518 - classification_loss: 0.0468
  680/10000 [=>............................] - ETA: 2:19:26 - loss: 0.3989 - regression_loss: 0.3521 - classification_loss: 0.0468
  681/10000 [=>............................] - ETA: 2:19:25 - loss: 0.3990 - regression_loss: 0.3521 - classification_loss: 0.0468
  682/10000 [=>............................] - ETA: 2:19:24 - loss: 0.3999 - regression_loss: 0.3529 - classification_loss: 0.0470
  683/10000 [=>............................] - ETA: 2:19:23 - loss: 0.4001 - regression_loss: 0.3531 - classification_loss: 0.0470
  684/10000 [=>............................] - ETA: 2:19:22 - loss: 0.4002 - regression_loss: 0.3531 - classification_loss: 0.0470
  685/10000 [=>............................] - ETA: 2:19:21 - loss: 0.4005 - regression_loss: 0.3534 - classification_loss: 0.0471
  686/10000 [=>............................] - ETA: 2:19:20 - loss: 0.3999 - regression_loss: 0.3529 - classification_loss: 0.0470
  687/10000 [=>............................] - ETA: 2:19:19 - loss: 0.3998 - regression_loss: 0.3529 - classification_loss: 0.0470
  688/10000 [=>............................] - ETA: 2:19:18 - loss: 0.4000 - regression_loss: 0.3530 - classification_loss: 0.0469
  689/10000 [=>............................] - ETA: 2:19:17 - loss: 0.4000 - regression_loss: 0.3531 - classification_loss: 0.0469
  690/10000 [=>............................] - ETA: 2:19:16 - loss: 0.3996 - regression_loss: 0.3526 - classification_loss: 0.0469
  691/10000 [=>............................] - ETA: 2:19:15 - loss: 0.3995 - regression_loss: 0.3526 - classification_loss: 0.0469
  692/10000 [=>............................] - ETA: 2:19:14 - loss: 0.3992 - regression_loss: 0.3524 - classification_loss: 0.0469
  693/10000 [=>............................] - ETA: 2:19:13 - loss: 0.3988 - regression_loss: 0.3521 - classification_loss: 0.0468
  694/10000 [=>............................] - ETA: 2:19:11 - loss: 0.3987 - regression_loss: 0.3520 - classification_loss: 0.0468
  695/10000 [=>............................] - ETA: 2:19:10 - loss: 0.3984 - regression_loss: 0.3517 - classification_loss: 0.0467
  696/10000 [=>............................] - ETA: 2:19:09 - loss: 0.3981 - regression_loss: 0.3514 - classification_loss: 0.0466
  697/10000 [=>............................] - ETA: 2:19:08 - loss: 0.3980 - regression_loss: 0.3513 - classification_loss: 0.0466
  698/10000 [=>............................] - ETA: 2:19:07 - loss: 0.3977 - regression_loss: 0.3511 - classification_loss: 0.0466
  699/10000 [=>............................] - ETA: 2:19:06 - loss: 0.3976 - regression_loss: 0.3510 - classification_loss: 0.0466
  700/10000 [=>............................] - ETA: 2:19:05 - loss: 0.3974 - regression_loss: 0.3509 - classification_loss: 0.0465
  701/10000 [=>............................] - ETA: 2:19:04 - loss: 0.3971 - regression_loss: 0.3506 - classification_loss: 0.0465
  702/10000 [=>............................] - ETA: 2:19:03 - loss: 0.3985 - regression_loss: 0.3514 - classification_loss: 0.0470
  703/10000 [=>............................] - ETA: 2:19:02 - loss: 0.3985 - regression_loss: 0.3515 - classification_loss: 0.0470
  704/10000 [=>............................] - ETA: 2:19:01 - loss: 0.3985 - regression_loss: 0.3515 - classification_loss: 0.0470
  705/10000 [=>............................] - ETA: 2:19:00 - loss: 0.3984 - regression_loss: 0.3513 - classification_loss: 0.0470
  706/10000 [=>............................] - ETA: 2:18:59 - loss: 0.3991 - regression_loss: 0.3520 - classification_loss: 0.0471
  707/10000 [=>............................] - ETA: 2:18:58 - loss: 0.3994 - regression_loss: 0.3523 - classification_loss: 0.0472
  708/10000 [=>............................] - ETA: 2:18:57 - loss: 0.3989 - regression_loss: 0.3518 - classification_loss: 0.0471
  709/10000 [=>............................] - ETA: 2:18:56 - loss: 0.3989 - regression_loss: 0.3518 - classification_loss: 0.0471
  710/10000 [=>............................] - ETA: 2:18:55 - loss: 0.3988 - regression_loss: 0.3517 - classification_loss: 0.0471
  711/10000 [=>............................] - ETA: 2:18:54 - loss: 0.3987 - regression_loss: 0.3517 - classification_loss: 0.0471
  712/10000 [=>............................] - ETA: 2:18:53 - loss: 0.3985 - regression_loss: 0.3515 - classification_loss: 0.0470
  713/10000 [=>............................] - ETA: 2:18:52 - loss: 0.3982 - regression_loss: 0.3512 - classification_loss: 0.0470
  714/10000 [=>............................] - ETA: 2:18:51 - loss: 0.3981 - regression_loss: 0.3511 - classification_loss: 0.0470
  715/10000 [=>............................] - ETA: 2:18:50 - loss: 0.3987 - regression_loss: 0.3517 - classification_loss: 0.0470
  716/10000 [=>............................] - ETA: 2:18:49 - loss: 0.3986 - regression_loss: 0.3517 - classification_loss: 0.0470
  717/10000 [=>............................] - ETA: 2:18:48 - loss: 0.3989 - regression_loss: 0.3520 - classification_loss: 0.0469
  718/10000 [=>............................] - ETA: 2:18:47 - loss: 0.3987 - regression_loss: 0.3518 - classification_loss: 0.0469
  719/10000 [=>............................] - ETA: 2:18:46 - loss: 0.3988 - regression_loss: 0.3519 - classification_loss: 0.0468
  720/10000 [=>............................] - ETA: 2:18:45 - loss: 0.3989 - regression_loss: 0.3520 - classification_loss: 0.0469
  721/10000 [=>............................] - ETA: 2:18:44 - loss: 0.3987 - regression_loss: 0.3519 - classification_loss: 0.0468
  722/10000 [=>............................] - ETA: 2:18:43 - loss: 0.3990 - regression_loss: 0.3522 - classification_loss: 0.0468
  723/10000 [=>............................] - ETA: 2:18:42 - loss: 0.3987 - regression_loss: 0.3520 - classification_loss: 0.0467
  724/10000 [=>............................] - ETA: 2:18:41 - loss: 0.3987 - regression_loss: 0.3520 - classification_loss: 0.0467
  725/10000 [=>............................] - ETA: 2:18:40 - loss: 0.3985 - regression_loss: 0.3518 - classification_loss: 0.0467
  726/10000 [=>............................] - ETA: 2:18:39 - loss: 0.3980 - regression_loss: 0.3514 - classification_loss: 0.0466
  727/10000 [=>............................] - ETA: 2:18:38 - loss: 0.3979 - regression_loss: 0.3513 - classification_loss: 0.0466
  728/10000 [=>............................] - ETA: 2:18:37 - loss: 0.3978 - regression_loss: 0.3512 - classification_loss: 0.0466
  729/10000 [=>............................] - ETA: 2:18:36 - loss: 0.3975 - regression_loss: 0.3510 - classification_loss: 0.0466
  730/10000 [=>............................] - ETA: 2:18:35 - loss: 0.3978 - regression_loss: 0.3512 - classification_loss: 0.0466
  731/10000 [=>............................] - ETA: 2:18:34 - loss: 0.3983 - regression_loss: 0.3518 - classification_loss: 0.0466
  732/10000 [=>............................] - ETA: 2:18:33 - loss: 0.3982 - regression_loss: 0.3517 - classification_loss: 0.0465
  733/10000 [=>............................] - ETA: 2:18:32 - loss: 0.3981 - regression_loss: 0.3515 - classification_loss: 0.0466
  734/10000 [=>............................] - ETA: 2:18:31 - loss: 0.3979 - regression_loss: 0.3514 - classification_loss: 0.0465
  735/10000 [=>............................] - ETA: 2:18:30 - loss: 0.3978 - regression_loss: 0.3514 - classification_loss: 0.0465
  736/10000 [=>............................] - ETA: 2:18:29 - loss: 0.3976 - regression_loss: 0.3512 - classification_loss: 0.0465
  737/10000 [=>............................] - ETA: 2:18:28 - loss: 0.3982 - regression_loss: 0.3517 - classification_loss: 0.0465
  738/10000 [=>............................] - ETA: 2:18:27 - loss: 0.4004 - regression_loss: 0.3534 - classification_loss: 0.0470
  739/10000 [=>............................] - ETA: 2:18:26 - loss: 0.4006 - regression_loss: 0.3536 - classification_loss: 0.0470
  740/10000 [=>............................] - ETA: 2:18:25 - loss: 0.4005 - regression_loss: 0.3536 - classification_loss: 0.0470
  741/10000 [=>............................] - ETA: 2:18:24 - loss: 0.4013 - regression_loss: 0.3541 - classification_loss: 0.0472
  742/10000 [=>............................] - ETA: 2:18:23 - loss: 0.4017 - regression_loss: 0.3545 - classification_loss: 0.0472
  743/10000 [=>............................] - ETA: 2:18:22 - loss: 0.4015 - regression_loss: 0.3544 - classification_loss: 0.0471
  744/10000 [=>............................] - ETA: 2:18:21 - loss: 0.4015 - regression_loss: 0.3544 - classification_loss: 0.0471
  745/10000 [=>............................] - ETA: 2:18:20 - loss: 0.4012 - regression_loss: 0.3542 - classification_loss: 0.0471
  746/10000 [=>............................] - ETA: 2:18:19 - loss: 0.4011 - regression_loss: 0.3540 - classification_loss: 0.0471
  747/10000 [=>............................] - ETA: 2:18:18 - loss: 0.4009 - regression_loss: 0.3538 - classification_loss: 0.0470
  748/10000 [=>............................] - ETA: 2:18:17 - loss: 0.4011 - regression_loss: 0.3541 - classification_loss: 0.0471
  749/10000 [=>............................] - ETA: 2:18:16 - loss: 0.4006 - regression_loss: 0.3536 - classification_loss: 0.0470
  750/10000 [=>............................] - ETA: 2:18:15 - loss: 0.4007 - regression_loss: 0.3538 - classification_loss: 0.0470
  751/10000 [=>............................] - ETA: 2:18:14 - loss: 0.4006 - regression_loss: 0.3537 - classification_loss: 0.0469
  752/10000 [=>............................] - ETA: 2:18:13 - loss: 0.4010 - regression_loss: 0.3541 - classification_loss: 0.0469
  753/10000 [=>............................] - ETA: 2:18:12 - loss: 0.4013 - regression_loss: 0.3544 - classification_loss: 0.0469
  754/10000 [=>............................] - ETA: 2:18:11 - loss: 0.4015 - regression_loss: 0.3546 - classification_loss: 0.0469
  755/10000 [=>............................] - ETA: 2:18:10 - loss: 0.4038 - regression_loss: 0.3562 - classification_loss: 0.0476
  756/10000 [=>............................] - ETA: 2:18:08 - loss: 0.4058 - regression_loss: 0.3578 - classification_loss: 0.0480
  757/10000 [=>............................] - ETA: 2:18:07 - loss: 0.4059 - regression_loss: 0.3579 - classification_loss: 0.0480
  758/10000 [=>............................] - ETA: 2:18:06 - loss: 0.4062 - regression_loss: 0.3582 - classification_loss: 0.0480
  759/10000 [=>............................] - ETA: 2:18:05 - loss: 0.4063 - regression_loss: 0.3584 - classification_loss: 0.0480
  760/10000 [=>............................] - ETA: 2:18:04 - loss: 0.4062 - regression_loss: 0.3583 - classification_loss: 0.0480
  761/10000 [=>............................] - ETA: 2:18:03 - loss: 0.4062 - regression_loss: 0.3581 - classification_loss: 0.0481
  762/10000 [=>............................] - ETA: 2:18:02 - loss: 0.4062 - regression_loss: 0.3582 - classification_loss: 0.0480
  763/10000 [=>............................] - ETA: 2:18:01 - loss: 0.4057 - regression_loss: 0.3577 - classification_loss: 0.0480
  764/10000 [=>............................] - ETA: 2:18:00 - loss: 0.4055 - regression_loss: 0.3575 - classification_loss: 0.0479
  765/10000 [=>............................] - ETA: 2:17:59 - loss: 0.4055 - regression_loss: 0.3576 - classification_loss: 0.0479
  766/10000 [=>............................] - ETA: 2:17:58 - loss: 0.4052 - regression_loss: 0.3573 - classification_loss: 0.0479
  767/10000 [=>............................] - ETA: 2:17:57 - loss: 0.4055 - regression_loss: 0.3576 - classification_loss: 0.0479
  768/10000 [=>............................] - ETA: 2:17:56 - loss: 0.4054 - regression_loss: 0.3575 - classification_loss: 0.0479
  769/10000 [=>............................] - ETA: 2:17:55 - loss: 0.4051 - regression_loss: 0.3573 - classification_loss: 0.0478
  770/10000 [=>............................] - ETA: 2:17:54 - loss: 0.4052 - regression_loss: 0.3574 - classification_loss: 0.0478
  771/10000 [=>............................] - ETA: 2:17:53 - loss: 0.4051 - regression_loss: 0.3574 - classification_loss: 0.0478
  772/10000 [=>............................] - ETA: 2:17:52 - loss: 0.4050 - regression_loss: 0.3573 - classification_loss: 0.0477
  773/10000 [=>............................] - ETA: 2:17:51 - loss: 0.4047 - regression_loss: 0.3570 - classification_loss: 0.0477
  774/10000 [=>............................] - ETA: 2:17:50 - loss: 0.4046 - regression_loss: 0.3570 - classification_loss: 0.0477
  775/10000 [=>............................] - ETA: 2:17:49 - loss: 0.4048 - regression_loss: 0.3571 - classification_loss: 0.0477
  776/10000 [=>............................] - ETA: 2:17:48 - loss: 0.4043 - regression_loss: 0.3567 - classification_loss: 0.0476
  777/10000 [=>............................] - ETA: 2:17:47 - loss: 0.4048 - regression_loss: 0.3571 - classification_loss: 0.0477
  778/10000 [=>............................] - ETA: 2:17:46 - loss: 0.4045 - regression_loss: 0.3568 - classification_loss: 0.0477
  779/10000 [=>............................] - ETA: 2:17:45 - loss: 0.4043 - regression_loss: 0.3567 - classification_loss: 0.0476
  780/10000 [=>............................] - ETA: 2:17:44 - loss: 0.4042 - regression_loss: 0.3567 - classification_loss: 0.0476
  781/10000 [=>............................] - ETA: 2:17:43 - loss: 0.4040 - regression_loss: 0.3564 - classification_loss: 0.0476
  782/10000 [=>............................] - ETA: 2:17:42 - loss: 0.4041 - regression_loss: 0.3566 - classification_loss: 0.0476
  783/10000 [=>............................] - ETA: 2:17:41 - loss: 0.4045 - regression_loss: 0.3570 - classification_loss: 0.0475
  784/10000 [=>............................] - ETA: 2:17:40 - loss: 0.4053 - regression_loss: 0.3578 - classification_loss: 0.0475
  785/10000 [=>............................] - ETA: 2:17:39 - loss: 0.4053 - regression_loss: 0.3578 - classification_loss: 0.0475
  786/10000 [=>............................] - ETA: 2:17:38 - loss: 0.4059 - regression_loss: 0.3583 - classification_loss: 0.0475
  787/10000 [=>............................] - ETA: 2:17:37 - loss: 0.4060 - regression_loss: 0.3585 - classification_loss: 0.0475
  788/10000 [=>............................] - ETA: 2:17:36 - loss: 0.4058 - regression_loss: 0.3583 - classification_loss: 0.0475
  789/10000 [=>............................] - ETA: 2:17:35 - loss: 0.4055 - regression_loss: 0.3580 - classification_loss: 0.0475
  790/10000 [=>............................] - ETA: 2:17:34 - loss: 0.4050 - regression_loss: 0.3576 - classification_loss: 0.0474
  791/10000 [=>............................] - ETA: 2:17:33 - loss: 0.4049 - regression_loss: 0.3575 - classification_loss: 0.0474
  792/10000 [=>............................] - ETA: 2:17:32 - loss: 0.4068 - regression_loss: 0.3574 - classification_loss: 0.0494
  793/10000 [=>............................] - ETA: 2:17:32 - loss: 0.4066 - regression_loss: 0.3572 - classification_loss: 0.0494
  794/10000 [=>............................] - ETA: 2:17:31 - loss: 0.4066 - regression_loss: 0.3573 - classification_loss: 0.0494
  795/10000 [=>............................] - ETA: 2:17:30 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0493
  796/10000 [=>............................] - ETA: 2:17:29 - loss: 0.4070 - regression_loss: 0.3576 - classification_loss: 0.0494
  797/10000 [=>............................] - ETA: 2:17:28 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0494
  798/10000 [=>............................] - ETA: 2:17:27 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0493
  799/10000 [=>............................] - ETA: 2:17:26 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0493
  800/10000 [=>............................] - ETA: 2:17:25 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
  801/10000 [=>............................] - ETA: 2:17:24 - loss: 0.4074 - regression_loss: 0.3582 - classification_loss: 0.0492
  802/10000 [=>............................] - ETA: 2:17:23 - loss: 0.4074 - regression_loss: 0.3582 - classification_loss: 0.0492
  803/10000 [=>............................] - ETA: 2:17:22 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
  804/10000 [=>............................] - ETA: 2:17:21 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
  805/10000 [=>............................] - ETA: 2:17:20 - loss: 0.4074 - regression_loss: 0.3582 - classification_loss: 0.0492
  806/10000 [=>............................] - ETA: 2:17:19 - loss: 0.4072 - regression_loss: 0.3581 - classification_loss: 0.0491
  807/10000 [=>............................] - ETA: 2:17:18 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0491
  808/10000 [=>............................] - ETA: 2:17:17 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
  809/10000 [=>............................] - ETA: 2:17:16 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
  810/10000 [=>............................] - ETA: 2:17:15 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0490
  811/10000 [=>............................] - ETA: 2:17:14 - loss: 0.4059 - regression_loss: 0.3570 - classification_loss: 0.0489
  812/10000 [=>............................] - ETA: 2:17:13 - loss: 0.4058 - regression_loss: 0.3569 - classification_loss: 0.0489
  813/10000 [=>............................] - ETA: 2:17:12 - loss: 0.4056 - regression_loss: 0.3567 - classification_loss: 0.0489
  814/10000 [=>............................] - ETA: 2:17:11 - loss: 0.4054 - regression_loss: 0.3564 - classification_loss: 0.0489
  815/10000 [=>............................] - ETA: 2:17:10 - loss: 0.4050 - regression_loss: 0.3561 - classification_loss: 0.0489
  816/10000 [=>............................] - ETA: 2:17:09 - loss: 0.4050 - regression_loss: 0.3561 - classification_loss: 0.0489
  817/10000 [=>............................] - ETA: 2:17:08 - loss: 0.4050 - regression_loss: 0.3560 - classification_loss: 0.0490
  818/10000 [=>............................] - ETA: 2:17:07 - loss: 0.4054 - regression_loss: 0.3563 - classification_loss: 0.0491
  819/10000 [=>............................] - ETA: 2:17:06 - loss: 0.4059 - regression_loss: 0.3568 - classification_loss: 0.0492
  820/10000 [=>............................] - ETA: 2:17:05 - loss: 0.4060 - regression_loss: 0.3568 - classification_loss: 0.0491
  821/10000 [=>............................] - ETA: 2:17:04 - loss: 0.4062 - regression_loss: 0.3570 - classification_loss: 0.0492
  822/10000 [=>............................] - ETA: 2:17:03 - loss: 0.4061 - regression_loss: 0.3570 - classification_loss: 0.0491
  823/10000 [=>............................] - ETA: 2:17:02 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
  824/10000 [=>............................] - ETA: 2:17:01 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
  825/10000 [=>............................] - ETA: 2:17:00 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0491
  826/10000 [=>............................] - ETA: 2:16:59 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
  827/10000 [=>............................] - ETA: 2:16:58 - loss: 0.4062 - regression_loss: 0.3571 - classification_loss: 0.0491
  828/10000 [=>............................] - ETA: 2:16:57 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0490
  829/10000 [=>............................] - ETA: 2:16:56 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
  830/10000 [=>............................] - ETA: 2:16:55 - loss: 0.4061 - regression_loss: 0.3570 - classification_loss: 0.0491
  831/10000 [=>............................] - ETA: 2:16:54 - loss: 0.4058 - regression_loss: 0.3568 - classification_loss: 0.0490
  832/10000 [=>............................] - ETA: 2:16:53 - loss: 0.4059 - regression_loss: 0.3568 - classification_loss: 0.0491
  833/10000 [=>............................] - ETA: 2:16:52 - loss: 0.4055 - regression_loss: 0.3565 - classification_loss: 0.0490
  834/10000 [=>............................] - ETA: 2:16:51 - loss: 0.4059 - regression_loss: 0.3568 - classification_loss: 0.0491
  835/10000 [=>............................] - ETA: 2:16:50 - loss: 0.4061 - regression_loss: 0.3569 - classification_loss: 0.0492
  836/10000 [=>............................] - ETA: 2:16:49 - loss: 0.4062 - regression_loss: 0.3570 - classification_loss: 0.0492
  837/10000 [=>............................] - ETA: 2:16:48 - loss: 0.4059 - regression_loss: 0.3567 - classification_loss: 0.0491
  838/10000 [=>............................] - ETA: 2:16:47 - loss: 0.4059 - regression_loss: 0.3568 - classification_loss: 0.0491
  839/10000 [=>............................] - ETA: 2:16:46 - loss: 0.4066 - regression_loss: 0.3575 - classification_loss: 0.0491
  840/10000 [=>............................] - ETA: 2:16:45 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
  841/10000 [=>............................] - ETA: 2:16:44 - loss: 0.4058 - regression_loss: 0.3568 - classification_loss: 0.0490
  842/10000 [=>............................] - ETA: 2:16:43 - loss: 0.4059 - regression_loss: 0.3569 - classification_loss: 0.0490
  843/10000 [=>............................] - ETA: 2:16:42 - loss: 0.4060 - regression_loss: 0.3569 - classification_loss: 0.0491
  844/10000 [=>............................] - ETA: 2:16:41 - loss: 0.4059 - regression_loss: 0.3568 - classification_loss: 0.0490
  845/10000 [=>............................] - ETA: 2:16:40 - loss: 0.4057 - regression_loss: 0.3567 - classification_loss: 0.0490
  846/10000 [=>............................] - ETA: 2:16:40 - loss: 0.4056 - regression_loss: 0.3566 - classification_loss: 0.0490
  847/10000 [=>............................] - ETA: 2:16:39 - loss: 0.4054 - regression_loss: 0.3564 - classification_loss: 0.0489
  848/10000 [=>............................] - ETA: 2:16:38 - loss: 0.4055 - regression_loss: 0.3565 - classification_loss: 0.0489
  849/10000 [=>............................] - ETA: 2:16:37 - loss: 0.4056 - regression_loss: 0.3566 - classification_loss: 0.0489
  850/10000 [=>............................] - ETA: 2:16:36 - loss: 0.4054 - regression_loss: 0.3565 - classification_loss: 0.0489
  851/10000 [=>............................] - ETA: 2:16:35 - loss: 0.4054 - regression_loss: 0.3565 - classification_loss: 0.0489
  852/10000 [=>............................] - ETA: 2:16:34 - loss: 0.4053 - regression_loss: 0.3565 - classification_loss: 0.0488
  853/10000 [=>............................] - ETA: 2:16:33 - loss: 0.4049 - regression_loss: 0.3561 - classification_loss: 0.0488
  854/10000 [=>............................] - ETA: 2:16:32 - loss: 0.4046 - regression_loss: 0.3559 - classification_loss: 0.0487
  855/10000 [=>............................] - ETA: 2:16:31 - loss: 0.4045 - regression_loss: 0.3558 - classification_loss: 0.0487
  856/10000 [=>............................] - ETA: 2:16:30 - loss: 0.4048 - regression_loss: 0.3561 - classification_loss: 0.0488
  857/10000 [=>............................] - ETA: 2:16:29 - loss: 0.4050 - regression_loss: 0.3562 - classification_loss: 0.0488
  858/10000 [=>............................] - ETA: 2:16:28 - loss: 0.4050 - regression_loss: 0.3562 - classification_loss: 0.0488
  859/10000 [=>............................] - ETA: 2:16:27 - loss: 0.4049 - regression_loss: 0.3561 - classification_loss: 0.0488
  860/10000 [=>............................] - ETA: 2:16:26 - loss: 0.4046 - regression_loss: 0.3559 - classification_loss: 0.0487
  861/10000 [=>............................] - ETA: 2:16:25 - loss: 0.4046 - regression_loss: 0.3559 - classification_loss: 0.0487
  862/10000 [=>............................] - ETA: 2:16:24 - loss: 0.4045 - regression_loss: 0.3558 - classification_loss: 0.0487
  863/10000 [=>............................] - ETA: 2:16:23 - loss: 0.4043 - regression_loss: 0.3557 - classification_loss: 0.0487
  864/10000 [=>............................] - ETA: 2:16:22 - loss: 0.4039 - regression_loss: 0.3553 - classification_loss: 0.0486
  865/10000 [=>............................] - ETA: 2:16:21 - loss: 0.4045 - regression_loss: 0.3560 - classification_loss: 0.0486
  866/10000 [=>............................] - ETA: 2:16:20 - loss: 0.4045 - regression_loss: 0.3560 - classification_loss: 0.0486
  867/10000 [=>............................] - ETA: 2:16:19 - loss: 0.4047 - regression_loss: 0.3561 - classification_loss: 0.0486
  868/10000 [=>............................] - ETA: 2:16:18 - loss: 0.4047 - regression_loss: 0.3562 - classification_loss: 0.0485
  869/10000 [=>............................] - ETA: 2:16:17 - loss: 0.4050 - regression_loss: 0.3564 - classification_loss: 0.0486
  870/10000 [=>............................] - ETA: 2:16:16 - loss: 0.4049 - regression_loss: 0.3563 - classification_loss: 0.0486
  871/10000 [=>............................] - ETA: 2:16:16 - loss: 0.4044 - regression_loss: 0.3559 - classification_loss: 0.0485
  872/10000 [=>............................] - ETA: 2:16:15 - loss: 0.4048 - regression_loss: 0.3562 - classification_loss: 0.0485
  873/10000 [=>............................] - ETA: 2:16:14 - loss: 0.4049 - regression_loss: 0.3564 - classification_loss: 0.0485
  874/10000 [=>............................] - ETA: 2:16:13 - loss: 0.4047 - regression_loss: 0.3561 - classification_loss: 0.0485
  875/10000 [=>............................] - ETA: 2:16:12 - loss: 0.4045 - regression_loss: 0.3560 - classification_loss: 0.0485
  876/10000 [=>............................] - ETA: 2:16:11 - loss: 0.4045 - regression_loss: 0.3561 - classification_loss: 0.0485
  877/10000 [=>............................] - ETA: 2:16:10 - loss: 0.4046 - regression_loss: 0.3561 - classification_loss: 0.0485
  878/10000 [=>............................] - ETA: 2:16:09 - loss: 0.4043 - regression_loss: 0.3559 - classification_loss: 0.0484
  879/10000 [=>............................] - ETA: 2:16:08 - loss: 0.4041 - regression_loss: 0.3557 - classification_loss: 0.0484
  880/10000 [=>............................] - ETA: 2:16:07 - loss: 0.4040 - regression_loss: 0.3556 - classification_loss: 0.0484
  881/10000 [=>............................] - ETA: 2:16:06 - loss: 0.4039 - regression_loss: 0.3556 - classification_loss: 0.0483
  882/10000 [=>............................] - ETA: 2:16:05 - loss: 0.4037 - regression_loss: 0.3554 - classification_loss: 0.0483
  883/10000 [=>............................] - ETA: 2:16:04 - loss: 0.4039 - regression_loss: 0.3555 - classification_loss: 0.0483
  884/10000 [=>............................] - ETA: 2:16:03 - loss: 0.4036 - regression_loss: 0.3553 - classification_loss: 0.0483
  885/10000 [=>............................] - ETA: 2:16:02 - loss: 0.4037 - regression_loss: 0.3554 - classification_loss: 0.0483
  886/10000 [=>............................] - ETA: 2:16:01 - loss: 0.4037 - regression_loss: 0.3554 - classification_loss: 0.0483
  887/10000 [=>............................] - ETA: 2:16:00 - loss: 0.4040 - regression_loss: 0.3557 - classification_loss: 0.0484
  888/10000 [=>............................] - ETA: 2:15:59 - loss: 0.4036 - regression_loss: 0.3553 - classification_loss: 0.0483
  889/10000 [=>............................] - ETA: 2:15:58 - loss: 0.4040 - regression_loss: 0.3556 - classification_loss: 0.0483
  890/10000 [=>............................] - ETA: 2:15:57 - loss: 0.4040 - regression_loss: 0.3557 - classification_loss: 0.0483
  891/10000 [=>............................] - ETA: 2:15:56 - loss: 0.4039 - regression_loss: 0.3556 - classification_loss: 0.0483
  892/10000 [=>............................] - ETA: 2:15:55 - loss: 0.4036 - regression_loss: 0.3553 - classification_loss: 0.0483
  893/10000 [=>............................] - ETA: 2:15:54 - loss: 0.4031 - regression_loss: 0.3549 - classification_loss: 0.0482
  894/10000 [=>............................] - ETA: 2:15:53 - loss: 0.4032 - regression_loss: 0.3550 - classification_loss: 0.0482
  895/10000 [=>............................] - ETA: 2:15:52 - loss: 0.4034 - regression_loss: 0.3552 - classification_loss: 0.0482
  896/10000 [=>............................] - ETA: 2:15:51 - loss: 0.4034 - regression_loss: 0.3552 - classification_loss: 0.0482
  897/10000 [=>............................] - ETA: 2:15:50 - loss: 0.4037 - regression_loss: 0.3554 - classification_loss: 0.0483
  898/10000 [=>............................] - ETA: 2:15:49 - loss: 0.4040 - regression_loss: 0.3557 - classification_loss: 0.0483
  899/10000 [=>............................] - ETA: 2:15:48 - loss: 0.4038 - regression_loss: 0.3556 - classification_loss: 0.0482
  900/10000 [=>............................] - ETA: 2:15:47 - loss: 0.4037 - regression_loss: 0.3555 - classification_loss: 0.0482
  901/10000 [=>............................] - ETA: 2:15:46 - loss: 0.4037 - regression_loss: 0.3555 - classification_loss: 0.0482
  902/10000 [=>............................] - ETA: 2:15:45 - loss: 0.4034 - regression_loss: 0.3553 - classification_loss: 0.0482
  903/10000 [=>............................] - ETA: 2:15:44 - loss: 0.4035 - regression_loss: 0.3551 - classification_loss: 0.0484
  904/10000 [=>............................] - ETA: 2:15:43 - loss: 0.4033 - regression_loss: 0.3549 - classification_loss: 0.0484
  905/10000 [=>............................] - ETA: 2:15:42 - loss: 0.4037 - regression_loss: 0.3553 - classification_loss: 0.0484
  906/10000 [=>............................] - ETA: 2:15:42 - loss: 0.4038 - regression_loss: 0.3555 - classification_loss: 0.0484
  907/10000 [=>............................] - ETA: 2:15:41 - loss: 0.4041 - regression_loss: 0.3557 - classification_loss: 0.0484
  908/10000 [=>............................] - ETA: 2:15:40 - loss: 0.4040 - regression_loss: 0.3557 - classification_loss: 0.0483
  909/10000 [=>............................] - ETA: 2:15:39 - loss: 0.4044 - regression_loss: 0.3560 - classification_loss: 0.0484
  910/10000 [=>............................] - ETA: 2:15:38 - loss: 0.4045 - regression_loss: 0.3561 - classification_loss: 0.0484
  911/10000 [=>............................] - ETA: 2:15:37 - loss: 0.4045 - regression_loss: 0.3562 - classification_loss: 0.0483
  912/10000 [=>............................] - ETA: 2:15:36 - loss: 0.4044 - regression_loss: 0.3561 - classification_loss: 0.0484
  913/10000 [=>............................] - ETA: 2:15:35 - loss: 0.4045 - regression_loss: 0.3562 - classification_loss: 0.0483
  914/10000 [=>............................] - ETA: 2:15:34 - loss: 0.4043 - regression_loss: 0.3561 - classification_loss: 0.0483
  915/10000 [=>............................] - ETA: 2:15:33 - loss: 0.4041 - regression_loss: 0.3558 - classification_loss: 0.0483
  916/10000 [=>............................] - ETA: 2:15:32 - loss: 0.4041 - regression_loss: 0.3558 - classification_loss: 0.0482
  917/10000 [=>............................] - ETA: 2:15:31 - loss: 0.4037 - regression_loss: 0.3555 - classification_loss: 0.0482
  918/10000 [=>............................] - ETA: 2:15:30 - loss: 0.4038 - regression_loss: 0.3556 - classification_loss: 0.0482
  919/10000 [=>............................] - ETA: 2:15:29 - loss: 0.4034 - regression_loss: 0.3552 - classification_loss: 0.0482
  920/10000 [=>............................] - ETA: 2:15:28 - loss: 0.4032 - regression_loss: 0.3551 - classification_loss: 0.0481
  921/10000 [=>............................] - ETA: 2:15:27 - loss: 0.4030 - regression_loss: 0.3549 - classification_loss: 0.0481
  922/10000 [=>............................] - ETA: 2:15:26 - loss: 0.4028 - regression_loss: 0.3547 - classification_loss: 0.0481
  923/10000 [=>............................] - ETA: 2:15:25 - loss: 0.4028 - regression_loss: 0.3548 - classification_loss: 0.0480
  924/10000 [=>............................] - ETA: 2:15:24 - loss: 0.4029 - regression_loss: 0.3548 - classification_loss: 0.0480
  925/10000 [=>............................] - ETA: 2:15:23 - loss: 0.4028 - regression_loss: 0.3547 - classification_loss: 0.0480
  926/10000 [=>............................] - ETA: 2:15:22 - loss: 0.4025 - regression_loss: 0.3545 - classification_loss: 0.0480
  927/10000 [=>............................] - ETA: 2:15:21 - loss: 0.4022 - regression_loss: 0.3542 - classification_loss: 0.0480
  928/10000 [=>............................] - ETA: 2:15:20 - loss: 0.4021 - regression_loss: 0.3542 - classification_loss: 0.0479
  929/10000 [=>............................] - ETA: 2:15:19 - loss: 0.4021 - regression_loss: 0.3541 - classification_loss: 0.0479
  930/10000 [=>............................] - ETA: 2:15:18 - loss: 0.4016 - regression_loss: 0.3538 - classification_loss: 0.0479
  931/10000 [=>............................] - ETA: 2:15:17 - loss: 0.4017 - regression_loss: 0.3539 - classification_loss: 0.0478
  932/10000 [=>............................] - ETA: 2:15:16 - loss: 0.4016 - regression_loss: 0.3538 - classification_loss: 0.0478
  933/10000 [=>............................] - ETA: 2:15:15 - loss: 0.4015 - regression_loss: 0.3537 - classification_loss: 0.0478
  934/10000 [=>............................] - ETA: 2:15:14 - loss: 0.4015 - regression_loss: 0.3537 - classification_loss: 0.0478
  935/10000 [=>............................] - ETA: 2:15:14 - loss: 0.4015 - regression_loss: 0.3537 - classification_loss: 0.0477
  936/10000 [=>............................] - ETA: 2:15:13 - loss: 0.4015 - regression_loss: 0.3538 - classification_loss: 0.0477
  937/10000 [=>............................] - ETA: 2:15:12 - loss: 0.4011 - regression_loss: 0.3534 - classification_loss: 0.0477
  938/10000 [=>............................] - ETA: 2:15:11 - loss: 0.4008 - regression_loss: 0.3532 - classification_loss: 0.0477
  939/10000 [=>............................] - ETA: 2:15:10 - loss: 0.4012 - regression_loss: 0.3536 - classification_loss: 0.0476
  940/10000 [=>............................] - ETA: 2:15:09 - loss: 0.4010 - regression_loss: 0.3534 - classification_loss: 0.0476
  941/10000 [=>............................] - ETA: 2:15:08 - loss: 0.4024 - regression_loss: 0.3544 - classification_loss: 0.0480
  942/10000 [=>............................] - ETA: 2:15:07 - loss: 0.4021 - regression_loss: 0.3542 - classification_loss: 0.0480
  943/10000 [=>............................] - ETA: 2:15:06 - loss: 0.4020 - regression_loss: 0.3540 - classification_loss: 0.0479
  944/10000 [=>............................] - ETA: 2:15:05 - loss: 0.4015 - regression_loss: 0.3537 - classification_loss: 0.0479
  945/10000 [=>............................] - ETA: 2:15:04 - loss: 0.4014 - regression_loss: 0.3536 - classification_loss: 0.0478
  946/10000 [=>............................] - ETA: 2:15:03 - loss: 0.4013 - regression_loss: 0.3535 - classification_loss: 0.0478
  947/10000 [=>............................] - ETA: 2:15:02 - loss: 0.4015 - regression_loss: 0.3537 - classification_loss: 0.0478
  948/10000 [=>............................] - ETA: 2:15:01 - loss: 0.4017 - regression_loss: 0.3539 - classification_loss: 0.0478
  949/10000 [=>............................] - ETA: 2:15:00 - loss: 0.4021 - regression_loss: 0.3542 - classification_loss: 0.0478
  950/10000 [=>............................] - ETA: 2:14:59 - loss: 0.4020 - regression_loss: 0.3542 - classification_loss: 0.0478
  951/10000 [=>............................] - ETA: 2:14:58 - loss: 0.4018 - regression_loss: 0.3541 - classification_loss: 0.0478
  952/10000 [=>............................] - ETA: 2:14:57 - loss: 0.4020 - regression_loss: 0.3542 - classification_loss: 0.0478
  953/10000 [=>............................] - ETA: 2:14:56 - loss: 0.4028 - regression_loss: 0.3550 - classification_loss: 0.0478
  954/10000 [=>............................] - ETA: 2:14:55 - loss: 0.4030 - regression_loss: 0.3552 - classification_loss: 0.0478
  955/10000 [=>............................] - ETA: 2:14:54 - loss: 0.4032 - regression_loss: 0.3554 - classification_loss: 0.0478
  956/10000 [=>............................] - ETA: 2:14:53 - loss: 0.4034 - regression_loss: 0.3556 - classification_loss: 0.0478
  957/10000 [=>............................] - ETA: 2:14:52 - loss: 0.4037 - regression_loss: 0.3559 - classification_loss: 0.0478
  958/10000 [=>............................] - ETA: 2:14:51 - loss: 0.4037 - regression_loss: 0.3559 - classification_loss: 0.0478
  959/10000 [=>............................] - ETA: 2:14:50 - loss: 0.4040 - regression_loss: 0.3562 - classification_loss: 0.0478
  960/10000 [=>............................] - ETA: 2:14:50 - loss: 0.4043 - regression_loss: 0.3565 - classification_loss: 0.0478
  961/10000 [=>............................] - ETA: 2:14:49 - loss: 0.4052 - regression_loss: 0.3571 - classification_loss: 0.0480
  962/10000 [=>............................] - ETA: 2:14:48 - loss: 0.4057 - regression_loss: 0.3577 - classification_loss: 0.0481
  963/10000 [=>............................] - ETA: 2:14:47 - loss: 0.4056 - regression_loss: 0.3576 - classification_loss: 0.0480
  964/10000 [=>............................] - ETA: 2:14:46 - loss: 0.4055 - regression_loss: 0.3575 - classification_loss: 0.0480
  965/10000 [=>............................] - ETA: 2:14:45 - loss: 0.4051 - regression_loss: 0.3571 - classification_loss: 0.0479
  966/10000 [=>............................] - ETA: 2:14:44 - loss: 0.4048 - regression_loss: 0.3569 - classification_loss: 0.0479
  967/10000 [=>............................] - ETA: 2:14:43 - loss: 0.4047 - regression_loss: 0.3569 - classification_loss: 0.0479
  968/10000 [=>............................] - ETA: 2:14:42 - loss: 0.4045 - regression_loss: 0.3567 - classification_loss: 0.0478
  969/10000 [=>............................] - ETA: 2:14:41 - loss: 0.4045 - regression_loss: 0.3567 - classification_loss: 0.0478
  970/10000 [=>............................] - ETA: 2:14:40 - loss: 0.4049 - regression_loss: 0.3572 - classification_loss: 0.0478
  971/10000 [=>............................] - ETA: 2:14:39 - loss: 0.4048 - regression_loss: 0.3571 - classification_loss: 0.0477
  972/10000 [=>............................] - ETA: 2:14:38 - loss: 0.4046 - regression_loss: 0.3569 - classification_loss: 0.0477
  973/10000 [=>............................] - ETA: 2:14:37 - loss: 0.4043 - regression_loss: 0.3567 - classification_loss: 0.0477
  974/10000 [=>............................] - ETA: 2:14:36 - loss: 0.4041 - regression_loss: 0.3564 - classification_loss: 0.0476
  975/10000 [=>............................] - ETA: 2:14:35 - loss: 0.4038 - regression_loss: 0.3562 - classification_loss: 0.0476
  976/10000 [=>............................] - ETA: 2:14:34 - loss: 0.4038 - regression_loss: 0.3562 - classification_loss: 0.0476
  977/10000 [=>............................] - ETA: 2:14:34 - loss: 0.4036 - regression_loss: 0.3561 - classification_loss: 0.0476
  978/10000 [=>............................] - ETA: 2:14:33 - loss: 0.4037 - regression_loss: 0.3562 - classification_loss: 0.0475
  979/10000 [=>............................] - ETA: 2:14:32 - loss: 0.4034 - regression_loss: 0.3560 - classification_loss: 0.0475
  980/10000 [=>............................] - ETA: 2:14:31 - loss: 0.4034 - regression_loss: 0.3559 - classification_loss: 0.0475
  981/10000 [=>............................] - ETA: 2:14:30 - loss: 0.4030 - regression_loss: 0.3556 - classification_loss: 0.0474
  982/10000 [=>............................] - ETA: 2:14:29 - loss: 0.4032 - regression_loss: 0.3557 - classification_loss: 0.0474
  983/10000 [=>............................] - ETA: 2:14:28 - loss: 0.4032 - regression_loss: 0.3557 - classification_loss: 0.0474
  984/10000 [=>............................] - ETA: 2:14:27 - loss: 0.4030 - regression_loss: 0.3556 - classification_loss: 0.0474
  985/10000 [=>............................] - ETA: 2:14:26 - loss: 0.4035 - regression_loss: 0.3561 - classification_loss: 0.0474
  986/10000 [=>............................] - ETA: 2:14:25 - loss: 0.4033 - regression_loss: 0.3559 - classification_loss: 0.0474
  987/10000 [=>............................] - ETA: 2:14:24 - loss: 0.4033 - regression_loss: 0.3559 - classification_loss: 0.0474
  988/10000 [=>............................] - ETA: 2:14:23 - loss: 0.4035 - regression_loss: 0.3561 - classification_loss: 0.0474
  989/10000 [=>............................] - ETA: 2:14:22 - loss: 0.4035 - regression_loss: 0.3562 - classification_loss: 0.0474
  990/10000 [=>............................] - ETA: 2:14:21 - loss: 0.4034 - regression_loss: 0.3561 - classification_loss: 0.0473
  991/10000 [=>............................] - ETA: 2:14:20 - loss: 0.4035 - regression_loss: 0.3562 - classification_loss: 0.0473
  992/10000 [=>............................] - ETA: 2:14:19 - loss: 0.4040 - regression_loss: 0.3566 - classification_loss: 0.0473
  993/10000 [=>............................] - ETA: 2:14:18 - loss: 0.4036 - regression_loss: 0.3563 - classification_loss: 0.0473
  994/10000 [=>............................] - ETA: 2:14:17 - loss: 0.4037 - regression_loss: 0.3564 - classification_loss: 0.0473
  995/10000 [=>............................] - ETA: 2:14:16 - loss: 0.4038 - regression_loss: 0.3566 - classification_loss: 0.0473
  996/10000 [=>............................] - ETA: 2:14:15 - loss: 0.4039 - regression_loss: 0.3566 - classification_loss: 0.0473
  997/10000 [=>............................] - ETA: 2:14:14 - loss: 0.4037 - regression_loss: 0.3565 - classification_loss: 0.0472
  998/10000 [=>............................] - ETA: 2:14:13 - loss: 0.4037 - regression_loss: 0.3565 - classification_loss: 0.0472
  999/10000 [=>............................] - ETA: 2:14:12 - loss: 0.4038 - regression_loss: 0.3566 - classification_loss: 0.0472
 1000/10000 [==>...........................] - ETA: 2:14:11 - loss: 0.4039 - regression_loss: 0.3566 - classification_loss: 0.0472
 1001/10000 [==>...........................] - ETA: 2:14:11 - loss: 0.4036 - regression_loss: 0.3564 - classification_loss: 0.0472
 1002/10000 [==>...........................] - ETA: 2:14:10 - loss: 0.4035 - regression_loss: 0.3563 - classification_loss: 0.0472
 1003/10000 [==>...........................] - ETA: 2:14:09 - loss: 0.4034 - regression_loss: 0.3562 - classification_loss: 0.0472
 1004/10000 [==>...........................] - ETA: 2:14:08 - loss: 0.4032 - regression_loss: 0.3561 - classification_loss: 0.0471
 1005/10000 [==>...........................] - ETA: 2:14:07 - loss: 0.4030 - regression_loss: 0.3559 - classification_loss: 0.0471
 1006/10000 [==>...........................] - ETA: 2:14:06 - loss: 0.4030 - regression_loss: 0.3559 - classification_loss: 0.0471
 1007/10000 [==>...........................] - ETA: 2:14:05 - loss: 0.4029 - regression_loss: 0.3559 - classification_loss: 0.0471
 1008/10000 [==>...........................] - ETA: 2:14:04 - loss: 0.4028 - regression_loss: 0.3558 - classification_loss: 0.0470
 1009/10000 [==>...........................] - ETA: 2:14:03 - loss: 0.4032 - regression_loss: 0.3562 - classification_loss: 0.0470
 1010/10000 [==>...........................] - ETA: 2:14:02 - loss: 0.4032 - regression_loss: 0.3562 - classification_loss: 0.0471
 1011/10000 [==>...........................] - ETA: 2:14:01 - loss: 0.4034 - regression_loss: 0.3563 - classification_loss: 0.0470
 1012/10000 [==>...........................] - ETA: 2:14:00 - loss: 0.4033 - regression_loss: 0.3563 - classification_loss: 0.0470
 1013/10000 [==>...........................] - ETA: 2:13:59 - loss: 0.4033 - regression_loss: 0.3563 - classification_loss: 0.0470
 1014/10000 [==>...........................] - ETA: 2:13:58 - loss: 0.4033 - regression_loss: 0.3563 - classification_loss: 0.0470
 1015/10000 [==>...........................] - ETA: 2:13:57 - loss: 0.4031 - regression_loss: 0.3561 - classification_loss: 0.0469
 1016/10000 [==>...........................] - ETA: 2:13:56 - loss: 0.4031 - regression_loss: 0.3562 - classification_loss: 0.0469
 1017/10000 [==>...........................] - ETA: 2:13:55 - loss: 0.4030 - regression_loss: 0.3561 - classification_loss: 0.0469
 1018/10000 [==>...........................] - ETA: 2:13:54 - loss: 0.4030 - regression_loss: 0.3561 - classification_loss: 0.0469
 1019/10000 [==>...........................] - ETA: 2:13:53 - loss: 0.4029 - regression_loss: 0.3560 - classification_loss: 0.0469
 1020/10000 [==>...........................] - ETA: 2:13:52 - loss: 0.4025 - regression_loss: 0.3557 - classification_loss: 0.0468
 1021/10000 [==>...........................] - ETA: 2:13:52 - loss: 0.4030 - regression_loss: 0.3561 - classification_loss: 0.0468
 1022/10000 [==>...........................] - ETA: 2:13:51 - loss: 0.4033 - regression_loss: 0.3565 - classification_loss: 0.0468
 1023/10000 [==>...........................] - ETA: 2:13:50 - loss: 0.4030 - regression_loss: 0.3563 - classification_loss: 0.0467
 1024/10000 [==>...........................] - ETA: 2:13:49 - loss: 0.4033 - regression_loss: 0.3566 - classification_loss: 0.0468
 1025/10000 [==>...........................] - ETA: 2:13:48 - loss: 0.4032 - regression_loss: 0.3564 - classification_loss: 0.0467
 1026/10000 [==>...........................] - ETA: 2:13:47 - loss: 0.4034 - regression_loss: 0.3566 - classification_loss: 0.0468
 1027/10000 [==>...........................] - ETA: 2:13:46 - loss: 0.4030 - regression_loss: 0.3563 - classification_loss: 0.0467
 1028/10000 [==>...........................] - ETA: 2:13:45 - loss: 0.4029 - regression_loss: 0.3562 - classification_loss: 0.0467
 1029/10000 [==>...........................] - ETA: 2:13:44 - loss: 0.4026 - regression_loss: 0.3559 - classification_loss: 0.0467
 1030/10000 [==>...........................] - ETA: 2:13:43 - loss: 0.4029 - regression_loss: 0.3561 - classification_loss: 0.0468
 1031/10000 [==>...........................] - ETA: 2:13:42 - loss: 0.4028 - regression_loss: 0.3560 - classification_loss: 0.0468
 1032/10000 [==>...........................] - ETA: 2:13:41 - loss: 0.4028 - regression_loss: 0.3560 - classification_loss: 0.0468
 1033/10000 [==>...........................] - ETA: 2:13:40 - loss: 0.4029 - regression_loss: 0.3561 - classification_loss: 0.0468
 1034/10000 [==>...........................] - ETA: 2:13:39 - loss: 0.4028 - regression_loss: 0.3560 - classification_loss: 0.0468
 1035/10000 [==>...........................] - ETA: 2:13:38 - loss: 0.4028 - regression_loss: 0.3560 - classification_loss: 0.0468
 1036/10000 [==>...........................] - ETA: 2:13:37 - loss: 0.4027 - regression_loss: 0.3560 - classification_loss: 0.0468
 1037/10000 [==>...........................] - ETA: 2:13:36 - loss: 0.4029 - regression_loss: 0.3562 - classification_loss: 0.0468
 1038/10000 [==>...........................] - ETA: 2:13:35 - loss: 0.4032 - regression_loss: 0.3564 - classification_loss: 0.0468
 1039/10000 [==>...........................] - ETA: 2:13:34 - loss: 0.4032 - regression_loss: 0.3564 - classification_loss: 0.0468
 1040/10000 [==>...........................] - ETA: 2:13:33 - loss: 0.4030 - regression_loss: 0.3562 - classification_loss: 0.0468
 1041/10000 [==>...........................] - ETA: 2:13:32 - loss: 0.4029 - regression_loss: 0.3562 - classification_loss: 0.0467
 1042/10000 [==>...........................] - ETA: 2:13:31 - loss: 0.4028 - regression_loss: 0.3561 - classification_loss: 0.0467
 1043/10000 [==>...........................] - ETA: 2:13:30 - loss: 0.4024 - regression_loss: 0.3557 - classification_loss: 0.0467
 1044/10000 [==>...........................] - ETA: 2:13:29 - loss: 0.4025 - regression_loss: 0.3558 - classification_loss: 0.0467
 1045/10000 [==>...........................] - ETA: 2:13:29 - loss: 0.4025 - regression_loss: 0.3558 - classification_loss: 0.0467
 1046/10000 [==>...........................] - ETA: 2:13:28 - loss: 0.4024 - regression_loss: 0.3557 - classification_loss: 0.0467
 1047/10000 [==>...........................] - ETA: 2:13:27 - loss: 0.4022 - regression_loss: 0.3556 - classification_loss: 0.0467
 1048/10000 [==>...........................] - ETA: 2:13:26 - loss: 0.4019 - regression_loss: 0.3553 - classification_loss: 0.0466
 1049/10000 [==>...........................] - ETA: 2:13:25 - loss: 0.4019 - regression_loss: 0.3553 - classification_loss: 0.0466
 1050/10000 [==>...........................] - ETA: 2:13:24 - loss: 0.4019 - regression_loss: 0.3553 - classification_loss: 0.0466
 1051/10000 [==>...........................] - ETA: 2:13:23 - loss: 0.4018 - regression_loss: 0.3551 - classification_loss: 0.0466
 1052/10000 [==>...........................] - ETA: 2:13:22 - loss: 0.4017 - regression_loss: 0.3551 - classification_loss: 0.0466
 1053/10000 [==>...........................] - ETA: 2:13:21 - loss: 0.4018 - regression_loss: 0.3552 - classification_loss: 0.0466
 1054/10000 [==>...........................] - ETA: 2:13:20 - loss: 0.4017 - regression_loss: 0.3551 - classification_loss: 0.0466
 1055/10000 [==>...........................] - ETA: 2:13:19 - loss: 0.4016 - regression_loss: 0.3550 - classification_loss: 0.0466
 1056/10000 [==>...........................] - ETA: 2:13:18 - loss: 0.4020 - regression_loss: 0.3554 - classification_loss: 0.0466
 1057/10000 [==>...........................] - ETA: 2:13:17 - loss: 0.4020 - regression_loss: 0.3553 - classification_loss: 0.0466
 1058/10000 [==>...........................] - ETA: 2:13:16 - loss: 0.4016 - regression_loss: 0.3550 - classification_loss: 0.0466
 1059/10000 [==>...........................] - ETA: 2:13:15 - loss: 0.4015 - regression_loss: 0.3549 - classification_loss: 0.0466
 1060/10000 [==>...........................] - ETA: 2:13:14 - loss: 0.4014 - regression_loss: 0.3548 - classification_loss: 0.0466
 1061/10000 [==>...........................] - ETA: 2:13:13 - loss: 0.4018 - regression_loss: 0.3552 - classification_loss: 0.0466
 1062/10000 [==>...........................] - ETA: 2:13:12 - loss: 0.4016 - regression_loss: 0.3550 - classification_loss: 0.0466
 1063/10000 [==>...........................] - ETA: 2:13:11 - loss: 0.4016 - regression_loss: 0.3550 - classification_loss: 0.0466
 1064/10000 [==>...........................] - ETA: 2:13:11 - loss: 0.4015 - regression_loss: 0.3550 - classification_loss: 0.0466
 1065/10000 [==>...........................] - ETA: 2:13:10 - loss: 0.4014 - regression_loss: 0.3548 - classification_loss: 0.0465
 1066/10000 [==>...........................] - ETA: 2:13:09 - loss: 0.4016 - regression_loss: 0.3551 - classification_loss: 0.0465
 1067/10000 [==>...........................] - ETA: 2:13:08 - loss: 0.4016 - regression_loss: 0.3552 - classification_loss: 0.0465
 1068/10000 [==>...........................] - ETA: 2:13:07 - loss: 0.4015 - regression_loss: 0.3550 - classification_loss: 0.0465
 1069/10000 [==>...........................] - ETA: 2:13:06 - loss: 0.4014 - regression_loss: 0.3549 - classification_loss: 0.0464
 1070/10000 [==>...........................] - ETA: 2:13:05 - loss: 0.4014 - regression_loss: 0.3550 - classification_loss: 0.0464
 1071/10000 [==>...........................] - ETA: 2:13:04 - loss: 0.4016 - regression_loss: 0.3552 - classification_loss: 0.0464
 1072/10000 [==>...........................] - ETA: 2:13:03 - loss: 0.4014 - regression_loss: 0.3550 - classification_loss: 0.0464
 1073/10000 [==>...........................] - ETA: 2:13:02 - loss: 0.4016 - regression_loss: 0.3552 - classification_loss: 0.0464
 1074/10000 [==>...........................] - ETA: 2:13:01 - loss: 0.4014 - regression_loss: 0.3550 - classification_loss: 0.0464
 1075/10000 [==>...........................] - ETA: 2:13:00 - loss: 0.4012 - regression_loss: 0.3548 - classification_loss: 0.0463
 1076/10000 [==>...........................] - ETA: 2:12:59 - loss: 0.4014 - regression_loss: 0.3551 - classification_loss: 0.0463
 1077/10000 [==>...........................] - ETA: 2:12:58 - loss: 0.4014 - regression_loss: 0.3551 - classification_loss: 0.0463
 1078/10000 [==>...........................] - ETA: 2:12:57 - loss: 0.4013 - regression_loss: 0.3551 - classification_loss: 0.0463
 1079/10000 [==>...........................] - ETA: 2:12:56 - loss: 0.4013 - regression_loss: 0.3551 - classification_loss: 0.0463
 1080/10000 [==>...........................] - ETA: 2:12:56 - loss: 0.4013 - regression_loss: 0.3551 - classification_loss: 0.0462
 1081/10000 [==>...........................] - ETA: 2:12:55 - loss: 0.4013 - regression_loss: 0.3551 - classification_loss: 0.0462
 1082/10000 [==>...........................] - ETA: 2:12:54 - loss: 0.4013 - regression_loss: 0.3551 - classification_loss: 0.0462
 1083/10000 [==>...........................] - ETA: 2:12:53 - loss: 0.4016 - regression_loss: 0.3554 - classification_loss: 0.0462
 1084/10000 [==>...........................] - ETA: 2:12:52 - loss: 0.4013 - regression_loss: 0.3552 - classification_loss: 0.0461
 1085/10000 [==>...........................] - ETA: 2:12:51 - loss: 0.4014 - regression_loss: 0.3553 - classification_loss: 0.0461
 1086/10000 [==>...........................] - ETA: 2:12:50 - loss: 0.4013 - regression_loss: 0.3552 - classification_loss: 0.0461
 1087/10000 [==>...........................] - ETA: 2:12:49 - loss: 0.4011 - regression_loss: 0.3550 - classification_loss: 0.0461
 1088/10000 [==>...........................] - ETA: 2:12:48 - loss: 0.4011 - regression_loss: 0.3550 - classification_loss: 0.0461
 1089/10000 [==>...........................] - ETA: 2:12:47 - loss: 0.4012 - regression_loss: 0.3551 - classification_loss: 0.0461
 1090/10000 [==>...........................] - ETA: 2:12:46 - loss: 0.4014 - regression_loss: 0.3553 - classification_loss: 0.0461
 1091/10000 [==>...........................] - ETA: 2:12:45 - loss: 0.4012 - regression_loss: 0.3551 - classification_loss: 0.0461
 1092/10000 [==>...........................] - ETA: 2:12:44 - loss: 0.4008 - regression_loss: 0.3548 - classification_loss: 0.0460
 1093/10000 [==>...........................] - ETA: 2:12:43 - loss: 0.4007 - regression_loss: 0.3547 - classification_loss: 0.0460
 1094/10000 [==>...........................] - ETA: 2:12:42 - loss: 0.4005 - regression_loss: 0.3545 - classification_loss: 0.0460
 1095/10000 [==>...........................] - ETA: 2:12:41 - loss: 0.4012 - regression_loss: 0.3549 - classification_loss: 0.0463
 1096/10000 [==>...........................] - ETA: 2:12:40 - loss: 0.4015 - regression_loss: 0.3552 - classification_loss: 0.0463
 1097/10000 [==>...........................] - ETA: 2:12:39 - loss: 0.4014 - regression_loss: 0.3551 - classification_loss: 0.0463
 1098/10000 [==>...........................] - ETA: 2:12:38 - loss: 0.4014 - regression_loss: 0.3552 - classification_loss: 0.0463
 1099/10000 [==>...........................] - ETA: 2:12:38 - loss: 0.4014 - regression_loss: 0.3551 - classification_loss: 0.0463
 1100/10000 [==>...........................] - ETA: 2:12:37 - loss: 0.4012 - regression_loss: 0.3550 - classification_loss: 0.0462
 1101/10000 [==>...........................] - ETA: 2:12:36 - loss: 0.4011 - regression_loss: 0.3549 - classification_loss: 0.0462
 1102/10000 [==>...........................] - ETA: 2:12:35 - loss: 0.4011 - regression_loss: 0.3549 - classification_loss: 0.0462
 1103/10000 [==>...........................] - ETA: 2:12:34 - loss: 0.4008 - regression_loss: 0.3546 - classification_loss: 0.0462
 1104/10000 [==>...........................] - ETA: 2:12:33 - loss: 0.4006 - regression_loss: 0.3544 - classification_loss: 0.0461
 1105/10000 [==>...........................] - ETA: 2:12:32 - loss: 0.4008 - regression_loss: 0.3547 - classification_loss: 0.0461
 1106/10000 [==>...........................] - ETA: 2:12:31 - loss: 0.4005 - regression_loss: 0.3544 - classification_loss: 0.0461
 1107/10000 [==>...........................] - ETA: 2:12:30 - loss: 0.4008 - regression_loss: 0.3546 - classification_loss: 0.0462
 1108/10000 [==>...........................] - ETA: 2:12:29 - loss: 0.4008 - regression_loss: 0.3546 - classification_loss: 0.0462
 1109/10000 [==>...........................] - ETA: 2:12:28 - loss: 0.4008 - regression_loss: 0.3547 - classification_loss: 0.0462
 1110/10000 [==>...........................] - ETA: 2:12:27 - loss: 0.4009 - regression_loss: 0.3547 - classification_loss: 0.0461
 1111/10000 [==>...........................] - ETA: 2:12:26 - loss: 0.4011 - regression_loss: 0.3549 - classification_loss: 0.0462
 1112/10000 [==>...........................] - ETA: 2:12:25 - loss: 0.4013 - regression_loss: 0.3551 - classification_loss: 0.0462
 1113/10000 [==>...........................] - ETA: 2:12:24 - loss: 0.4013 - regression_loss: 0.3551 - classification_loss: 0.0462
 1114/10000 [==>...........................] - ETA: 2:12:23 - loss: 0.4013 - regression_loss: 0.3551 - classification_loss: 0.0462
 1115/10000 [==>...........................] - ETA: 2:12:22 - loss: 0.4015 - regression_loss: 0.3553 - classification_loss: 0.0462
 1116/10000 [==>...........................] - ETA: 2:12:21 - loss: 0.4018 - regression_loss: 0.3556 - classification_loss: 0.0462
 1117/10000 [==>...........................] - ETA: 2:12:20 - loss: 0.4016 - regression_loss: 0.3554 - classification_loss: 0.0462
 1118/10000 [==>...........................] - ETA: 2:12:20 - loss: 0.4019 - regression_loss: 0.3557 - classification_loss: 0.0462
 1119/10000 [==>...........................] - ETA: 2:12:19 - loss: 0.4020 - regression_loss: 0.3558 - classification_loss: 0.0462
 1120/10000 [==>...........................] - ETA: 2:12:18 - loss: 0.4018 - regression_loss: 0.3556 - classification_loss: 0.0462
 1121/10000 [==>...........................] - ETA: 2:12:17 - loss: 0.4019 - regression_loss: 0.3557 - classification_loss: 0.0462
 1122/10000 [==>...........................] - ETA: 2:12:16 - loss: 0.4019 - regression_loss: 0.3557 - classification_loss: 0.0462
 1123/10000 [==>...........................] - ETA: 2:12:15 - loss: 0.4017 - regression_loss: 0.3556 - classification_loss: 0.0461
 1124/10000 [==>...........................] - ETA: 2:12:14 - loss: 0.4016 - regression_loss: 0.3555 - classification_loss: 0.0461
 1125/10000 [==>...........................] - ETA: 2:12:13 - loss: 0.4017 - regression_loss: 0.3556 - classification_loss: 0.0461
 1126/10000 [==>...........................] - ETA: 2:12:12 - loss: 0.4028 - regression_loss: 0.3565 - classification_loss: 0.0462
 1127/10000 [==>...........................] - ETA: 2:12:11 - loss: 0.4028 - regression_loss: 0.3566 - classification_loss: 0.0462
 1128/10000 [==>...........................] - ETA: 2:12:10 - loss: 0.4038 - regression_loss: 0.3576 - classification_loss: 0.0462
 1129/10000 [==>...........................] - ETA: 2:12:09 - loss: 0.4038 - regression_loss: 0.3576 - classification_loss: 0.0462
 1130/10000 [==>...........................] - ETA: 2:12:08 - loss: 0.4036 - regression_loss: 0.3574 - classification_loss: 0.0462
 1131/10000 [==>...........................] - ETA: 2:12:07 - loss: 0.4037 - regression_loss: 0.3575 - classification_loss: 0.0462
 1132/10000 [==>...........................] - ETA: 2:12:06 - loss: 0.4041 - regression_loss: 0.3579 - classification_loss: 0.0462
 1133/10000 [==>...........................] - ETA: 2:12:05 - loss: 0.4041 - regression_loss: 0.3579 - classification_loss: 0.0461
 1134/10000 [==>...........................] - ETA: 2:12:04 - loss: 0.4040 - regression_loss: 0.3579 - classification_loss: 0.0461
 1135/10000 [==>...........................] - ETA: 2:12:03 - loss: 0.4039 - regression_loss: 0.3578 - classification_loss: 0.0461
 1136/10000 [==>...........................] - ETA: 2:12:02 - loss: 0.4041 - regression_loss: 0.3580 - classification_loss: 0.0461
 1137/10000 [==>...........................] - ETA: 2:12:02 - loss: 0.4040 - regression_loss: 0.3578 - classification_loss: 0.0461
 1138/10000 [==>...........................] - ETA: 2:12:01 - loss: 0.4038 - regression_loss: 0.3577 - classification_loss: 0.0461
 1139/10000 [==>...........................] - ETA: 2:12:00 - loss: 0.4037 - regression_loss: 0.3576 - classification_loss: 0.0461
 1140/10000 [==>...........................] - ETA: 2:11:59 - loss: 0.4035 - regression_loss: 0.3574 - classification_loss: 0.0461
 1141/10000 [==>...........................] - ETA: 2:11:58 - loss: 0.4033 - regression_loss: 0.3572 - classification_loss: 0.0461
 1142/10000 [==>...........................] - ETA: 2:11:57 - loss: 0.4032 - regression_loss: 0.3572 - classification_loss: 0.0461
 1143/10000 [==>...........................] - ETA: 2:11:56 - loss: 0.4030 - regression_loss: 0.3570 - classification_loss: 0.0460
 1144/10000 [==>...........................] - ETA: 2:11:55 - loss: 0.4032 - regression_loss: 0.3571 - classification_loss: 0.0460
 1145/10000 [==>...........................] - ETA: 2:11:54 - loss: 0.4032 - regression_loss: 0.3571 - classification_loss: 0.0461
 1146/10000 [==>...........................] - ETA: 2:11:53 - loss: 0.4032 - regression_loss: 0.3571 - classification_loss: 0.0461
 1147/10000 [==>...........................] - ETA: 2:11:52 - loss: 0.4031 - regression_loss: 0.3570 - classification_loss: 0.0461
 1148/10000 [==>...........................] - ETA: 2:11:51 - loss: 0.4029 - regression_loss: 0.3569 - classification_loss: 0.0461
 1149/10000 [==>...........................] - ETA: 2:11:50 - loss: 0.4029 - regression_loss: 0.3569 - classification_loss: 0.0460
 1150/10000 [==>...........................] - ETA: 2:11:49 - loss: 0.4028 - regression_loss: 0.3568 - classification_loss: 0.0460
 1151/10000 [==>...........................] - ETA: 2:11:49 - loss: 0.4028 - regression_loss: 0.3568 - classification_loss: 0.0460
 1152/10000 [==>...........................] - ETA: 2:11:48 - loss: 0.4028 - regression_loss: 0.3568 - classification_loss: 0.0460
 1153/10000 [==>...........................] - ETA: 2:11:47 - loss: 0.4027 - regression_loss: 0.3567 - classification_loss: 0.0460
 1154/10000 [==>...........................] - ETA: 2:11:46 - loss: 0.4026 - regression_loss: 0.3566 - classification_loss: 0.0460
 1155/10000 [==>...........................] - ETA: 2:11:45 - loss: 0.4024 - regression_loss: 0.3564 - classification_loss: 0.0460
 1156/10000 [==>...........................] - ETA: 2:11:44 - loss: 0.4022 - regression_loss: 0.3563 - classification_loss: 0.0460
 1157/10000 [==>...........................] - ETA: 2:11:43 - loss: 0.4022 - regression_loss: 0.3563 - classification_loss: 0.0459
 1158/10000 [==>...........................] - ETA: 2:11:42 - loss: 0.4020 - regression_loss: 0.3561 - classification_loss: 0.0459
 1159/10000 [==>...........................] - ETA: 2:11:41 - loss: 0.4021 - regression_loss: 0.3562 - classification_loss: 0.0459
 1160/10000 [==>...........................] - ETA: 2:11:40 - loss: 0.4020 - regression_loss: 0.3561 - classification_loss: 0.0459
 1161/10000 [==>...........................] - ETA: 2:11:39 - loss: 0.4018 - regression_loss: 0.3560 - classification_loss: 0.0458
 1162/10000 [==>...........................] - ETA: 2:11:38 - loss: 0.4019 - regression_loss: 0.3561 - classification_loss: 0.0458
 1163/10000 [==>...........................] - ETA: 2:11:37 - loss: 0.4018 - regression_loss: 0.3560 - classification_loss: 0.0458
 1164/10000 [==>...........................] - ETA: 2:11:36 - loss: 0.4017 - regression_loss: 0.3559 - classification_loss: 0.0458
 1165/10000 [==>...........................] - ETA: 2:11:35 - loss: 0.4016 - regression_loss: 0.3558 - classification_loss: 0.0457
 1166/10000 [==>...........................] - ETA: 2:11:34 - loss: 0.4015 - regression_loss: 0.3558 - classification_loss: 0.0457
 1167/10000 [==>...........................] - ETA: 2:11:34 - loss: 0.4015 - regression_loss: 0.3558 - classification_loss: 0.0457
 1168/10000 [==>...........................] - ETA: 2:11:33 - loss: 0.4014 - regression_loss: 0.3557 - classification_loss: 0.0457
 1169/10000 [==>...........................] - ETA: 2:11:32 - loss: 0.4011 - regression_loss: 0.3554 - classification_loss: 0.0456
 1170/10000 [==>...........................] - ETA: 2:11:31 - loss: 0.4010 - regression_loss: 0.3554 - classification_loss: 0.0456
 1171/10000 [==>...........................] - ETA: 2:11:30 - loss: 0.4010 - regression_loss: 0.3554 - classification_loss: 0.0456
 1172/10000 [==>...........................] - ETA: 2:11:29 - loss: 0.4010 - regression_loss: 0.3555 - classification_loss: 0.0455
 1173/10000 [==>...........................] - ETA: 2:11:28 - loss: 0.4011 - regression_loss: 0.3556 - classification_loss: 0.0456
 1174/10000 [==>...........................] - ETA: 2:11:27 - loss: 0.4011 - regression_loss: 0.3555 - classification_loss: 0.0456
 1175/10000 [==>...........................] - ETA: 2:11:26 - loss: 0.4010 - regression_loss: 0.3554 - classification_loss: 0.0456
 1176/10000 [==>...........................] - ETA: 2:11:25 - loss: 0.4011 - regression_loss: 0.3555 - classification_loss: 0.0456
 1177/10000 [==>...........................] - ETA: 2:11:24 - loss: 0.4014 - regression_loss: 0.3557 - classification_loss: 0.0457
 1178/10000 [==>...........................] - ETA: 2:11:23 - loss: 0.4014 - regression_loss: 0.3558 - classification_loss: 0.0456
 1179/10000 [==>...........................] - ETA: 2:11:22 - loss: 0.4014 - regression_loss: 0.3558 - classification_loss: 0.0456
 1180/10000 [==>...........................] - ETA: 2:11:21 - loss: 0.4013 - regression_loss: 0.3557 - classification_loss: 0.0456
 1181/10000 [==>...........................] - ETA: 2:11:20 - loss: 0.4012 - regression_loss: 0.3556 - classification_loss: 0.0456
 1182/10000 [==>...........................] - ETA: 2:11:19 - loss: 0.4011 - regression_loss: 0.3556 - classification_loss: 0.0456
 1183/10000 [==>...........................] - ETA: 2:11:19 - loss: 0.4009 - regression_loss: 0.3554 - classification_loss: 0.0455
 1184/10000 [==>...........................] - ETA: 2:11:18 - loss: 0.4006 - regression_loss: 0.3551 - classification_loss: 0.0455
 1185/10000 [==>...........................] - ETA: 2:11:17 - loss: 0.4007 - regression_loss: 0.3552 - classification_loss: 0.0455
 1186/10000 [==>...........................] - ETA: 2:11:16 - loss: 0.4005 - regression_loss: 0.3551 - classification_loss: 0.0455
 1187/10000 [==>...........................] - ETA: 2:11:15 - loss: 0.4003 - regression_loss: 0.3549 - classification_loss: 0.0454
 1188/10000 [==>...........................] - ETA: 2:11:14 - loss: 0.4002 - regression_loss: 0.3548 - classification_loss: 0.0454
 1189/10000 [==>...........................] - ETA: 2:11:13 - loss: 0.4002 - regression_loss: 0.3547 - classification_loss: 0.0454
 1190/10000 [==>...........................] - ETA: 2:11:12 - loss: 0.4000 - regression_loss: 0.3546 - classification_loss: 0.0454
 1191/10000 [==>...........................] - ETA: 2:11:11 - loss: 0.3999 - regression_loss: 0.3546 - classification_loss: 0.0454
 1192/10000 [==>...........................] - ETA: 2:11:10 - loss: 0.3996 - regression_loss: 0.3543 - classification_loss: 0.0453
 1193/10000 [==>...........................] - ETA: 2:11:09 - loss: 0.3993 - regression_loss: 0.3540 - classification_loss: 0.0453
 1194/10000 [==>...........................] - ETA: 2:11:08 - loss: 0.3991 - regression_loss: 0.3539 - classification_loss: 0.0453
 1195/10000 [==>...........................] - ETA: 2:11:07 - loss: 0.3990 - regression_loss: 0.3537 - classification_loss: 0.0453
 1196/10000 [==>...........................] - ETA: 2:11:06 - loss: 0.3990 - regression_loss: 0.3538 - classification_loss: 0.0453
 1197/10000 [==>...........................] - ETA: 2:11:05 - loss: 0.3988 - regression_loss: 0.3536 - classification_loss: 0.0452
 1198/10000 [==>...........................] - ETA: 2:11:04 - loss: 0.3987 - regression_loss: 0.3535 - classification_loss: 0.0452
 1199/10000 [==>...........................] - ETA: 2:11:03 - loss: 0.3987 - regression_loss: 0.3535 - classification_loss: 0.0452
 1200/10000 [==>...........................] - ETA: 2:11:03 - loss: 0.3987 - regression_loss: 0.3535 - classification_loss: 0.0452
 1201/10000 [==>...........................] - ETA: 2:11:02 - loss: 0.3987 - regression_loss: 0.3536 - classification_loss: 0.0452
 1202/10000 [==>...........................] - ETA: 2:11:01 - loss: 0.3986 - regression_loss: 0.3535 - classification_loss: 0.0451
 1203/10000 [==>...........................] - ETA: 2:11:00 - loss: 0.3987 - regression_loss: 0.3535 - classification_loss: 0.0451
 1204/10000 [==>...........................] - ETA: 2:10:59 - loss: 0.3984 - regression_loss: 0.3533 - classification_loss: 0.0451
 1205/10000 [==>...........................] - ETA: 2:10:58 - loss: 0.3986 - regression_loss: 0.3535 - classification_loss: 0.0451
 1206/10000 [==>...........................] - ETA: 2:10:57 - loss: 0.3986 - regression_loss: 0.3535 - classification_loss: 0.0451
 1207/10000 [==>...........................] - ETA: 2:10:56 - loss: 0.3986 - regression_loss: 0.3535 - classification_loss: 0.0451
 1208/10000 [==>...........................] - ETA: 2:10:55 - loss: 0.3987 - regression_loss: 0.3535 - classification_loss: 0.0451
 1209/10000 [==>...........................] - ETA: 2:10:54 - loss: 0.3986 - regression_loss: 0.3535 - classification_loss: 0.0451
 1210/10000 [==>...........................] - ETA: 2:10:53 - loss: 0.3984 - regression_loss: 0.3533 - classification_loss: 0.0451
 1211/10000 [==>...........................] - ETA: 2:10:52 - loss: 0.3984 - regression_loss: 0.3533 - classification_loss: 0.0451
 1212/10000 [==>...........................] - ETA: 2:10:51 - loss: 0.3980 - regression_loss: 0.3530 - classification_loss: 0.0451
 1213/10000 [==>...........................] - ETA: 2:10:51 - loss: 0.3979 - regression_loss: 0.3529 - classification_loss: 0.0450
 1214/10000 [==>...........................] - ETA: 2:10:50 - loss: 0.3978 - regression_loss: 0.3528 - classification_loss: 0.0450
 1215/10000 [==>...........................] - ETA: 2:10:49 - loss: 0.3977 - regression_loss: 0.3527 - classification_loss: 0.0450
 1216/10000 [==>...........................] - ETA: 2:10:48 - loss: 0.3977 - regression_loss: 0.3527 - classification_loss: 0.0449
 1217/10000 [==>...........................] - ETA: 2:10:47 - loss: 0.3978 - regression_loss: 0.3528 - classification_loss: 0.0450
 1218/10000 [==>...........................] - ETA: 2:10:46 - loss: 0.3976 - regression_loss: 0.3527 - classification_loss: 0.0449
 1219/10000 [==>...........................] - ETA: 2:10:45 - loss: 0.3980 - regression_loss: 0.3531 - classification_loss: 0.0449
 1220/10000 [==>...........................] - ETA: 2:10:44 - loss: 0.3980 - regression_loss: 0.3530 - classification_loss: 0.0449
 1221/10000 [==>...........................] - ETA: 2:10:43 - loss: 0.3976 - regression_loss: 0.3528 - classification_loss: 0.0449
 1222/10000 [==>...........................] - ETA: 2:10:42 - loss: 0.3978 - regression_loss: 0.3529 - classification_loss: 0.0449
 1223/10000 [==>...........................] - ETA: 2:10:41 - loss: 0.3985 - regression_loss: 0.3535 - classification_loss: 0.0450
 1224/10000 [==>...........................] - ETA: 2:10:40 - loss: 0.3982 - regression_loss: 0.3533 - classification_loss: 0.0449
 1225/10000 [==>...........................] - ETA: 2:10:39 - loss: 0.3984 - regression_loss: 0.3535 - classification_loss: 0.0449
 1226/10000 [==>...........................] - ETA: 2:10:38 - loss: 0.3992 - regression_loss: 0.3542 - classification_loss: 0.0450
 1227/10000 [==>...........................] - ETA: 2:10:38 - loss: 0.3990 - regression_loss: 0.3541 - classification_loss: 0.0449
 1228/10000 [==>...........................] - ETA: 2:10:37 - loss: 0.3989 - regression_loss: 0.3540 - classification_loss: 0.0449
 1229/10000 [==>...........................] - ETA: 2:10:36 - loss: 0.3991 - regression_loss: 0.3541 - classification_loss: 0.0450
 1230/10000 [==>...........................] - ETA: 2:10:35 - loss: 0.3999 - regression_loss: 0.3544 - classification_loss: 0.0455
 1231/10000 [==>...........................] - ETA: 2:10:34 - loss: 0.3998 - regression_loss: 0.3544 - classification_loss: 0.0455
 1232/10000 [==>...........................] - ETA: 2:10:33 - loss: 0.3998 - regression_loss: 0.3544 - classification_loss: 0.0454
 1233/10000 [==>...........................] - ETA: 2:10:32 - loss: 0.3999 - regression_loss: 0.3545 - classification_loss: 0.0454
 1234/10000 [==>...........................] - ETA: 2:10:31 - loss: 0.4001 - regression_loss: 0.3546 - classification_loss: 0.0456
 1235/10000 [==>...........................] - ETA: 2:10:30 - loss: 0.4002 - regression_loss: 0.3546 - classification_loss: 0.0456
 1236/10000 [==>...........................] - ETA: 2:10:29 - loss: 0.4002 - regression_loss: 0.3546 - classification_loss: 0.0456
 1237/10000 [==>...........................] - ETA: 2:10:28 - loss: 0.3998 - regression_loss: 0.3543 - classification_loss: 0.0455
 1238/10000 [==>...........................] - ETA: 2:10:27 - loss: 0.3997 - regression_loss: 0.3542 - classification_loss: 0.0455
 1239/10000 [==>...........................] - ETA: 2:10:26 - loss: 0.3997 - regression_loss: 0.3542 - classification_loss: 0.0455
 1240/10000 [==>...........................] - ETA: 2:10:26 - loss: 0.3996 - regression_loss: 0.3541 - classification_loss: 0.0455
 1241/10000 [==>...........................] - ETA: 2:10:25 - loss: 0.3993 - regression_loss: 0.3539 - classification_loss: 0.0454
 1242/10000 [==>...........................] - ETA: 2:10:24 - loss: 0.3994 - regression_loss: 0.3540 - classification_loss: 0.0454
 1243/10000 [==>...........................] - ETA: 2:10:23 - loss: 0.3996 - regression_loss: 0.3541 - classification_loss: 0.0454
 1244/10000 [==>...........................] - ETA: 2:10:22 - loss: 0.3997 - regression_loss: 0.3542 - classification_loss: 0.0454
 1245/10000 [==>...........................] - ETA: 2:10:21 - loss: 0.3995 - regression_loss: 0.3541 - classification_loss: 0.0454
 1246/10000 [==>...........................] - ETA: 2:10:20 - loss: 0.3998 - regression_loss: 0.3544 - classification_loss: 0.0454
 1247/10000 [==>...........................] - ETA: 2:10:19 - loss: 0.3996 - regression_loss: 0.3543 - classification_loss: 0.0454
 1248/10000 [==>...........................] - ETA: 2:10:18 - loss: 0.3997 - regression_loss: 0.3543 - classification_loss: 0.0453
 1249/10000 [==>...........................] - ETA: 2:10:17 - loss: 0.3995 - regression_loss: 0.3542 - classification_loss: 0.0453
 1250/10000 [==>...........................] - ETA: 2:10:16 - loss: 0.3995 - regression_loss: 0.3542 - classification_loss: 0.0453
 1251/10000 [==>...........................] - ETA: 2:10:15 - loss: 0.3995 - regression_loss: 0.3542 - classification_loss: 0.0453
 1252/10000 [==>...........................] - ETA: 2:10:14 - loss: 0.3995 - regression_loss: 0.3542 - classification_loss: 0.0453
 1253/10000 [==>...........................] - ETA: 2:10:13 - loss: 0.3995 - regression_loss: 0.3543 - classification_loss: 0.0452
 1254/10000 [==>...........................] - ETA: 2:10:12 - loss: 0.3995 - regression_loss: 0.3542 - classification_loss: 0.0453
 1255/10000 [==>...........................] - ETA: 2:10:11 - loss: 0.3993 - regression_loss: 0.3541 - classification_loss: 0.0452
 1256/10000 [==>...........................] - ETA: 2:10:11 - loss: 0.3993 - regression_loss: 0.3541 - classification_loss: 0.0453
 1257/10000 [==>...........................] - ETA: 2:10:10 - loss: 0.3992 - regression_loss: 0.3540 - classification_loss: 0.0452
 1258/10000 [==>...........................] - ETA: 2:10:09 - loss: 0.3989 - regression_loss: 0.3537 - classification_loss: 0.0452
 1259/10000 [==>...........................] - ETA: 2:10:08 - loss: 0.3993 - regression_loss: 0.3540 - classification_loss: 0.0453
 1260/10000 [==>...........................] - ETA: 2:10:07 - loss: 0.3993 - regression_loss: 0.3540 - classification_loss: 0.0453
 1261/10000 [==>...........................] - ETA: 2:10:06 - loss: 0.3997 - regression_loss: 0.3544 - classification_loss: 0.0453
 1262/10000 [==>...........................] - ETA: 2:10:05 - loss: 0.3995 - regression_loss: 0.3543 - classification_loss: 0.0453
 1263/10000 [==>...........................] - ETA: 2:10:04 - loss: 0.3994 - regression_loss: 0.3541 - classification_loss: 0.0453
 1264/10000 [==>...........................] - ETA: 2:10:03 - loss: 0.3994 - regression_loss: 0.3542 - classification_loss: 0.0453
 1265/10000 [==>...........................] - ETA: 2:10:02 - loss: 0.3994 - regression_loss: 0.3542 - classification_loss: 0.0452
 1266/10000 [==>...........................] - ETA: 2:10:01 - loss: 0.3996 - regression_loss: 0.3544 - classification_loss: 0.0452
 1267/10000 [==>...........................] - ETA: 2:10:00 - loss: 0.3996 - regression_loss: 0.3544 - classification_loss: 0.0452
 1268/10000 [==>...........................] - ETA: 2:09:59 - loss: 0.3993 - regression_loss: 0.3541 - classification_loss: 0.0452
 1269/10000 [==>...........................] - ETA: 2:09:58 - loss: 0.3993 - regression_loss: 0.3541 - classification_loss: 0.0452
 1270/10000 [==>...........................] - ETA: 2:09:58 - loss: 0.3994 - regression_loss: 0.3542 - classification_loss: 0.0452
 1271/10000 [==>...........................] - ETA: 2:09:57 - loss: 0.3992 - regression_loss: 0.3540 - classification_loss: 0.0452
 1272/10000 [==>...........................] - ETA: 2:09:56 - loss: 0.3990 - regression_loss: 0.3538 - classification_loss: 0.0451
 1273/10000 [==>...........................] - ETA: 2:09:55 - loss: 0.3989 - regression_loss: 0.3538 - classification_loss: 0.0451
 1274/10000 [==>...........................] - ETA: 2:09:54 - loss: 0.3988 - regression_loss: 0.3537 - classification_loss: 0.0452
 1275/10000 [==>...........................] - ETA: 2:09:53 - loss: 0.3990 - regression_loss: 0.3537 - classification_loss: 0.0453
 1276/10000 [==>...........................] - ETA: 2:09:52 - loss: 0.3987 - regression_loss: 0.3535 - classification_loss: 0.0452
 1277/10000 [==>...........................] - ETA: 2:09:51 - loss: 0.3988 - regression_loss: 0.3535 - classification_loss: 0.0453
 1278/10000 [==>...........................] - ETA: 2:09:50 - loss: 0.3991 - regression_loss: 0.3537 - classification_loss: 0.0453
 1279/10000 [==>...........................] - ETA: 2:09:49 - loss: 0.3989 - regression_loss: 0.3536 - classification_loss: 0.0453
 1280/10000 [==>...........................] - ETA: 2:09:48 - loss: 0.3988 - regression_loss: 0.3536 - classification_loss: 0.0453
 1281/10000 [==>...........................] - ETA: 2:09:47 - loss: 0.3988 - regression_loss: 0.3535 - classification_loss: 0.0453
 1282/10000 [==>...........................] - ETA: 2:09:46 - loss: 0.3989 - regression_loss: 0.3536 - classification_loss: 0.0453
 1283/10000 [==>...........................] - ETA: 2:09:45 - loss: 0.3987 - regression_loss: 0.3534 - classification_loss: 0.0453
 1284/10000 [==>...........................] - ETA: 2:09:45 - loss: 0.3985 - regression_loss: 0.3532 - classification_loss: 0.0453
 1285/10000 [==>...........................] - ETA: 2:09:44 - loss: 0.3984 - regression_loss: 0.3532 - classification_loss: 0.0452
 1286/10000 [==>...........................] - ETA: 2:09:43 - loss: 0.3995 - regression_loss: 0.3542 - classification_loss: 0.0453
 1287/10000 [==>...........................] - ETA: 2:09:42 - loss: 0.3996 - regression_loss: 0.3543 - classification_loss: 0.0453
 1288/10000 [==>...........................] - ETA: 2:09:41 - loss: 0.3996 - regression_loss: 0.3543 - classification_loss: 0.0453
 1289/10000 [==>...........................] - ETA: 2:09:40 - loss: 0.3996 - regression_loss: 0.3544 - classification_loss: 0.0453
 1290/10000 [==>...........................] - ETA: 2:09:39 - loss: 0.3996 - regression_loss: 0.3543 - classification_loss: 0.0453
 1291/10000 [==>...........................] - ETA: 2:09:38 - loss: 0.3993 - regression_loss: 0.3541 - classification_loss: 0.0453
 1292/10000 [==>...........................] - ETA: 2:09:37 - loss: 0.3993 - regression_loss: 0.3541 - classification_loss: 0.0452
 1293/10000 [==>...........................] - ETA: 2:09:36 - loss: 0.3991 - regression_loss: 0.3539 - classification_loss: 0.0452
 1294/10000 [==>...........................] - ETA: 2:09:35 - loss: 0.3993 - regression_loss: 0.3540 - classification_loss: 0.0453
 1295/10000 [==>...........................] - ETA: 2:09:34 - loss: 0.3996 - regression_loss: 0.3543 - classification_loss: 0.0453
 1296/10000 [==>...........................] - ETA: 2:09:33 - loss: 0.3995 - regression_loss: 0.3543 - classification_loss: 0.0453
 1297/10000 [==>...........................] - ETA: 2:09:32 - loss: 0.3994 - regression_loss: 0.3541 - classification_loss: 0.0453
 1298/10000 [==>...........................] - ETA: 2:09:32 - loss: 0.3992 - regression_loss: 0.3540 - classification_loss: 0.0452
 1299/10000 [==>...........................] - ETA: 2:09:31 - loss: 0.3994 - regression_loss: 0.3542 - classification_loss: 0.0452
 1300/10000 [==>...........................] - ETA: 2:09:30 - loss: 0.3993 - regression_loss: 0.3541 - classification_loss: 0.0452
 1301/10000 [==>...........................] - ETA: 2:09:29 - loss: 0.3991 - regression_loss: 0.3539 - classification_loss: 0.0452
 1302/10000 [==>...........................] - ETA: 2:09:28 - loss: 0.3989 - regression_loss: 0.3537 - classification_loss: 0.0452
 1303/10000 [==>...........................] - ETA: 2:09:27 - loss: 0.3989 - regression_loss: 0.3538 - classification_loss: 0.0451
 1304/10000 [==>...........................] - ETA: 2:09:26 - loss: 0.3987 - regression_loss: 0.3536 - classification_loss: 0.0451
 1305/10000 [==>...........................] - ETA: 2:09:25 - loss: 0.3986 - regression_loss: 0.3535 - classification_loss: 0.0451
 1306/10000 [==>...........................] - ETA: 2:09:24 - loss: 0.3984 - regression_loss: 0.3533 - classification_loss: 0.0451
 1307/10000 [==>...........................] - ETA: 2:09:23 - loss: 0.3983 - regression_loss: 0.3532 - classification_loss: 0.0451
 1308/10000 [==>...........................] - ETA: 2:09:22 - loss: 0.3984 - regression_loss: 0.3533 - classification_loss: 0.0451
 1309/10000 [==>...........................] - ETA: 2:09:21 - loss: 0.3986 - regression_loss: 0.3535 - classification_loss: 0.0451
 1310/10000 [==>...........................] - ETA: 2:09:21 - loss: 0.3988 - regression_loss: 0.3537 - classification_loss: 0.0451
 1311/10000 [==>...........................] - ETA: 2:09:20 - loss: 0.3986 - regression_loss: 0.3535 - classification_loss: 0.0451
 1312/10000 [==>...........................] - ETA: 2:09:19 - loss: 0.3990 - regression_loss: 0.3539 - classification_loss: 0.0451
 1313/10000 [==>...........................] - ETA: 2:09:18 - loss: 0.3989 - regression_loss: 0.3538 - classification_loss: 0.0451
 1314/10000 [==>...........................] - ETA: 2:09:17 - loss: 0.3986 - regression_loss: 0.3535 - classification_loss: 0.0451
 1315/10000 [==>...........................] - ETA: 2:09:16 - loss: 0.3987 - regression_loss: 0.3536 - classification_loss: 0.0451
 1316/10000 [==>...........................] - ETA: 2:09:15 - loss: 0.3991 - regression_loss: 0.3540 - classification_loss: 0.0451
 1317/10000 [==>...........................] - ETA: 2:09:14 - loss: 0.3990 - regression_loss: 0.3540 - classification_loss: 0.0451
 1318/10000 [==>...........................] - ETA: 2:09:13 - loss: 0.3989 - regression_loss: 0.3538 - classification_loss: 0.0451
 1319/10000 [==>...........................] - ETA: 2:09:12 - loss: 0.3994 - regression_loss: 0.3542 - classification_loss: 0.0452
 1320/10000 [==>...........................] - ETA: 2:09:11 - loss: 0.3993 - regression_loss: 0.3542 - classification_loss: 0.0452
 1321/10000 [==>...........................] - ETA: 2:09:10 - loss: 0.3994 - regression_loss: 0.3543 - classification_loss: 0.0451
 1322/10000 [==>...........................] - ETA: 2:09:09 - loss: 0.3993 - regression_loss: 0.3541 - classification_loss: 0.0452
 1323/10000 [==>...........................] - ETA: 2:09:09 - loss: 0.3990 - regression_loss: 0.3539 - classification_loss: 0.0451
 1324/10000 [==>...........................] - ETA: 2:09:08 - loss: 0.3988 - regression_loss: 0.3537 - classification_loss: 0.0451
 1325/10000 [==>...........................] - ETA: 2:09:07 - loss: 0.3985 - regression_loss: 0.3535 - classification_loss: 0.0451
 1326/10000 [==>...........................] - ETA: 2:09:06 - loss: 0.3986 - regression_loss: 0.3535 - classification_loss: 0.0450
 1327/10000 [==>...........................] - ETA: 2:09:05 - loss: 0.3989 - regression_loss: 0.3538 - classification_loss: 0.0450
 1328/10000 [==>...........................] - ETA: 2:09:04 - loss: 0.3991 - regression_loss: 0.3540 - classification_loss: 0.0451
 1329/10000 [==>...........................] - ETA: 2:09:03 - loss: 0.3994 - regression_loss: 0.3543 - classification_loss: 0.0451
 1330/10000 [==>...........................] - ETA: 2:09:02 - loss: 0.3993 - regression_loss: 0.3542 - classification_loss: 0.0451
 1331/10000 [==>...........................] - ETA: 2:09:01 - loss: 0.3992 - regression_loss: 0.3542 - classification_loss: 0.0451
 1332/10000 [==>...........................] - ETA: 2:09:00 - loss: 0.3992 - regression_loss: 0.3541 - classification_loss: 0.0451
 1333/10000 [==>...........................] - ETA: 2:08:59 - loss: 0.3991 - regression_loss: 0.3541 - classification_loss: 0.0450
 1334/10000 [===>..........................] - ETA: 2:08:58 - loss: 0.3990 - regression_loss: 0.3540 - classification_loss: 0.0450
 1335/10000 [===>..........................] - ETA: 2:08:58 - loss: 0.3992 - regression_loss: 0.3542 - classification_loss: 0.0450
 1336/10000 [===>..........................] - ETA: 2:08:57 - loss: 0.3991 - regression_loss: 0.3541 - classification_loss: 0.0450
 1337/10000 [===>..........................] - ETA: 2:08:56 - loss: 0.3991 - regression_loss: 0.3541 - classification_loss: 0.0449
 1338/10000 [===>..........................] - ETA: 2:08:55 - loss: 0.3989 - regression_loss: 0.3540 - classification_loss: 0.0449
 1339/10000 [===>..........................] - ETA: 2:08:54 - loss: 0.3989 - regression_loss: 0.3540 - classification_loss: 0.0449
 1340/10000 [===>..........................] - ETA: 2:08:53 - loss: 0.3987 - regression_loss: 0.3539 - classification_loss: 0.0449
 1341/10000 [===>..........................] - ETA: 2:08:52 - loss: 0.3986 - regression_loss: 0.3537 - classification_loss: 0.0448
 1342/10000 [===>..........................] - ETA: 2:08:51 - loss: 0.3984 - regression_loss: 0.3536 - classification_loss: 0.0448
 1343/10000 [===>..........................] - ETA: 2:08:50 - loss: 0.3984 - regression_loss: 0.3536 - classification_loss: 0.0448
 1344/10000 [===>..........................] - ETA: 2:08:49 - loss: 0.3986 - regression_loss: 0.3538 - classification_loss: 0.0448
 1345/10000 [===>..........................] - ETA: 2:08:48 - loss: 0.3987 - regression_loss: 0.3538 - classification_loss: 0.0448
 1346/10000 [===>..........................] - ETA: 2:08:47 - loss: 0.3988 - regression_loss: 0.3540 - classification_loss: 0.0448
 1347/10000 [===>..........................] - ETA: 2:08:46 - loss: 0.3985 - regression_loss: 0.3537 - classification_loss: 0.0448
 1348/10000 [===>..........................] - ETA: 2:08:46 - loss: 0.3984 - regression_loss: 0.3536 - classification_loss: 0.0448
 1349/10000 [===>..........................] - ETA: 2:08:45 - loss: 0.3986 - regression_loss: 0.3538 - classification_loss: 0.0448
 1350/10000 [===>..........................] - ETA: 2:08:44 - loss: 0.3990 - regression_loss: 0.3542 - classification_loss: 0.0448
 1351/10000 [===>..........................] - ETA: 2:08:43 - loss: 0.3990 - regression_loss: 0.3542 - classification_loss: 0.0448
 1352/10000 [===>..........................] - ETA: 2:08:42 - loss: 0.3990 - regression_loss: 0.3542 - classification_loss: 0.0448
 1353/10000 [===>..........................] - ETA: 2:08:41 - loss: 0.3991 - regression_loss: 0.3543 - classification_loss: 0.0448
 1354/10000 [===>..........................] - ETA: 2:08:40 - loss: 0.3991 - regression_loss: 0.3543 - classification_loss: 0.0448
 1355/10000 [===>..........................] - ETA: 2:08:39 - loss: 0.3990 - regression_loss: 0.3543 - classification_loss: 0.0447
 1356/10000 [===>..........................] - ETA: 2:08:38 - loss: 0.3989 - regression_loss: 0.3542 - classification_loss: 0.0447
 1357/10000 [===>..........................] - ETA: 2:08:37 - loss: 0.3990 - regression_loss: 0.3543 - classification_loss: 0.0447
 1358/10000 [===>..........................] - ETA: 2:08:36 - loss: 0.3989 - regression_loss: 0.3542 - classification_loss: 0.0447
 1359/10000 [===>..........................] - ETA: 2:08:35 - loss: 0.3986 - regression_loss: 0.3540 - classification_loss: 0.0446
 1360/10000 [===>..........................] - ETA: 2:08:34 - loss: 0.3987 - regression_loss: 0.3541 - classification_loss: 0.0446
 1361/10000 [===>..........................] - ETA: 2:08:33 - loss: 0.3987 - regression_loss: 0.3540 - classification_loss: 0.0447
 1362/10000 [===>..........................] - ETA: 2:08:33 - loss: 0.3986 - regression_loss: 0.3540 - classification_loss: 0.0447
 1363/10000 [===>..........................] - ETA: 2:08:32 - loss: 0.3985 - regression_loss: 0.3539 - classification_loss: 0.0446
 1364/10000 [===>..........................] - ETA: 2:08:31 - loss: 0.3986 - regression_loss: 0.3540 - classification_loss: 0.0447
 1365/10000 [===>..........................] - ETA: 2:08:30 - loss: 0.3987 - regression_loss: 0.3541 - classification_loss: 0.0446
 1366/10000 [===>..........................] - ETA: 2:08:29 - loss: 0.3989 - regression_loss: 0.3543 - classification_loss: 0.0446
 1367/10000 [===>..........................] - ETA: 2:08:28 - loss: 0.3987 - regression_loss: 0.3541 - classification_loss: 0.0446
 1368/10000 [===>..........................] - ETA: 2:08:27 - loss: 0.3987 - regression_loss: 0.3541 - classification_loss: 0.0446
 1369/10000 [===>..........................] - ETA: 2:08:26 - loss: 0.3985 - regression_loss: 0.3539 - classification_loss: 0.0445
 1370/10000 [===>..........................] - ETA: 2:08:25 - loss: 0.3982 - regression_loss: 0.3537 - classification_loss: 0.0445
 1371/10000 [===>..........................] - ETA: 2:08:24 - loss: 0.3982 - regression_loss: 0.3536 - classification_loss: 0.0445
 1372/10000 [===>..........................] - ETA: 2:08:23 - loss: 0.3982 - regression_loss: 0.3537 - classification_loss: 0.0446
 1373/10000 [===>..........................] - ETA: 2:08:22 - loss: 0.3982 - regression_loss: 0.3537 - classification_loss: 0.0445
 1374/10000 [===>..........................] - ETA: 2:08:21 - loss: 0.3981 - regression_loss: 0.3536 - classification_loss: 0.0445
 1375/10000 [===>..........................] - ETA: 2:08:21 - loss: 0.3981 - regression_loss: 0.3535 - classification_loss: 0.0445
 1376/10000 [===>..........................] - ETA: 2:08:20 - loss: 0.3981 - regression_loss: 0.3535 - classification_loss: 0.0445
 1377/10000 [===>..........................] - ETA: 2:08:19 - loss: 0.3980 - regression_loss: 0.3535 - classification_loss: 0.0445
 1378/10000 [===>..........................] - ETA: 2:08:18 - loss: 0.3981 - regression_loss: 0.3536 - classification_loss: 0.0445
 1379/10000 [===>..........................] - ETA: 2:08:17 - loss: 0.3980 - regression_loss: 0.3535 - classification_loss: 0.0445
 1380/10000 [===>..........................] - ETA: 2:08:16 - loss: 0.3982 - regression_loss: 0.3537 - classification_loss: 0.0445
 1381/10000 [===>..........................] - ETA: 2:08:15 - loss: 0.3981 - regression_loss: 0.3536 - classification_loss: 0.0445
 1382/10000 [===>..........................] - ETA: 2:08:14 - loss: 0.3982 - regression_loss: 0.3537 - classification_loss: 0.0445
 1383/10000 [===>..........................] - ETA: 2:08:13 - loss: 0.3983 - regression_loss: 0.3537 - classification_loss: 0.0445
 1384/10000 [===>..........................] - ETA: 2:08:12 - loss: 0.3983 - regression_loss: 0.3537 - classification_loss: 0.0445
 1385/10000 [===>..........................] - ETA: 2:08:11 - loss: 0.3983 - regression_loss: 0.3537 - classification_loss: 0.0445
 1386/10000 [===>..........................] - ETA: 2:08:10 - loss: 0.3985 - regression_loss: 0.3539 - classification_loss: 0.0445
 1387/10000 [===>..........................] - ETA: 2:08:10 - loss: 0.3982 - regression_loss: 0.3537 - classification_loss: 0.0445
 1388/10000 [===>..........................] - ETA: 2:08:09 - loss: 0.3981 - regression_loss: 0.3535 - classification_loss: 0.0445
 1389/10000 [===>..........................] - ETA: 2:08:08 - loss: 0.3980 - regression_loss: 0.3535 - classification_loss: 0.0445
 1390/10000 [===>..........................] - ETA: 2:08:07 - loss: 0.3981 - regression_loss: 0.3536 - classification_loss: 0.0445
 1391/10000 [===>..........................] - ETA: 2:08:06 - loss: 0.3981 - regression_loss: 0.3537 - classification_loss: 0.0445
 1392/10000 [===>..........................] - ETA: 2:08:05 - loss: 0.3987 - regression_loss: 0.3541 - classification_loss: 0.0446
 1393/10000 [===>..........................] - ETA: 2:08:04 - loss: 0.3986 - regression_loss: 0.3541 - classification_loss: 0.0445
 1394/10000 [===>..........................] - ETA: 2:08:03 - loss: 0.3983 - regression_loss: 0.3538 - classification_loss: 0.0445
 1395/10000 [===>..........................] - ETA: 2:08:02 - loss: 0.3983 - regression_loss: 0.3538 - classification_loss: 0.0445
 1396/10000 [===>..........................] - ETA: 2:08:01 - loss: 0.3980 - regression_loss: 0.3535 - classification_loss: 0.0445
 1397/10000 [===>..........................] - ETA: 2:08:00 - loss: 0.3981 - regression_loss: 0.3536 - classification_loss: 0.0445
 1398/10000 [===>..........................] - ETA: 2:07:59 - loss: 0.3980 - regression_loss: 0.3535 - classification_loss: 0.0445
 1399/10000 [===>..........................] - ETA: 2:07:58 - loss: 0.3980 - regression_loss: 0.3535 - classification_loss: 0.0445
 1400/10000 [===>..........................] - ETA: 2:07:58 - loss: 0.3982 - regression_loss: 0.3537 - classification_loss: 0.0445
 1401/10000 [===>..........................] - ETA: 2:07:57 - loss: 0.3982 - regression_loss: 0.3537 - classification_loss: 0.0445
 1402/10000 [===>..........................] - ETA: 2:07:56 - loss: 0.3979 - regression_loss: 0.3534 - classification_loss: 0.0445
 1403/10000 [===>..........................] - ETA: 2:07:55 - loss: 0.3981 - regression_loss: 0.3536 - classification_loss: 0.0445
 1404/10000 [===>..........................] - ETA: 2:07:54 - loss: 0.3981 - regression_loss: 0.3535 - classification_loss: 0.0446
 1405/10000 [===>..........................] - ETA: 2:07:53 - loss: 0.3979 - regression_loss: 0.3534 - classification_loss: 0.0445
 1406/10000 [===>..........................] - ETA: 2:07:52 - loss: 0.3979 - regression_loss: 0.3534 - classification_loss: 0.0445
 1407/10000 [===>..........................] - ETA: 2:07:51 - loss: 0.3977 - regression_loss: 0.3532 - classification_loss: 0.0445
 1408/10000 [===>..........................] - ETA: 2:07:50 - loss: 0.3979 - regression_loss: 0.3533 - classification_loss: 0.0446
 1409/10000 [===>..........................] - ETA: 2:07:49 - loss: 0.3978 - regression_loss: 0.3533 - classification_loss: 0.0446
 1410/10000 [===>..........................] - ETA: 2:07:48 - loss: 0.3978 - regression_loss: 0.3532 - classification_loss: 0.0445
 1411/10000 [===>..........................] - ETA: 2:07:48 - loss: 0.3977 - regression_loss: 0.3532 - classification_loss: 0.0445
 1412/10000 [===>..........................] - ETA: 2:07:47 - loss: 0.3977 - regression_loss: 0.3532 - classification_loss: 0.0445
 1413/10000 [===>..........................] - ETA: 2:07:46 - loss: 0.3978 - regression_loss: 0.3533 - classification_loss: 0.0445
 1414/10000 [===>..........................] - ETA: 2:07:45 - loss: 0.3978 - regression_loss: 0.3533 - classification_loss: 0.0445
 1415/10000 [===>..........................] - ETA: 2:07:44 - loss: 0.3977 - regression_loss: 0.3532 - classification_loss: 0.0445
 1416/10000 [===>..........................] - ETA: 2:07:43 - loss: 0.3977 - regression_loss: 0.3532 - classification_loss: 0.0445
 1417/10000 [===>..........................] - ETA: 2:07:42 - loss: 0.3975 - regression_loss: 0.3530 - classification_loss: 0.0445
 1418/10000 [===>..........................] - ETA: 2:07:41 - loss: 0.3973 - regression_loss: 0.3529 - classification_loss: 0.0444
 1419/10000 [===>..........................] - ETA: 2:07:40 - loss: 0.3970 - regression_loss: 0.3526 - classification_loss: 0.0444
 1420/10000 [===>..........................] - ETA: 2:07:39 - loss: 0.3970 - regression_loss: 0.3526 - classification_loss: 0.0444
 1421/10000 [===>..........................] - ETA: 2:07:38 - loss: 0.3971 - regression_loss: 0.3527 - classification_loss: 0.0444
 1422/10000 [===>..........................] - ETA: 2:07:37 - loss: 0.3972 - regression_loss: 0.3528 - classification_loss: 0.0444
 1423/10000 [===>..........................] - ETA: 2:07:36 - loss: 0.3972 - regression_loss: 0.3528 - classification_loss: 0.0444
 1424/10000 [===>..........................] - ETA: 2:07:36 - loss: 0.3972 - regression_loss: 0.3528 - classification_loss: 0.0444
 1425/10000 [===>..........................] - ETA: 2:07:35 - loss: 0.3971 - regression_loss: 0.3527 - classification_loss: 0.0444
 1426/10000 [===>..........................] - ETA: 2:07:34 - loss: 0.3969 - regression_loss: 0.3525 - classification_loss: 0.0444
 1427/10000 [===>..........................] - ETA: 2:07:33 - loss: 0.3966 - regression_loss: 0.3523 - classification_loss: 0.0443
 1428/10000 [===>..........................] - ETA: 2:07:32 - loss: 0.3964 - regression_loss: 0.3521 - classification_loss: 0.0443
 1429/10000 [===>..........................] - ETA: 2:07:31 - loss: 0.3965 - regression_loss: 0.3522 - classification_loss: 0.0443
 1430/10000 [===>..........................] - ETA: 2:07:30 - loss: 0.3964 - regression_loss: 0.3521 - classification_loss: 0.0443
 1431/10000 [===>..........................] - ETA: 2:07:29 - loss: 0.3963 - regression_loss: 0.3520 - classification_loss: 0.0443
 1432/10000 [===>..........................] - ETA: 2:07:28 - loss: 0.3963 - regression_loss: 0.3521 - classification_loss: 0.0442
 1433/10000 [===>..........................] - ETA: 2:07:27 - loss: 0.3962 - regression_loss: 0.3520 - classification_loss: 0.0442
 1434/10000 [===>..........................] - ETA: 2:07:26 - loss: 0.3962 - regression_loss: 0.3519 - classification_loss: 0.0443
 1435/10000 [===>..........................] - ETA: 2:07:25 - loss: 0.3961 - regression_loss: 0.3518 - classification_loss: 0.0443
 1436/10000 [===>..........................] - ETA: 2:07:25 - loss: 0.3960 - regression_loss: 0.3517 - classification_loss: 0.0442
 1437/10000 [===>..........................] - ETA: 2:07:24 - loss: 0.3958 - regression_loss: 0.3515 - classification_loss: 0.0442
 1438/10000 [===>..........................] - ETA: 2:07:23 - loss: 0.3957 - regression_loss: 0.3515 - classification_loss: 0.0442
 1439/10000 [===>..........................] - ETA: 2:07:22 - loss: 0.3958 - regression_loss: 0.3516 - classification_loss: 0.0442
 1440/10000 [===>..........................] - ETA: 2:07:21 - loss: 0.3957 - regression_loss: 0.3514 - classification_loss: 0.0442
 1441/10000 [===>..........................] - ETA: 2:07:20 - loss: 0.3956 - regression_loss: 0.3513 - classification_loss: 0.0442
 1442/10000 [===>..........................] - ETA: 2:07:19 - loss: 0.3957 - regression_loss: 0.3514 - classification_loss: 0.0443
 1443/10000 [===>..........................] - ETA: 2:07:18 - loss: 0.3956 - regression_loss: 0.3514 - classification_loss: 0.0442
 1444/10000 [===>..........................] - ETA: 2:07:17 - loss: 0.3953 - regression_loss: 0.3511 - classification_loss: 0.0442
 1445/10000 [===>..........................] - ETA: 2:07:16 - loss: 0.3955 - regression_loss: 0.3513 - classification_loss: 0.0442
 1446/10000 [===>..........................] - ETA: 2:07:15 - loss: 0.3955 - regression_loss: 0.3513 - classification_loss: 0.0442
 1447/10000 [===>..........................] - ETA: 2:07:15 - loss: 0.3954 - regression_loss: 0.3512 - classification_loss: 0.0442
 1448/10000 [===>..........................] - ETA: 2:07:14 - loss: 0.3953 - regression_loss: 0.3511 - classification_loss: 0.0442
 1449/10000 [===>..........................] - ETA: 2:07:13 - loss: 0.3954 - regression_loss: 0.3512 - classification_loss: 0.0442
 1450/10000 [===>..........................] - ETA: 2:07:12 - loss: 0.3954 - regression_loss: 0.3512 - classification_loss: 0.0442
 1451/10000 [===>..........................] - ETA: 2:07:11 - loss: 0.3957 - regression_loss: 0.3515 - classification_loss: 0.0442
 1452/10000 [===>..........................] - ETA: 2:07:10 - loss: 0.3958 - regression_loss: 0.3516 - classification_loss: 0.0442
 1453/10000 [===>..........................] - ETA: 2:07:09 - loss: 0.3960 - regression_loss: 0.3517 - classification_loss: 0.0443
 1454/10000 [===>..........................] - ETA: 2:07:08 - loss: 0.3960 - regression_loss: 0.3517 - classification_loss: 0.0443
 1455/10000 [===>..........................] - ETA: 2:07:07 - loss: 0.3963 - regression_loss: 0.3520 - classification_loss: 0.0443
 1456/10000 [===>..........................] - ETA: 2:07:06 - loss: 0.3961 - regression_loss: 0.3518 - classification_loss: 0.0443
 1457/10000 [===>..........................] - ETA: 2:07:05 - loss: 0.3962 - regression_loss: 0.3519 - classification_loss: 0.0443
 1458/10000 [===>..........................] - ETA: 2:07:04 - loss: 0.3963 - regression_loss: 0.3519 - classification_loss: 0.0443
 1459/10000 [===>..........................] - ETA: 2:07:04 - loss: 0.3961 - regression_loss: 0.3518 - classification_loss: 0.0443
 1460/10000 [===>..........................] - ETA: 2:07:03 - loss: 0.3961 - regression_loss: 0.3518 - classification_loss: 0.0443
 1461/10000 [===>..........................] - ETA: 2:07:02 - loss: 0.3961 - regression_loss: 0.3518 - classification_loss: 0.0443
 1462/10000 [===>..........................] - ETA: 2:07:01 - loss: 0.3962 - regression_loss: 0.3519 - classification_loss: 0.0444
 1463/10000 [===>..........................] - ETA: 2:07:00 - loss: 0.3963 - regression_loss: 0.3519 - classification_loss: 0.0443
 1464/10000 [===>..........................] - ETA: 2:06:59 - loss: 0.3961 - regression_loss: 0.3518 - classification_loss: 0.0443
 1465/10000 [===>..........................] - ETA: 2:06:58 - loss: 0.3961 - regression_loss: 0.3518 - classification_loss: 0.0443
 1466/10000 [===>..........................] - ETA: 2:06:57 - loss: 0.3960 - regression_loss: 0.3518 - classification_loss: 0.0443
 1467/10000 [===>..........................] - ETA: 2:06:56 - loss: 0.3962 - regression_loss: 0.3519 - classification_loss: 0.0443
 1468/10000 [===>..........................] - ETA: 2:06:55 - loss: 0.3961 - regression_loss: 0.3518 - classification_loss: 0.0443
 1469/10000 [===>..........................] - ETA: 2:06:54 - loss: 0.3961 - regression_loss: 0.3518 - classification_loss: 0.0443
 1470/10000 [===>..........................] - ETA: 2:06:53 - loss: 0.3960 - regression_loss: 0.3518 - classification_loss: 0.0442
 1471/10000 [===>..........................] - ETA: 2:06:53 - loss: 0.3961 - regression_loss: 0.3518 - classification_loss: 0.0442
 1472/10000 [===>..........................] - ETA: 2:06:52 - loss: 0.3960 - regression_loss: 0.3518 - classification_loss: 0.0442
 1473/10000 [===>..........................] - ETA: 2:06:51 - loss: 0.3961 - regression_loss: 0.3519 - classification_loss: 0.0442
 1474/10000 [===>..........................] - ETA: 2:06:50 - loss: 0.3961 - regression_loss: 0.3519 - classification_loss: 0.0442
 1475/10000 [===>..........................] - ETA: 2:06:49 - loss: 0.3960 - regression_loss: 0.3518 - classification_loss: 0.0442
 1476/10000 [===>..........................] - ETA: 2:06:48 - loss: 0.3960 - regression_loss: 0.3518 - classification_loss: 0.0442
 1477/10000 [===>..........................] - ETA: 2:06:47 - loss: 0.3963 - regression_loss: 0.3520 - classification_loss: 0.0442
 1478/10000 [===>..........................] - ETA: 2:06:46 - loss: 0.3963 - regression_loss: 0.3520 - classification_loss: 0.0443
 1479/10000 [===>..........................] - ETA: 2:06:45 - loss: 0.3965 - regression_loss: 0.3522 - classification_loss: 0.0443
 1480/10000 [===>..........................] - ETA: 2:06:44 - loss: 0.3965 - regression_loss: 0.3522 - classification_loss: 0.0442
 1481/10000 [===>..........................] - ETA: 2:06:43 - loss: 0.3966 - regression_loss: 0.3524 - classification_loss: 0.0443
 1482/10000 [===>..........................] - ETA: 2:06:42 - loss: 0.3966 - regression_loss: 0.3524 - classification_loss: 0.0442
 1483/10000 [===>..........................] - ETA: 2:06:41 - loss: 0.3966 - regression_loss: 0.3524 - classification_loss: 0.0442
 1484/10000 [===>..........................] - ETA: 2:06:41 - loss: 0.3965 - regression_loss: 0.3523 - classification_loss: 0.0442
 1485/10000 [===>..........................] - ETA: 2:06:40 - loss: 0.3965 - regression_loss: 0.3523 - classification_loss: 0.0442
 1486/10000 [===>..........................] - ETA: 2:06:39 - loss: 0.3964 - regression_loss: 0.3522 - classification_loss: 0.0442
 1487/10000 [===>..........................] - ETA: 2:06:38 - loss: 0.3971 - regression_loss: 0.3529 - classification_loss: 0.0442
 1488/10000 [===>..........................] - ETA: 2:06:37 - loss: 0.3973 - regression_loss: 0.3530 - classification_loss: 0.0443
 1489/10000 [===>..........................] - ETA: 2:06:36 - loss: 0.3973 - regression_loss: 0.3530 - classification_loss: 0.0443
 1490/10000 [===>..........................] - ETA: 2:06:35 - loss: 0.3972 - regression_loss: 0.3529 - classification_loss: 0.0443
 1491/10000 [===>..........................] - ETA: 2:06:34 - loss: 0.3977 - regression_loss: 0.3534 - classification_loss: 0.0443
 1492/10000 [===>..........................] - ETA: 2:06:33 - loss: 0.3976 - regression_loss: 0.3533 - classification_loss: 0.0443
 1493/10000 [===>..........................] - ETA: 2:06:32 - loss: 0.3976 - regression_loss: 0.3533 - classification_loss: 0.0443
 1494/10000 [===>..........................] - ETA: 2:06:31 - loss: 0.3977 - regression_loss: 0.3535 - classification_loss: 0.0443
 1495/10000 [===>..........................] - ETA: 2:06:30 - loss: 0.3975 - regression_loss: 0.3532 - classification_loss: 0.0442
 1496/10000 [===>..........................] - ETA: 2:06:30 - loss: 0.3976 - regression_loss: 0.3534 - classification_loss: 0.0442
 1497/10000 [===>..........................] - ETA: 2:06:29 - loss: 0.3975 - regression_loss: 0.3533 - classification_loss: 0.0442
 1498/10000 [===>..........................] - ETA: 2:06:28 - loss: 0.3975 - regression_loss: 0.3533 - classification_loss: 0.0442
 1499/10000 [===>..........................] - ETA: 2:06:27 - loss: 0.3977 - regression_loss: 0.3535 - classification_loss: 0.0442
 1500/10000 [===>..........................] - ETA: 2:06:26 - loss: 0.3978 - regression_loss: 0.3536 - classification_loss: 0.0442
 1501/10000 [===>..........................] - ETA: 2:06:25 - loss: 0.3980 - regression_loss: 0.3538 - classification_loss: 0.0442
 1502/10000 [===>..........................] - ETA: 2:06:24 - loss: 0.3979 - regression_loss: 0.3537 - classification_loss: 0.0442
 1503/10000 [===>..........................] - ETA: 2:06:23 - loss: 0.3980 - regression_loss: 0.3538 - classification_loss: 0.0442
 1504/10000 [===>..........................] - ETA: 2:06:22 - loss: 0.3977 - regression_loss: 0.3536 - classification_loss: 0.0441
 1505/10000 [===>..........................] - ETA: 2:06:21 - loss: 0.3976 - regression_loss: 0.3535 - classification_loss: 0.0441
 1506/10000 [===>..........................] - ETA: 2:06:20 - loss: 0.3976 - regression_loss: 0.3535 - classification_loss: 0.0441
 1507/10000 [===>..........................] - ETA: 2:06:19 - loss: 0.3976 - regression_loss: 0.3534 - classification_loss: 0.0441
 1508/10000 [===>..........................] - ETA: 2:06:19 - loss: 0.3975 - regression_loss: 0.3534 - classification_loss: 0.0441
 1509/10000 [===>..........................] - ETA: 2:06:18 - loss: 0.3973 - regression_loss: 0.3532 - classification_loss: 0.0441
 1510/10000 [===>..........................] - ETA: 2:06:17 - loss: 0.3976 - regression_loss: 0.3534 - classification_loss: 0.0441
 1511/10000 [===>..........................] - ETA: 2:06:16 - loss: 0.3975 - regression_loss: 0.3534 - classification_loss: 0.0441
 1512/10000 [===>..........................] - ETA: 2:06:15 - loss: 0.3977 - regression_loss: 0.3536 - classification_loss: 0.0441
 1513/10000 [===>..........................] - ETA: 2:06:14 - loss: 0.3978 - regression_loss: 0.3537 - classification_loss: 0.0441
 1514/10000 [===>..........................] - ETA: 2:06:13 - loss: 0.3977 - regression_loss: 0.3536 - classification_loss: 0.0441
 1515/10000 [===>..........................] - ETA: 2:06:12 - loss: 0.3978 - regression_loss: 0.3537 - classification_loss: 0.0441
 1516/10000 [===>..........................] - ETA: 2:06:11 - loss: 0.3977 - regression_loss: 0.3537 - classification_loss: 0.0441
 1517/10000 [===>..........................] - ETA: 2:06:10 - loss: 0.3977 - regression_loss: 0.3536 - classification_loss: 0.0441
 1518/10000 [===>..........................] - ETA: 2:06:09 - loss: 0.3977 - regression_loss: 0.3536 - classification_loss: 0.0441
 1519/10000 [===>..........................] - ETA: 2:06:08 - loss: 0.3974 - regression_loss: 0.3534 - classification_loss: 0.0440
 1520/10000 [===>..........................] - ETA: 2:06:08 - loss: 0.3973 - regression_loss: 0.3533 - classification_loss: 0.0440
 1521/10000 [===>..........................] - ETA: 2:06:07 - loss: 0.3973 - regression_loss: 0.3531 - classification_loss: 0.0441
 1522/10000 [===>..........................] - ETA: 2:06:06 - loss: 0.3972 - regression_loss: 0.3530 - classification_loss: 0.0441
 1523/10000 [===>..........................] - ETA: 2:06:05 - loss: 0.3971 - regression_loss: 0.3530 - classification_loss: 0.0441
 1524/10000 [===>..........................] - ETA: 2:06:04 - loss: 0.3970 - regression_loss: 0.3529 - classification_loss: 0.0441
 1525/10000 [===>..........................] - ETA: 2:06:03 - loss: 0.3970 - regression_loss: 0.3529 - classification_loss: 0.0442
 1526/10000 [===>..........................] - ETA: 2:06:02 - loss: 0.3969 - regression_loss: 0.3527 - classification_loss: 0.0441
 1527/10000 [===>..........................] - ETA: 2:06:01 - loss: 0.3969 - regression_loss: 0.3527 - classification_loss: 0.0441
 1528/10000 [===>..........................] - ETA: 2:06:00 - loss: 0.3968 - regression_loss: 0.3527 - classification_loss: 0.0441
 1529/10000 [===>..........................] - ETA: 2:05:59 - loss: 0.3966 - regression_loss: 0.3525 - classification_loss: 0.0441
 1530/10000 [===>..........................] - ETA: 2:05:58 - loss: 0.3966 - regression_loss: 0.3524 - classification_loss: 0.0441
 1531/10000 [===>..........................] - ETA: 2:05:58 - loss: 0.3964 - regression_loss: 0.3523 - classification_loss: 0.0441
 1532/10000 [===>..........................] - ETA: 2:05:57 - loss: 0.3964 - regression_loss: 0.3523 - classification_loss: 0.0441
 1533/10000 [===>..........................] - ETA: 2:05:56 - loss: 0.3963 - regression_loss: 0.3522 - classification_loss: 0.0441
 1534/10000 [===>..........................] - ETA: 2:05:55 - loss: 0.3962 - regression_loss: 0.3521 - classification_loss: 0.0440
 1535/10000 [===>..........................] - ETA: 2:05:54 - loss: 0.3965 - regression_loss: 0.3524 - classification_loss: 0.0441
 1536/10000 [===>..........................] - ETA: 2:05:53 - loss: 0.3964 - regression_loss: 0.3523 - classification_loss: 0.0441
 1537/10000 [===>..........................] - ETA: 2:05:52 - loss: 0.3964 - regression_loss: 0.3523 - classification_loss: 0.0441
 1538/10000 [===>..........................] - ETA: 2:05:51 - loss: 0.3968 - regression_loss: 0.3527 - classification_loss: 0.0441
 1539/10000 [===>..........................] - ETA: 2:05:50 - loss: 0.3969 - regression_loss: 0.3528 - classification_loss: 0.0441
 1540/10000 [===>..........................] - ETA: 2:05:49 - loss: 0.3969 - regression_loss: 0.3529 - classification_loss: 0.0441
 1541/10000 [===>..........................] - ETA: 2:05:48 - loss: 0.3969 - regression_loss: 0.3528 - classification_loss: 0.0441
 1542/10000 [===>..........................] - ETA: 2:05:47 - loss: 0.3968 - regression_loss: 0.3527 - classification_loss: 0.0441
 1543/10000 [===>..........................] - ETA: 2:05:47 - loss: 0.3970 - regression_loss: 0.3528 - classification_loss: 0.0441
 1544/10000 [===>..........................] - ETA: 2:05:46 - loss: 0.3970 - regression_loss: 0.3529 - classification_loss: 0.0441
 1545/10000 [===>..........................] - ETA: 2:05:45 - loss: 0.3971 - regression_loss: 0.3530 - classification_loss: 0.0442
 1546/10000 [===>..........................] - ETA: 2:05:44 - loss: 0.3971 - regression_loss: 0.3529 - classification_loss: 0.0442
 1547/10000 [===>..........................] - ETA: 2:05:43 - loss: 0.3972 - regression_loss: 0.3530 - classification_loss: 0.0442
 1548/10000 [===>..........................] - ETA: 2:05:42 - loss: 0.3970 - regression_loss: 0.3529 - classification_loss: 0.0442
 1549/10000 [===>..........................] - ETA: 2:05:41 - loss: 0.3969 - regression_loss: 0.3528 - classification_loss: 0.0441
 1550/10000 [===>..........................] - ETA: 2:05:40 - loss: 0.3967 - regression_loss: 0.3526 - classification_loss: 0.0441
 1551/10000 [===>..........................] - ETA: 2:05:39 - loss: 0.3965 - regression_loss: 0.3524 - classification_loss: 0.0441
 1552/10000 [===>..........................] - ETA: 2:05:38 - loss: 0.3963 - regression_loss: 0.3522 - classification_loss: 0.0441
 1553/10000 [===>..........................] - ETA: 2:05:37 - loss: 0.3964 - regression_loss: 0.3523 - classification_loss: 0.0441
 1554/10000 [===>..........................] - ETA: 2:05:36 - loss: 0.3964 - regression_loss: 0.3523 - classification_loss: 0.0441
 1555/10000 [===>..........................] - ETA: 2:05:36 - loss: 0.3963 - regression_loss: 0.3523 - classification_loss: 0.0440
 1556/10000 [===>..........................] - ETA: 2:05:35 - loss: 0.3961 - regression_loss: 0.3521 - classification_loss: 0.0440
 1557/10000 [===>..........................] - ETA: 2:05:34 - loss: 0.3961 - regression_loss: 0.3521 - classification_loss: 0.0440
 1558/10000 [===>..........................] - ETA: 2:05:33 - loss: 0.3960 - regression_loss: 0.3520 - classification_loss: 0.0440
 1559/10000 [===>..........................] - ETA: 2:05:32 - loss: 0.3960 - regression_loss: 0.3520 - classification_loss: 0.0441
 1560/10000 [===>..........................] - ETA: 2:05:31 - loss: 0.3965 - regression_loss: 0.3524 - classification_loss: 0.0441
 1561/10000 [===>..........................] - ETA: 2:05:30 - loss: 0.3965 - regression_loss: 0.3524 - classification_loss: 0.0441
 1562/10000 [===>..........................] - ETA: 2:05:29 - loss: 0.3966 - regression_loss: 0.3524 - classification_loss: 0.0441
 1563/10000 [===>..........................] - ETA: 2:05:28 - loss: 0.3966 - regression_loss: 0.3524 - classification_loss: 0.0441
 1564/10000 [===>..........................] - ETA: 2:05:27 - loss: 0.3965 - regression_loss: 0.3524 - classification_loss: 0.0441
 1565/10000 [===>..........................] - ETA: 2:05:26 - loss: 0.3964 - regression_loss: 0.3523 - classification_loss: 0.0441
 1566/10000 [===>..........................] - ETA: 2:05:25 - loss: 0.3964 - regression_loss: 0.3523 - classification_loss: 0.0442
 1567/10000 [===>..........................] - ETA: 2:05:25 - loss: 0.3963 - regression_loss: 0.3522 - classification_loss: 0.0442
 1568/10000 [===>..........................] - ETA: 2:05:24 - loss: 0.3962 - regression_loss: 0.3521 - classification_loss: 0.0442
 1569/10000 [===>..........................] - ETA: 2:05:23 - loss: 0.3962 - regression_loss: 0.3521 - classification_loss: 0.0441
 1570/10000 [===>..........................] - ETA: 2:05:22 - loss: 0.3962 - regression_loss: 0.3520 - classification_loss: 0.0441
 1571/10000 [===>..........................] - ETA: 2:05:21 - loss: 0.3961 - regression_loss: 0.3520 - classification_loss: 0.0441
 1572/10000 [===>..........................] - ETA: 2:05:20 - loss: 0.3959 - regression_loss: 0.3518 - classification_loss: 0.0441
 1573/10000 [===>..........................] - ETA: 2:05:19 - loss: 0.3958 - regression_loss: 0.3517 - classification_loss: 0.0441
 1574/10000 [===>..........................] - ETA: 2:05:18 - loss: 0.3958 - regression_loss: 0.3517 - classification_loss: 0.0441
 1575/10000 [===>..........................] - ETA: 2:05:17 - loss: 0.3960 - regression_loss: 0.3519 - classification_loss: 0.0441
 1576/10000 [===>..........................] - ETA: 2:05:16 - loss: 0.3959 - regression_loss: 0.3519 - classification_loss: 0.0441
 1577/10000 [===>..........................] - ETA: 2:05:15 - loss: 0.3960 - regression_loss: 0.3519 - classification_loss: 0.0441
 1578/10000 [===>..........................] - ETA: 2:05:15 - loss: 0.3959 - regression_loss: 0.3518 - classification_loss: 0.0441
 1579/10000 [===>..........................] - ETA: 2:05:14 - loss: 0.3958 - regression_loss: 0.3517 - classification_loss: 0.0441
 1580/10000 [===>..........................] - ETA: 2:05:13 - loss: 0.3959 - regression_loss: 0.3517 - classification_loss: 0.0442
 1581/10000 [===>..........................] - ETA: 2:05:12 - loss: 0.3958 - regression_loss: 0.3516 - classification_loss: 0.0442
 1582/10000 [===>..........................] - ETA: 2:05:11 - loss: 0.3957 - regression_loss: 0.3516 - classification_loss: 0.0442
 1583/10000 [===>..........................] - ETA: 2:05:10 - loss: 0.3957 - regression_loss: 0.3516 - classification_loss: 0.0441
 1584/10000 [===>..........................] - ETA: 2:05:09 - loss: 0.3956 - regression_loss: 0.3515 - classification_loss: 0.0441
 1585/10000 [===>..........................] - ETA: 2:05:08 - loss: 0.3955 - regression_loss: 0.3514 - classification_loss: 0.0441
 1586/10000 [===>..........................] - ETA: 2:05:07 - loss: 0.3955 - regression_loss: 0.3514 - classification_loss: 0.0441
 1587/10000 [===>..........................] - ETA: 2:05:06 - loss: 0.3954 - regression_loss: 0.3513 - classification_loss: 0.0441
 1588/10000 [===>..........................] - ETA: 2:05:05 - loss: 0.3953 - regression_loss: 0.3512 - classification_loss: 0.0441
 1589/10000 [===>..........................] - ETA: 2:05:04 - loss: 0.3954 - regression_loss: 0.3513 - classification_loss: 0.0441
 1590/10000 [===>..........................] - ETA: 2:05:04 - loss: 0.3955 - regression_loss: 0.3513 - classification_loss: 0.0441
 1591/10000 [===>..........................] - ETA: 2:05:03 - loss: 0.3955 - regression_loss: 0.3514 - classification_loss: 0.0441
 1592/10000 [===>..........................] - ETA: 2:05:02 - loss: 0.3954 - regression_loss: 0.3513 - classification_loss: 0.0441
 1593/10000 [===>..........................] - ETA: 2:05:01 - loss: 0.3955 - regression_loss: 0.3513 - classification_loss: 0.0442
 1594/10000 [===>..........................] - ETA: 2:05:00 - loss: 0.3960 - regression_loss: 0.3515 - classification_loss: 0.0444
 1595/10000 [===>..........................] - ETA: 2:04:59 - loss: 0.3962 - regression_loss: 0.3518 - classification_loss: 0.0445
 1596/10000 [===>..........................] - ETA: 2:04:58 - loss: 0.3962 - regression_loss: 0.3517 - classification_loss: 0.0445
 1597/10000 [===>..........................] - ETA: 2:04:57 - loss: 0.3961 - regression_loss: 0.3516 - classification_loss: 0.0445
 1598/10000 [===>..........................] - ETA: 2:04:56 - loss: 0.3959 - regression_loss: 0.3515 - classification_loss: 0.0444
 1599/10000 [===>..........................] - ETA: 2:04:55 - loss: 0.3959 - regression_loss: 0.3514 - classification_loss: 0.0445
 1600/10000 [===>..........................] - ETA: 2:04:54 - loss: 0.3958 - regression_loss: 0.3513 - classification_loss: 0.0445
 1601/10000 [===>..........................] - ETA: 2:04:54 - loss: 0.3957 - regression_loss: 0.3512 - classification_loss: 0.0444
 1602/10000 [===>..........................] - ETA: 2:04:53 - loss: 0.3957 - regression_loss: 0.3513 - classification_loss: 0.0444
 1603/10000 [===>..........................] - ETA: 2:04:52 - loss: 0.3958 - regression_loss: 0.3513 - classification_loss: 0.0444
 1604/10000 [===>..........................] - ETA: 2:04:51 - loss: 0.3959 - regression_loss: 0.3515 - classification_loss: 0.0444
 1605/10000 [===>..........................] - ETA: 2:04:50 - loss: 0.3958 - regression_loss: 0.3514 - classification_loss: 0.0444
 1606/10000 [===>..........................] - ETA: 2:04:49 - loss: 0.3958 - regression_loss: 0.3514 - classification_loss: 0.0444
 1607/10000 [===>..........................] - ETA: 2:04:48 - loss: 0.3956 - regression_loss: 0.3512 - classification_loss: 0.0444
 1608/10000 [===>..........................] - ETA: 2:04:47 - loss: 0.3953 - regression_loss: 0.3510 - classification_loss: 0.0444
 1609/10000 [===>..........................] - ETA: 2:04:46 - loss: 0.3952 - regression_loss: 0.3509 - classification_loss: 0.0443
 1610/10000 [===>..........................] - ETA: 2:04:45 - loss: 0.3955 - regression_loss: 0.3511 - classification_loss: 0.0444
 1611/10000 [===>..........................] - ETA: 2:04:44 - loss: 0.3955 - regression_loss: 0.3511 - classification_loss: 0.0444
 1612/10000 [===>..........................] - ETA: 2:04:43 - loss: 0.3955 - regression_loss: 0.3511 - classification_loss: 0.0444
 1613/10000 [===>..........................] - ETA: 2:04:43 - loss: 0.3955 - regression_loss: 0.3511 - classification_loss: 0.0444
 1614/10000 [===>..........................] - ETA: 2:04:42 - loss: 0.3956 - regression_loss: 0.3512 - classification_loss: 0.0444
 1615/10000 [===>..........................] - ETA: 2:04:41 - loss: 0.3958 - regression_loss: 0.3514 - classification_loss: 0.0444
 1616/10000 [===>..........................] - ETA: 2:04:40 - loss: 0.3959 - regression_loss: 0.3515 - classification_loss: 0.0444
 1617/10000 [===>..........................] - ETA: 2:04:39 - loss: 0.3956 - regression_loss: 0.3513 - classification_loss: 0.0444
 1618/10000 [===>..........................] - ETA: 2:04:38 - loss: 0.3957 - regression_loss: 0.3513 - classification_loss: 0.0444
 1619/10000 [===>..........................] - ETA: 2:04:37 - loss: 0.3957 - regression_loss: 0.3513 - classification_loss: 0.0444
 1620/10000 [===>..........................] - ETA: 2:04:36 - loss: 0.3958 - regression_loss: 0.3514 - classification_loss: 0.0444
 1621/10000 [===>..........................] - ETA: 2:04:35 - loss: 0.3960 - regression_loss: 0.3515 - classification_loss: 0.0444
 1622/10000 [===>..........................] - ETA: 2:04:34 - loss: 0.3960 - regression_loss: 0.3516 - classification_loss: 0.0444
 1623/10000 [===>..........................] - ETA: 2:04:33 - loss: 0.3960 - regression_loss: 0.3516 - classification_loss: 0.0444
 1624/10000 [===>..........................] - ETA: 2:04:33 - loss: 0.3962 - regression_loss: 0.3518 - classification_loss: 0.0444
 1625/10000 [===>..........................] - ETA: 2:04:32 - loss: 0.3964 - regression_loss: 0.3520 - classification_loss: 0.0444
 1626/10000 [===>..........................] - ETA: 2:04:31 - loss: 0.3965 - regression_loss: 0.3521 - classification_loss: 0.0444
 1627/10000 [===>..........................] - ETA: 2:04:30 - loss: 0.3964 - regression_loss: 0.3520 - classification_loss: 0.0444
 1628/10000 [===>..........................] - ETA: 2:04:29 - loss: 0.3964 - regression_loss: 0.3520 - classification_loss: 0.0444
 1629/10000 [===>..........................] - ETA: 2:04:28 - loss: 0.3963 - regression_loss: 0.3519 - classification_loss: 0.0444
 1630/10000 [===>..........................] - ETA: 2:04:27 - loss: 0.3968 - regression_loss: 0.3524 - classification_loss: 0.0444
 1631/10000 [===>..........................] - ETA: 2:04:26 - loss: 0.3967 - regression_loss: 0.3524 - classification_loss: 0.0444
 1632/10000 [===>..........................] - ETA: 2:04:25 - loss: 0.3969 - regression_loss: 0.3525 - classification_loss: 0.0444
 1633/10000 [===>..........................] - ETA: 2:04:24 - loss: 0.3974 - regression_loss: 0.3529 - classification_loss: 0.0445
 1634/10000 [===>..........................] - ETA: 2:04:23 - loss: 0.3974 - regression_loss: 0.3529 - classification_loss: 0.0445
 1635/10000 [===>..........................] - ETA: 2:04:23 - loss: 0.3973 - regression_loss: 0.3528 - classification_loss: 0.0445
 1636/10000 [===>..........................] - ETA: 2:04:22 - loss: 0.3971 - regression_loss: 0.3526 - classification_loss: 0.0445
 1637/10000 [===>..........................] - ETA: 2:04:21 - loss: 0.3970 - regression_loss: 0.3525 - classification_loss: 0.0445
 1638/10000 [===>..........................] - ETA: 2:04:20 - loss: 0.3969 - regression_loss: 0.3524 - classification_loss: 0.0445
 1639/10000 [===>..........................] - ETA: 2:04:19 - loss: 0.3974 - regression_loss: 0.3529 - classification_loss: 0.0445
 1640/10000 [===>..........................] - ETA: 2:04:18 - loss: 0.3976 - regression_loss: 0.3531 - classification_loss: 0.0445
 1641/10000 [===>..........................] - ETA: 2:04:17 - loss: 0.3978 - regression_loss: 0.3532 - classification_loss: 0.0446
 1642/10000 [===>..........................] - ETA: 2:04:16 - loss: 0.3980 - regression_loss: 0.3532 - classification_loss: 0.0447
 1643/10000 [===>..........................] - ETA: 2:04:15 - loss: 0.3980 - regression_loss: 0.3533 - classification_loss: 0.0447
 1644/10000 [===>..........................] - ETA: 2:04:14 - loss: 0.3983 - regression_loss: 0.3536 - classification_loss: 0.0447
 1645/10000 [===>..........................] - ETA: 2:04:13 - loss: 0.3984 - regression_loss: 0.3537 - classification_loss: 0.0448
 1646/10000 [===>..........................] - ETA: 2:04:13 - loss: 0.3984 - regression_loss: 0.3536 - classification_loss: 0.0448
 1647/10000 [===>..........................] - ETA: 2:04:12 - loss: 0.3983 - regression_loss: 0.3535 - classification_loss: 0.0448
 1648/10000 [===>..........................] - ETA: 2:04:11 - loss: 0.3983 - regression_loss: 0.3535 - classification_loss: 0.0449
 1649/10000 [===>..........................] - ETA: 2:04:10 - loss: 0.3982 - regression_loss: 0.3534 - classification_loss: 0.0449
 1650/10000 [===>..........................] - ETA: 2:04:09 - loss: 0.3983 - regression_loss: 0.3534 - classification_loss: 0.0450
 1651/10000 [===>..........................] - ETA: 2:04:08 - loss: 0.3983 - regression_loss: 0.3533 - classification_loss: 0.0450
 1652/10000 [===>..........................] - ETA: 2:04:07 - loss: 0.3983 - regression_loss: 0.3533 - classification_loss: 0.0450
 1653/10000 [===>..........................] - ETA: 2:04:06 - loss: 0.3982 - regression_loss: 0.3533 - classification_loss: 0.0449
 1654/10000 [===>..........................] - ETA: 2:04:05 - loss: 0.3982 - regression_loss: 0.3533 - classification_loss: 0.0449
 1655/10000 [===>..........................] - ETA: 2:04:04 - loss: 0.3981 - regression_loss: 0.3532 - classification_loss: 0.0449
 1656/10000 [===>..........................] - ETA: 2:04:03 - loss: 0.3981 - regression_loss: 0.3532 - classification_loss: 0.0449
 1657/10000 [===>..........................] - ETA: 2:04:02 - loss: 0.3981 - regression_loss: 0.3532 - classification_loss: 0.0449
 1658/10000 [===>..........................] - ETA: 2:04:02 - loss: 0.3984 - regression_loss: 0.3535 - classification_loss: 0.0449
 1659/10000 [===>..........................] - ETA: 2:04:01 - loss: 0.3983 - regression_loss: 0.3534 - classification_loss: 0.0450
 1660/10000 [===>..........................] - ETA: 2:04:00 - loss: 0.3992 - regression_loss: 0.3538 - classification_loss: 0.0455
 1661/10000 [===>..........................] - ETA: 2:03:59 - loss: 0.3991 - regression_loss: 0.3537 - classification_loss: 0.0454
 1662/10000 [===>..........................] - ETA: 2:03:58 - loss: 0.3991 - regression_loss: 0.3536 - classification_loss: 0.0454
 1663/10000 [===>..........................] - ETA: 2:03:57 - loss: 0.3995 - regression_loss: 0.3540 - classification_loss: 0.0455
 1664/10000 [===>..........................] - ETA: 2:03:56 - loss: 0.3996 - regression_loss: 0.3540 - classification_loss: 0.0455
 1665/10000 [===>..........................] - ETA: 2:03:55 - loss: 0.3996 - regression_loss: 0.3540 - classification_loss: 0.0455
 1666/10000 [===>..........................] - ETA: 2:03:54 - loss: 0.3997 - regression_loss: 0.3542 - classification_loss: 0.0455
 1667/10000 [====>.........................] - ETA: 2:03:53 - loss: 0.3997 - regression_loss: 0.3541 - classification_loss: 0.0455
 1668/10000 [====>.........................] - ETA: 2:03:52 - loss: 0.3996 - regression_loss: 0.3540 - classification_loss: 0.0455
 1669/10000 [====>.........................] - ETA: 2:03:52 - loss: 0.3994 - regression_loss: 0.3539 - classification_loss: 0.0455
 1670/10000 [====>.........................] - ETA: 2:03:51 - loss: 0.3994 - regression_loss: 0.3539 - classification_loss: 0.0455
 1671/10000 [====>.........................] - ETA: 2:03:50 - loss: 0.3992 - regression_loss: 0.3537 - classification_loss: 0.0455
 1672/10000 [====>.........................] - ETA: 2:03:49 - loss: 0.3990 - regression_loss: 0.3535 - classification_loss: 0.0455
 1673/10000 [====>.........................] - ETA: 2:03:48 - loss: 0.3991 - regression_loss: 0.3536 - classification_loss: 0.0455
 1674/10000 [====>.........................] - ETA: 2:03:47 - loss: 0.3988 - regression_loss: 0.3534 - classification_loss: 0.0454
 1675/10000 [====>.........................] - ETA: 2:03:46 - loss: 0.3988 - regression_loss: 0.3534 - classification_loss: 0.0454
 1676/10000 [====>.........................] - ETA: 2:03:45 - loss: 0.3991 - regression_loss: 0.3536 - classification_loss: 0.0455
 1677/10000 [====>.........................] - ETA: 2:03:44 - loss: 0.3993 - regression_loss: 0.3537 - classification_loss: 0.0456
 1678/10000 [====>.........................] - ETA: 2:03:43 - loss: 0.3991 - regression_loss: 0.3535 - classification_loss: 0.0456
 1679/10000 [====>.........................] - ETA: 2:03:42 - loss: 0.3989 - regression_loss: 0.3534 - classification_loss: 0.0455
 1680/10000 [====>.........................] - ETA: 2:03:41 - loss: 0.3990 - regression_loss: 0.3535 - classification_loss: 0.0456
 1681/10000 [====>.........................] - ETA: 2:03:41 - loss: 0.3990 - regression_loss: 0.3534 - classification_loss: 0.0456
 1682/10000 [====>.........................] - ETA: 2:03:40 - loss: 0.3989 - regression_loss: 0.3533 - classification_loss: 0.0456
 1683/10000 [====>.........................] - ETA: 2:03:39 - loss: 0.3989 - regression_loss: 0.3534 - classification_loss: 0.0456
 1684/10000 [====>.........................] - ETA: 2:03:38 - loss: 0.3990 - regression_loss: 0.3535 - classification_loss: 0.0456
 1685/10000 [====>.........................] - ETA: 2:03:37 - loss: 0.3995 - regression_loss: 0.3539 - classification_loss: 0.0455
 1686/10000 [====>.........................] - ETA: 2:03:36 - loss: 0.3994 - regression_loss: 0.3539 - classification_loss: 0.0455
 1687/10000 [====>.........................] - ETA: 2:03:35 - loss: 0.3995 - regression_loss: 0.3540 - classification_loss: 0.0455
 1688/10000 [====>.........................] - ETA: 2:03:34 - loss: 0.3996 - regression_loss: 0.3540 - classification_loss: 0.0455
 1689/10000 [====>.........................] - ETA: 2:03:33 - loss: 0.3995 - regression_loss: 0.3540 - classification_loss: 0.0455
 1690/10000 [====>.........................] - ETA: 2:03:32 - loss: 0.3993 - regression_loss: 0.3538 - classification_loss: 0.0455
 1691/10000 [====>.........................] - ETA: 2:03:31 - loss: 0.3994 - regression_loss: 0.3539 - classification_loss: 0.0455
 1692/10000 [====>.........................] - ETA: 2:03:30 - loss: 0.3994 - regression_loss: 0.3539 - classification_loss: 0.0455
 1693/10000 [====>.........................] - ETA: 2:03:30 - loss: 0.3995 - regression_loss: 0.3540 - classification_loss: 0.0455
 1694/10000 [====>.........................] - ETA: 2:03:29 - loss: 0.3995 - regression_loss: 0.3540 - classification_loss: 0.0455
 1695/10000 [====>.........................] - ETA: 2:03:28 - loss: 0.3993 - regression_loss: 0.3539 - classification_loss: 0.0455
 1696/10000 [====>.........................] - ETA: 2:03:27 - loss: 0.3992 - regression_loss: 0.3538 - classification_loss: 0.0455
 1697/10000 [====>.........................] - ETA: 2:03:26 - loss: 0.3993 - regression_loss: 0.3538 - classification_loss: 0.0455
 1698/10000 [====>.........................] - ETA: 2:03:25 - loss: 0.3992 - regression_loss: 0.3538 - classification_loss: 0.0454
 1699/10000 [====>.........................] - ETA: 2:03:24 - loss: 0.3991 - regression_loss: 0.3536 - classification_loss: 0.0454
 1700/10000 [====>.........................] - ETA: 2:03:23 - loss: 0.3991 - regression_loss: 0.3537 - classification_loss: 0.0454
 1701/10000 [====>.........................] - ETA: 2:03:22 - loss: 0.3991 - regression_loss: 0.3537 - classification_loss: 0.0454
 1702/10000 [====>.........................] - ETA: 2:03:21 - loss: 0.3990 - regression_loss: 0.3536 - classification_loss: 0.0454
 1703/10000 [====>.........................] - ETA: 2:03:20 - loss: 0.3990 - regression_loss: 0.3536 - classification_loss: 0.0454
 1704/10000 [====>.........................] - ETA: 2:03:20 - loss: 0.3989 - regression_loss: 0.3535 - classification_loss: 0.0454
 1705/10000 [====>.........................] - ETA: 2:03:19 - loss: 0.3989 - regression_loss: 0.3534 - classification_loss: 0.0454
 1706/10000 [====>.........................] - ETA: 2:03:18 - loss: 0.3988 - regression_loss: 0.3534 - classification_loss: 0.0454
 1707/10000 [====>.........................] - ETA: 2:03:17 - loss: 0.3988 - regression_loss: 0.3534 - classification_loss: 0.0454
 1708/10000 [====>.........................] - ETA: 2:03:16 - loss: 0.3987 - regression_loss: 0.3534 - classification_loss: 0.0454
 1709/10000 [====>.........................] - ETA: 2:03:15 - loss: 0.3989 - regression_loss: 0.3535 - classification_loss: 0.0453
 1710/10000 [====>.........................] - ETA: 2:03:14 - loss: 0.3988 - regression_loss: 0.3534 - classification_loss: 0.0453
 1711/10000 [====>.........................] - ETA: 2:03:13 - loss: 0.3990 - regression_loss: 0.3536 - classification_loss: 0.0453
 1712/10000 [====>.........................] - ETA: 2:03:12 - loss: 0.3989 - regression_loss: 0.3536 - classification_loss: 0.0453
 1713/10000 [====>.........................] - ETA: 2:03:11 - loss: 0.3989 - regression_loss: 0.3536 - classification_loss: 0.0453
 1714/10000 [====>.........................] - ETA: 2:03:10 - loss: 0.3989 - regression_loss: 0.3536 - classification_loss: 0.0453
 1715/10000 [====>.........................] - ETA: 2:03:10 - loss: 0.3988 - regression_loss: 0.3535 - classification_loss: 0.0453
 1716/10000 [====>.........................] - ETA: 2:03:09 - loss: 0.3987 - regression_loss: 0.3534 - classification_loss: 0.0453
 1717/10000 [====>.........................] - ETA: 2:03:08 - loss: 0.3987 - regression_loss: 0.3533 - classification_loss: 0.0453
 1718/10000 [====>.........................] - ETA: 2:03:07 - loss: 0.3984 - regression_loss: 0.3531 - classification_loss: 0.0453
 1719/10000 [====>.........................] - ETA: 2:03:06 - loss: 0.3982 - regression_loss: 0.3530 - classification_loss: 0.0453
 1720/10000 [====>.........................] - ETA: 2:03:05 - loss: 0.3983 - regression_loss: 0.3530 - classification_loss: 0.0453
 1721/10000 [====>.........................] - ETA: 2:03:04 - loss: 0.3981 - regression_loss: 0.3529 - classification_loss: 0.0452
 1722/10000 [====>.........................] - ETA: 2:03:03 - loss: 0.3982 - regression_loss: 0.3529 - classification_loss: 0.0452
 1723/10000 [====>.........................] - ETA: 2:03:02 - loss: 0.3981 - regression_loss: 0.3528 - classification_loss: 0.0452
 1724/10000 [====>.........................] - ETA: 2:03:01 - loss: 0.3981 - regression_loss: 0.3528 - classification_loss: 0.0452
 1725/10000 [====>.........................] - ETA: 2:03:00 - loss: 0.3982 - regression_loss: 0.3530 - classification_loss: 0.0452
 1726/10000 [====>.........................] - ETA: 2:03:00 - loss: 0.3983 - regression_loss: 0.3530 - classification_loss: 0.0452
 1727/10000 [====>.........................] - ETA: 2:02:59 - loss: 0.3985 - regression_loss: 0.3532 - classification_loss: 0.0453
 1728/10000 [====>.........................] - ETA: 2:02:58 - loss: 0.3985 - regression_loss: 0.3532 - classification_loss: 0.0453
 1729/10000 [====>.........................] - ETA: 2:02:57 - loss: 0.3986 - regression_loss: 0.3533 - classification_loss: 0.0453
 1730/10000 [====>.........................] - ETA: 2:02:56 - loss: 0.3988 - regression_loss: 0.3534 - classification_loss: 0.0453
 1731/10000 [====>.........................] - ETA: 2:02:55 - loss: 0.3985 - regression_loss: 0.3532 - classification_loss: 0.0453
 1732/10000 [====>.........................] - ETA: 2:02:54 - loss: 0.3985 - regression_loss: 0.3532 - classification_loss: 0.0453
 1733/10000 [====>.........................] - ETA: 2:02:53 - loss: 0.3984 - regression_loss: 0.3530 - classification_loss: 0.0453
 1734/10000 [====>.........................] - ETA: 2:02:52 - loss: 0.3984 - regression_loss: 0.3530 - classification_loss: 0.0453
 1735/10000 [====>.........................] - ETA: 2:02:51 - loss: 0.3983 - regression_loss: 0.3530 - classification_loss: 0.0453
 1736/10000 [====>.........................] - ETA: 2:02:50 - loss: 0.3983 - regression_loss: 0.3530 - classification_loss: 0.0453
 1737/10000 [====>.........................] - ETA: 2:02:50 - loss: 0.3982 - regression_loss: 0.3529 - classification_loss: 0.0453
 1738/10000 [====>.........................] - ETA: 2:02:49 - loss: 0.3981 - regression_loss: 0.3528 - classification_loss: 0.0453
 1739/10000 [====>.........................] - ETA: 2:02:48 - loss: 0.3981 - regression_loss: 0.3529 - classification_loss: 0.0453
 1740/10000 [====>.........................] - ETA: 2:02:47 - loss: 0.3982 - regression_loss: 0.3529 - classification_loss: 0.0453
 1741/10000 [====>.........................] - ETA: 2:02:46 - loss: 0.3982 - regression_loss: 0.3529 - classification_loss: 0.0453
 1742/10000 [====>.........................] - ETA: 2:02:45 - loss: 0.3984 - regression_loss: 0.3531 - classification_loss: 0.0453
 1743/10000 [====>.........................] - ETA: 2:02:44 - loss: 0.3986 - regression_loss: 0.3532 - classification_loss: 0.0454
 1744/10000 [====>.........................] - ETA: 2:02:43 - loss: 0.3987 - regression_loss: 0.3532 - classification_loss: 0.0454
 1745/10000 [====>.........................] - ETA: 2:02:42 - loss: 0.3986 - regression_loss: 0.3532 - classification_loss: 0.0454
 1746/10000 [====>.........................] - ETA: 2:02:41 - loss: 0.3983 - regression_loss: 0.3530 - classification_loss: 0.0454
 1747/10000 [====>.........................] - ETA: 2:02:40 - loss: 0.3988 - regression_loss: 0.3534 - classification_loss: 0.0454
 1748/10000 [====>.........................] - ETA: 2:02:39 - loss: 0.3987 - regression_loss: 0.3533 - classification_loss: 0.0454
 1749/10000 [====>.........................] - ETA: 2:02:39 - loss: 0.3989 - regression_loss: 0.3535 - classification_loss: 0.0454
 1750/10000 [====>.........................] - ETA: 2:02:38 - loss: 0.3990 - regression_loss: 0.3536 - classification_loss: 0.0454
 1751/10000 [====>.........................] - ETA: 2:02:37 - loss: 0.3988 - regression_loss: 0.3535 - classification_loss: 0.0454
 1752/10000 [====>.........................] - ETA: 2:02:36 - loss: 0.3988 - regression_loss: 0.3535 - classification_loss: 0.0454
 1753/10000 [====>.........................] - ETA: 2:02:35 - loss: 0.3988 - regression_loss: 0.3535 - classification_loss: 0.0453
 1754/10000 [====>.........................] - ETA: 2:02:34 - loss: 0.3988 - regression_loss: 0.3535 - classification_loss: 0.0453
 1755/10000 [====>.........................] - ETA: 2:02:33 - loss: 0.3986 - regression_loss: 0.3533 - classification_loss: 0.0453
 1756/10000 [====>.........................] - ETA: 2:02:32 - loss: 0.3985 - regression_loss: 0.3532 - classification_loss: 0.0453
 1757/10000 [====>.........................] - ETA: 2:02:31 - loss: 0.3984 - regression_loss: 0.3531 - classification_loss: 0.0453
 1758/10000 [====>.........................] - ETA: 2:02:30 - loss: 0.3983 - regression_loss: 0.3530 - classification_loss: 0.0453
 1759/10000 [====>.........................] - ETA: 2:02:29 - loss: 0.3983 - regression_loss: 0.3530 - classification_loss: 0.0452
 1760/10000 [====>.........................] - ETA: 2:02:29 - loss: 0.3981 - regression_loss: 0.3529 - classification_loss: 0.0452
 1761/10000 [====>.........................] - ETA: 2:02:28 - loss: 0.3983 - regression_loss: 0.3530 - classification_loss: 0.0453
 1762/10000 [====>.........................] - ETA: 2:02:27 - loss: 0.3985 - regression_loss: 0.3532 - classification_loss: 0.0453
 1763/10000 [====>.........................] - ETA: 2:02:26 - loss: 0.3985 - regression_loss: 0.3532 - classification_loss: 0.0454
 1764/10000 [====>.........................] - ETA: 2:02:25 - loss: 0.3988 - regression_loss: 0.3534 - classification_loss: 0.0454
 1765/10000 [====>.........................] - ETA: 2:02:24 - loss: 0.3987 - regression_loss: 0.3533 - classification_loss: 0.0454
 1766/10000 [====>.........................] - ETA: 2:02:23 - loss: 0.3991 - regression_loss: 0.3537 - classification_loss: 0.0454
 1767/10000 [====>.........................] - ETA: 2:02:22 - loss: 0.3994 - regression_loss: 0.3539 - classification_loss: 0.0455
 1768/10000 [====>.........................] - ETA: 2:02:21 - loss: 0.3993 - regression_loss: 0.3539 - classification_loss: 0.0455
 1769/10000 [====>.........................] - ETA: 2:02:20 - loss: 0.3991 - regression_loss: 0.3537 - classification_loss: 0.0454
 1770/10000 [====>.........................] - ETA: 2:02:19 - loss: 0.3990 - regression_loss: 0.3535 - classification_loss: 0.0454
 1771/10000 [====>.........................] - ETA: 2:02:19 - loss: 0.3989 - regression_loss: 0.3535 - classification_loss: 0.0454
 1772/10000 [====>.........................] - ETA: 2:02:18 - loss: 0.3988 - regression_loss: 0.3534 - classification_loss: 0.0454
 1773/10000 [====>.........................] - ETA: 2:02:17 - loss: 0.3987 - regression_loss: 0.3533 - classification_loss: 0.0454
 1774/10000 [====>.........................] - ETA: 2:02:16 - loss: 0.3987 - regression_loss: 0.3533 - classification_loss: 0.0454
 1775/10000 [====>.........................] - ETA: 2:02:15 - loss: 0.3987 - regression_loss: 0.3533 - classification_loss: 0.0454
 1776/10000 [====>.........................] - ETA: 2:02:14 - loss: 0.3985 - regression_loss: 0.3531 - classification_loss: 0.0454
 1777/10000 [====>.........................] - ETA: 2:02:13 - loss: 0.3985 - regression_loss: 0.3531 - classification_loss: 0.0454
 1778/10000 [====>.........................] - ETA: 2:02:12 - loss: 0.3986 - regression_loss: 0.3532 - classification_loss: 0.0454
 1779/10000 [====>.........................] - ETA: 2:02:11 - loss: 0.3985 - regression_loss: 0.3531 - classification_loss: 0.0454
 1780/10000 [====>.........................] - ETA: 2:02:10 - loss: 0.3984 - regression_loss: 0.3530 - classification_loss: 0.0454
 1781/10000 [====>.........................] - ETA: 2:02:09 - loss: 0.3984 - regression_loss: 0.3530 - classification_loss: 0.0454
 1782/10000 [====>.........................] - ETA: 2:02:08 - loss: 0.3983 - regression_loss: 0.3529 - classification_loss: 0.0454
 1783/10000 [====>.........................] - ETA: 2:02:08 - loss: 0.3982 - regression_loss: 0.3528 - classification_loss: 0.0454
 1784/10000 [====>.........................] - ETA: 2:02:07 - loss: 0.3985 - regression_loss: 0.3530 - classification_loss: 0.0455
 1785/10000 [====>.........................] - ETA: 2:02:06 - loss: 0.3984 - regression_loss: 0.3529 - classification_loss: 0.0455
 1786/10000 [====>.........................] - ETA: 2:02:05 - loss: 0.3983 - regression_loss: 0.3528 - classification_loss: 0.0455
 1787/10000 [====>.........................] - ETA: 2:02:04 - loss: 0.3987 - regression_loss: 0.3530 - classification_loss: 0.0457
 1788/10000 [====>.........................] - ETA: 2:02:03 - loss: 0.3986 - regression_loss: 0.3530 - classification_loss: 0.0457
 1789/10000 [====>.........................] - ETA: 2:02:02 - loss: 0.3988 - regression_loss: 0.3531 - classification_loss: 0.0457
 1790/10000 [====>.........................] - ETA: 2:02:01 - loss: 0.3987 - regression_loss: 0.3530 - classification_loss: 0.0457
 1791/10000 [====>.........................] - ETA: 2:02:00 - loss: 0.3988 - regression_loss: 0.3531 - classification_loss: 0.0457
 1792/10000 [====>.........................] - ETA: 2:01:59 - loss: 0.3987 - regression_loss: 0.3531 - classification_loss: 0.0457
 1793/10000 [====>.........................] - ETA: 2:01:58 - loss: 0.3986 - regression_loss: 0.3530 - classification_loss: 0.0457
 1794/10000 [====>.........................] - ETA: 2:01:58 - loss: 0.3986 - regression_loss: 0.3529 - classification_loss: 0.0456
 1795/10000 [====>.........................] - ETA: 2:01:57 - loss: 0.3985 - regression_loss: 0.3529 - classification_loss: 0.0456
 1796/10000 [====>.........................] - ETA: 2:01:56 - loss: 0.3986 - regression_loss: 0.3530 - classification_loss: 0.0456
 1797/10000 [====>.........................] - ETA: 2:01:55 - loss: 0.3985 - regression_loss: 0.3529 - classification_loss: 0.0456
 1798/10000 [====>.........................] - ETA: 2:01:54 - loss: 0.3983 - regression_loss: 0.3527 - classification_loss: 0.0456
 1799/10000 [====>.........................] - ETA: 2:01:53 - loss: 0.3982 - regression_loss: 0.3526 - classification_loss: 0.0456
 1800/10000 [====>.........................] - ETA: 2:01:52 - loss: 0.3983 - regression_loss: 0.3527 - classification_loss: 0.0456
 1801/10000 [====>.........................] - ETA: 2:01:51 - loss: 0.3983 - regression_loss: 0.3527 - classification_loss: 0.0456
 1802/10000 [====>.........................] - ETA: 2:01:50 - loss: 0.3985 - regression_loss: 0.3529 - classification_loss: 0.0456
 1803/10000 [====>.........................] - ETA: 2:01:49 - loss: 0.3983 - regression_loss: 0.3527 - classification_loss: 0.0456
 1804/10000 [====>.........................] - ETA: 2:01:49 - loss: 0.3981 - regression_loss: 0.3525 - classification_loss: 0.0456
 1805/10000 [====>.........................] - ETA: 2:01:48 - loss: 0.3979 - regression_loss: 0.3523 - classification_loss: 0.0455
 1806/10000 [====>.........................] - ETA: 2:01:47 - loss: 0.3978 - regression_loss: 0.3523 - classification_loss: 0.0455
 1807/10000 [====>.........................] - ETA: 2:01:46 - loss: 0.3977 - regression_loss: 0.3522 - classification_loss: 0.0455
 1808/10000 [====>.........................] - ETA: 2:01:45 - loss: 0.3978 - regression_loss: 0.3523 - classification_loss: 0.0455
 1809/10000 [====>.........................] - ETA: 2:01:44 - loss: 0.3977 - regression_loss: 0.3522 - classification_loss: 0.0455
 1810/10000 [====>.........................] - ETA: 2:01:43 - loss: 0.3977 - regression_loss: 0.3522 - classification_loss: 0.0455
 1811/10000 [====>.........................] - ETA: 2:01:42 - loss: 0.3976 - regression_loss: 0.3521 - classification_loss: 0.0455
 1812/10000 [====>.........................] - ETA: 2:01:41 - loss: 0.3974 - regression_loss: 0.3519 - classification_loss: 0.0455
 1813/10000 [====>.........................] - ETA: 2:01:40 - loss: 0.3974 - regression_loss: 0.3519 - classification_loss: 0.0455
 1814/10000 [====>.........................] - ETA: 2:01:39 - loss: 0.3974 - regression_loss: 0.3518 - classification_loss: 0.0455
 1815/10000 [====>.........................] - ETA: 2:01:38 - loss: 0.3977 - regression_loss: 0.3520 - classification_loss: 0.0457
 1816/10000 [====>.........................] - ETA: 2:01:38 - loss: 0.3982 - regression_loss: 0.3524 - classification_loss: 0.0457
 1817/10000 [====>.........................] - ETA: 2:01:37 - loss: 0.3982 - regression_loss: 0.3524 - classification_loss: 0.0457
 1818/10000 [====>.........................] - ETA: 2:01:36 - loss: 0.3979 - regression_loss: 0.3522 - classification_loss: 0.0457
 1819/10000 [====>.........................] - ETA: 2:01:35 - loss: 0.3978 - regression_loss: 0.3521 - classification_loss: 0.0457
 1820/10000 [====>.........................] - ETA: 2:01:34 - loss: 0.3978 - regression_loss: 0.3521 - classification_loss: 0.0457
 1821/10000 [====>.........................] - ETA: 2:01:33 - loss: 0.3977 - regression_loss: 0.3520 - classification_loss: 0.0457
 1822/10000 [====>.........................] - ETA: 2:01:32 - loss: 0.3977 - regression_loss: 0.3521 - classification_loss: 0.0457
 1823/10000 [====>.........................] - ETA: 2:01:31 - loss: 0.3976 - regression_loss: 0.3520 - classification_loss: 0.0456
 1824/10000 [====>.........................] - ETA: 2:01:30 - loss: 0.3977 - regression_loss: 0.3520 - classification_loss: 0.0456
 1825/10000 [====>.........................] - ETA: 2:01:29 - loss: 0.3976 - regression_loss: 0.3519 - classification_loss: 0.0457
 1826/10000 [====>.........................] - ETA: 2:01:28 - loss: 0.3976 - regression_loss: 0.3519 - classification_loss: 0.0457
 1827/10000 [====>.........................] - ETA: 2:01:28 - loss: 0.3976 - regression_loss: 0.3519 - classification_loss: 0.0457
 1828/10000 [====>.........................] - ETA: 2:01:27 - loss: 0.3975 - regression_loss: 0.3518 - classification_loss: 0.0457
 1829/10000 [====>.........................] - ETA: 2:01:26 - loss: 0.3974 - regression_loss: 0.3517 - classification_loss: 0.0457
 1830/10000 [====>.........................] - ETA: 2:01:25 - loss: 0.3975 - regression_loss: 0.3518 - classification_loss: 0.0456
 1831/10000 [====>.........................] - ETA: 2:01:24 - loss: 0.3973 - regression_loss: 0.3517 - classification_loss: 0.0456
 1832/10000 [====>.........................] - ETA: 2:01:23 - loss: 0.3973 - regression_loss: 0.3517 - classification_loss: 0.0456
 1833/10000 [====>.........................] - ETA: 2:01:22 - loss: 0.3974 - regression_loss: 0.3518 - classification_loss: 0.0456
 1834/10000 [====>.........................] - ETA: 2:01:21 - loss: 0.3972 - regression_loss: 0.3516 - classification_loss: 0.0456
 1835/10000 [====>.........................] - ETA: 2:01:20 - loss: 0.3973 - regression_loss: 0.3517 - classification_loss: 0.0456
 1836/10000 [====>.........................] - ETA: 2:01:19 - loss: 0.3973 - regression_loss: 0.3518 - classification_loss: 0.0456
 1837/10000 [====>.........................] - ETA: 2:01:18 - loss: 0.3976 - regression_loss: 0.3520 - classification_loss: 0.0456
 1838/10000 [====>.........................] - ETA: 2:01:18 - loss: 0.3974 - regression_loss: 0.3518 - classification_loss: 0.0456
 1839/10000 [====>.........................] - ETA: 2:01:17 - loss: 0.3974 - regression_loss: 0.3518 - classification_loss: 0.0456
 1840/10000 [====>.........................] - ETA: 2:01:16 - loss: 0.3973 - regression_loss: 0.3517 - classification_loss: 0.0456
 1841/10000 [====>.........................] - ETA: 2:01:15 - loss: 0.3973 - regression_loss: 0.3517 - classification_loss: 0.0456
 1842/10000 [====>.........................] - ETA: 2:01:14 - loss: 0.3974 - regression_loss: 0.3518 - classification_loss: 0.0456
 1843/10000 [====>.........................] - ETA: 2:01:13 - loss: 0.3973 - regression_loss: 0.3518 - classification_loss: 0.0456
 1844/10000 [====>.........................] - ETA: 2:01:12 - loss: 0.3971 - regression_loss: 0.3516 - classification_loss: 0.0455
 1845/10000 [====>.........................] - ETA: 2:01:11 - loss: 0.3971 - regression_loss: 0.3516 - classification_loss: 0.0455
 1846/10000 [====>.........................] - ETA: 2:01:10 - loss: 0.3969 - regression_loss: 0.3514 - classification_loss: 0.0455
 1847/10000 [====>.........................] - ETA: 2:01:09 - loss: 0.3968 - regression_loss: 0.3513 - classification_loss: 0.0455
 1848/10000 [====>.........................] - ETA: 2:01:08 - loss: 0.3968 - regression_loss: 0.3513 - classification_loss: 0.0455
 1849/10000 [====>.........................] - ETA: 2:01:08 - loss: 0.3967 - regression_loss: 0.3512 - classification_loss: 0.0455
 1850/10000 [====>.........................] - ETA: 2:01:07 - loss: 0.3966 - regression_loss: 0.3511 - classification_loss: 0.0454
 1851/10000 [====>.........................] - ETA: 2:01:06 - loss: 0.3966 - regression_loss: 0.3511 - classification_loss: 0.0454
 1852/10000 [====>.........................] - ETA: 2:01:05 - loss: 0.3965 - regression_loss: 0.3511 - classification_loss: 0.0454
 1853/10000 [====>.........................] - ETA: 2:01:04 - loss: 0.3966 - regression_loss: 0.3511 - classification_loss: 0.0454
 1854/10000 [====>.........................] - ETA: 2:01:03 - loss: 0.3966 - regression_loss: 0.3511 - classification_loss: 0.0455
 1855/10000 [====>.........................] - ETA: 2:01:02 - loss: 0.3966 - regression_loss: 0.3511 - classification_loss: 0.0454
 1856/10000 [====>.........................] - ETA: 2:01:01 - loss: 0.3965 - regression_loss: 0.3511 - classification_loss: 0.0454
 1857/10000 [====>.........................] - ETA: 2:01:00 - loss: 0.3966 - regression_loss: 0.3512 - classification_loss: 0.0454
 1858/10000 [====>.........................] - ETA: 2:00:59 - loss: 0.3965 - regression_loss: 0.3511 - classification_loss: 0.0454
 1859/10000 [====>.........................] - ETA: 2:00:58 - loss: 0.3965 - regression_loss: 0.3511 - classification_loss: 0.0454
 1860/10000 [====>.........................] - ETA: 2:00:58 - loss: 0.3963 - regression_loss: 0.3509 - classification_loss: 0.0454
 1861/10000 [====>.........................] - ETA: 2:00:57 - loss: 0.3961 - regression_loss: 0.3507 - classification_loss: 0.0453
 1862/10000 [====>.........................] - ETA: 2:00:56 - loss: 0.3961 - regression_loss: 0.3508 - classification_loss: 0.0454
 1863/10000 [====>.........................] - ETA: 2:00:55 - loss: 0.3960 - regression_loss: 0.3507 - classification_loss: 0.0453
 1864/10000 [====>.........................] - ETA: 2:00:54 - loss: 0.3959 - regression_loss: 0.3506 - classification_loss: 0.0453
 1865/10000 [====>.........................] - ETA: 2:00:53 - loss: 0.3959 - regression_loss: 0.3506 - classification_loss: 0.0453
 1866/10000 [====>.........................] - ETA: 2:00:52 - loss: 0.3967 - regression_loss: 0.3509 - classification_loss: 0.0458
 1867/10000 [====>.........................] - ETA: 2:00:51 - loss: 0.3966 - regression_loss: 0.3508 - classification_loss: 0.0457
 1868/10000 [====>.........................] - ETA: 2:00:50 - loss: 0.3964 - regression_loss: 0.3506 - classification_loss: 0.0457
 1869/10000 [====>.........................] - ETA: 2:00:49 - loss: 0.3963 - regression_loss: 0.3506 - classification_loss: 0.0457
 1870/10000 [====>.........................] - ETA: 2:00:48 - loss: 0.3964 - regression_loss: 0.3507 - classification_loss: 0.0457
 1871/10000 [====>.........................] - ETA: 2:00:48 - loss: 0.3962 - regression_loss: 0.3505 - classification_loss: 0.0457
 1872/10000 [====>.........................] - ETA: 2:00:47 - loss: 0.3962 - regression_loss: 0.3505 - classification_loss: 0.0457
 1873/10000 [====>.........................] - ETA: 2:00:46 - loss: 0.3960 - regression_loss: 0.3503 - classification_loss: 0.0456
 1874/10000 [====>.........................] - ETA: 2:00:45 - loss: 0.3959 - regression_loss: 0.3503 - classification_loss: 0.0456
 1875/10000 [====>.........................] - ETA: 2:00:44 - loss: 0.3959 - regression_loss: 0.3503 - classification_loss: 0.0457
 1876/10000 [====>.........................] - ETA: 2:00:43 - loss: 0.3960 - regression_loss: 0.3504 - classification_loss: 0.0457
 1877/10000 [====>.........................] - ETA: 2:00:42 - loss: 0.3961 - regression_loss: 0.3504 - classification_loss: 0.0457
 1878/10000 [====>.........................] - ETA: 2:00:41 - loss: 0.3960 - regression_loss: 0.3503 - classification_loss: 0.0457
 1879/10000 [====>.........................] - ETA: 2:00:40 - loss: 0.3959 - regression_loss: 0.3503 - classification_loss: 0.0456
 1880/10000 [====>.........................] - ETA: 2:00:39 - loss: 0.3960 - regression_loss: 0.3503 - classification_loss: 0.0456
 1881/10000 [====>.........................] - ETA: 2:00:38 - loss: 0.3960 - regression_loss: 0.3503 - classification_loss: 0.0456
 1882/10000 [====>.........................] - ETA: 2:00:38 - loss: 0.3958 - regression_loss: 0.3502 - classification_loss: 0.0456
 1883/10000 [====>.........................] - ETA: 2:00:37 - loss: 0.3961 - regression_loss: 0.3505 - classification_loss: 0.0456
 1884/10000 [====>.........................] - ETA: 2:00:36 - loss: 0.3961 - regression_loss: 0.3505 - classification_loss: 0.0456
 1885/10000 [====>.........................] - ETA: 2:00:35 - loss: 0.3961 - regression_loss: 0.3505 - classification_loss: 0.0456
 1886/10000 [====>.........................] - ETA: 2:00:34 - loss: 0.3962 - regression_loss: 0.3506 - classification_loss: 0.0456
 1887/10000 [====>.........................] - ETA: 2:00:33 - loss: 0.3962 - regression_loss: 0.3506 - classification_loss: 0.0456
 1888/10000 [====>.........................] - ETA: 2:00:32 - loss: 0.3961 - regression_loss: 0.3505 - classification_loss: 0.0456
 1889/10000 [====>.........................] - ETA: 2:00:31 - loss: 0.3961 - regression_loss: 0.3505 - classification_loss: 0.0456
 1890/10000 [====>.........................] - ETA: 2:00:30 - loss: 0.3962 - regression_loss: 0.3505 - classification_loss: 0.0456
 1891/10000 [====>.........................] - ETA: 2:00:29 - loss: 0.3961 - regression_loss: 0.3504 - classification_loss: 0.0456
 1892/10000 [====>.........................] - ETA: 2:00:29 - loss: 0.3960 - regression_loss: 0.3504 - classification_loss: 0.0456
 1893/10000 [====>.........................] - ETA: 2:00:28 - loss: 0.3959 - regression_loss: 0.3503 - classification_loss: 0.0456
 1894/10000 [====>.........................] - ETA: 2:00:27 - loss: 0.3957 - regression_loss: 0.3501 - classification_loss: 0.0456
 1895/10000 [====>.........................] - ETA: 2:00:26 - loss: 0.3957 - regression_loss: 0.3502 - classification_loss: 0.0456
 1896/10000 [====>.........................] - ETA: 2:00:25 - loss: 0.3956 - regression_loss: 0.3501 - classification_loss: 0.0456
 1897/10000 [====>.........................] - ETA: 2:00:24 - loss: 0.3954 - regression_loss: 0.3499 - classification_loss: 0.0455
 1898/10000 [====>.........................] - ETA: 2:00:23 - loss: 0.3954 - regression_loss: 0.3498 - classification_loss: 0.0455
 1899/10000 [====>.........................] - ETA: 2:00:22 - loss: 0.3953 - regression_loss: 0.3498 - classification_loss: 0.0455
 1900/10000 [====>.........................] - ETA: 2:00:21 - loss: 0.3951 - regression_loss: 0.3496 - classification_loss: 0.0455
 1901/10000 [====>.........................] - ETA: 2:00:20 - loss: 0.3950 - regression_loss: 0.3496 - classification_loss: 0.0455
 1902/10000 [====>.........................] - ETA: 2:00:19 - loss: 0.3950 - regression_loss: 0.3495 - classification_loss: 0.0454
 1903/10000 [====>.........................] - ETA: 2:00:19 - loss: 0.3949 - regression_loss: 0.3494 - classification_loss: 0.0454
 1904/10000 [====>.........................] - ETA: 2:00:18 - loss: 0.3948 - regression_loss: 0.3493 - classification_loss: 0.0454
 1905/10000 [====>.........................] - ETA: 2:00:17 - loss: 0.3946 - regression_loss: 0.3492 - classification_loss: 0.0454
 1906/10000 [====>.........................] - ETA: 2:00:16 - loss: 0.3947 - regression_loss: 0.3493 - classification_loss: 0.0454
 1907/10000 [====>.........................] - ETA: 2:00:15 - loss: 0.3946 - regression_loss: 0.3492 - classification_loss: 0.0454
 1908/10000 [====>.........................] - ETA: 2:00:14 - loss: 0.3945 - regression_loss: 0.3492 - classification_loss: 0.0454
 1909/10000 [====>.........................] - ETA: 2:00:13 - loss: 0.3947 - regression_loss: 0.3493 - classification_loss: 0.0454
 1910/10000 [====>.........................] - ETA: 2:00:12 - loss: 0.3946 - regression_loss: 0.3492 - classification_loss: 0.0454
 1911/10000 [====>.........................] - ETA: 2:00:11 - loss: 0.3946 - regression_loss: 0.3492 - classification_loss: 0.0454
 1912/10000 [====>.........................] - ETA: 2:00:10 - loss: 0.3946 - regression_loss: 0.3493 - classification_loss: 0.0454
 1913/10000 [====>.........................] - ETA: 2:00:09 - loss: 0.3947 - regression_loss: 0.3493 - classification_loss: 0.0454
 1914/10000 [====>.........................] - ETA: 2:00:09 - loss: 0.3948 - regression_loss: 0.3494 - classification_loss: 0.0454
 1915/10000 [====>.........................] - ETA: 2:00:08 - loss: 0.3947 - regression_loss: 0.3493 - classification_loss: 0.0454
 1916/10000 [====>.........................] - ETA: 2:00:07 - loss: 0.3946 - regression_loss: 0.3492 - classification_loss: 0.0454
 1917/10000 [====>.........................] - ETA: 2:00:06 - loss: 0.3945 - regression_loss: 0.3492 - classification_loss: 0.0454
 1918/10000 [====>.........................] - ETA: 2:00:05 - loss: 0.3944 - regression_loss: 0.3491 - classification_loss: 0.0453
 1919/10000 [====>.........................] - ETA: 2:00:04 - loss: 0.3946 - regression_loss: 0.3492 - classification_loss: 0.0454
 1920/10000 [====>.........................] - ETA: 2:00:03 - loss: 0.3945 - regression_loss: 0.3491 - classification_loss: 0.0454
 1921/10000 [====>.........................] - ETA: 2:00:02 - loss: 0.3944 - regression_loss: 0.3491 - classification_loss: 0.0454
 1922/10000 [====>.........................] - ETA: 2:00:01 - loss: 0.3946 - regression_loss: 0.3493 - classification_loss: 0.0454
 1923/10000 [====>.........................] - ETA: 2:00:00 - loss: 0.3948 - regression_loss: 0.3494 - classification_loss: 0.0454
 1924/10000 [====>.........................] - ETA: 2:00:00 - loss: 0.3946 - regression_loss: 0.3492 - classification_loss: 0.0454
 1925/10000 [====>.........................] - ETA: 1:59:59 - loss: 0.3945 - regression_loss: 0.3492 - classification_loss: 0.0454
 1926/10000 [====>.........................] - ETA: 1:59:58 - loss: 0.3945 - regression_loss: 0.3491 - classification_loss: 0.0454
 1927/10000 [====>.........................] - ETA: 1:59:57 - loss: 0.3945 - regression_loss: 0.3492 - classification_loss: 0.0454
 1928/10000 [====>.........................] - ETA: 1:59:56 - loss: 0.3944 - regression_loss: 0.3491 - classification_loss: 0.0453
 1929/10000 [====>.........................] - ETA: 1:59:55 - loss: 0.3944 - regression_loss: 0.3490 - classification_loss: 0.0453
 1930/10000 [====>.........................] - ETA: 1:59:54 - loss: 0.3944 - regression_loss: 0.3491 - classification_loss: 0.0453
 1931/10000 [====>.........................] - ETA: 1:59:53 - loss: 0.3946 - regression_loss: 0.3492 - classification_loss: 0.0454
 1932/10000 [====>.........................] - ETA: 1:59:52 - loss: 0.3945 - regression_loss: 0.3491 - classification_loss: 0.0454
 1933/10000 [====>.........................] - ETA: 1:59:51 - loss: 0.3946 - regression_loss: 0.3492 - classification_loss: 0.0454
 1934/10000 [====>.........................] - ETA: 1:59:50 - loss: 0.3947 - regression_loss: 0.3492 - classification_loss: 0.0454
 1935/10000 [====>.........................] - ETA: 1:59:50 - loss: 0.3947 - regression_loss: 0.3493 - classification_loss: 0.0454
 1936/10000 [====>.........................] - ETA: 1:59:49 - loss: 0.3946 - regression_loss: 0.3492 - classification_loss: 0.0454
 1937/10000 [====>.........................] - ETA: 1:59:48 - loss: 0.3945 - regression_loss: 0.3491 - classification_loss: 0.0454
 1938/10000 [====>.........................] - ETA: 1:59:47 - loss: 0.3944 - regression_loss: 0.3490 - classification_loss: 0.0454
 1939/10000 [====>.........................] - ETA: 1:59:46 - loss: 0.3944 - regression_loss: 0.3490 - classification_loss: 0.0453
 1940/10000 [====>.........................] - ETA: 1:59:45 - loss: 0.3943 - regression_loss: 0.3490 - classification_loss: 0.0453
 1941/10000 [====>.........................] - ETA: 1:59:44 - loss: 0.3943 - regression_loss: 0.3490 - classification_loss: 0.0453
 1942/10000 [====>.........................] - ETA: 1:59:43 - loss: 0.3946 - regression_loss: 0.3492 - classification_loss: 0.0453
 1943/10000 [====>.........................] - ETA: 1:59:42 - loss: 0.3945 - regression_loss: 0.3492 - classification_loss: 0.0453
 1944/10000 [====>.........................] - ETA: 1:59:41 - loss: 0.3945 - regression_loss: 0.3492 - classification_loss: 0.0453
 1945/10000 [====>.........................] - ETA: 1:59:41 - loss: 0.3945 - regression_loss: 0.3492 - classification_loss: 0.0453
 1946/10000 [====>.........................] - ETA: 1:59:40 - loss: 0.3944 - regression_loss: 0.3491 - classification_loss: 0.0453
 1947/10000 [====>.........................] - ETA: 1:59:39 - loss: 0.3942 - regression_loss: 0.3490 - classification_loss: 0.0453
 1948/10000 [====>.........................] - ETA: 1:59:38 - loss: 0.3945 - regression_loss: 0.3492 - classification_loss: 0.0453
 1949/10000 [====>.........................] - ETA: 1:59:37 - loss: 0.3945 - regression_loss: 0.3492 - classification_loss: 0.0453
 1950/10000 [====>.........................] - ETA: 1:59:36 - loss: 0.3947 - regression_loss: 0.3494 - classification_loss: 0.0453
 1951/10000 [====>.........................] - ETA: 1:59:35 - loss: 0.3947 - regression_loss: 0.3494 - classification_loss: 0.0453
 1952/10000 [====>.........................] - ETA: 1:59:34 - loss: 0.3950 - regression_loss: 0.3497 - classification_loss: 0.0453
 1953/10000 [====>.........................] - ETA: 1:59:33 - loss: 0.3949 - regression_loss: 0.3496 - classification_loss: 0.0453
 1954/10000 [====>.........................] - ETA: 1:59:33 - loss: 0.3947 - regression_loss: 0.3494 - classification_loss: 0.0453
 1955/10000 [====>.........................] - ETA: 1:59:32 - loss: 0.3948 - regression_loss: 0.3495 - classification_loss: 0.0453
 1956/10000 [====>.........................] - ETA: 1:59:31 - loss: 0.3952 - regression_loss: 0.3499 - classification_loss: 0.0454
 1957/10000 [====>.........................] - ETA: 1:59:30 - loss: 0.3950 - regression_loss: 0.3497 - classification_loss: 0.0453
 1958/10000 [====>.........................] - ETA: 1:59:29 - loss: 0.3951 - regression_loss: 0.3497 - classification_loss: 0.0454
 1959/10000 [====>.........................] - ETA: 1:59:28 - loss: 0.3951 - regression_loss: 0.3497 - classification_loss: 0.0454
 1960/10000 [====>.........................] - ETA: 1:59:27 - loss: 0.3950 - regression_loss: 0.3497 - classification_loss: 0.0453
 1961/10000 [====>.........................] - ETA: 1:59:26 - loss: 0.3950 - regression_loss: 0.3497 - classification_loss: 0.0453
 1962/10000 [====>.........................] - ETA: 1:59:25 - loss: 0.3950 - regression_loss: 0.3497 - classification_loss: 0.0453
 1963/10000 [====>.........................] - ETA: 1:59:24 - loss: 0.3950 - regression_loss: 0.3497 - classification_loss: 0.0453
 1964/10000 [====>.........................] - ETA: 1:59:23 - loss: 0.3949 - regression_loss: 0.3497 - classification_loss: 0.0453
 1965/10000 [====>.........................] - ETA: 1:59:23 - loss: 0.3949 - regression_loss: 0.3496 - classification_loss: 0.0453
 1966/10000 [====>.........................] - ETA: 1:59:22 - loss: 0.3950 - regression_loss: 0.3498 - classification_loss: 0.0453
 1967/10000 [====>.........................] - ETA: 1:59:21 - loss: 0.3951 - regression_loss: 0.3498 - classification_loss: 0.0453
 1968/10000 [====>.........................] - ETA: 1:59:20 - loss: 0.3950 - regression_loss: 0.3497 - classification_loss: 0.0453
 1969/10000 [====>.........................] - ETA: 1:59:19 - loss: 0.3950 - regression_loss: 0.3498 - classification_loss: 0.0453
 1970/10000 [====>.........................] - ETA: 1:59:18 - loss: 0.3949 - regression_loss: 0.3497 - classification_loss: 0.0452
 1971/10000 [====>.........................] - ETA: 1:59:17 - loss: 0.3949 - regression_loss: 0.3497 - classification_loss: 0.0452
 1972/10000 [====>.........................] - ETA: 1:59:16 - loss: 0.3947 - regression_loss: 0.3495 - classification_loss: 0.0452
 1973/10000 [====>.........................] - ETA: 1:59:15 - loss: 0.3952 - regression_loss: 0.3500 - classification_loss: 0.0452
 1974/10000 [====>.........................] - ETA: 1:59:14 - loss: 0.3951 - regression_loss: 0.3499 - classification_loss: 0.0452
 1975/10000 [====>.........................] - ETA: 1:59:14 - loss: 0.3951 - regression_loss: 0.3499 - classification_loss: 0.0452
 1976/10000 [====>.........................] - ETA: 1:59:13 - loss: 0.3950 - regression_loss: 0.3499 - classification_loss: 0.0452
 1977/10000 [====>.........................] - ETA: 1:59:12 - loss: 0.3951 - regression_loss: 0.3499 - classification_loss: 0.0452
 1978/10000 [====>.........................] - ETA: 1:59:11 - loss: 0.3953 - regression_loss: 0.3501 - classification_loss: 0.0452
 1979/10000 [====>.........................] - ETA: 1:59:10 - loss: 0.3953 - regression_loss: 0.3502 - classification_loss: 0.0452
 1980/10000 [====>.........................] - ETA: 1:59:09 - loss: 0.3954 - regression_loss: 0.3503 - classification_loss: 0.0452
 1981/10000 [====>.........................] - ETA: 1:59:08 - loss: 0.3954 - regression_loss: 0.3502 - classification_loss: 0.0452
 1982/10000 [====>.........................] - ETA: 1:59:07 - loss: 0.3953 - regression_loss: 0.3501 - classification_loss: 0.0451
 1983/10000 [====>.........................] - ETA: 1:59:06 - loss: 0.3953 - regression_loss: 0.3502 - classification_loss: 0.0451
 1984/10000 [====>.........................] - ETA: 1:59:05 - loss: 0.3954 - regression_loss: 0.3503 - classification_loss: 0.0451
 1985/10000 [====>.........................] - ETA: 1:59:05 - loss: 0.3956 - regression_loss: 0.3504 - classification_loss: 0.0453
 1986/10000 [====>.........................] - ETA: 1:59:04 - loss: 0.3956 - regression_loss: 0.3503 - classification_loss: 0.0453
 1987/10000 [====>.........................] - ETA: 1:59:03 - loss: 0.3959 - regression_loss: 0.3507 - classification_loss: 0.0452
 1988/10000 [====>.........................] - ETA: 1:59:02 - loss: 0.3957 - regression_loss: 0.3505 - classification_loss: 0.0452
 1989/10000 [====>.........................] - ETA: 1:59:01 - loss: 0.3958 - regression_loss: 0.3506 - classification_loss: 0.0452
 1990/10000 [====>.........................] - ETA: 1:59:00 - loss: 0.3958 - regression_loss: 0.3506 - classification_loss: 0.0452
 1991/10000 [====>.........................] - ETA: 1:58:59 - loss: 0.3959 - regression_loss: 0.3507 - classification_loss: 0.0452
 1992/10000 [====>.........................] - ETA: 1:58:58 - loss: 0.3959 - regression_loss: 0.3507 - classification_loss: 0.0452
 1993/10000 [====>.........................] - ETA: 1:58:57 - loss: 0.3960 - regression_loss: 0.3508 - classification_loss: 0.0452
 1994/10000 [====>.........................] - ETA: 1:58:56 - loss: 0.3959 - regression_loss: 0.3507 - classification_loss: 0.0452
 1995/10000 [====>.........................] - ETA: 1:58:55 - loss: 0.3958 - regression_loss: 0.3506 - classification_loss: 0.0452
 1996/10000 [====>.........................] - ETA: 1:58:55 - loss: 0.3958 - regression_loss: 0.3506 - classification_loss: 0.0452
 1997/10000 [====>.........................] - ETA: 1:58:54 - loss: 0.3957 - regression_loss: 0.3505 - classification_loss: 0.0452
 1998/10000 [====>.........................] - ETA: 1:58:53 - loss: 0.3956 - regression_loss: 0.3505 - classification_loss: 0.0452
 1999/10000 [====>.........................] - ETA: 1:58:52 - loss: 0.3956 - regression_loss: 0.3504 - classification_loss: 0.0452
 2000/10000 [=====>........................] - ETA: 1:58:51 - loss: 0.3955 - regression_loss: 0.3504 - classification_loss: 0.0452
 2001/10000 [=====>........................] - ETA: 1:58:50 - loss: 0.3955 - regression_loss: 0.3504 - classification_loss: 0.0452
 2002/10000 [=====>........................] - ETA: 1:58:49 - loss: 0.3956 - regression_loss: 0.3504 - classification_loss: 0.0452
 2003/10000 [=====>........................] - ETA: 1:58:48 - loss: 0.3956 - regression_loss: 0.3505 - classification_loss: 0.0451
 2004/10000 [=====>........................] - ETA: 1:58:47 - loss: 0.3956 - regression_loss: 0.3504 - classification_loss: 0.0451
 2005/10000 [=====>........................] - ETA: 1:58:46 - loss: 0.3955 - regression_loss: 0.3504 - classification_loss: 0.0451
 2006/10000 [=====>........................] - ETA: 1:58:46 - loss: 0.3957 - regression_loss: 0.3505 - classification_loss: 0.0451
 2007/10000 [=====>........................] - ETA: 1:58:45 - loss: 0.3957 - regression_loss: 0.3505 - classification_loss: 0.0452
 2008/10000 [=====>........................] - ETA: 1:58:44 - loss: 0.3956 - regression_loss: 0.3504 - classification_loss: 0.0452
 2009/10000 [=====>........................] - ETA: 1:58:43 - loss: 0.3957 - regression_loss: 0.3506 - classification_loss: 0.0452
 2010/10000 [=====>........................] - ETA: 1:58:42 - loss: 0.3957 - regression_loss: 0.3505 - classification_loss: 0.0451
 2011/10000 [=====>........................] - ETA: 1:58:41 - loss: 0.3956 - regression_loss: 0.3505 - classification_loss: 0.0451
 2012/10000 [=====>........................] - ETA: 1:58:40 - loss: 0.3954 - regression_loss: 0.3503 - classification_loss: 0.0451
 2013/10000 [=====>........................] - ETA: 1:58:39 - loss: 0.3954 - regression_loss: 0.3503 - classification_loss: 0.0451
 2014/10000 [=====>........................] - ETA: 1:58:38 - loss: 0.3954 - regression_loss: 0.3503 - classification_loss: 0.0451
 2015/10000 [=====>........................] - ETA: 1:58:37 - loss: 0.3953 - regression_loss: 0.3503 - classification_loss: 0.0451
 2016/10000 [=====>........................] - ETA: 1:58:37 - loss: 0.3955 - regression_loss: 0.3504 - classification_loss: 0.0451
 2017/10000 [=====>........................] - ETA: 1:58:36 - loss: 0.3955 - regression_loss: 0.3504 - classification_loss: 0.0451
 2018/10000 [=====>........................] - ETA: 1:58:35 - loss: 0.3958 - regression_loss: 0.3507 - classification_loss: 0.0451
 2019/10000 [=====>........................] - ETA: 1:58:34 - loss: 0.3957 - regression_loss: 0.3506 - classification_loss: 0.0451
 2020/10000 [=====>........................] - ETA: 1:58:33 - loss: 0.3957 - regression_loss: 0.3507 - classification_loss: 0.0451
 2021/10000 [=====>........................] - ETA: 1:58:32 - loss: 0.3955 - regression_loss: 0.3505 - classification_loss: 0.0450
 2022/10000 [=====>........................] - ETA: 1:58:31 - loss: 0.3960 - regression_loss: 0.3509 - classification_loss: 0.0451
 2023/10000 [=====>........................] - ETA: 1:58:30 - loss: 0.3963 - regression_loss: 0.3512 - classification_loss: 0.0451
 2024/10000 [=====>........................] - ETA: 1:58:29 - loss: 0.3963 - regression_loss: 0.3512 - classification_loss: 0.0451
 2025/10000 [=====>........................] - ETA: 1:58:28 - loss: 0.3964 - regression_loss: 0.3513 - classification_loss: 0.0451
 2026/10000 [=====>........................] - ETA: 1:58:27 - loss: 0.3963 - regression_loss: 0.3512 - classification_loss: 0.0451
 2027/10000 [=====>........................] - ETA: 1:58:27 - loss: 0.3964 - regression_loss: 0.3513 - classification_loss: 0.0451
 2028/10000 [=====>........................] - ETA: 1:58:26 - loss: 0.3963 - regression_loss: 0.3512 - classification_loss: 0.0451
 2029/10000 [=====>........................] - ETA: 1:58:25 - loss: 0.3965 - regression_loss: 0.3513 - classification_loss: 0.0451
 2030/10000 [=====>........................] - ETA: 1:58:24 - loss: 0.3966 - regression_loss: 0.3514 - classification_loss: 0.0452
 2031/10000 [=====>........................] - ETA: 1:58:23 - loss: 0.3965 - regression_loss: 0.3513 - classification_loss: 0.0452
 2032/10000 [=====>........................] - ETA: 1:58:22 - loss: 0.3969 - regression_loss: 0.3517 - classification_loss: 0.0452
 2033/10000 [=====>........................] - ETA: 1:58:21 - loss: 0.3969 - regression_loss: 0.3517 - classification_loss: 0.0452
 2034/10000 [=====>........................] - ETA: 1:58:20 - loss: 0.3971 - regression_loss: 0.3518 - classification_loss: 0.0452
 2035/10000 [=====>........................] - ETA: 1:58:19 - loss: 0.3971 - regression_loss: 0.3518 - classification_loss: 0.0452
 2036/10000 [=====>........................] - ETA: 1:58:18 - loss: 0.3972 - regression_loss: 0.3519 - classification_loss: 0.0452
 2037/10000 [=====>........................] - ETA: 1:58:18 - loss: 0.3973 - regression_loss: 0.3521 - classification_loss: 0.0452
 2038/10000 [=====>........................] - ETA: 1:58:17 - loss: 0.3972 - regression_loss: 0.3520 - classification_loss: 0.0452
 2039/10000 [=====>........................] - ETA: 1:58:16 - loss: 0.3972 - regression_loss: 0.3520 - classification_loss: 0.0452
 2040/10000 [=====>........................] - ETA: 1:58:15 - loss: 0.3974 - regression_loss: 0.3522 - classification_loss: 0.0452
 2041/10000 [=====>........................] - ETA: 1:58:14 - loss: 0.3973 - regression_loss: 0.3521 - classification_loss: 0.0452
 2042/10000 [=====>........................] - ETA: 1:58:13 - loss: 0.3974 - regression_loss: 0.3522 - classification_loss: 0.0452
 2043/10000 [=====>........................] - ETA: 1:58:12 - loss: 0.3974 - regression_loss: 0.3522 - classification_loss: 0.0452
 2044/10000 [=====>........................] - ETA: 1:58:11 - loss: 0.3973 - regression_loss: 0.3522 - classification_loss: 0.0452
 2045/10000 [=====>........................] - ETA: 1:58:10 - loss: 0.3973 - regression_loss: 0.3522 - classification_loss: 0.0451
 2046/10000 [=====>........................] - ETA: 1:58:09 - loss: 0.3972 - regression_loss: 0.3521 - classification_loss: 0.0451
 2047/10000 [=====>........................] - ETA: 1:58:09 - loss: 0.3972 - regression_loss: 0.3521 - classification_loss: 0.0451
 2048/10000 [=====>........................] - ETA: 1:58:08 - loss: 0.3971 - regression_loss: 0.3520 - classification_loss: 0.0451
 2049/10000 [=====>........................] - ETA: 1:58:07 - loss: 0.3972 - regression_loss: 0.3520 - classification_loss: 0.0451
 2050/10000 [=====>........................] - ETA: 1:58:06 - loss: 0.3971 - regression_loss: 0.3520 - classification_loss: 0.0451
 2051/10000 [=====>........................] - ETA: 1:58:05 - loss: 0.3969 - regression_loss: 0.3518 - classification_loss: 0.0451
 2052/10000 [=====>........................] - ETA: 1:58:04 - loss: 0.3969 - regression_loss: 0.3518 - classification_loss: 0.0451
 2053/10000 [=====>........................] - ETA: 1:58:03 - loss: 0.3969 - regression_loss: 0.3518 - classification_loss: 0.0451
 2054/10000 [=====>........................] - ETA: 1:58:02 - loss: 0.3969 - regression_loss: 0.3519 - classification_loss: 0.0451
 2055/10000 [=====>........................] - ETA: 1:58:01 - loss: 0.3972 - regression_loss: 0.3520 - classification_loss: 0.0451
 2056/10000 [=====>........................] - ETA: 1:58:00 - loss: 0.3972 - regression_loss: 0.3521 - classification_loss: 0.0451
 2057/10000 [=====>........................] - ETA: 1:57:59 - loss: 0.3971 - regression_loss: 0.3520 - classification_loss: 0.0451
 2058/10000 [=====>........................] - ETA: 1:57:59 - loss: 0.3972 - regression_loss: 0.3521 - classification_loss: 0.0451
 2059/10000 [=====>........................] - ETA: 1:57:58 - loss: 0.3973 - regression_loss: 0.3522 - classification_loss: 0.0451
 2060/10000 [=====>........................] - ETA: 1:57:57 - loss: 0.3973 - regression_loss: 0.3522 - classification_loss: 0.0451
 2061/10000 [=====>........................] - ETA: 1:57:56 - loss: 0.3972 - regression_loss: 0.3522 - classification_loss: 0.0450
 2062/10000 [=====>........................] - ETA: 1:57:55 - loss: 0.3973 - regression_loss: 0.3522 - classification_loss: 0.0451
 2063/10000 [=====>........................] - ETA: 1:57:54 - loss: 0.3974 - regression_loss: 0.3523 - classification_loss: 0.0451
 2064/10000 [=====>........................] - ETA: 1:57:53 - loss: 0.3973 - regression_loss: 0.3522 - classification_loss: 0.0451
 2065/10000 [=====>........................] - ETA: 1:57:52 - loss: 0.3975 - regression_loss: 0.3524 - classification_loss: 0.0451
 2066/10000 [=====>........................] - ETA: 1:57:51 - loss: 0.3974 - regression_loss: 0.3523 - classification_loss: 0.0451
 2067/10000 [=====>........................] - ETA: 1:57:50 - loss: 0.3975 - regression_loss: 0.3524 - classification_loss: 0.0451
 2068/10000 [=====>........................] - ETA: 1:57:50 - loss: 0.3975 - regression_loss: 0.3524 - classification_loss: 0.0451
 2069/10000 [=====>........................] - ETA: 1:57:49 - loss: 0.3976 - regression_loss: 0.3525 - classification_loss: 0.0451
 2070/10000 [=====>........................] - ETA: 1:57:48 - loss: 0.3974 - regression_loss: 0.3524 - classification_loss: 0.0451
 2071/10000 [=====>........................] - ETA: 1:57:47 - loss: 0.3976 - regression_loss: 0.3525 - classification_loss: 0.0451
 2072/10000 [=====>........................] - ETA: 1:57:46 - loss: 0.3974 - regression_loss: 0.3523 - classification_loss: 0.0451
 2073/10000 [=====>........................] - ETA: 1:57:45 - loss: 0.3974 - regression_loss: 0.3523 - classification_loss: 0.0451
 2074/10000 [=====>........................] - ETA: 1:57:44 - loss: 0.3975 - regression_loss: 0.3524 - classification_loss: 0.0451
 2075/10000 [=====>........................] - ETA: 1:57:43 - loss: 0.3975 - regression_loss: 0.3524 - classification_loss: 0.0450
 2076/10000 [=====>........................] - ETA: 1:57:42 - loss: 0.3974 - regression_loss: 0.3524 - classification_loss: 0.0451
 2077/10000 [=====>........................] - ETA: 1:57:41 - loss: 0.3974 - regression_loss: 0.3523 - classification_loss: 0.0450
 2078/10000 [=====>........................] - ETA: 1:57:40 - loss: 0.3974 - regression_loss: 0.3524 - classification_loss: 0.0450
 2079/10000 [=====>........................] - ETA: 1:57:40 - loss: 0.3974 - regression_loss: 0.3524 - classification_loss: 0.0450
 2080/10000 [=====>........................] - ETA: 1:57:39 - loss: 0.3973 - regression_loss: 0.3523 - classification_loss: 0.0450
 2081/10000 [=====>........................] - ETA: 1:57:38 - loss: 0.3974 - regression_loss: 0.3524 - classification_loss: 0.0450
 2082/10000 [=====>........................] - ETA: 1:57:37 - loss: 0.3975 - regression_loss: 0.3524 - classification_loss: 0.0450
 2083/10000 [=====>........................] - ETA: 1:57:36 - loss: 0.3975 - regression_loss: 0.3525 - classification_loss: 0.0450
 2084/10000 [=====>........................] - ETA: 1:57:35 - loss: 0.3976 - regression_loss: 0.3526 - classification_loss: 0.0450
 2085/10000 [=====>........................] - ETA: 1:57:34 - loss: 0.3976 - regression_loss: 0.3526 - classification_loss: 0.0450
 2086/10000 [=====>........................] - ETA: 1:57:33 - loss: 0.3976 - regression_loss: 0.3526 - classification_loss: 0.0450
 2087/10000 [=====>........................] - ETA: 1:57:32 - loss: 0.3977 - regression_loss: 0.3527 - classification_loss: 0.0450
 2088/10000 [=====>........................] - ETA: 1:57:31 - loss: 0.3977 - regression_loss: 0.3527 - classification_loss: 0.0450
 2089/10000 [=====>........................] - ETA: 1:57:31 - loss: 0.3977 - regression_loss: 0.3528 - classification_loss: 0.0449
 2090/10000 [=====>........................] - ETA: 1:57:30 - loss: 0.3977 - regression_loss: 0.3528 - classification_loss: 0.0449
 2091/10000 [=====>........................] - ETA: 1:57:29 - loss: 0.3976 - regression_loss: 0.3527 - classification_loss: 0.0449
 2092/10000 [=====>........................] - ETA: 1:57:28 - loss: 0.3980 - regression_loss: 0.3530 - classification_loss: 0.0450
 2093/10000 [=====>........................] - ETA: 1:57:27 - loss: 0.3979 - regression_loss: 0.3530 - classification_loss: 0.0450
 2094/10000 [=====>........................] - ETA: 1:57:26 - loss: 0.3980 - regression_loss: 0.3530 - classification_loss: 0.0450
 2095/10000 [=====>........................] - ETA: 1:57:25 - loss: 0.3980 - regression_loss: 0.3530 - classification_loss: 0.0450
 2096/10000 [=====>........................] - ETA: 1:57:24 - loss: 0.3980 - regression_loss: 0.3530 - classification_loss: 0.0450
 2097/10000 [=====>........................] - ETA: 1:57:23 - loss: 0.3980 - regression_loss: 0.3530 - classification_loss: 0.0450
 2098/10000 [=====>........................] - ETA: 1:57:22 - loss: 0.3979 - regression_loss: 0.3529 - classification_loss: 0.0450
 2099/10000 [=====>........................] - ETA: 1:57:21 - loss: 0.3977 - regression_loss: 0.3528 - classification_loss: 0.0449
 2100/10000 [=====>........................] - ETA: 1:57:21 - loss: 0.3977 - regression_loss: 0.3528 - classification_loss: 0.0449
 2101/10000 [=====>........................] - ETA: 1:57:20 - loss: 0.3977 - regression_loss: 0.3528 - classification_loss: 0.0449
 2102/10000 [=====>........................] - ETA: 1:57:19 - loss: 0.3976 - regression_loss: 0.3527 - classification_loss: 0.0449
 2103/10000 [=====>........................] - ETA: 1:57:18 - loss: 0.3977 - regression_loss: 0.3528 - classification_loss: 0.0449
 2104/10000 [=====>........................] - ETA: 1:57:17 - loss: 0.3978 - regression_loss: 0.3528 - classification_loss: 0.0449
 2105/10000 [=====>........................] - ETA: 1:57:16 - loss: 0.3978 - regression_loss: 0.3529 - classification_loss: 0.0450
 2106/10000 [=====>........................] - ETA: 1:57:15 - loss: 0.3978 - regression_loss: 0.3528 - classification_loss: 0.0450
 2107/10000 [=====>........................] - ETA: 1:57:14 - loss: 0.3978 - regression_loss: 0.3529 - classification_loss: 0.0449
 2108/10000 [=====>........................] - ETA: 1:57:13 - loss: 0.3979 - regression_loss: 0.3530 - classification_loss: 0.0449
 2109/10000 [=====>........................] - ETA: 1:57:13 - loss: 0.3979 - regression_loss: 0.3530 - classification_loss: 0.0449
 2110/10000 [=====>........................] - ETA: 1:57:12 - loss: 0.3977 - regression_loss: 0.3528 - classification_loss: 0.0449
 2111/10000 [=====>........................] - ETA: 1:57:11 - loss: 0.3976 - regression_loss: 0.3528 - classification_loss: 0.0449
 2112/10000 [=====>........................] - ETA: 1:57:10 - loss: 0.3977 - regression_loss: 0.3528 - classification_loss: 0.0449
 2113/10000 [=====>........................] - ETA: 1:57:09 - loss: 0.3977 - regression_loss: 0.3528 - classification_loss: 0.0448
 2114/10000 [=====>........................] - ETA: 1:57:08 - loss: 0.3976 - regression_loss: 0.3528 - classification_loss: 0.0448
 2115/10000 [=====>........................] - ETA: 1:57:07 - loss: 0.3977 - regression_loss: 0.3529 - classification_loss: 0.0448
 2116/10000 [=====>........................] - ETA: 1:57:06 - loss: 0.3984 - regression_loss: 0.3535 - classification_loss: 0.0449
 2117/10000 [=====>........................] - ETA: 1:57:05 - loss: 0.3982 - regression_loss: 0.3533 - classification_loss: 0.0449
 2118/10000 [=====>........................] - ETA: 1:57:04 - loss: 0.3982 - regression_loss: 0.3533 - classification_loss: 0.0449
 2119/10000 [=====>........................] - ETA: 1:57:03 - loss: 0.3981 - regression_loss: 0.3532 - classification_loss: 0.0449
 2120/10000 [=====>........................] - ETA: 1:57:03 - loss: 0.3980 - regression_loss: 0.3532 - classification_loss: 0.0449
 2121/10000 [=====>........................] - ETA: 1:57:02 - loss: 0.3980 - regression_loss: 0.3532 - classification_loss: 0.0449
 2122/10000 [=====>........................] - ETA: 1:57:01 - loss: 0.3980 - regression_loss: 0.3531 - classification_loss: 0.0449
 2123/10000 [=====>........................] - ETA: 1:57:00 - loss: 0.3981 - regression_loss: 0.3531 - classification_loss: 0.0449
 2124/10000 [=====>........................] - ETA: 1:56:59 - loss: 0.3981 - regression_loss: 0.3532 - classification_loss: 0.0449
 2125/10000 [=====>........................] - ETA: 1:56:58 - loss: 0.3979 - regression_loss: 0.3530 - classification_loss: 0.0449
 2126/10000 [=====>........................] - ETA: 1:56:57 - loss: 0.3979 - regression_loss: 0.3530 - classification_loss: 0.0449
 2127/10000 [=====>........................] - ETA: 1:56:56 - loss: 0.3979 - regression_loss: 0.3530 - classification_loss: 0.0449
 2128/10000 [=====>........................] - ETA: 1:56:55 - loss: 0.3987 - regression_loss: 0.3538 - classification_loss: 0.0449
 2129/10000 [=====>........................] - ETA: 1:56:54 - loss: 0.3987 - regression_loss: 0.3538 - classification_loss: 0.0449
 2130/10000 [=====>........................] - ETA: 1:56:53 - loss: 0.3986 - regression_loss: 0.3537 - classification_loss: 0.0449
 2131/10000 [=====>........................] - ETA: 1:56:53 - loss: 0.3985 - regression_loss: 0.3537 - classification_loss: 0.0449
 2132/10000 [=====>........................] - ETA: 1:56:52 - loss: 0.3985 - regression_loss: 0.3537 - classification_loss: 0.0449
 2133/10000 [=====>........................] - ETA: 1:56:51 - loss: 0.3985 - regression_loss: 0.3537 - classification_loss: 0.0449
 2134/10000 [=====>........................] - ETA: 1:56:50 - loss: 0.3986 - regression_loss: 0.3538 - classification_loss: 0.0449
 2135/10000 [=====>........................] - ETA: 1:56:49 - loss: 0.3990 - regression_loss: 0.3542 - classification_loss: 0.0448
 2136/10000 [=====>........................] - ETA: 1:56:48 - loss: 0.3990 - regression_loss: 0.3542 - classification_loss: 0.0448
 2137/10000 [=====>........................] - ETA: 1:56:47 - loss: 0.3990 - regression_loss: 0.3542 - classification_loss: 0.0448
 2138/10000 [=====>........................] - ETA: 1:56:46 - loss: 0.3990 - regression_loss: 0.3542 - classification_loss: 0.0448
 2139/10000 [=====>........................] - ETA: 1:56:45 - loss: 0.3989 - regression_loss: 0.3541 - classification_loss: 0.0448
 2140/10000 [=====>........................] - ETA: 1:56:44 - loss: 0.3988 - regression_loss: 0.3540 - classification_loss: 0.0448
 2141/10000 [=====>........................] - ETA: 1:56:44 - loss: 0.3988 - regression_loss: 0.3540 - classification_loss: 0.0448
 2142/10000 [=====>........................] - ETA: 1:56:43 - loss: 0.3988 - regression_loss: 0.3540 - classification_loss: 0.0448
 2143/10000 [=====>........................] - ETA: 1:56:42 - loss: 0.3988 - regression_loss: 0.3540 - classification_loss: 0.0448
 2144/10000 [=====>........................] - ETA: 1:56:41 - loss: 0.3987 - regression_loss: 0.3538 - classification_loss: 0.0448
 2145/10000 [=====>........................] - ETA: 1:56:40 - loss: 0.3987 - regression_loss: 0.3539 - classification_loss: 0.0448
 2146/10000 [=====>........................] - ETA: 1:56:39 - loss: 0.3986 - regression_loss: 0.3539 - classification_loss: 0.0448
 2147/10000 [=====>........................] - ETA: 1:56:38 - loss: 0.3986 - regression_loss: 0.3538 - classification_loss: 0.0448
 2148/10000 [=====>........................] - ETA: 1:56:37 - loss: 0.3987 - regression_loss: 0.3539 - classification_loss: 0.0448
 2149/10000 [=====>........................] - ETA: 1:56:36 - loss: 0.3987 - regression_loss: 0.3539 - classification_loss: 0.0448
 2150/10000 [=====>........................] - ETA: 1:56:35 - loss: 0.3987 - regression_loss: 0.3539 - classification_loss: 0.0447
 2151/10000 [=====>........................] - ETA: 1:56:35 - loss: 0.3986 - regression_loss: 0.3538 - classification_loss: 0.0447
 2152/10000 [=====>........................] - ETA: 1:56:34 - loss: 0.3989 - regression_loss: 0.3541 - classification_loss: 0.0448
 2153/10000 [=====>........................] - ETA: 1:56:33 - loss: 0.3988 - regression_loss: 0.3541 - classification_loss: 0.0448
 2154/10000 [=====>........................] - ETA: 1:56:32 - loss: 0.3990 - regression_loss: 0.3542 - classification_loss: 0.0448
 2155/10000 [=====>........................] - ETA: 1:56:31 - loss: 0.3991 - regression_loss: 0.3543 - classification_loss: 0.0448
 2156/10000 [=====>........................] - ETA: 1:56:30 - loss: 0.3990 - regression_loss: 0.3542 - classification_loss: 0.0448
 2157/10000 [=====>........................] - ETA: 1:56:29 - loss: 0.3989 - regression_loss: 0.3542 - classification_loss: 0.0448
 2158/10000 [=====>........................] - ETA: 1:56:28 - loss: 0.3989 - regression_loss: 0.3541 - classification_loss: 0.0448
 2159/10000 [=====>........................] - ETA: 1:56:27 - loss: 0.3990 - regression_loss: 0.3541 - classification_loss: 0.0448
 2160/10000 [=====>........................] - ETA: 1:56:26 - loss: 0.3990 - regression_loss: 0.3542 - classification_loss: 0.0448
 2161/10000 [=====>........................] - ETA: 1:56:25 - loss: 0.3990 - regression_loss: 0.3542 - classification_loss: 0.0448
 2162/10000 [=====>........................] - ETA: 1:56:25 - loss: 0.3990 - regression_loss: 0.3542 - classification_loss: 0.0448
 2163/10000 [=====>........................] - ETA: 1:56:24 - loss: 0.3991 - regression_loss: 0.3542 - classification_loss: 0.0449
 2164/10000 [=====>........................] - ETA: 1:56:23 - loss: 0.3990 - regression_loss: 0.3541 - classification_loss: 0.0449
 2165/10000 [=====>........................] - ETA: 1:56:22 - loss: 0.3989 - regression_loss: 0.3541 - classification_loss: 0.0449
 2166/10000 [=====>........................] - ETA: 1:56:21 - loss: 0.3990 - regression_loss: 0.3541 - classification_loss: 0.0448
 2167/10000 [=====>........................] - ETA: 1:56:20 - loss: 0.3990 - regression_loss: 0.3541 - classification_loss: 0.0448
 2168/10000 [=====>........................] - ETA: 1:56:19 - loss: 0.3989 - regression_loss: 0.3541 - classification_loss: 0.0448
 2169/10000 [=====>........................] - ETA: 1:56:18 - loss: 0.3988 - regression_loss: 0.3541 - classification_loss: 0.0448
 2170/10000 [=====>........................] - ETA: 1:56:17 - loss: 0.3988 - regression_loss: 0.3540 - classification_loss: 0.0448
 2171/10000 [=====>........................] - ETA: 1:56:16 - loss: 0.3988 - regression_loss: 0.3540 - classification_loss: 0.0448
 2172/10000 [=====>........................] - ETA: 1:56:16 - loss: 0.3989 - regression_loss: 0.3541 - classification_loss: 0.0448
 2173/10000 [=====>........................] - ETA: 1:56:15 - loss: 0.3988 - regression_loss: 0.3541 - classification_loss: 0.0448
 2174/10000 [=====>........................] - ETA: 1:56:14 - loss: 0.3989 - regression_loss: 0.3540 - classification_loss: 0.0449
 2175/10000 [=====>........................] - ETA: 1:56:13 - loss: 0.3989 - regression_loss: 0.3540 - classification_loss: 0.0449
 2176/10000 [=====>........................] - ETA: 1:56:12 - loss: 0.3988 - regression_loss: 0.3539 - classification_loss: 0.0449
 2177/10000 [=====>........................] - ETA: 1:56:11 - loss: 0.3988 - regression_loss: 0.3539 - classification_loss: 0.0449
 2178/10000 [=====>........................] - ETA: 1:56:10 - loss: 0.3988 - regression_loss: 0.3540 - classification_loss: 0.0449
 2179/10000 [=====>........................] - ETA: 1:56:09 - loss: 0.3988 - regression_loss: 0.3539 - classification_loss: 0.0448
 2180/10000 [=====>........................] - ETA: 1:56:08 - loss: 0.3988 - regression_loss: 0.3539 - classification_loss: 0.0449
 2181/10000 [=====>........................] - ETA: 1:56:07 - loss: 0.3988 - regression_loss: 0.3539 - classification_loss: 0.0448
 2182/10000 [=====>........................] - ETA: 1:56:07 - loss: 0.3987 - regression_loss: 0.3539 - classification_loss: 0.0448
 2183/10000 [=====>........................] - ETA: 1:56:06 - loss: 0.3987 - regression_loss: 0.3539 - classification_loss: 0.0448
 2184/10000 [=====>........................] - ETA: 1:56:05 - loss: 0.3987 - regression_loss: 0.3538 - classification_loss: 0.0448
 2185/10000 [=====>........................] - ETA: 1:56:04 - loss: 0.3986 - regression_loss: 0.3538 - classification_loss: 0.0448
 2186/10000 [=====>........................] - ETA: 1:56:03 - loss: 0.3984 - regression_loss: 0.3536 - classification_loss: 0.0448
 2187/10000 [=====>........................] - ETA: 1:56:02 - loss: 0.3984 - regression_loss: 0.3536 - classification_loss: 0.0448
 2188/10000 [=====>........................] - ETA: 1:56:01 - loss: 0.3984 - regression_loss: 0.3536 - classification_loss: 0.0448
 2189/10000 [=====>........................] - ETA: 1:56:00 - loss: 0.3984 - regression_loss: 0.3536 - classification_loss: 0.0448
 2190/10000 [=====>........................] - ETA: 1:55:59 - loss: 0.3986 - regression_loss: 0.3538 - classification_loss: 0.0448
 2191/10000 [=====>........................] - ETA: 1:55:58 - loss: 0.3986 - regression_loss: 0.3537 - classification_loss: 0.0448
 2192/10000 [=====>........................] - ETA: 1:55:58 - loss: 0.3985 - regression_loss: 0.3537 - classification_loss: 0.0448
 2193/10000 [=====>........................] - ETA: 1:55:57 - loss: 0.3983 - regression_loss: 0.3535 - classification_loss: 0.0448
 2194/10000 [=====>........................] - ETA: 1:55:56 - loss: 0.3984 - regression_loss: 0.3537 - classification_loss: 0.0448
 2195/10000 [=====>........................] - ETA: 1:55:55 - loss: 0.3983 - regression_loss: 0.3535 - classification_loss: 0.0448
 2196/10000 [=====>........................] - ETA: 1:55:54 - loss: 0.3982 - regression_loss: 0.3535 - classification_loss: 0.0447
 2197/10000 [=====>........................] - ETA: 1:55:53 - loss: 0.3981 - regression_loss: 0.3534 - classification_loss: 0.0447
 2198/10000 [=====>........................] - ETA: 1:55:52 - loss: 0.3981 - regression_loss: 0.3533 - classification_loss: 0.0447
 2199/10000 [=====>........................] - ETA: 1:55:51 - loss: 0.3980 - regression_loss: 0.3533 - classification_loss: 0.0447
 2200/10000 [=====>........................] - ETA: 1:55:50 - loss: 0.3981 - regression_loss: 0.3534 - classification_loss: 0.0447
 2201/10000 [=====>........................] - ETA: 1:55:49 - loss: 0.3981 - regression_loss: 0.3534 - classification_loss: 0.0447
 2202/10000 [=====>........................] - ETA: 1:55:48 - loss: 0.3980 - regression_loss: 0.3533 - classification_loss: 0.0447
 2203/10000 [=====>........................] - ETA: 1:55:48 - loss: 0.3979 - regression_loss: 0.3533 - classification_loss: 0.0447
 2204/10000 [=====>........................] - ETA: 1:55:47 - loss: 0.3983 - regression_loss: 0.3536 - classification_loss: 0.0447
 2205/10000 [=====>........................] - ETA: 1:55:46 - loss: 0.3984 - regression_loss: 0.3536 - classification_loss: 0.0447
 2206/10000 [=====>........................] - ETA: 1:55:45 - loss: 0.3982 - regression_loss: 0.3535 - classification_loss: 0.0447
 2207/10000 [=====>........................] - ETA: 1:55:44 - loss: 0.3982 - regression_loss: 0.3535 - classification_loss: 0.0447
 2208/10000 [=====>........................] - ETA: 1:55:43 - loss: 0.3981 - regression_loss: 0.3534 - classification_loss: 0.0447
 2209/10000 [=====>........................] - ETA: 1:55:42 - loss: 0.3979 - regression_loss: 0.3533 - classification_loss: 0.0447
 2210/10000 [=====>........................] - ETA: 1:55:41 - loss: 0.3979 - regression_loss: 0.3533 - classification_loss: 0.0447
 2211/10000 [=====>........................] - ETA: 1:55:40 - loss: 0.3978 - regression_loss: 0.3532 - classification_loss: 0.0446
 2212/10000 [=====>........................] - ETA: 1:55:39 - loss: 0.3978 - regression_loss: 0.3531 - classification_loss: 0.0446
 2213/10000 [=====>........................] - ETA: 1:55:39 - loss: 0.3979 - regression_loss: 0.3532 - classification_loss: 0.0447
 2214/10000 [=====>........................] - ETA: 1:55:38 - loss: 0.3978 - regression_loss: 0.3532 - classification_loss: 0.0447
 2215/10000 [=====>........................] - ETA: 1:55:37 - loss: 0.3978 - regression_loss: 0.3532 - classification_loss: 0.0447
 2216/10000 [=====>........................] - ETA: 1:55:36 - loss: 0.3978 - regression_loss: 0.3532 - classification_loss: 0.0446
 2217/10000 [=====>........................] - ETA: 1:55:35 - loss: 0.3977 - regression_loss: 0.3531 - classification_loss: 0.0446
 2218/10000 [=====>........................] - ETA: 1:55:34 - loss: 0.3978 - regression_loss: 0.3532 - classification_loss: 0.0446
 2219/10000 [=====>........................] - ETA: 1:55:33 - loss: 0.3977 - regression_loss: 0.3531 - classification_loss: 0.0446
 2220/10000 [=====>........................] - ETA: 1:55:32 - loss: 0.3980 - regression_loss: 0.3533 - classification_loss: 0.0447
 2221/10000 [=====>........................] - ETA: 1:55:31 - loss: 0.3980 - regression_loss: 0.3533 - classification_loss: 0.0447
 2222/10000 [=====>........................] - ETA: 1:55:30 - loss: 0.3979 - regression_loss: 0.3532 - classification_loss: 0.0447
 2223/10000 [=====>........................] - ETA: 1:55:29 - loss: 0.3980 - regression_loss: 0.3533 - classification_loss: 0.0447
 2224/10000 [=====>........................] - ETA: 1:55:29 - loss: 0.3979 - regression_loss: 0.3532 - classification_loss: 0.0447
 2225/10000 [=====>........................] - ETA: 1:55:28 - loss: 0.3979 - regression_loss: 0.3532 - classification_loss: 0.0447
 2226/10000 [=====>........................] - ETA: 1:55:27 - loss: 0.3979 - regression_loss: 0.3532 - classification_loss: 0.0447
 2227/10000 [=====>........................] - ETA: 1:55:26 - loss: 0.3979 - regression_loss: 0.3533 - classification_loss: 0.0447
 2228/10000 [=====>........................] - ETA: 1:55:25 - loss: 0.3980 - regression_loss: 0.3533 - classification_loss: 0.0447
 2229/10000 [=====>........................] - ETA: 1:55:24 - loss: 0.3980 - regression_loss: 0.3533 - classification_loss: 0.0447
 2230/10000 [=====>........................] - ETA: 1:55:23 - loss: 0.3982 - regression_loss: 0.3536 - classification_loss: 0.0447
 2231/10000 [=====>........................] - ETA: 1:55:22 - loss: 0.3981 - regression_loss: 0.3535 - classification_loss: 0.0447
 2232/10000 [=====>........................] - ETA: 1:55:21 - loss: 0.3982 - regression_loss: 0.3536 - classification_loss: 0.0446
 2233/10000 [=====>........................] - ETA: 1:55:20 - loss: 0.3983 - regression_loss: 0.3536 - classification_loss: 0.0447
 2234/10000 [=====>........................] - ETA: 1:55:20 - loss: 0.3985 - regression_loss: 0.3538 - classification_loss: 0.0447
 2235/10000 [=====>........................] - ETA: 1:55:19 - loss: 0.3983 - regression_loss: 0.3536 - classification_loss: 0.0447
 2236/10000 [=====>........................] - ETA: 1:55:18 - loss: 0.3982 - regression_loss: 0.3535 - classification_loss: 0.0447
 2237/10000 [=====>........................] - ETA: 1:55:17 - loss: 0.3986 - regression_loss: 0.3539 - classification_loss: 0.0447
 2238/10000 [=====>........................] - ETA: 1:55:16 - loss: 0.3986 - regression_loss: 0.3539 - classification_loss: 0.0447
 2239/10000 [=====>........................] - ETA: 1:55:15 - loss: 0.3988 - regression_loss: 0.3540 - classification_loss: 0.0448
 2240/10000 [=====>........................] - ETA: 1:55:14 - loss: 0.3988 - regression_loss: 0.3540 - classification_loss: 0.0448
 2241/10000 [=====>........................] - ETA: 1:55:13 - loss: 0.3988 - regression_loss: 0.3541 - classification_loss: 0.0447
 2242/10000 [=====>........................] - ETA: 1:55:12 - loss: 0.3987 - regression_loss: 0.3540 - classification_loss: 0.0447
 2243/10000 [=====>........................] - ETA: 1:55:11 - loss: 0.3987 - regression_loss: 0.3540 - classification_loss: 0.0447
 2244/10000 [=====>........................] - ETA: 1:55:11 - loss: 0.3986 - regression_loss: 0.3539 - classification_loss: 0.0447
 2245/10000 [=====>........................] - ETA: 1:55:10 - loss: 0.3985 - regression_loss: 0.3537 - classification_loss: 0.0447
 2246/10000 [=====>........................] - ETA: 1:55:09 - loss: 0.3987 - regression_loss: 0.3540 - classification_loss: 0.0447
 2247/10000 [=====>........................] - ETA: 1:55:08 - loss: 0.3987 - regression_loss: 0.3540 - classification_loss: 0.0447
 2248/10000 [=====>........................] - ETA: 1:55:07 - loss: 0.3987 - regression_loss: 0.3540 - classification_loss: 0.0447
 2249/10000 [=====>........................] - ETA: 1:55:06 - loss: 0.3987 - regression_loss: 0.3540 - classification_loss: 0.0447
 2250/10000 [=====>........................] - ETA: 1:55:05 - loss: 0.3987 - regression_loss: 0.3540 - classification_loss: 0.0447
 2251/10000 [=====>........................] - ETA: 1:55:04 - loss: 0.3986 - regression_loss: 0.3539 - classification_loss: 0.0446
 2252/10000 [=====>........................] - ETA: 1:55:03 - loss: 0.3985 - regression_loss: 0.3539 - classification_loss: 0.0446
 2253/10000 [=====>........................] - ETA: 1:55:02 - loss: 0.3985 - regression_loss: 0.3539 - classification_loss: 0.0446
 2254/10000 [=====>........................] - ETA: 1:55:01 - loss: 0.3986 - regression_loss: 0.3540 - classification_loss: 0.0446
 2255/10000 [=====>........................] - ETA: 1:55:01 - loss: 0.3985 - regression_loss: 0.3539 - classification_loss: 0.0446
 2256/10000 [=====>........................] - ETA: 1:55:00 - loss: 0.3987 - regression_loss: 0.3540 - classification_loss: 0.0447
 2257/10000 [=====>........................] - ETA: 1:54:59 - loss: 0.3986 - regression_loss: 0.3540 - classification_loss: 0.0447
 2258/10000 [=====>........................] - ETA: 1:54:58 - loss: 0.3988 - regression_loss: 0.3541 - classification_loss: 0.0447
 2259/10000 [=====>........................] - ETA: 1:54:57 - loss: 0.3987 - regression_loss: 0.3540 - classification_loss: 0.0447
 2260/10000 [=====>........................] - ETA: 1:54:56 - loss: 0.3986 - regression_loss: 0.3539 - classification_loss: 0.0446
 2261/10000 [=====>........................] - ETA: 1:54:55 - loss: 0.3985 - regression_loss: 0.3538 - classification_loss: 0.0446
 2262/10000 [=====>........................] - ETA: 1:54:54 - loss: 0.3985 - regression_loss: 0.3539 - classification_loss: 0.0446
 2263/10000 [=====>........................] - ETA: 1:54:53 - loss: 0.3987 - regression_loss: 0.3541 - classification_loss: 0.0446
 2264/10000 [=====>........................] - ETA: 1:54:52 - loss: 0.3987 - regression_loss: 0.3541 - classification_loss: 0.0446
 2265/10000 [=====>........................] - ETA: 1:54:52 - loss: 0.3986 - regression_loss: 0.3540 - classification_loss: 0.0446
 2266/10000 [=====>........................] - ETA: 1:54:51 - loss: 0.3990 - regression_loss: 0.3544 - classification_loss: 0.0446
 2267/10000 [=====>........................] - ETA: 1:54:50 - loss: 0.3988 - regression_loss: 0.3542 - classification_loss: 0.0446
 2268/10000 [=====>........................] - ETA: 1:54:49 - loss: 0.3991 - regression_loss: 0.3545 - classification_loss: 0.0446
 2269/10000 [=====>........................] - ETA: 1:54:48 - loss: 0.3992 - regression_loss: 0.3546 - classification_loss: 0.0447
 2270/10000 [=====>........................] - ETA: 1:54:47 - loss: 0.3992 - regression_loss: 0.3545 - classification_loss: 0.0446
 2271/10000 [=====>........................] - ETA: 1:54:46 - loss: 0.3992 - regression_loss: 0.3545 - classification_loss: 0.0446
 2272/10000 [=====>........................] - ETA: 1:54:45 - loss: 0.3994 - regression_loss: 0.3547 - classification_loss: 0.0446
 2273/10000 [=====>........................] - ETA: 1:54:44 - loss: 0.3993 - regression_loss: 0.3547 - classification_loss: 0.0446
 2274/10000 [=====>........................] - ETA: 1:54:43 - loss: 0.3991 - regression_loss: 0.3545 - classification_loss: 0.0446
 2275/10000 [=====>........................] - ETA: 1:54:43 - loss: 0.3993 - regression_loss: 0.3546 - classification_loss: 0.0447
 2276/10000 [=====>........................] - ETA: 1:54:42 - loss: 0.3993 - regression_loss: 0.3546 - classification_loss: 0.0446
 2277/10000 [=====>........................] - ETA: 1:54:41 - loss: 0.3992 - regression_loss: 0.3545 - classification_loss: 0.0446
 2278/10000 [=====>........................] - ETA: 1:54:40 - loss: 0.3990 - regression_loss: 0.3544 - classification_loss: 0.0446
 2279/10000 [=====>........................] - ETA: 1:54:39 - loss: 0.3988 - regression_loss: 0.3542 - classification_loss: 0.0446
 2280/10000 [=====>........................] - ETA: 1:54:38 - loss: 0.3986 - regression_loss: 0.3541 - classification_loss: 0.0446
 2281/10000 [=====>........................] - ETA: 1:54:37 - loss: 0.3986 - regression_loss: 0.3541 - classification_loss: 0.0446
 2282/10000 [=====>........................] - ETA: 1:54:36 - loss: 0.3987 - regression_loss: 0.3541 - classification_loss: 0.0446
 2283/10000 [=====>........................] - ETA: 1:54:35 - loss: 0.3986 - regression_loss: 0.3540 - classification_loss: 0.0446
 2284/10000 [=====>........................] - ETA: 1:54:34 - loss: 0.3987 - regression_loss: 0.3542 - classification_loss: 0.0446
 2285/10000 [=====>........................] - ETA: 1:54:34 - loss: 0.3987 - regression_loss: 0.3541 - classification_loss: 0.0445
 2286/10000 [=====>........................] - ETA: 1:54:33 - loss: 0.3985 - regression_loss: 0.3540 - classification_loss: 0.0445
 2287/10000 [=====>........................] - ETA: 1:54:32 - loss: 0.3986 - regression_loss: 0.3541 - classification_loss: 0.0445
 2288/10000 [=====>........................] - ETA: 1:54:31 - loss: 0.3986 - regression_loss: 0.3541 - classification_loss: 0.0445
 2289/10000 [=====>........................] - ETA: 1:54:30 - loss: 0.3986 - regression_loss: 0.3540 - classification_loss: 0.0445
 2290/10000 [=====>........................] - ETA: 1:54:29 - loss: 0.3985 - regression_loss: 0.3540 - classification_loss: 0.0445
 2291/10000 [=====>........................] - ETA: 1:54:28 - loss: 0.3994 - regression_loss: 0.3545 - classification_loss: 0.0449
 2292/10000 [=====>........................] - ETA: 1:54:27 - loss: 0.3993 - regression_loss: 0.3544 - classification_loss: 0.0449
 2293/10000 [=====>........................] - ETA: 1:54:26 - loss: 0.3995 - regression_loss: 0.3546 - classification_loss: 0.0449
 2294/10000 [=====>........................] - ETA: 1:54:25 - loss: 0.3995 - regression_loss: 0.3546 - classification_loss: 0.0449
 2295/10000 [=====>........................] - ETA: 1:54:25 - loss: 0.3996 - regression_loss: 0.3547 - classification_loss: 0.0449
 2296/10000 [=====>........................] - ETA: 1:54:24 - loss: 0.3996 - regression_loss: 0.3547 - classification_loss: 0.0449
 2297/10000 [=====>........................] - ETA: 1:54:23 - loss: 0.3996 - regression_loss: 0.3547 - classification_loss: 0.0449
 2298/10000 [=====>........................] - ETA: 1:54:22 - loss: 0.3995 - regression_loss: 0.3546 - classification_loss: 0.0449
 2299/10000 [=====>........................] - ETA: 1:54:21 - loss: 0.3998 - regression_loss: 0.3549 - classification_loss: 0.0449
 2300/10000 [=====>........................] - ETA: 1:54:20 - loss: 0.3996 - regression_loss: 0.3547 - classification_loss: 0.0449
 2301/10000 [=====>........................] - ETA: 1:54:19 - loss: 0.3997 - regression_loss: 0.3548 - classification_loss: 0.0449
 2302/10000 [=====>........................] - ETA: 1:54:18 - loss: 0.3996 - regression_loss: 0.3547 - classification_loss: 0.0449
 2303/10000 [=====>........................] - ETA: 1:54:17 - loss: 0.3996 - regression_loss: 0.3547 - classification_loss: 0.0449
 2304/10000 [=====>........................] - ETA: 1:54:16 - loss: 0.3997 - regression_loss: 0.3547 - classification_loss: 0.0450
 2305/10000 [=====>........................] - ETA: 1:54:16 - loss: 0.3998 - regression_loss: 0.3548 - classification_loss: 0.0450
 2306/10000 [=====>........................] - ETA: 1:54:15 - loss: 0.3997 - regression_loss: 0.3547 - classification_loss: 0.0450
 2307/10000 [=====>........................] - ETA: 1:54:14 - loss: 0.3996 - regression_loss: 0.3547 - classification_loss: 0.0450
 2308/10000 [=====>........................] - ETA: 1:54:13 - loss: 0.3996 - regression_loss: 0.3546 - classification_loss: 0.0449
 2309/10000 [=====>........................] - ETA: 1:54:12 - loss: 0.3996 - regression_loss: 0.3546 - classification_loss: 0.0449
 2310/10000 [=====>........................] - ETA: 1:54:11 - loss: 0.3996 - regression_loss: 0.3547 - classification_loss: 0.0450
 2311/10000 [=====>........................] - ETA: 1:54:10 - loss: 0.3996 - regression_loss: 0.3547 - classification_loss: 0.0450
 2312/10000 [=====>........................] - ETA: 1:54:09 - loss: 0.3996 - regression_loss: 0.3546 - classification_loss: 0.0449
 2313/10000 [=====>........................] - ETA: 1:54:08 - loss: 0.3996 - regression_loss: 0.3547 - classification_loss: 0.0449
 2314/10000 [=====>........................] - ETA: 1:54:07 - loss: 0.3996 - regression_loss: 0.3547 - classification_loss: 0.0449
 2315/10000 [=====>........................] - ETA: 1:54:07 - loss: 0.3997 - regression_loss: 0.3548 - classification_loss: 0.0449
 2316/10000 [=====>........................] - ETA: 1:54:06 - loss: 0.3997 - regression_loss: 0.3548 - classification_loss: 0.0449
 2317/10000 [=====>........................] - ETA: 1:54:05 - loss: 0.3996 - regression_loss: 0.3547 - classification_loss: 0.0449
 2318/10000 [=====>........................] - ETA: 1:54:04 - loss: 0.3996 - regression_loss: 0.3547 - classification_loss: 0.0449
 2319/10000 [=====>........................] - ETA: 1:54:03 - loss: 0.3995 - regression_loss: 0.3546 - classification_loss: 0.0449
 2320/10000 [=====>........................] - ETA: 1:54:02 - loss: 0.3995 - regression_loss: 0.3546 - classification_loss: 0.0449
 2321/10000 [=====>........................] - ETA: 1:54:01 - loss: 0.3994 - regression_loss: 0.3546 - classification_loss: 0.0449
 2322/10000 [=====>........................] - ETA: 1:54:00 - loss: 0.3994 - regression_loss: 0.3546 - classification_loss: 0.0449
 2323/10000 [=====>........................] - ETA: 1:53:59 - loss: 0.3995 - regression_loss: 0.3546 - classification_loss: 0.0449
 2324/10000 [=====>........................] - ETA: 1:53:58 - loss: 0.3997 - regression_loss: 0.3546 - classification_loss: 0.0451
 2325/10000 [=====>........................] - ETA: 1:53:57 - loss: 0.3998 - regression_loss: 0.3546 - classification_loss: 0.0451
 2326/10000 [=====>........................] - ETA: 1:53:57 - loss: 0.3998 - regression_loss: 0.3547 - classification_loss: 0.0451
 2327/10000 [=====>........................] - ETA: 1:53:56 - loss: 0.3997 - regression_loss: 0.3546 - classification_loss: 0.0451
 2328/10000 [=====>........................] - ETA: 1:53:55 - loss: 0.3997 - regression_loss: 0.3546 - classification_loss: 0.0451
 2329/10000 [=====>........................] - ETA: 1:53:54 - loss: 0.3999 - regression_loss: 0.3548 - classification_loss: 0.0451
 2330/10000 [=====>........................] - ETA: 1:53:53 - loss: 0.3998 - regression_loss: 0.3547 - classification_loss: 0.0451
 2331/10000 [=====>........................] - ETA: 1:53:52 - loss: 0.3997 - regression_loss: 0.3546 - classification_loss: 0.0451
 2332/10000 [=====>........................] - ETA: 1:53:51 - loss: 0.3997 - regression_loss: 0.3546 - classification_loss: 0.0451
 2333/10000 [=====>........................] - ETA: 1:53:50 - loss: 0.3997 - regression_loss: 0.3546 - classification_loss: 0.0451
 2334/10000 [======>.......................] - ETA: 1:53:49 - loss: 0.3997 - regression_loss: 0.3546 - classification_loss: 0.0451
 2335/10000 [======>.......................] - ETA: 1:53:49 - loss: 0.3998 - regression_loss: 0.3547 - classification_loss: 0.0451
 2336/10000 [======>.......................] - ETA: 1:53:48 - loss: 0.3997 - regression_loss: 0.3546 - classification_loss: 0.0451
 2337/10000 [======>.......................] - ETA: 1:53:47 - loss: 0.3998 - regression_loss: 0.3547 - classification_loss: 0.0451
 2338/10000 [======>.......................] - ETA: 1:53:46 - loss: 0.3997 - regression_loss: 0.3546 - classification_loss: 0.0451
 2339/10000 [======>.......................] - ETA: 1:53:45 - loss: 0.3996 - regression_loss: 0.3546 - classification_loss: 0.0451
 2340/10000 [======>.......................] - ETA: 1:53:44 - loss: 0.4001 - regression_loss: 0.3545 - classification_loss: 0.0456
 2341/10000 [======>.......................] - ETA: 1:53:43 - loss: 0.4000 - regression_loss: 0.3544 - classification_loss: 0.0456
 2342/10000 [======>.......................] - ETA: 1:53:42 - loss: 0.3998 - regression_loss: 0.3543 - classification_loss: 0.0455
 2343/10000 [======>.......................] - ETA: 1:53:41 - loss: 0.3998 - regression_loss: 0.3543 - classification_loss: 0.0455
 2344/10000 [======>.......................] - ETA: 1:53:40 - loss: 0.3998 - regression_loss: 0.3543 - classification_loss: 0.0455
 2345/10000 [======>.......................] - ETA: 1:53:39 - loss: 0.3998 - regression_loss: 0.3543 - classification_loss: 0.0455
 2346/10000 [======>.......................] - ETA: 1:53:39 - loss: 0.3999 - regression_loss: 0.3543 - classification_loss: 0.0455
 2347/10000 [======>.......................] - ETA: 1:53:38 - loss: 0.3999 - regression_loss: 0.3544 - classification_loss: 0.0455
 2348/10000 [======>.......................] - ETA: 1:53:37 - loss: 0.3997 - regression_loss: 0.3542 - classification_loss: 0.0455
 2349/10000 [======>.......................] - ETA: 1:53:36 - loss: 0.3998 - regression_loss: 0.3543 - classification_loss: 0.0455
 2350/10000 [======>.......................] - ETA: 1:53:35 - loss: 0.3999 - regression_loss: 0.3544 - classification_loss: 0.0455
 2351/10000 [======>.......................] - ETA: 1:53:34 - loss: 0.3997 - regression_loss: 0.3543 - classification_loss: 0.0455
 2352/10000 [======>.......................] - ETA: 1:53:33 - loss: 0.3997 - regression_loss: 0.3542 - classification_loss: 0.0455
 2353/10000 [======>.......................] - ETA: 1:53:32 - loss: 0.3997 - regression_loss: 0.3542 - classification_loss: 0.0455
 2354/10000 [======>.......................] - ETA: 1:53:31 - loss: 0.3995 - regression_loss: 0.3541 - classification_loss: 0.0455
 2355/10000 [======>.......................] - ETA: 1:53:30 - loss: 0.3994 - regression_loss: 0.3540 - classification_loss: 0.0455
 2356/10000 [======>.......................] - ETA: 1:53:30 - loss: 0.3994 - regression_loss: 0.3539 - classification_loss: 0.0455
 2357/10000 [======>.......................] - ETA: 1:53:29 - loss: 0.3994 - regression_loss: 0.3539 - classification_loss: 0.0455
 2358/10000 [======>.......................] - ETA: 1:53:28 - loss: 0.3993 - regression_loss: 0.3538 - classification_loss: 0.0455
 2359/10000 [======>.......................] - ETA: 1:53:27 - loss: 0.3993 - regression_loss: 0.3538 - classification_loss: 0.0455
 2360/10000 [======>.......................] - ETA: 1:53:26 - loss: 0.3993 - regression_loss: 0.3538 - classification_loss: 0.0455
 2361/10000 [======>.......................] - ETA: 1:53:25 - loss: 0.3993 - regression_loss: 0.3538 - classification_loss: 0.0455
 2362/10000 [======>.......................] - ETA: 1:53:24 - loss: 0.3992 - regression_loss: 0.3538 - classification_loss: 0.0455
 2363/10000 [======>.......................] - ETA: 1:53:23 - loss: 0.3993 - regression_loss: 0.3538 - classification_loss: 0.0455
 2364/10000 [======>.......................] - ETA: 1:53:22 - loss: 0.3993 - regression_loss: 0.3538 - classification_loss: 0.0455
 2365/10000 [======>.......................] - ETA: 1:53:22 - loss: 0.3991 - regression_loss: 0.3537 - classification_loss: 0.0454
 2366/10000 [======>.......................] - ETA: 1:53:21 - loss: 0.3990 - regression_loss: 0.3536 - classification_loss: 0.0454
 2367/10000 [======>.......................] - ETA: 1:53:20 - loss: 0.3990 - regression_loss: 0.3536 - classification_loss: 0.0454
 2368/10000 [======>.......................] - ETA: 1:53:19 - loss: 0.3989 - regression_loss: 0.3535 - classification_loss: 0.0454
 2369/10000 [======>.......................] - ETA: 1:53:18 - loss: 0.3989 - regression_loss: 0.3535 - classification_loss: 0.0454
 2370/10000 [======>.......................] - ETA: 1:53:17 - loss: 0.3988 - regression_loss: 0.3534 - classification_loss: 0.0454
 2371/10000 [======>.......................] - ETA: 1:53:16 - loss: 0.3989 - regression_loss: 0.3535 - classification_loss: 0.0454
 2372/10000 [======>.......................] - ETA: 1:53:15 - loss: 0.3989 - regression_loss: 0.3535 - classification_loss: 0.0454
 2373/10000 [======>.......................] - ETA: 1:53:14 - loss: 0.3990 - regression_loss: 0.3536 - classification_loss: 0.0454
 2374/10000 [======>.......................] - ETA: 1:53:13 - loss: 0.3990 - regression_loss: 0.3535 - classification_loss: 0.0454
 2375/10000 [======>.......................] - ETA: 1:53:13 - loss: 0.3988 - regression_loss: 0.3534 - classification_loss: 0.0454
 2376/10000 [======>.......................] - ETA: 1:53:12 - loss: 0.3988 - regression_loss: 0.3534 - classification_loss: 0.0454
 2377/10000 [======>.......................] - ETA: 1:53:11 - loss: 0.3987 - regression_loss: 0.3533 - classification_loss: 0.0454
 2378/10000 [======>.......................] - ETA: 1:53:10 - loss: 0.3988 - regression_loss: 0.3534 - classification_loss: 0.0454
 2379/10000 [======>.......................] - ETA: 1:53:09 - loss: 0.3987 - regression_loss: 0.3533 - classification_loss: 0.0454
 2380/10000 [======>.......................] - ETA: 1:53:08 - loss: 0.3990 - regression_loss: 0.3536 - classification_loss: 0.0454
 2381/10000 [======>.......................] - ETA: 1:53:07 - loss: 0.3991 - regression_loss: 0.3537 - classification_loss: 0.0454
 2382/10000 [======>.......................] - ETA: 1:53:06 - loss: 0.3994 - regression_loss: 0.3540 - classification_loss: 0.0454
 2383/10000 [======>.......................] - ETA: 1:53:05 - loss: 0.3993 - regression_loss: 0.3539 - classification_loss: 0.0454
 2384/10000 [======>.......................] - ETA: 1:53:05 - loss: 0.3994 - regression_loss: 0.3540 - classification_loss: 0.0454
 2385/10000 [======>.......................] - ETA: 1:53:04 - loss: 0.3993 - regression_loss: 0.3539 - classification_loss: 0.0454
 2386/10000 [======>.......................] - ETA: 1:53:03 - loss: 0.3993 - regression_loss: 0.3539 - classification_loss: 0.0454
 2387/10000 [======>.......................] - ETA: 1:53:02 - loss: 0.3993 - regression_loss: 0.3539 - classification_loss: 0.0454
 2388/10000 [======>.......................] - ETA: 1:53:01 - loss: 0.3993 - regression_loss: 0.3539 - classification_loss: 0.0454
 2389/10000 [======>.......................] - ETA: 1:53:00 - loss: 0.3994 - regression_loss: 0.3540 - classification_loss: 0.0454
 2390/10000 [======>.......................] - ETA: 1:52:59 - loss: 0.3994 - regression_loss: 0.3540 - classification_loss: 0.0454
 2391/10000 [======>.......................] - ETA: 1:52:58 - loss: 0.3994 - regression_loss: 0.3540 - classification_loss: 0.0454
 2392/10000 [======>.......................] - ETA: 1:52:57 - loss: 0.3995 - regression_loss: 0.3541 - classification_loss: 0.0454
 2393/10000 [======>.......................] - ETA: 1:52:56 - loss: 0.3994 - regression_loss: 0.3541 - classification_loss: 0.0454
 2394/10000 [======>.......................] - ETA: 1:52:55 - loss: 0.3994 - regression_loss: 0.3540 - classification_loss: 0.0454
 2395/10000 [======>.......................] - ETA: 1:52:55 - loss: 0.3993 - regression_loss: 0.3540 - classification_loss: 0.0454
 2396/10000 [======>.......................] - ETA: 1:52:54 - loss: 0.3994 - regression_loss: 0.3540 - classification_loss: 0.0454
 2397/10000 [======>.......................] - ETA: 1:52:53 - loss: 0.3993 - regression_loss: 0.3540 - classification_loss: 0.0454
 2398/10000 [======>.......................] - ETA: 1:52:52 - loss: 0.3992 - regression_loss: 0.3539 - classification_loss: 0.0453
 2399/10000 [======>.......................] - ETA: 1:52:51 - loss: 0.3993 - regression_loss: 0.3539 - classification_loss: 0.0453
 2400/10000 [======>.......................] - ETA: 1:52:50 - loss: 0.3992 - regression_loss: 0.3539 - classification_loss: 0.0453
 2401/10000 [======>.......................] - ETA: 1:52:49 - loss: 0.3992 - regression_loss: 0.3538 - classification_loss: 0.0453
 2402/10000 [======>.......................] - ETA: 1:52:48 - loss: 0.3991 - regression_loss: 0.3538 - classification_loss: 0.0453
 2403/10000 [======>.......................] - ETA: 1:52:47 - loss: 0.3992 - regression_loss: 0.3539 - classification_loss: 0.0453
 2404/10000 [======>.......................] - ETA: 1:52:46 - loss: 0.3992 - regression_loss: 0.3539 - classification_loss: 0.0453
 2405/10000 [======>.......................] - ETA: 1:52:46 - loss: 0.3992 - regression_loss: 0.3539 - classification_loss: 0.0453
 2406/10000 [======>.......................] - ETA: 1:52:45 - loss: 0.3991 - regression_loss: 0.3538 - classification_loss: 0.0453
 2407/10000 [======>.......................] - ETA: 1:52:44 - loss: 0.3991 - regression_loss: 0.3538 - classification_loss: 0.0453
 2408/10000 [======>.......................] - ETA: 1:52:43 - loss: 0.3989 - regression_loss: 0.3536 - classification_loss: 0.0453
 2409/10000 [======>.......................] - ETA: 1:52:42 - loss: 0.3989 - regression_loss: 0.3536 - classification_loss: 0.0453
 2410/10000 [======>.......................] - ETA: 1:52:41 - loss: 0.3988 - regression_loss: 0.3535 - classification_loss: 0.0452
 2411/10000 [======>.......................] - ETA: 1:52:40 - loss: 0.3995 - regression_loss: 0.3542 - classification_loss: 0.0453
 2412/10000 [======>.......................] - ETA: 1:52:39 - loss: 0.3994 - regression_loss: 0.3541 - classification_loss: 0.0453
 2413/10000 [======>.......................] - ETA: 1:52:38 - loss: 0.3994 - regression_loss: 0.3541 - classification_loss: 0.0453
 2414/10000 [======>.......................] - ETA: 1:52:38 - loss: 0.3997 - regression_loss: 0.3544 - classification_loss: 0.0453
 2415/10000 [======>.......................] - ETA: 1:52:37 - loss: 0.4002 - regression_loss: 0.3548 - classification_loss: 0.0454
 2416/10000 [======>.......................] - ETA: 1:52:36 - loss: 0.4000 - regression_loss: 0.3547 - classification_loss: 0.0453
 2417/10000 [======>.......................] - ETA: 1:52:35 - loss: 0.4002 - regression_loss: 0.3549 - classification_loss: 0.0453
 2418/10000 [======>.......................] - ETA: 1:52:34 - loss: 0.4003 - regression_loss: 0.3549 - classification_loss: 0.0454
 2419/10000 [======>.......................] - ETA: 1:52:33 - loss: 0.4001 - regression_loss: 0.3548 - classification_loss: 0.0453
 2420/10000 [======>.......................] - ETA: 1:52:32 - loss: 0.4001 - regression_loss: 0.3547 - classification_loss: 0.0453
 2421/10000 [======>.......................] - ETA: 1:52:31 - loss: 0.4000 - regression_loss: 0.3547 - classification_loss: 0.0453
 2422/10000 [======>.......................] - ETA: 1:52:30 - loss: 0.4000 - regression_loss: 0.3547 - classification_loss: 0.0453
 2423/10000 [======>.......................] - ETA: 1:52:29 - loss: 0.4001 - regression_loss: 0.3548 - classification_loss: 0.0453
 2424/10000 [======>.......................] - ETA: 1:52:29 - loss: 0.4001 - regression_loss: 0.3548 - classification_loss: 0.0453
 2425/10000 [======>.......................] - ETA: 1:52:28 - loss: 0.4000 - regression_loss: 0.3547 - classification_loss: 0.0453
 2426/10000 [======>.......................] - ETA: 1:52:27 - loss: 0.4000 - regression_loss: 0.3548 - classification_loss: 0.0453
 2427/10000 [======>.......................] - ETA: 1:52:26 - loss: 0.4000 - regression_loss: 0.3547 - classification_loss: 0.0453
 2428/10000 [======>.......................] - ETA: 1:52:25 - loss: 0.4001 - regression_loss: 0.3548 - classification_loss: 0.0453
 2429/10000 [======>.......................] - ETA: 1:52:24 - loss: 0.4000 - regression_loss: 0.3547 - classification_loss: 0.0453
 2430/10000 [======>.......................] - ETA: 1:52:23 - loss: 0.4000 - regression_loss: 0.3547 - classification_loss: 0.0453
 2431/10000 [======>.......................] - ETA: 1:52:22 - loss: 0.4001 - regression_loss: 0.3548 - classification_loss: 0.0453
 2432/10000 [======>.......................] - ETA: 1:52:21 - loss: 0.4001 - regression_loss: 0.3548 - classification_loss: 0.0453
 2433/10000 [======>.......................] - ETA: 1:52:20 - loss: 0.4002 - regression_loss: 0.3549 - classification_loss: 0.0453
 2434/10000 [======>.......................] - ETA: 1:52:20 - loss: 0.4002 - regression_loss: 0.3549 - classification_loss: 0.0453
 2435/10000 [======>.......................] - ETA: 1:52:19 - loss: 0.4002 - regression_loss: 0.3550 - classification_loss: 0.0453
 2436/10000 [======>.......................] - ETA: 1:52:18 - loss: 0.4002 - regression_loss: 0.3549 - classification_loss: 0.0453
 2437/10000 [======>.......................] - ETA: 1:52:17 - loss: 0.4003 - regression_loss: 0.3550 - classification_loss: 0.0453
 2438/10000 [======>.......................] - ETA: 1:52:16 - loss: 0.4003 - regression_loss: 0.3550 - classification_loss: 0.0453
 2439/10000 [======>.......................] - ETA: 1:52:15 - loss: 0.4004 - regression_loss: 0.3551 - classification_loss: 0.0453
 2440/10000 [======>.......................] - ETA: 1:52:14 - loss: 0.4004 - regression_loss: 0.3552 - classification_loss: 0.0453
 2441/10000 [======>.......................] - ETA: 1:52:13 - loss: 0.4004 - regression_loss: 0.3551 - classification_loss: 0.0453
 2442/10000 [======>.......................] - ETA: 1:52:12 - loss: 0.4002 - regression_loss: 0.3550 - classification_loss: 0.0452
 2443/10000 [======>.......................] - ETA: 1:52:11 - loss: 0.4004 - regression_loss: 0.3551 - classification_loss: 0.0453
 2444/10000 [======>.......................] - ETA: 1:52:11 - loss: 0.4007 - regression_loss: 0.3554 - classification_loss: 0.0453
 2445/10000 [======>.......................] - ETA: 1:52:10 - loss: 0.4008 - regression_loss: 0.3555 - classification_loss: 0.0453
 2446/10000 [======>.......................] - ETA: 1:52:09 - loss: 0.4009 - regression_loss: 0.3555 - classification_loss: 0.0454
 2447/10000 [======>.......................] - ETA: 1:52:08 - loss: 0.4009 - regression_loss: 0.3555 - classification_loss: 0.0454
 2448/10000 [======>.......................] - ETA: 1:52:07 - loss: 0.4008 - regression_loss: 0.3554 - classification_loss: 0.0454
 2449/10000 [======>.......................] - ETA: 1:52:06 - loss: 0.4009 - regression_loss: 0.3555 - classification_loss: 0.0453
 2450/10000 [======>.......................] - ETA: 1:52:05 - loss: 0.4009 - regression_loss: 0.3556 - classification_loss: 0.0453
 2451/10000 [======>.......................] - ETA: 1:52:04 - loss: 0.4009 - regression_loss: 0.3556 - classification_loss: 0.0453
 2452/10000 [======>.......................] - ETA: 1:52:03 - loss: 0.4009 - regression_loss: 0.3555 - classification_loss: 0.0453
 2453/10000 [======>.......................] - ETA: 1:52:02 - loss: 0.4009 - regression_loss: 0.3555 - classification_loss: 0.0454
 2454/10000 [======>.......................] - ETA: 1:52:02 - loss: 0.4008 - regression_loss: 0.3555 - classification_loss: 0.0453
 2455/10000 [======>.......................] - ETA: 1:52:01 - loss: 0.4009 - regression_loss: 0.3556 - classification_loss: 0.0453
 2456/10000 [======>.......................] - ETA: 1:52:00 - loss: 0.4010 - regression_loss: 0.3557 - classification_loss: 0.0454
 2457/10000 [======>.......................] - ETA: 1:51:59 - loss: 0.4012 - regression_loss: 0.3559 - classification_loss: 0.0454
 2458/10000 [======>.......................] - ETA: 1:51:58 - loss: 0.4014 - regression_loss: 0.3560 - classification_loss: 0.0454
 2459/10000 [======>.......................] - ETA: 1:51:57 - loss: 0.4014 - regression_loss: 0.3560 - classification_loss: 0.0454
 2460/10000 [======>.......................] - ETA: 1:51:56 - loss: 0.4014 - regression_loss: 0.3560 - classification_loss: 0.0454
 2461/10000 [======>.......................] - ETA: 1:51:55 - loss: 0.4014 - regression_loss: 0.3560 - classification_loss: 0.0454
 2462/10000 [======>.......................] - ETA: 1:51:54 - loss: 0.4013 - regression_loss: 0.3560 - classification_loss: 0.0454
 2463/10000 [======>.......................] - ETA: 1:51:53 - loss: 0.4014 - regression_loss: 0.3561 - classification_loss: 0.0454
 2464/10000 [======>.......................] - ETA: 1:51:52 - loss: 0.4015 - regression_loss: 0.3561 - classification_loss: 0.0454
 2465/10000 [======>.......................] - ETA: 1:51:52 - loss: 0.4016 - regression_loss: 0.3562 - classification_loss: 0.0454
 2466/10000 [======>.......................] - ETA: 1:51:51 - loss: 0.4015 - regression_loss: 0.3561 - classification_loss: 0.0454
 2467/10000 [======>.......................] - ETA: 1:51:50 - loss: 0.4018 - regression_loss: 0.3564 - classification_loss: 0.0454
 2468/10000 [======>.......................] - ETA: 1:51:49 - loss: 0.4018 - regression_loss: 0.3564 - classification_loss: 0.0454
 2469/10000 [======>.......................] - ETA: 1:51:48 - loss: 0.4017 - regression_loss: 0.3563 - classification_loss: 0.0454
 2470/10000 [======>.......................] - ETA: 1:51:47 - loss: 0.4016 - regression_loss: 0.3562 - classification_loss: 0.0454
 2471/10000 [======>.......................] - ETA: 1:51:46 - loss: 0.4015 - regression_loss: 0.3561 - classification_loss: 0.0454
 2472/10000 [======>.......................] - ETA: 1:51:45 - loss: 0.4014 - regression_loss: 0.3560 - classification_loss: 0.0454
 2473/10000 [======>.......................] - ETA: 1:51:44 - loss: 0.4020 - regression_loss: 0.3565 - classification_loss: 0.0455
 2474/10000 [======>.......................] - ETA: 1:51:44 - loss: 0.4019 - regression_loss: 0.3564 - classification_loss: 0.0455
 2475/10000 [======>.......................] - ETA: 1:51:43 - loss: 0.4018 - regression_loss: 0.3564 - classification_loss: 0.0455
 2476/10000 [======>.......................] - ETA: 1:51:42 - loss: 0.4020 - regression_loss: 0.3565 - classification_loss: 0.0455
 2477/10000 [======>.......................] - ETA: 1:51:41 - loss: 0.4020 - regression_loss: 0.3565 - classification_loss: 0.0455
 2478/10000 [======>.......................] - ETA: 1:51:40 - loss: 0.4019 - regression_loss: 0.3565 - classification_loss: 0.0455
 2479/10000 [======>.......................] - ETA: 1:51:39 - loss: 0.4019 - regression_loss: 0.3565 - classification_loss: 0.0455
 2480/10000 [======>.......................] - ETA: 1:51:38 - loss: 0.4022 - regression_loss: 0.3567 - classification_loss: 0.0455
 2481/10000 [======>.......................] - ETA: 1:51:37 - loss: 0.4022 - regression_loss: 0.3567 - classification_loss: 0.0455
 2482/10000 [======>.......................] - ETA: 1:51:36 - loss: 0.4020 - regression_loss: 0.3566 - classification_loss: 0.0454
 2483/10000 [======>.......................] - ETA: 1:51:35 - loss: 0.4021 - regression_loss: 0.3567 - classification_loss: 0.0454
 2484/10000 [======>.......................] - ETA: 1:51:34 - loss: 0.4021 - regression_loss: 0.3567 - classification_loss: 0.0454
 2485/10000 [======>.......................] - ETA: 1:51:34 - loss: 0.4021 - regression_loss: 0.3566 - classification_loss: 0.0454
 2486/10000 [======>.......................] - ETA: 1:51:33 - loss: 0.4019 - regression_loss: 0.3565 - classification_loss: 0.0454
 2487/10000 [======>.......................] - ETA: 1:51:32 - loss: 0.4019 - regression_loss: 0.3565 - classification_loss: 0.0454
 2488/10000 [======>.......................] - ETA: 1:51:31 - loss: 0.4020 - regression_loss: 0.3566 - classification_loss: 0.0454
 2489/10000 [======>.......................] - ETA: 1:51:30 - loss: 0.4020 - regression_loss: 0.3566 - classification_loss: 0.0454
 2490/10000 [======>.......................] - ETA: 1:51:29 - loss: 0.4020 - regression_loss: 0.3566 - classification_loss: 0.0454
 2491/10000 [======>.......................] - ETA: 1:51:28 - loss: 0.4019 - regression_loss: 0.3565 - classification_loss: 0.0454
 2492/10000 [======>.......................] - ETA: 1:51:27 - loss: 0.4019 - regression_loss: 0.3566 - classification_loss: 0.0454
 2493/10000 [======>.......................] - ETA: 1:51:26 - loss: 0.4026 - regression_loss: 0.3571 - classification_loss: 0.0454
 2494/10000 [======>.......................] - ETA: 1:51:25 - loss: 0.4025 - regression_loss: 0.3571 - classification_loss: 0.0454
 2495/10000 [======>.......................] - ETA: 1:51:25 - loss: 0.4025 - regression_loss: 0.3571 - classification_loss: 0.0455
 2496/10000 [======>.......................] - ETA: 1:51:24 - loss: 0.4025 - regression_loss: 0.3570 - classification_loss: 0.0454
 2497/10000 [======>.......................] - ETA: 1:51:23 - loss: 0.4024 - regression_loss: 0.3569 - classification_loss: 0.0454
 2498/10000 [======>.......................] - ETA: 1:51:22 - loss: 0.4023 - regression_loss: 0.3569 - classification_loss: 0.0454
 2499/10000 [======>.......................] - ETA: 1:51:21 - loss: 0.4022 - regression_loss: 0.3568 - classification_loss: 0.0454
 2500/10000 [======>.......................] - ETA: 1:51:20 - loss: 0.4022 - regression_loss: 0.3568 - classification_loss: 0.0454
 2501/10000 [======>.......................] - ETA: 1:51:19 - loss: 0.4020 - regression_loss: 0.3567 - classification_loss: 0.0454
 2502/10000 [======>.......................] - ETA: 1:51:18 - loss: 0.4020 - regression_loss: 0.3567 - classification_loss: 0.0454
 2503/10000 [======>.......................] - ETA: 1:51:17 - loss: 0.4020 - regression_loss: 0.3566 - classification_loss: 0.0454
 2504/10000 [======>.......................] - ETA: 1:51:16 - loss: 0.4019 - regression_loss: 0.3566 - classification_loss: 0.0454
 2505/10000 [======>.......................] - ETA: 1:51:16 - loss: 0.4019 - regression_loss: 0.3565 - classification_loss: 0.0453
 2506/10000 [======>.......................] - ETA: 1:51:15 - loss: 0.4017 - regression_loss: 0.3564 - classification_loss: 0.0453
 2507/10000 [======>.......................] - ETA: 1:51:14 - loss: 0.4018 - regression_loss: 0.3565 - classification_loss: 0.0453
 2508/10000 [======>.......................] - ETA: 1:51:13 - loss: 0.4018 - regression_loss: 0.3564 - classification_loss: 0.0453
 2509/10000 [======>.......................] - ETA: 1:51:12 - loss: 0.4017 - regression_loss: 0.3564 - classification_loss: 0.0453
 2510/10000 [======>.......................] - ETA: 1:51:11 - loss: 0.4017 - regression_loss: 0.3564 - classification_loss: 0.0453
 2511/10000 [======>.......................] - ETA: 1:51:10 - loss: 0.4018 - regression_loss: 0.3565 - classification_loss: 0.0453
 2512/10000 [======>.......................] - ETA: 1:51:09 - loss: 0.4017 - regression_loss: 0.3564 - classification_loss: 0.0453
 2513/10000 [======>.......................] - ETA: 1:51:08 - loss: 0.4017 - regression_loss: 0.3564 - classification_loss: 0.0453
 2514/10000 [======>.......................] - ETA: 1:51:07 - loss: 0.4025 - regression_loss: 0.3570 - classification_loss: 0.0455
 2515/10000 [======>.......................] - ETA: 1:51:07 - loss: 0.4025 - regression_loss: 0.3570 - classification_loss: 0.0455
 2516/10000 [======>.......................] - ETA: 1:51:06 - loss: 0.4025 - regression_loss: 0.3570 - classification_loss: 0.0455
 2517/10000 [======>.......................] - ETA: 1:51:05 - loss: 0.4027 - regression_loss: 0.3572 - classification_loss: 0.0456
 2518/10000 [======>.......................] - ETA: 1:51:04 - loss: 0.4026 - regression_loss: 0.3570 - classification_loss: 0.0455
 2519/10000 [======>.......................] - ETA: 1:51:03 - loss: 0.4026 - regression_loss: 0.3571 - classification_loss: 0.0455
 2520/10000 [======>.......................] - ETA: 1:51:02 - loss: 0.4025 - regression_loss: 0.3570 - classification_loss: 0.0455
 2521/10000 [======>.......................] - ETA: 1:51:01 - loss: 0.4024 - regression_loss: 0.3569 - classification_loss: 0.0455
 2522/10000 [======>.......................] - ETA: 1:51:00 - loss: 0.4024 - regression_loss: 0.3569 - classification_loss: 0.0455
 2523/10000 [======>.......................] - ETA: 1:50:59 - loss: 0.4023 - regression_loss: 0.3568 - classification_loss: 0.0455
 2524/10000 [======>.......................] - ETA: 1:50:58 - loss: 0.4022 - regression_loss: 0.3568 - classification_loss: 0.0455
 2525/10000 [======>.......................] - ETA: 1:50:58 - loss: 0.4022 - regression_loss: 0.3568 - classification_loss: 0.0454
 2526/10000 [======>.......................] - ETA: 1:50:57 - loss: 0.4022 - regression_loss: 0.3568 - classification_loss: 0.0454
 2527/10000 [======>.......................] - ETA: 1:50:56 - loss: 0.4022 - regression_loss: 0.3567 - classification_loss: 0.0454
 2528/10000 [======>.......................] - ETA: 1:50:55 - loss: 0.4020 - regression_loss: 0.3566 - classification_loss: 0.0454
 2529/10000 [======>.......................] - ETA: 1:50:54 - loss: 0.4020 - regression_loss: 0.3566 - classification_loss: 0.0454
 2530/10000 [======>.......................] - ETA: 1:50:53 - loss: 0.4020 - regression_loss: 0.3566 - classification_loss: 0.0454
 2531/10000 [======>.......................] - ETA: 1:50:52 - loss: 0.4019 - regression_loss: 0.3565 - classification_loss: 0.0454
 2532/10000 [======>.......................] - ETA: 1:50:51 - loss: 0.4019 - regression_loss: 0.3564 - classification_loss: 0.0454
 2533/10000 [======>.......................] - ETA: 1:50:50 - loss: 0.4019 - regression_loss: 0.3565 - classification_loss: 0.0454
 2534/10000 [======>.......................] - ETA: 1:50:50 - loss: 0.4017 - regression_loss: 0.3563 - classification_loss: 0.0454
 2535/10000 [======>.......................] - ETA: 1:50:49 - loss: 0.4017 - regression_loss: 0.3563 - classification_loss: 0.0454
 2536/10000 [======>.......................] - ETA: 1:50:48 - loss: 0.4017 - regression_loss: 0.3564 - classification_loss: 0.0454
 2537/10000 [======>.......................] - ETA: 1:50:47 - loss: 0.4017 - regression_loss: 0.3563 - classification_loss: 0.0454
 2538/10000 [======>.......................] - ETA: 1:50:46 - loss: 0.4018 - regression_loss: 0.3564 - classification_loss: 0.0454
 2539/10000 [======>.......................] - ETA: 1:50:45 - loss: 0.4018 - regression_loss: 0.3565 - classification_loss: 0.0454
 2540/10000 [======>.......................] - ETA: 1:50:44 - loss: 0.4018 - regression_loss: 0.3564 - classification_loss: 0.0454
 2541/10000 [======>.......................] - ETA: 1:50:43 - loss: 0.4017 - regression_loss: 0.3564 - classification_loss: 0.0453
 2542/10000 [======>.......................] - ETA: 1:50:42 - loss: 0.4017 - regression_loss: 0.3564 - classification_loss: 0.0453
 2543/10000 [======>.......................] - ETA: 1:50:41 - loss: 0.4017 - regression_loss: 0.3564 - classification_loss: 0.0453
 2544/10000 [======>.......................] - ETA: 1:50:41 - loss: 0.4016 - regression_loss: 0.3563 - classification_loss: 0.0453
 2545/10000 [======>.......................] - ETA: 1:50:40 - loss: 0.4015 - regression_loss: 0.3562 - classification_loss: 0.0453
 2546/10000 [======>.......................] - ETA: 1:50:39 - loss: 0.4014 - regression_loss: 0.3561 - classification_loss: 0.0453
 2547/10000 [======>.......................] - ETA: 1:50:38 - loss: 0.4013 - regression_loss: 0.3560 - classification_loss: 0.0453
 2548/10000 [======>.......................] - ETA: 1:50:37 - loss: 0.4012 - regression_loss: 0.3560 - classification_loss: 0.0453
 2549/10000 [======>.......................] - ETA: 1:50:36 - loss: 0.4014 - regression_loss: 0.3562 - classification_loss: 0.0453
 2550/10000 [======>.......................] - ETA: 1:50:35 - loss: 0.4015 - regression_loss: 0.3563 - classification_loss: 0.0453
 2551/10000 [======>.......................] - ETA: 1:50:34 - loss: 0.4016 - regression_loss: 0.3563 - classification_loss: 0.0453
 2552/10000 [======>.......................] - ETA: 1:50:33 - loss: 0.4014 - regression_loss: 0.3561 - classification_loss: 0.0453
 2553/10000 [======>.......................] - ETA: 1:50:32 - loss: 0.4015 - regression_loss: 0.3562 - classification_loss: 0.0453
 2554/10000 [======>.......................] - ETA: 1:50:32 - loss: 0.4014 - regression_loss: 0.3562 - classification_loss: 0.0453
 2555/10000 [======>.......................] - ETA: 1:50:31 - loss: 0.4015 - regression_loss: 0.3562 - classification_loss: 0.0452
 2556/10000 [======>.......................] - ETA: 1:50:30 - loss: 0.4015 - regression_loss: 0.3562 - classification_loss: 0.0452
 2557/10000 [======>.......................] - ETA: 1:50:29 - loss: 0.4014 - regression_loss: 0.3561 - classification_loss: 0.0452
 2558/10000 [======>.......................] - ETA: 1:50:28 - loss: 0.4013 - regression_loss: 0.3561 - classification_loss: 0.0452
 2559/10000 [======>.......................] - ETA: 1:50:27 - loss: 0.4013 - regression_loss: 0.3560 - classification_loss: 0.0452
 2560/10000 [======>.......................] - ETA: 1:50:26 - loss: 0.4012 - regression_loss: 0.3560 - classification_loss: 0.0452
 2561/10000 [======>.......................] - ETA: 1:50:25 - loss: 0.4012 - regression_loss: 0.3560 - classification_loss: 0.0452
 2562/10000 [======>.......................] - ETA: 1:50:24 - loss: 0.4014 - regression_loss: 0.3563 - classification_loss: 0.0452
 2563/10000 [======>.......................] - ETA: 1:50:23 - loss: 0.4015 - regression_loss: 0.3563 - classification_loss: 0.0452
 2564/10000 [======>.......................] - ETA: 1:50:23 - loss: 0.4014 - regression_loss: 0.3562 - classification_loss: 0.0452
 2565/10000 [======>.......................] - ETA: 1:50:22 - loss: 0.4016 - regression_loss: 0.3563 - classification_loss: 0.0453
 2566/10000 [======>.......................] - ETA: 1:50:21 - loss: 0.4016 - regression_loss: 0.3563 - classification_loss: 0.0453
 2567/10000 [======>.......................] - ETA: 1:50:20 - loss: 0.4020 - regression_loss: 0.3567 - classification_loss: 0.0453
 2568/10000 [======>.......................] - ETA: 1:50:19 - loss: 0.4021 - regression_loss: 0.3568 - classification_loss: 0.0453
 2569/10000 [======>.......................] - ETA: 1:50:18 - loss: 0.4021 - regression_loss: 0.3568 - classification_loss: 0.0453
 2570/10000 [======>.......................] - ETA: 1:50:17 - loss: 0.4019 - regression_loss: 0.3566 - classification_loss: 0.0453
 2571/10000 [======>.......................] - ETA: 1:50:16 - loss: 0.4020 - regression_loss: 0.3566 - classification_loss: 0.0453
 2572/10000 [======>.......................] - ETA: 1:50:15 - loss: 0.4019 - regression_loss: 0.3566 - classification_loss: 0.0453
 2573/10000 [======>.......................] - ETA: 1:50:15 - loss: 0.4019 - regression_loss: 0.3566 - classification_loss: 0.0453
 2574/10000 [======>.......................] - ETA: 1:50:14 - loss: 0.4018 - regression_loss: 0.3565 - classification_loss: 0.0453
 2575/10000 [======>.......................] - ETA: 1:50:13 - loss: 0.4017 - regression_loss: 0.3565 - classification_loss: 0.0453
 2576/10000 [======>.......................] - ETA: 1:50:12 - loss: 0.4016 - regression_loss: 0.3563 - classification_loss: 0.0453
 2577/10000 [======>.......................] - ETA: 1:50:11 - loss: 0.4015 - regression_loss: 0.3563 - classification_loss: 0.0453
 2578/10000 [======>.......................] - ETA: 1:50:10 - loss: 0.4015 - regression_loss: 0.3563 - classification_loss: 0.0452
 2579/10000 [======>.......................] - ETA: 1:50:09 - loss: 0.4016 - regression_loss: 0.3564 - classification_loss: 0.0452
 2580/10000 [======>.......................] - ETA: 1:50:08 - loss: 0.4015 - regression_loss: 0.3563 - classification_loss: 0.0452
 2581/10000 [======>.......................] - ETA: 1:50:07 - loss: 0.4016 - regression_loss: 0.3564 - classification_loss: 0.0452
 2582/10000 [======>.......................] - ETA: 1:50:06 - loss: 0.4016 - regression_loss: 0.3564 - classification_loss: 0.0452
 2583/10000 [======>.......................] - ETA: 1:50:06 - loss: 0.4016 - regression_loss: 0.3564 - classification_loss: 0.0452
 2584/10000 [======>.......................] - ETA: 1:50:05 - loss: 0.4018 - regression_loss: 0.3566 - classification_loss: 0.0452
 2585/10000 [======>.......................] - ETA: 1:50:04 - loss: 0.4018 - regression_loss: 0.3565 - classification_loss: 0.0452
 2586/10000 [======>.......................] - ETA: 1:50:03 - loss: 0.4018 - regression_loss: 0.3566 - classification_loss: 0.0452
 2587/10000 [======>.......................] - ETA: 1:50:02 - loss: 0.4018 - regression_loss: 0.3566 - classification_loss: 0.0452
 2588/10000 [======>.......................] - ETA: 1:50:01 - loss: 0.4017 - regression_loss: 0.3565 - classification_loss: 0.0452
 2589/10000 [======>.......................] - ETA: 1:50:00 - loss: 0.4017 - regression_loss: 0.3565 - classification_loss: 0.0452
 2590/10000 [======>.......................] - ETA: 1:49:59 - loss: 0.4015 - regression_loss: 0.3563 - classification_loss: 0.0452
 2591/10000 [======>.......................] - ETA: 1:49:58 - loss: 0.4015 - regression_loss: 0.3563 - classification_loss: 0.0452
 2592/10000 [======>.......................] - ETA: 1:49:57 - loss: 0.4014 - regression_loss: 0.3562 - classification_loss: 0.0452
 2593/10000 [======>.......................] - ETA: 1:49:57 - loss: 0.4013 - regression_loss: 0.3562 - classification_loss: 0.0452
 2594/10000 [======>.......................] - ETA: 1:49:56 - loss: 0.4015 - regression_loss: 0.3563 - classification_loss: 0.0452
 2595/10000 [======>.......................] - ETA: 1:49:55 - loss: 0.4014 - regression_loss: 0.3562 - classification_loss: 0.0452
 2596/10000 [======>.......................] - ETA: 1:49:54 - loss: 0.4013 - regression_loss: 0.3562 - classification_loss: 0.0452
 2597/10000 [======>.......................] - ETA: 1:49:53 - loss: 0.4016 - regression_loss: 0.3564 - classification_loss: 0.0452
 2598/10000 [======>.......................] - ETA: 1:49:52 - loss: 0.4015 - regression_loss: 0.3563 - classification_loss: 0.0452
 2599/10000 [======>.......................] - ETA: 1:49:51 - loss: 0.4014 - regression_loss: 0.3563 - classification_loss: 0.0451
 2600/10000 [======>.......................] - ETA: 1:49:50 - loss: 0.4016 - regression_loss: 0.3564 - classification_loss: 0.0452
 2601/10000 [======>.......................] - ETA: 1:49:49 - loss: 0.4016 - regression_loss: 0.3564 - classification_loss: 0.0452
 2602/10000 [======>.......................] - ETA: 1:49:49 - loss: 0.4019 - regression_loss: 0.3567 - classification_loss: 0.0452
 2603/10000 [======>.......................] - ETA: 1:49:48 - loss: 0.4019 - regression_loss: 0.3567 - classification_loss: 0.0452
 2604/10000 [======>.......................] - ETA: 1:49:47 - loss: 0.4019 - regression_loss: 0.3567 - classification_loss: 0.0452
 2605/10000 [======>.......................] - ETA: 1:49:46 - loss: 0.4019 - regression_loss: 0.3567 - classification_loss: 0.0452
 2606/10000 [======>.......................] - ETA: 1:49:45 - loss: 0.4019 - regression_loss: 0.3568 - classification_loss: 0.0451
 2607/10000 [======>.......................] - ETA: 1:49:44 - loss: 0.4019 - regression_loss: 0.3568 - classification_loss: 0.0451
 2608/10000 [======>.......................] - ETA: 1:49:43 - loss: 0.4018 - regression_loss: 0.3567 - classification_loss: 0.0451
 2609/10000 [======>.......................] - ETA: 1:49:42 - loss: 0.4017 - regression_loss: 0.3566 - classification_loss: 0.0451
 2610/10000 [======>.......................] - ETA: 1:49:41 - loss: 0.4021 - regression_loss: 0.3569 - classification_loss: 0.0452
 2611/10000 [======>.......................] - ETA: 1:49:40 - loss: 0.4021 - regression_loss: 0.3569 - classification_loss: 0.0452
 2612/10000 [======>.......................] - ETA: 1:49:40 - loss: 0.4023 - regression_loss: 0.3571 - classification_loss: 0.0452
 2613/10000 [======>.......................] - ETA: 1:49:39 - loss: 0.4023 - regression_loss: 0.3571 - classification_loss: 0.0452
 2614/10000 [======>.......................] - ETA: 1:49:38 - loss: 0.4023 - regression_loss: 0.3571 - classification_loss: 0.0452
 2615/10000 [======>.......................] - ETA: 1:49:37 - loss: 0.4023 - regression_loss: 0.3571 - classification_loss: 0.0452
 2616/10000 [======>.......................] - ETA: 1:49:36 - loss: 0.4023 - regression_loss: 0.3571 - classification_loss: 0.0452
 2617/10000 [======>.......................] - ETA: 1:49:35 - loss: 0.4022 - regression_loss: 0.3570 - classification_loss: 0.0452
 2618/10000 [======>.......................] - ETA: 1:49:34 - loss: 0.4021 - regression_loss: 0.3570 - classification_loss: 0.0452
 2619/10000 [======>.......................] - ETA: 1:49:33 - loss: 0.4020 - regression_loss: 0.3569 - classification_loss: 0.0452
 2620/10000 [======>.......................] - ETA: 1:49:32 - loss: 0.4020 - regression_loss: 0.3569 - classification_loss: 0.0452
 2621/10000 [======>.......................] - ETA: 1:49:31 - loss: 0.4020 - regression_loss: 0.3568 - classification_loss: 0.0452
 2622/10000 [======>.......................] - ETA: 1:49:31 - loss: 0.4020 - regression_loss: 0.3568 - classification_loss: 0.0452
 2623/10000 [======>.......................] - ETA: 1:49:30 - loss: 0.4019 - regression_loss: 0.3568 - classification_loss: 0.0451
 2624/10000 [======>.......................] - ETA: 1:49:29 - loss: 0.4019 - regression_loss: 0.3568 - classification_loss: 0.0451
 2625/10000 [======>.......................] - ETA: 1:49:28 - loss: 0.4020 - regression_loss: 0.3568 - classification_loss: 0.0451
 2626/10000 [======>.......................] - ETA: 1:49:27 - loss: 0.4022 - regression_loss: 0.3570 - classification_loss: 0.0451
 2627/10000 [======>.......................] - ETA: 1:49:26 - loss: 0.4023 - regression_loss: 0.3572 - classification_loss: 0.0452
 2628/10000 [======>.......................] - ETA: 1:49:25 - loss: 0.4023 - regression_loss: 0.3572 - classification_loss: 0.0452
 2629/10000 [======>.......................] - ETA: 1:49:24 - loss: 0.4022 - regression_loss: 0.3571 - classification_loss: 0.0452
 2630/10000 [======>.......................] - ETA: 1:49:23 - loss: 0.4022 - regression_loss: 0.3570 - classification_loss: 0.0451
 2631/10000 [======>.......................] - ETA: 1:49:22 - loss: 0.4021 - regression_loss: 0.3570 - classification_loss: 0.0451
 2632/10000 [======>.......................] - ETA: 1:49:22 - loss: 0.4020 - regression_loss: 0.3569 - classification_loss: 0.0451
 2633/10000 [======>.......................] - ETA: 1:49:21 - loss: 0.4020 - regression_loss: 0.3569 - classification_loss: 0.0451
 2634/10000 [======>.......................] - ETA: 1:49:20 - loss: 0.4018 - regression_loss: 0.3568 - classification_loss: 0.0451
 2635/10000 [======>.......................] - ETA: 1:49:19 - loss: 0.4018 - regression_loss: 0.3567 - classification_loss: 0.0451
 2636/10000 [======>.......................] - ETA: 1:49:18 - loss: 0.4018 - regression_loss: 0.3567 - classification_loss: 0.0451
 2637/10000 [======>.......................] - ETA: 1:49:17 - loss: 0.4017 - regression_loss: 0.3566 - classification_loss: 0.0451
 2638/10000 [======>.......................] - ETA: 1:49:16 - loss: 0.4018 - regression_loss: 0.3568 - classification_loss: 0.0451
 2639/10000 [======>.......................] - ETA: 1:49:15 - loss: 0.4018 - regression_loss: 0.3568 - classification_loss: 0.0451
 2640/10000 [======>.......................] - ETA: 1:49:14 - loss: 0.4019 - regression_loss: 0.3569 - classification_loss: 0.0451
 2641/10000 [======>.......................] - ETA: 1:49:13 - loss: 0.4019 - regression_loss: 0.3568 - classification_loss: 0.0451
 2642/10000 [======>.......................] - ETA: 1:49:13 - loss: 0.4019 - regression_loss: 0.3568 - classification_loss: 0.0451
 2643/10000 [======>.......................] - ETA: 1:49:12 - loss: 0.4018 - regression_loss: 0.3567 - classification_loss: 0.0451
 2644/10000 [======>.......................] - ETA: 1:49:11 - loss: 0.4018 - regression_loss: 0.3568 - classification_loss: 0.0450
 2645/10000 [======>.......................] - ETA: 1:49:10 - loss: 0.4017 - regression_loss: 0.3567 - classification_loss: 0.0450
 2646/10000 [======>.......................] - ETA: 1:49:09 - loss: 0.4020 - regression_loss: 0.3569 - classification_loss: 0.0451
 2647/10000 [======>.......................] - ETA: 1:49:08 - loss: 0.4020 - regression_loss: 0.3569 - classification_loss: 0.0450
 2648/10000 [======>.......................] - ETA: 1:49:07 - loss: 0.4022 - regression_loss: 0.3572 - classification_loss: 0.0451
 2649/10000 [======>.......................] - ETA: 1:49:06 - loss: 0.4021 - regression_loss: 0.3571 - classification_loss: 0.0450
 2650/10000 [======>.......................] - ETA: 1:49:05 - loss: 0.4023 - regression_loss: 0.3572 - classification_loss: 0.0451
 2651/10000 [======>.......................] - ETA: 1:49:04 - loss: 0.4023 - regression_loss: 0.3572 - classification_loss: 0.0451
 2652/10000 [======>.......................] - ETA: 1:49:04 - loss: 0.4023 - regression_loss: 0.3572 - classification_loss: 0.0451
 2653/10000 [======>.......................] - ETA: 1:49:03 - loss: 0.4021 - regression_loss: 0.3571 - classification_loss: 0.0450
 2654/10000 [======>.......................] - ETA: 1:49:02 - loss: 0.4021 - regression_loss: 0.3570 - classification_loss: 0.0450
 2655/10000 [======>.......................] - ETA: 1:49:01 - loss: 0.4021 - regression_loss: 0.3570 - classification_loss: 0.0450
 2656/10000 [======>.......................] - ETA: 1:49:00 - loss: 0.4021 - regression_loss: 0.3570 - classification_loss: 0.0450
 2657/10000 [======>.......................] - ETA: 1:48:59 - loss: 0.4020 - regression_loss: 0.3570 - classification_loss: 0.0450
 2658/10000 [======>.......................] - ETA: 1:48:58 - loss: 0.4021 - regression_loss: 0.3571 - classification_loss: 0.0450
 2659/10000 [======>.......................] - ETA: 1:48:57 - loss: 0.4021 - regression_loss: 0.3570 - classification_loss: 0.0450
 2660/10000 [======>.......................] - ETA: 1:48:56 - loss: 0.4021 - regression_loss: 0.3570 - classification_loss: 0.0450
 2661/10000 [======>.......................] - ETA: 1:48:56 - loss: 0.4021 - regression_loss: 0.3570 - classification_loss: 0.0450
 2662/10000 [======>.......................] - ETA: 1:48:55 - loss: 0.4020 - regression_loss: 0.3570 - classification_loss: 0.0450
 2663/10000 [======>.......................] - ETA: 1:48:54 - loss: 0.4020 - regression_loss: 0.3570 - classification_loss: 0.0450
 2664/10000 [======>.......................] - ETA: 1:48:53 - loss: 0.4020 - regression_loss: 0.3570 - classification_loss: 0.0450
 2665/10000 [======>.......................] - ETA: 1:48:52 - loss: 0.4019 - regression_loss: 0.3569 - classification_loss: 0.0450
 2666/10000 [======>.......................] - ETA: 1:48:51 - loss: 0.4018 - regression_loss: 0.3569 - classification_loss: 0.0450
 2667/10000 [=======>......................] - ETA: 1:48:50 - loss: 0.4018 - regression_loss: 0.3568 - classification_loss: 0.0450
 2668/10000 [=======>......................] - ETA: 1:48:49 - loss: 0.4020 - regression_loss: 0.3570 - classification_loss: 0.0450
 2669/10000 [=======>......................] - ETA: 1:48:48 - loss: 0.4020 - regression_loss: 0.3570 - classification_loss: 0.0450
 2670/10000 [=======>......................] - ETA: 1:48:47 - loss: 0.4020 - regression_loss: 0.3570 - classification_loss: 0.0450
 2671/10000 [=======>......................] - ETA: 1:48:47 - loss: 0.4020 - regression_loss: 0.3570 - classification_loss: 0.0450
 2672/10000 [=======>......................] - ETA: 1:48:46 - loss: 0.4020 - regression_loss: 0.3570 - classification_loss: 0.0450
 2673/10000 [=======>......................] - ETA: 1:48:45 - loss: 0.4019 - regression_loss: 0.3569 - classification_loss: 0.0450
 2674/10000 [=======>......................] - ETA: 1:48:44 - loss: 0.4019 - regression_loss: 0.3569 - classification_loss: 0.0450
 2675/10000 [=======>......................] - ETA: 1:48:43 - loss: 0.4019 - regression_loss: 0.3569 - classification_loss: 0.0450
 2676/10000 [=======>......................] - ETA: 1:48:42 - loss: 0.4019 - regression_loss: 0.3569 - classification_loss: 0.0450
 2677/10000 [=======>......................] - ETA: 1:48:41 - loss: 0.4020 - regression_loss: 0.3570 - classification_loss: 0.0450
 2678/10000 [=======>......................] - ETA: 1:48:40 - loss: 0.4020 - regression_loss: 0.3570 - classification_loss: 0.0450
 2679/10000 [=======>......................] - ETA: 1:48:39 - loss: 0.4020 - regression_loss: 0.3570 - classification_loss: 0.0450
 2680/10000 [=======>......................] - ETA: 1:48:38 - loss: 0.4019 - regression_loss: 0.3570 - classification_loss: 0.0450
 2681/10000 [=======>......................] - ETA: 1:48:38 - loss: 0.4019 - regression_loss: 0.3570 - classification_loss: 0.0450
 2682/10000 [=======>......................] - ETA: 1:48:37 - loss: 0.4021 - regression_loss: 0.3571 - classification_loss: 0.0450
 2683/10000 [=======>......................] - ETA: 1:48:36 - loss: 0.4021 - regression_loss: 0.3572 - classification_loss: 0.0449
 2684/10000 [=======>......................] - ETA: 1:48:35 - loss: 0.4023 - regression_loss: 0.3572 - classification_loss: 0.0450
 2685/10000 [=======>......................] - ETA: 1:48:34 - loss: 0.4024 - regression_loss: 0.3573 - classification_loss: 0.0450
 2686/10000 [=======>......................] - ETA: 1:48:33 - loss: 0.4023 - regression_loss: 0.3573 - classification_loss: 0.0450
 2687/10000 [=======>......................] - ETA: 1:48:32 - loss: 0.4024 - regression_loss: 0.3574 - classification_loss: 0.0450
 2688/10000 [=======>......................] - ETA: 1:48:31 - loss: 0.4024 - regression_loss: 0.3574 - classification_loss: 0.0450
 2689/10000 [=======>......................] - ETA: 1:48:30 - loss: 0.4024 - regression_loss: 0.3574 - classification_loss: 0.0450
 2690/10000 [=======>......................] - ETA: 1:48:29 - loss: 0.4024 - regression_loss: 0.3574 - classification_loss: 0.0450
 2691/10000 [=======>......................] - ETA: 1:48:29 - loss: 0.4023 - regression_loss: 0.3573 - classification_loss: 0.0450
 2692/10000 [=======>......................] - ETA: 1:48:28 - loss: 0.4022 - regression_loss: 0.3572 - classification_loss: 0.0450
 2693/10000 [=======>......................] - ETA: 1:48:27 - loss: 0.4021 - regression_loss: 0.3571 - classification_loss: 0.0450
 2694/10000 [=======>......................] - ETA: 1:48:26 - loss: 0.4020 - regression_loss: 0.3570 - classification_loss: 0.0450
 2695/10000 [=======>......................] - ETA: 1:48:25 - loss: 0.4020 - regression_loss: 0.3570 - classification_loss: 0.0450
 2696/10000 [=======>......................] - ETA: 1:48:24 - loss: 0.4023 - regression_loss: 0.3573 - classification_loss: 0.0450
 2697/10000 [=======>......................] - ETA: 1:48:23 - loss: 0.4023 - regression_loss: 0.3573 - classification_loss: 0.0450
 2698/10000 [=======>......................] - ETA: 1:48:22 - loss: 0.4022 - regression_loss: 0.3572 - classification_loss: 0.0450
 2699/10000 [=======>......................] - ETA: 1:48:21 - loss: 0.4022 - regression_loss: 0.3572 - classification_loss: 0.0450
 2700/10000 [=======>......................] - ETA: 1:48:20 - loss: 0.4024 - regression_loss: 0.3574 - classification_loss: 0.0450
 2701/10000 [=======>......................] - ETA: 1:48:20 - loss: 0.4026 - regression_loss: 0.3576 - classification_loss: 0.0450
 2702/10000 [=======>......................] - ETA: 1:48:19 - loss: 0.4027 - regression_loss: 0.3577 - classification_loss: 0.0450
 2703/10000 [=======>......................] - ETA: 1:48:18 - loss: 0.4026 - regression_loss: 0.3576 - classification_loss: 0.0450
 2704/10000 [=======>......................] - ETA: 1:48:17 - loss: 0.4026 - regression_loss: 0.3576 - classification_loss: 0.0450
 2705/10000 [=======>......................] - ETA: 1:48:16 - loss: 0.4026 - regression_loss: 0.3576 - classification_loss: 0.0450
 2706/10000 [=======>......................] - ETA: 1:48:15 - loss: 0.4024 - regression_loss: 0.3575 - classification_loss: 0.0449
 2707/10000 [=======>......................] - ETA: 1:48:14 - loss: 0.4023 - regression_loss: 0.3574 - classification_loss: 0.0449
 2708/10000 [=======>......................] - ETA: 1:48:13 - loss: 0.4023 - regression_loss: 0.3574 - classification_loss: 0.0449
 2709/10000 [=======>......................] - ETA: 1:48:12 - loss: 0.4022 - regression_loss: 0.3573 - classification_loss: 0.0449
 2710/10000 [=======>......................] - ETA: 1:48:12 - loss: 0.4024 - regression_loss: 0.3575 - classification_loss: 0.0449
 2711/10000 [=======>......................] - ETA: 1:48:11 - loss: 0.4025 - regression_loss: 0.3576 - classification_loss: 0.0449
 2712/10000 [=======>......................] - ETA: 1:48:10 - loss: 0.4024 - regression_loss: 0.3575 - classification_loss: 0.0449
 2713/10000 [=======>......................] - ETA: 1:48:09 - loss: 0.4025 - regression_loss: 0.3576 - classification_loss: 0.0449
 2714/10000 [=======>......................] - ETA: 1:48:08 - loss: 0.4023 - regression_loss: 0.3575 - classification_loss: 0.0449
 2715/10000 [=======>......................] - ETA: 1:48:07 - loss: 0.4024 - regression_loss: 0.3575 - classification_loss: 0.0449
 2716/10000 [=======>......................] - ETA: 1:48:06 - loss: 0.4023 - regression_loss: 0.3574 - classification_loss: 0.0448
 2717/10000 [=======>......................] - ETA: 1:48:05 - loss: 0.4022 - regression_loss: 0.3574 - classification_loss: 0.0448
 2718/10000 [=======>......................] - ETA: 1:48:04 - loss: 0.4021 - regression_loss: 0.3573 - classification_loss: 0.0448
 2719/10000 [=======>......................] - ETA: 1:48:03 - loss: 0.4021 - regression_loss: 0.3573 - classification_loss: 0.0448
 2720/10000 [=======>......................] - ETA: 1:48:03 - loss: 0.4020 - regression_loss: 0.3572 - classification_loss: 0.0448
 2721/10000 [=======>......................] - ETA: 1:48:02 - loss: 0.4020 - regression_loss: 0.3572 - classification_loss: 0.0448
 2722/10000 [=======>......................] - ETA: 1:48:01 - loss: 0.4020 - regression_loss: 0.3572 - classification_loss: 0.0448
 2723/10000 [=======>......................] - ETA: 1:48:00 - loss: 0.4018 - regression_loss: 0.3570 - classification_loss: 0.0448
 2724/10000 [=======>......................] - ETA: 1:47:59 - loss: 0.4017 - regression_loss: 0.3570 - classification_loss: 0.0448
 2725/10000 [=======>......................] - ETA: 1:47:58 - loss: 0.4017 - regression_loss: 0.3569 - classification_loss: 0.0448
 2726/10000 [=======>......................] - ETA: 1:47:57 - loss: 0.4017 - regression_loss: 0.3570 - classification_loss: 0.0448
 2727/10000 [=======>......................] - ETA: 1:47:56 - loss: 0.4017 - regression_loss: 0.3570 - classification_loss: 0.0448
 2728/10000 [=======>......................] - ETA: 1:47:55 - loss: 0.4016 - regression_loss: 0.3568 - classification_loss: 0.0447
 2729/10000 [=======>......................] - ETA: 1:47:54 - loss: 0.4016 - regression_loss: 0.3568 - classification_loss: 0.0448
 2730/10000 [=======>......................] - ETA: 1:47:54 - loss: 0.4015 - regression_loss: 0.3567 - classification_loss: 0.0448
 2731/10000 [=======>......................] - ETA: 1:47:53 - loss: 0.4015 - regression_loss: 0.3567 - classification_loss: 0.0448
 2732/10000 [=======>......................] - ETA: 1:47:52 - loss: 0.4016 - regression_loss: 0.3569 - classification_loss: 0.0448
 2733/10000 [=======>......................] - ETA: 1:47:51 - loss: 0.4015 - regression_loss: 0.3567 - classification_loss: 0.0447
 2734/10000 [=======>......................] - ETA: 1:47:50 - loss: 0.4016 - regression_loss: 0.3568 - classification_loss: 0.0447
 2735/10000 [=======>......................] - ETA: 1:47:49 - loss: 0.4015 - regression_loss: 0.3568 - classification_loss: 0.0448
 2736/10000 [=======>......................] - ETA: 1:47:48 - loss: 0.4015 - regression_loss: 0.3567 - classification_loss: 0.0447
 2737/10000 [=======>......................] - ETA: 1:47:47 - loss: 0.4014 - regression_loss: 0.3567 - classification_loss: 0.0447
 2738/10000 [=======>......................] - ETA: 1:47:46 - loss: 0.4016 - regression_loss: 0.3568 - classification_loss: 0.0447
 2739/10000 [=======>......................] - ETA: 1:47:45 - loss: 0.4015 - regression_loss: 0.3568 - classification_loss: 0.0447
 2740/10000 [=======>......................] - ETA: 1:47:45 - loss: 0.4015 - regression_loss: 0.3567 - classification_loss: 0.0447
 2741/10000 [=======>......................] - ETA: 1:47:44 - loss: 0.4014 - regression_loss: 0.3567 - classification_loss: 0.0447
 2742/10000 [=======>......................] - ETA: 1:47:43 - loss: 0.4014 - regression_loss: 0.3566 - classification_loss: 0.0447
 2743/10000 [=======>......................] - ETA: 1:47:42 - loss: 0.4015 - regression_loss: 0.3568 - classification_loss: 0.0447
 2744/10000 [=======>......................] - ETA: 1:47:41 - loss: 0.4015 - regression_loss: 0.3568 - classification_loss: 0.0447
 2745/10000 [=======>......................] - ETA: 1:47:40 - loss: 0.4014 - regression_loss: 0.3567 - classification_loss: 0.0447
 2746/10000 [=======>......................] - ETA: 1:47:39 - loss: 0.4015 - regression_loss: 0.3568 - classification_loss: 0.0447
 2747/10000 [=======>......................] - ETA: 1:47:38 - loss: 0.4015 - regression_loss: 0.3568 - classification_loss: 0.0447
 2748/10000 [=======>......................] - ETA: 1:47:37 - loss: 0.4014 - regression_loss: 0.3567 - classification_loss: 0.0447
 2749/10000 [=======>......................] - ETA: 1:47:37 - loss: 0.4014 - regression_loss: 0.3567 - classification_loss: 0.0447
 2750/10000 [=======>......................] - ETA: 1:47:36 - loss: 0.4013 - regression_loss: 0.3565 - classification_loss: 0.0447
 2751/10000 [=======>......................] - ETA: 1:47:35 - loss: 0.4012 - regression_loss: 0.3565 - classification_loss: 0.0447
 2752/10000 [=======>......................] - ETA: 1:47:34 - loss: 0.4011 - regression_loss: 0.3564 - classification_loss: 0.0447
 2753/10000 [=======>......................] - ETA: 1:47:33 - loss: 0.4010 - regression_loss: 0.3564 - classification_loss: 0.0447
 2754/10000 [=======>......................] - ETA: 1:47:32 - loss: 0.4011 - regression_loss: 0.3564 - classification_loss: 0.0447
 2755/10000 [=======>......................] - ETA: 1:47:31 - loss: 0.4010 - regression_loss: 0.3564 - classification_loss: 0.0447
 2756/10000 [=======>......................] - ETA: 1:47:30 - loss: 0.4010 - regression_loss: 0.3564 - classification_loss: 0.0447
 2757/10000 [=======>......................] - ETA: 1:47:29 - loss: 0.4009 - regression_loss: 0.3562 - classification_loss: 0.0446
 2758/10000 [=======>......................] - ETA: 1:47:28 - loss: 0.4008 - regression_loss: 0.3561 - classification_loss: 0.0446
 2759/10000 [=======>......................] - ETA: 1:47:28 - loss: 0.4007 - regression_loss: 0.3561 - classification_loss: 0.0446
 2760/10000 [=======>......................] - ETA: 1:47:27 - loss: 0.4007 - regression_loss: 0.3561 - classification_loss: 0.0446
 2761/10000 [=======>......................] - ETA: 1:47:26 - loss: 0.4006 - regression_loss: 0.3560 - classification_loss: 0.0446
 2762/10000 [=======>......................] - ETA: 1:47:25 - loss: 0.4006 - regression_loss: 0.3560 - classification_loss: 0.0446
 2763/10000 [=======>......................] - ETA: 1:47:24 - loss: 0.4005 - regression_loss: 0.3560 - classification_loss: 0.0446
 2764/10000 [=======>......................] - ETA: 1:47:23 - loss: 0.4006 - regression_loss: 0.3560 - classification_loss: 0.0446
 2765/10000 [=======>......................] - ETA: 1:47:22 - loss: 0.4005 - regression_loss: 0.3559 - classification_loss: 0.0446
 2766/10000 [=======>......................] - ETA: 1:47:21 - loss: 0.4005 - regression_loss: 0.3559 - classification_loss: 0.0446
 2767/10000 [=======>......................] - ETA: 1:47:20 - loss: 0.4004 - regression_loss: 0.3559 - classification_loss: 0.0446
 2768/10000 [=======>......................] - ETA: 1:47:19 - loss: 0.4004 - regression_loss: 0.3558 - classification_loss: 0.0446
 2769/10000 [=======>......................] - ETA: 1:47:19 - loss: 0.4003 - regression_loss: 0.3557 - classification_loss: 0.0445
 2770/10000 [=======>......................] - ETA: 1:47:18 - loss: 0.4002 - regression_loss: 0.3557 - classification_loss: 0.0445
 2771/10000 [=======>......................] - ETA: 1:47:17 - loss: 0.4002 - regression_loss: 0.3556 - classification_loss: 0.0445
 2772/10000 [=======>......................] - ETA: 1:47:16 - loss: 0.4001 - regression_loss: 0.3556 - classification_loss: 0.0445
 2773/10000 [=======>......................] - ETA: 1:47:15 - loss: 0.4000 - regression_loss: 0.3555 - classification_loss: 0.0445
 2774/10000 [=======>......................] - ETA: 1:47:14 - loss: 0.4000 - regression_loss: 0.3555 - classification_loss: 0.0445
 2775/10000 [=======>......................] - ETA: 1:47:13 - loss: 0.4000 - regression_loss: 0.3555 - classification_loss: 0.0445
 2776/10000 [=======>......................] - ETA: 1:47:12 - loss: 0.4000 - regression_loss: 0.3555 - classification_loss: 0.0445
 2777/10000 [=======>......................] - ETA: 1:47:11 - loss: 0.3999 - regression_loss: 0.3554 - classification_loss: 0.0445
 2778/10000 [=======>......................] - ETA: 1:47:10 - loss: 0.3999 - regression_loss: 0.3554 - classification_loss: 0.0444
 2779/10000 [=======>......................] - ETA: 1:47:10 - loss: 0.3999 - regression_loss: 0.3555 - classification_loss: 0.0444
 2780/10000 [=======>......................] - ETA: 1:47:09 - loss: 0.3999 - regression_loss: 0.3555 - classification_loss: 0.0444
 2781/10000 [=======>......................] - ETA: 1:47:08 - loss: 0.3998 - regression_loss: 0.3554 - classification_loss: 0.0444
 2782/10000 [=======>......................] - ETA: 1:47:07 - loss: 0.4001 - regression_loss: 0.3557 - classification_loss: 0.0444
 2783/10000 [=======>......................] - ETA: 1:47:06 - loss: 0.4000 - regression_loss: 0.3555 - classification_loss: 0.0444
 2784/10000 [=======>......................] - ETA: 1:47:05 - loss: 0.3998 - regression_loss: 0.3554 - classification_loss: 0.0444
 2785/10000 [=======>......................] - ETA: 1:47:04 - loss: 0.3999 - regression_loss: 0.3555 - classification_loss: 0.0444
 2786/10000 [=======>......................] - ETA: 1:47:03 - loss: 0.3998 - regression_loss: 0.3554 - classification_loss: 0.0444
 2787/10000 [=======>......................] - ETA: 1:47:02 - loss: 0.3998 - regression_loss: 0.3555 - classification_loss: 0.0444
 2788/10000 [=======>......................] - ETA: 1:47:02 - loss: 0.3998 - regression_loss: 0.3555 - classification_loss: 0.0444
 2789/10000 [=======>......................] - ETA: 1:47:01 - loss: 0.3998 - regression_loss: 0.3554 - classification_loss: 0.0444
 2790/10000 [=======>......................] - ETA: 1:47:00 - loss: 0.3997 - regression_loss: 0.3553 - classification_loss: 0.0444
 2791/10000 [=======>......................] - ETA: 1:46:59 - loss: 0.3997 - regression_loss: 0.3554 - classification_loss: 0.0444
 2792/10000 [=======>......................] - ETA: 1:46:58 - loss: 0.3998 - regression_loss: 0.3555 - classification_loss: 0.0444
 2793/10000 [=======>......................] - ETA: 1:46:57 - loss: 0.3998 - regression_loss: 0.3554 - classification_loss: 0.0444
 2794/10000 [=======>......................] - ETA: 1:46:56 - loss: 0.3998 - regression_loss: 0.3554 - classification_loss: 0.0444
 2795/10000 [=======>......................] - ETA: 1:46:55 - loss: 0.3999 - regression_loss: 0.3555 - classification_loss: 0.0444
 2796/10000 [=======>......................] - ETA: 1:46:54 - loss: 0.3999 - regression_loss: 0.3555 - classification_loss: 0.0444
 2797/10000 [=======>......................] - ETA: 1:46:53 - loss: 0.4000 - regression_loss: 0.3556 - classification_loss: 0.0444
 2798/10000 [=======>......................] - ETA: 1:46:53 - loss: 0.4000 - regression_loss: 0.3556 - classification_loss: 0.0445
 2799/10000 [=======>......................] - ETA: 1:46:52 - loss: 0.4003 - regression_loss: 0.3558 - classification_loss: 0.0445
 2800/10000 [=======>......................] - ETA: 1:46:51 - loss: 0.4002 - regression_loss: 0.3557 - classification_loss: 0.0445
 2801/10000 [=======>......................] - ETA: 1:46:50 - loss: 0.4006 - regression_loss: 0.3561 - classification_loss: 0.0445
 2802/10000 [=======>......................] - ETA: 1:46:49 - loss: 0.4006 - regression_loss: 0.3560 - classification_loss: 0.0445
 2803/10000 [=======>......................] - ETA: 1:46:48 - loss: 0.4006 - regression_loss: 0.3561 - classification_loss: 0.0445
 2804/10000 [=======>......................] - ETA: 1:46:47 - loss: 0.4005 - regression_loss: 0.3560 - classification_loss: 0.0445
 2805/10000 [=======>......................] - ETA: 1:46:46 - loss: 0.4005 - regression_loss: 0.3560 - classification_loss: 0.0445
 2806/10000 [=======>......................] - ETA: 1:46:45 - loss: 0.4005 - regression_loss: 0.3560 - classification_loss: 0.0445
 2807/10000 [=======>......................] - ETA: 1:46:44 - loss: 0.4004 - regression_loss: 0.3560 - classification_loss: 0.0445
 2808/10000 [=======>......................] - ETA: 1:46:44 - loss: 0.4004 - regression_loss: 0.3559 - classification_loss: 0.0445
 2809/10000 [=======>......................] - ETA: 1:46:43 - loss: 0.4004 - regression_loss: 0.3559 - classification_loss: 0.0445
 2810/10000 [=======>......................] - ETA: 1:46:42 - loss: 0.4004 - regression_loss: 0.3560 - classification_loss: 0.0445
 2811/10000 [=======>......................] - ETA: 1:46:41 - loss: 0.4005 - regression_loss: 0.3560 - classification_loss: 0.0445
 2812/10000 [=======>......................] - ETA: 1:46:40 - loss: 0.4004 - regression_loss: 0.3560 - classification_loss: 0.0445
 2813/10000 [=======>......................] - ETA: 1:46:39 - loss: 0.4003 - regression_loss: 0.3558 - classification_loss: 0.0445
 2814/10000 [=======>......................] - ETA: 1:46:38 - loss: 0.4003 - regression_loss: 0.3559 - classification_loss: 0.0445
 2815/10000 [=======>......................] - ETA: 1:46:37 - loss: 0.4002 - regression_loss: 0.3558 - classification_loss: 0.0444
 2816/10000 [=======>......................] - ETA: 1:46:36 - loss: 0.4004 - regression_loss: 0.3560 - classification_loss: 0.0445
 2817/10000 [=======>......................] - ETA: 1:46:36 - loss: 0.4004 - regression_loss: 0.3559 - classification_loss: 0.0444
 2818/10000 [=======>......................] - ETA: 1:46:35 - loss: 0.4039 - regression_loss: 0.3561 - classification_loss: 0.0479
 2819/10000 [=======>......................] - ETA: 1:46:34 - loss: 0.4039 - regression_loss: 0.3561 - classification_loss: 0.0478
 2820/10000 [=======>......................] - ETA: 1:46:33 - loss: 0.4039 - regression_loss: 0.3561 - classification_loss: 0.0478
 2821/10000 [=======>......................] - ETA: 1:46:32 - loss: 0.4040 - regression_loss: 0.3562 - classification_loss: 0.0478
 2822/10000 [=======>......................] - ETA: 1:46:31 - loss: 0.4042 - regression_loss: 0.3564 - classification_loss: 0.0479
 2823/10000 [=======>......................] - ETA: 1:46:30 - loss: 0.4041 - regression_loss: 0.3562 - classification_loss: 0.0478
 2824/10000 [=======>......................] - ETA: 1:46:29 - loss: 0.4040 - regression_loss: 0.3562 - classification_loss: 0.0478
 2825/10000 [=======>......................] - ETA: 1:46:28 - loss: 0.4039 - regression_loss: 0.3561 - classification_loss: 0.0478
 2826/10000 [=======>......................] - ETA: 1:46:27 - loss: 0.4039 - regression_loss: 0.3561 - classification_loss: 0.0478
 2827/10000 [=======>......................] - ETA: 1:46:27 - loss: 0.4038 - regression_loss: 0.3560 - classification_loss: 0.0478
 2828/10000 [=======>......................] - ETA: 1:46:26 - loss: 0.4037 - regression_loss: 0.3559 - classification_loss: 0.0478
 2829/10000 [=======>......................] - ETA: 1:46:25 - loss: 0.4037 - regression_loss: 0.3559 - classification_loss: 0.0478
 2830/10000 [=======>......................] - ETA: 1:46:24 - loss: 0.4037 - regression_loss: 0.3559 - classification_loss: 0.0478
 2831/10000 [=======>......................] - ETA: 1:46:23 - loss: 0.4038 - regression_loss: 0.3559 - classification_loss: 0.0478
 2832/10000 [=======>......................] - ETA: 1:46:22 - loss: 0.4039 - regression_loss: 0.3560 - classification_loss: 0.0479
 2833/10000 [=======>......................] - ETA: 1:46:21 - loss: 0.4039 - regression_loss: 0.3560 - classification_loss: 0.0478
 2834/10000 [=======>......................] - ETA: 1:46:20 - loss: 0.4042 - regression_loss: 0.3563 - classification_loss: 0.0479
 2835/10000 [=======>......................] - ETA: 1:46:19 - loss: 0.4042 - regression_loss: 0.3563 - classification_loss: 0.0479
 2836/10000 [=======>......................] - ETA: 1:46:18 - loss: 0.4042 - regression_loss: 0.3563 - classification_loss: 0.0479
 2837/10000 [=======>......................] - ETA: 1:46:18 - loss: 0.4041 - regression_loss: 0.3563 - classification_loss: 0.0478
 2838/10000 [=======>......................] - ETA: 1:46:17 - loss: 0.4040 - regression_loss: 0.3561 - classification_loss: 0.0478
 2839/10000 [=======>......................] - ETA: 1:46:16 - loss: 0.4039 - regression_loss: 0.3561 - classification_loss: 0.0478
 2840/10000 [=======>......................] - ETA: 1:46:15 - loss: 0.4039 - regression_loss: 0.3561 - classification_loss: 0.0478
 2841/10000 [=======>......................] - ETA: 1:46:14 - loss: 0.4042 - regression_loss: 0.3564 - classification_loss: 0.0478
 2842/10000 [=======>......................] - ETA: 1:46:13 - loss: 0.4042 - regression_loss: 0.3564 - classification_loss: 0.0478
 2843/10000 [=======>......................] - ETA: 1:46:12 - loss: 0.4042 - regression_loss: 0.3564 - classification_loss: 0.0478
 2844/10000 [=======>......................] - ETA: 1:46:11 - loss: 0.4042 - regression_loss: 0.3564 - classification_loss: 0.0478
 2845/10000 [=======>......................] - ETA: 1:46:10 - loss: 0.4042 - regression_loss: 0.3564 - classification_loss: 0.0478
 2846/10000 [=======>......................] - ETA: 1:46:10 - loss: 0.4043 - regression_loss: 0.3565 - classification_loss: 0.0478
 2847/10000 [=======>......................] - ETA: 1:46:09 - loss: 0.4046 - regression_loss: 0.3567 - classification_loss: 0.0479
 2848/10000 [=======>......................] - ETA: 1:46:08 - loss: 0.4045 - regression_loss: 0.3566 - classification_loss: 0.0478
 2849/10000 [=======>......................] - ETA: 1:46:07 - loss: 0.4044 - regression_loss: 0.3566 - classification_loss: 0.0478
 2850/10000 [=======>......................] - ETA: 1:46:06 - loss: 0.4044 - regression_loss: 0.3566 - classification_loss: 0.0478
 2851/10000 [=======>......................] - ETA: 1:46:05 - loss: 0.4045 - regression_loss: 0.3567 - classification_loss: 0.0478
 2852/10000 [=======>......................] - ETA: 1:46:04 - loss: 0.4044 - regression_loss: 0.3566 - classification_loss: 0.0478
 2853/10000 [=======>......................] - ETA: 1:46:03 - loss: 0.4043 - regression_loss: 0.3565 - classification_loss: 0.0478
 2854/10000 [=======>......................] - ETA: 1:46:02 - loss: 0.4043 - regression_loss: 0.3565 - classification_loss: 0.0478
 2855/10000 [=======>......................] - ETA: 1:46:01 - loss: 0.4043 - regression_loss: 0.3565 - classification_loss: 0.0478
 2856/10000 [=======>......................] - ETA: 1:46:01 - loss: 0.4043 - regression_loss: 0.3565 - classification_loss: 0.0478
 2857/10000 [=======>......................] - ETA: 1:46:00 - loss: 0.4043 - regression_loss: 0.3565 - classification_loss: 0.0478
 2858/10000 [=======>......................] - ETA: 1:45:59 - loss: 0.4043 - regression_loss: 0.3565 - classification_loss: 0.0478
 2859/10000 [=======>......................] - ETA: 1:45:58 - loss: 0.4044 - regression_loss: 0.3566 - classification_loss: 0.0478
 2860/10000 [=======>......................] - ETA: 1:45:57 - loss: 0.4043 - regression_loss: 0.3566 - classification_loss: 0.0478
 2861/10000 [=======>......................] - ETA: 1:45:56 - loss: 0.4043 - regression_loss: 0.3565 - classification_loss: 0.0478
 2862/10000 [=======>......................] - ETA: 1:45:55 - loss: 0.4043 - regression_loss: 0.3565 - classification_loss: 0.0478
 2863/10000 [=======>......................] - ETA: 1:45:54 - loss: 0.4043 - regression_loss: 0.3566 - classification_loss: 0.0478
 2864/10000 [=======>......................] - ETA: 1:45:53 - loss: 0.4042 - regression_loss: 0.3564 - classification_loss: 0.0478
 2865/10000 [=======>......................] - ETA: 1:45:52 - loss: 0.4041 - regression_loss: 0.3564 - classification_loss: 0.0478
 2866/10000 [=======>......................] - ETA: 1:45:52 - loss: 0.4041 - regression_loss: 0.3563 - classification_loss: 0.0478
 2867/10000 [=======>......................] - ETA: 1:45:51 - loss: 0.4041 - regression_loss: 0.3564 - classification_loss: 0.0477
 2868/10000 [=======>......................] - ETA: 1:45:50 - loss: 0.4040 - regression_loss: 0.3562 - classification_loss: 0.0477
 2869/10000 [=======>......................] - ETA: 1:45:49 - loss: 0.4040 - regression_loss: 0.3562 - classification_loss: 0.0477
 2870/10000 [=======>......................] - ETA: 1:45:48 - loss: 0.4043 - regression_loss: 0.3565 - classification_loss: 0.0478
 2871/10000 [=======>......................] - ETA: 1:45:47 - loss: 0.4041 - regression_loss: 0.3564 - classification_loss: 0.0477
 2872/10000 [=======>......................] - ETA: 1:45:46 - loss: 0.4041 - regression_loss: 0.3563 - classification_loss: 0.0477
 2873/10000 [=======>......................] - ETA: 1:45:45 - loss: 0.4040 - regression_loss: 0.3563 - classification_loss: 0.0477
 2874/10000 [=======>......................] - ETA: 1:45:44 - loss: 0.4039 - regression_loss: 0.3562 - classification_loss: 0.0477
 2875/10000 [=======>......................] - ETA: 1:45:44 - loss: 0.4039 - regression_loss: 0.3562 - classification_loss: 0.0477
 2876/10000 [=======>......................] - ETA: 1:45:43 - loss: 0.4038 - regression_loss: 0.3562 - classification_loss: 0.0477
 2877/10000 [=======>......................] - ETA: 1:45:42 - loss: 0.4037 - regression_loss: 0.3560 - classification_loss: 0.0477
 2878/10000 [=======>......................] - ETA: 1:45:41 - loss: 0.4037 - regression_loss: 0.3560 - classification_loss: 0.0477
 2879/10000 [=======>......................] - ETA: 1:45:40 - loss: 0.4036 - regression_loss: 0.3560 - classification_loss: 0.0477
 2880/10000 [=======>......................] - ETA: 1:45:39 - loss: 0.4036 - regression_loss: 0.3560 - classification_loss: 0.0477
 2881/10000 [=======>......................] - ETA: 1:45:38 - loss: 0.4036 - regression_loss: 0.3560 - classification_loss: 0.0477
 2882/10000 [=======>......................] - ETA: 1:45:37 - loss: 0.4037 - regression_loss: 0.3560 - classification_loss: 0.0477
 2883/10000 [=======>......................] - ETA: 1:45:36 - loss: 0.4037 - regression_loss: 0.3560 - classification_loss: 0.0477
 2884/10000 [=======>......................] - ETA: 1:45:35 - loss: 0.4039 - regression_loss: 0.3562 - classification_loss: 0.0477
 2885/10000 [=======>......................] - ETA: 1:45:35 - loss: 0.4038 - regression_loss: 0.3561 - classification_loss: 0.0477
 2886/10000 [=======>......................] - ETA: 1:45:34 - loss: 0.4039 - regression_loss: 0.3562 - classification_loss: 0.0477
 2887/10000 [=======>......................] - ETA: 1:45:33 - loss: 0.4038 - regression_loss: 0.3561 - classification_loss: 0.0477
 2888/10000 [=======>......................] - ETA: 1:45:32 - loss: 0.4038 - regression_loss: 0.3561 - classification_loss: 0.0477
 2889/10000 [=======>......................] - ETA: 1:45:31 - loss: 0.4038 - regression_loss: 0.3561 - classification_loss: 0.0477
 2890/10000 [=======>......................] - ETA: 1:45:30 - loss: 0.4038 - regression_loss: 0.3560 - classification_loss: 0.0477
 2891/10000 [=======>......................] - ETA: 1:45:29 - loss: 0.4038 - regression_loss: 0.3561 - classification_loss: 0.0477
 2892/10000 [=======>......................] - ETA: 1:45:28 - loss: 0.4039 - regression_loss: 0.3562 - classification_loss: 0.0477
 2893/10000 [=======>......................] - ETA: 1:45:27 - loss: 0.4039 - regression_loss: 0.3561 - classification_loss: 0.0477
 2894/10000 [=======>......................] - ETA: 1:45:27 - loss: 0.4041 - regression_loss: 0.3563 - classification_loss: 0.0478
 2895/10000 [=======>......................] - ETA: 1:45:26 - loss: 0.4041 - regression_loss: 0.3563 - classification_loss: 0.0478
 2896/10000 [=======>......................] - ETA: 1:45:25 - loss: 0.4040 - regression_loss: 0.3563 - classification_loss: 0.0477
 2897/10000 [=======>......................] - ETA: 1:45:24 - loss: 0.4039 - regression_loss: 0.3562 - classification_loss: 0.0477
 2898/10000 [=======>......................] - ETA: 1:45:23 - loss: 0.4038 - regression_loss: 0.3561 - classification_loss: 0.0477
 2899/10000 [=======>......................] - ETA: 1:45:22 - loss: 0.4039 - regression_loss: 0.3561 - classification_loss: 0.0477
 2900/10000 [=======>......................] - ETA: 1:45:21 - loss: 0.4039 - regression_loss: 0.3562 - classification_loss: 0.0477
 2901/10000 [=======>......................] - ETA: 1:45:20 - loss: 0.4039 - regression_loss: 0.3562 - classification_loss: 0.0477
 2902/10000 [=======>......................] - ETA: 1:45:19 - loss: 0.4039 - regression_loss: 0.3562 - classification_loss: 0.0477
 2903/10000 [=======>......................] - ETA: 1:45:18 - loss: 0.4039 - regression_loss: 0.3562 - classification_loss: 0.0477
 2904/10000 [=======>......................] - ETA: 1:45:18 - loss: 0.4039 - regression_loss: 0.3562 - classification_loss: 0.0477
 2905/10000 [=======>......................] - ETA: 1:45:17 - loss: 0.4040 - regression_loss: 0.3562 - classification_loss: 0.0477
 2906/10000 [=======>......................] - ETA: 1:45:16 - loss: 0.4040 - regression_loss: 0.3563 - classification_loss: 0.0477
 2907/10000 [=======>......................] - ETA: 1:45:15 - loss: 0.4040 - regression_loss: 0.3563 - classification_loss: 0.0477
 2908/10000 [=======>......................] - ETA: 1:45:14 - loss: 0.4039 - regression_loss: 0.3562 - classification_loss: 0.0477
 2909/10000 [=======>......................] - ETA: 1:45:13 - loss: 0.4040 - regression_loss: 0.3562 - classification_loss: 0.0477
 2910/10000 [=======>......................] - ETA: 1:45:12 - loss: 0.4040 - regression_loss: 0.3563 - classification_loss: 0.0477
 2911/10000 [=======>......................] - ETA: 1:45:11 - loss: 0.4040 - regression_loss: 0.3563 - classification_loss: 0.0477
 2912/10000 [=======>......................] - ETA: 1:45:10 - loss: 0.4038 - regression_loss: 0.3562 - classification_loss: 0.0477
 2913/10000 [=======>......................] - ETA: 1:45:09 - loss: 0.4039 - regression_loss: 0.3562 - classification_loss: 0.0477
 2914/10000 [=======>......................] - ETA: 1:45:09 - loss: 0.4039 - regression_loss: 0.3562 - classification_loss: 0.0477
 2915/10000 [=======>......................] - ETA: 1:45:08 - loss: 0.4039 - regression_loss: 0.3562 - classification_loss: 0.0476
 2916/10000 [=======>......................] - ETA: 1:45:07 - loss: 0.4038 - regression_loss: 0.3562 - classification_loss: 0.0476
 2917/10000 [=======>......................] - ETA: 1:45:06 - loss: 0.4039 - regression_loss: 0.3563 - classification_loss: 0.0476
 2918/10000 [=======>......................] - ETA: 1:45:05 - loss: 0.4038 - regression_loss: 0.3562 - classification_loss: 0.0476
 2919/10000 [=======>......................] - ETA: 1:45:04 - loss: 0.4038 - regression_loss: 0.3562 - classification_loss: 0.0476
 2920/10000 [=======>......................] - ETA: 1:45:03 - loss: 0.4039 - regression_loss: 0.3563 - classification_loss: 0.0476
 2921/10000 [=======>......................] - ETA: 1:45:02 - loss: 0.4038 - regression_loss: 0.3562 - classification_loss: 0.0476
 2922/10000 [=======>......................] - ETA: 1:45:01 - loss: 0.4038 - regression_loss: 0.3562 - classification_loss: 0.0476
 2923/10000 [=======>......................] - ETA: 1:45:01 - loss: 0.4037 - regression_loss: 0.3561 - classification_loss: 0.0476
 2924/10000 [=======>......................] - ETA: 1:45:00 - loss: 0.4037 - regression_loss: 0.3561 - classification_loss: 0.0476
 2925/10000 [=======>......................] - ETA: 1:44:59 - loss: 0.4036 - regression_loss: 0.3560 - classification_loss: 0.0476
 2926/10000 [=======>......................] - ETA: 1:44:58 - loss: 0.4036 - regression_loss: 0.3560 - classification_loss: 0.0476
 2927/10000 [=======>......................] - ETA: 1:44:57 - loss: 0.4036 - regression_loss: 0.3560 - classification_loss: 0.0476
 2928/10000 [=======>......................] - ETA: 1:44:56 - loss: 0.4036 - regression_loss: 0.3560 - classification_loss: 0.0476
 2929/10000 [=======>......................] - ETA: 1:44:55 - loss: 0.4036 - regression_loss: 0.3560 - classification_loss: 0.0476
 2930/10000 [=======>......................] - ETA: 1:44:54 - loss: 0.4036 - regression_loss: 0.3560 - classification_loss: 0.0476
 2931/10000 [=======>......................] - ETA: 1:44:53 - loss: 0.4036 - regression_loss: 0.3559 - classification_loss: 0.0476
 2932/10000 [=======>......................] - ETA: 1:44:52 - loss: 0.4036 - regression_loss: 0.3560 - classification_loss: 0.0476
 2933/10000 [=======>......................] - ETA: 1:44:52 - loss: 0.4036 - regression_loss: 0.3560 - classification_loss: 0.0476
 2934/10000 [=======>......................] - ETA: 1:44:51 - loss: 0.4036 - regression_loss: 0.3560 - classification_loss: 0.0476
 2935/10000 [=======>......................] - ETA: 1:44:50 - loss: 0.4037 - regression_loss: 0.3561 - classification_loss: 0.0476
 2936/10000 [=======>......................] - ETA: 1:44:49 - loss: 0.4036 - regression_loss: 0.3560 - classification_loss: 0.0476
 2937/10000 [=======>......................] - ETA: 1:44:48 - loss: 0.4035 - regression_loss: 0.3560 - classification_loss: 0.0476
 2938/10000 [=======>......................] - ETA: 1:44:47 - loss: 0.4035 - regression_loss: 0.3559 - classification_loss: 0.0476
 2939/10000 [=======>......................] - ETA: 1:44:46 - loss: 0.4034 - regression_loss: 0.3558 - classification_loss: 0.0475
 2940/10000 [=======>......................] - ETA: 1:44:45 - loss: 0.4033 - regression_loss: 0.3557 - classification_loss: 0.0475
 2941/10000 [=======>......................] - ETA: 1:44:44 - loss: 0.4033 - regression_loss: 0.3557 - classification_loss: 0.0475
 2942/10000 [=======>......................] - ETA: 1:44:43 - loss: 0.4033 - regression_loss: 0.3557 - classification_loss: 0.0475
 2943/10000 [=======>......................] - ETA: 1:44:43 - loss: 0.4033 - regression_loss: 0.3558 - classification_loss: 0.0475
 2944/10000 [=======>......................] - ETA: 1:44:42 - loss: 0.4033 - regression_loss: 0.3558 - classification_loss: 0.0475
 2945/10000 [=======>......................] - ETA: 1:44:41 - loss: 0.4033 - regression_loss: 0.3558 - classification_loss: 0.0475
 2946/10000 [=======>......................] - ETA: 1:44:40 - loss: 0.4032 - regression_loss: 0.3557 - classification_loss: 0.0475
 2947/10000 [=======>......................] - ETA: 1:44:39 - loss: 0.4032 - regression_loss: 0.3557 - classification_loss: 0.0475
 2948/10000 [=======>......................] - ETA: 1:44:38 - loss: 0.4032 - regression_loss: 0.3557 - classification_loss: 0.0475
 2949/10000 [=======>......................] - ETA: 1:44:37 - loss: 0.4032 - regression_loss: 0.3557 - classification_loss: 0.0475
 2950/10000 [=======>......................] - ETA: 1:44:36 - loss: 0.4032 - regression_loss: 0.3557 - classification_loss: 0.0475
 2951/10000 [=======>......................] - ETA: 1:44:35 - loss: 0.4034 - regression_loss: 0.3559 - classification_loss: 0.0475
 2952/10000 [=======>......................] - ETA: 1:44:35 - loss: 0.4034 - regression_loss: 0.3559 - classification_loss: 0.0475
 2953/10000 [=======>......................] - ETA: 1:44:34 - loss: 0.4033 - regression_loss: 0.3558 - classification_loss: 0.0475
 2954/10000 [=======>......................] - ETA: 1:44:33 - loss: 0.4032 - regression_loss: 0.3557 - classification_loss: 0.0475
 2955/10000 [=======>......................] - ETA: 1:44:32 - loss: 0.4031 - regression_loss: 0.3557 - classification_loss: 0.0475
 2956/10000 [=======>......................] - ETA: 1:44:31 - loss: 0.4031 - regression_loss: 0.3557 - classification_loss: 0.0474
 2957/10000 [=======>......................] - ETA: 1:44:30 - loss: 0.4031 - regression_loss: 0.3557 - classification_loss: 0.0475
 2958/10000 [=======>......................] - ETA: 1:44:29 - loss: 0.4031 - regression_loss: 0.3556 - classification_loss: 0.0474
 2959/10000 [=======>......................] - ETA: 1:44:28 - loss: 0.4031 - regression_loss: 0.3557 - classification_loss: 0.0475
 2960/10000 [=======>......................] - ETA: 1:44:27 - loss: 0.4032 - regression_loss: 0.3557 - classification_loss: 0.0475
 2961/10000 [=======>......................] - ETA: 1:44:26 - loss: 0.4032 - regression_loss: 0.3557 - classification_loss: 0.0475
 2962/10000 [=======>......................] - ETA: 1:44:26 - loss: 0.4032 - regression_loss: 0.3557 - classification_loss: 0.0475
 2963/10000 [=======>......................] - ETA: 1:44:25 - loss: 0.4032 - regression_loss: 0.3557 - classification_loss: 0.0475
 2964/10000 [=======>......................] - ETA: 1:44:24 - loss: 0.4033 - regression_loss: 0.3558 - classification_loss: 0.0475
 2965/10000 [=======>......................] - ETA: 1:44:23 - loss: 0.4034 - regression_loss: 0.3559 - classification_loss: 0.0475
 2966/10000 [=======>......................] - ETA: 1:44:22 - loss: 0.4035 - regression_loss: 0.3559 - classification_loss: 0.0475
 2967/10000 [=======>......................] - ETA: 1:44:21 - loss: 0.4035 - regression_loss: 0.3560 - classification_loss: 0.0475
 2968/10000 [=======>......................] - ETA: 1:44:20 - loss: 0.4034 - regression_loss: 0.3559 - classification_loss: 0.0475
 2969/10000 [=======>......................] - ETA: 1:44:19 - loss: 0.4034 - regression_loss: 0.3559 - classification_loss: 0.0475
 2970/10000 [=======>......................] - ETA: 1:44:18 - loss: 0.4035 - regression_loss: 0.3560 - classification_loss: 0.0475
 2971/10000 [=======>......................] - ETA: 1:44:18 - loss: 0.4035 - regression_loss: 0.3560 - classification_loss: 0.0475
 2972/10000 [=======>......................] - ETA: 1:44:17 - loss: 0.4035 - regression_loss: 0.3559 - classification_loss: 0.0475
 2973/10000 [=======>......................] - ETA: 1:44:16 - loss: 0.4034 - regression_loss: 0.3559 - classification_loss: 0.0475
 2974/10000 [=======>......................] - ETA: 1:44:15 - loss: 0.4034 - regression_loss: 0.3559 - classification_loss: 0.0475
 2975/10000 [=======>......................] - ETA: 1:44:14 - loss: 0.4033 - regression_loss: 0.3558 - classification_loss: 0.0475
 2976/10000 [=======>......................] - ETA: 1:44:13 - loss: 0.4034 - regression_loss: 0.3559 - classification_loss: 0.0475
 2977/10000 [=======>......................] - ETA: 1:44:12 - loss: 0.4033 - regression_loss: 0.3558 - classification_loss: 0.0475
 2978/10000 [=======>......................] - ETA: 1:44:11 - loss: 0.4033 - regression_loss: 0.3559 - classification_loss: 0.0475
 2979/10000 [=======>......................] - ETA: 1:44:10 - loss: 0.4033 - regression_loss: 0.3559 - classification_loss: 0.0475
 2980/10000 [=======>......................] - ETA: 1:44:09 - loss: 0.4032 - regression_loss: 0.3558 - classification_loss: 0.0475
 2981/10000 [=======>......................] - ETA: 1:44:09 - loss: 0.4032 - regression_loss: 0.3557 - classification_loss: 0.0475
 2982/10000 [=======>......................] - ETA: 1:44:08 - loss: 0.4032 - regression_loss: 0.3558 - classification_loss: 0.0475
 2983/10000 [=======>......................] - ETA: 1:44:07 - loss: 0.4033 - regression_loss: 0.3558 - classification_loss: 0.0475
 2984/10000 [=======>......................] - ETA: 1:44:06 - loss: 0.4034 - regression_loss: 0.3559 - classification_loss: 0.0475
 2985/10000 [=======>......................] - ETA: 1:44:05 - loss: 0.4033 - regression_loss: 0.3559 - classification_loss: 0.0475
 2986/10000 [=======>......................] - ETA: 1:44:04 - loss: 0.4034 - regression_loss: 0.3559 - classification_loss: 0.0474
 2987/10000 [=======>......................] - ETA: 1:44:03 - loss: 0.4033 - regression_loss: 0.3558 - classification_loss: 0.0474
 2988/10000 [=======>......................] - ETA: 1:44:02 - loss: 0.4032 - regression_loss: 0.3558 - classification_loss: 0.0474
 2989/10000 [=======>......................] - ETA: 1:44:01 - loss: 0.4032 - regression_loss: 0.3558 - classification_loss: 0.0474
 2990/10000 [=======>......................] - ETA: 1:44:00 - loss: 0.4033 - regression_loss: 0.3558 - classification_loss: 0.0474
 2991/10000 [=======>......................] - ETA: 1:44:00 - loss: 0.4033 - regression_loss: 0.3558 - classification_loss: 0.0474
 2992/10000 [=======>......................] - ETA: 1:43:59 - loss: 0.4032 - regression_loss: 0.3558 - classification_loss: 0.0474
 2993/10000 [=======>......................] - ETA: 1:43:58 - loss: 0.4035 - regression_loss: 0.3560 - classification_loss: 0.0474
 2994/10000 [=======>......................] - ETA: 1:43:58 - loss: 0.4034 - regression_loss: 0.3559 - classification_loss: 0.0474
 2995/10000 [=======>......................] - ETA: 1:43:57 - loss: 0.4033 - regression_loss: 0.3559 - classification_loss: 0.0474
 2996/10000 [=======>......................] - ETA: 1:43:56 - loss: 0.4032 - regression_loss: 0.3558 - classification_loss: 0.0474
 2997/10000 [=======>......................] - ETA: 1:43:55 - loss: 0.4032 - regression_loss: 0.3558 - classification_loss: 0.0474
 2998/10000 [=======>......................] - ETA: 1:43:54 - loss: 0.4031 - regression_loss: 0.3557 - classification_loss: 0.0474
 2999/10000 [=======>......................] - ETA: 1:43:53 - loss: 0.4032 - regression_loss: 0.3557 - classification_loss: 0.0474
 3000/10000 [========>.....................] - ETA: 1:43:52 - loss: 0.4032 - regression_loss: 0.3558 - classification_loss: 0.0474
 3001/10000 [========>.....................] - ETA: 1:43:51 - loss: 0.4031 - regression_loss: 0.3557 - classification_loss: 0.0474
 3002/10000 [========>.....................] - ETA: 1:43:50 - loss: 0.4030 - regression_loss: 0.3556 - classification_loss: 0.0474
 3003/10000 [========>.....................] - ETA: 1:43:49 - loss: 0.4029 - regression_loss: 0.3555 - classification_loss: 0.0474
 3004/10000 [========>.....................] - ETA: 1:43:49 - loss: 0.4028 - regression_loss: 0.3554 - classification_loss: 0.0474
 3005/10000 [========>.....................] - ETA: 1:43:48 - loss: 0.4027 - regression_loss: 0.3554 - classification_loss: 0.0474
 3006/10000 [========>.....................] - ETA: 1:43:47 - loss: 0.4028 - regression_loss: 0.3554 - classification_loss: 0.0474
 3007/10000 [========>.....................] - ETA: 1:43:46 - loss: 0.4030 - regression_loss: 0.3556 - classification_loss: 0.0474
 3008/10000 [========>.....................] - ETA: 1:43:45 - loss: 0.4030 - regression_loss: 0.3556 - classification_loss: 0.0474
 3009/10000 [========>.....................] - ETA: 1:43:44 - loss: 0.4030 - regression_loss: 0.3556 - classification_loss: 0.0474
 3010/10000 [========>.....................] - ETA: 1:43:43 - loss: 0.4030 - regression_loss: 0.3557 - classification_loss: 0.0474
 3011/10000 [========>.....................] - ETA: 1:43:42 - loss: 0.4031 - regression_loss: 0.3557 - classification_loss: 0.0474
 3012/10000 [========>.....................] - ETA: 1:43:41 - loss: 0.4031 - regression_loss: 0.3558 - classification_loss: 0.0474
 3013/10000 [========>.....................] - ETA: 1:43:40 - loss: 0.4031 - regression_loss: 0.3557 - classification_loss: 0.0474
 3014/10000 [========>.....................] - ETA: 1:43:40 - loss: 0.4034 - regression_loss: 0.3560 - classification_loss: 0.0474
 3015/10000 [========>.....................] - ETA: 1:43:39 - loss: 0.4033 - regression_loss: 0.3559 - classification_loss: 0.0474
 3016/10000 [========>.....................] - ETA: 1:43:38 - loss: 0.4033 - regression_loss: 0.3559 - classification_loss: 0.0474
 3017/10000 [========>.....................] - ETA: 1:43:37 - loss: 0.4034 - regression_loss: 0.3560 - classification_loss: 0.0473
 3018/10000 [========>.....................] - ETA: 1:43:36 - loss: 0.4036 - regression_loss: 0.3563 - classification_loss: 0.0473
 3019/10000 [========>.....................] - ETA: 1:43:35 - loss: 0.4039 - regression_loss: 0.3565 - classification_loss: 0.0474
 3020/10000 [========>.....................] - ETA: 1:43:34 - loss: 0.4038 - regression_loss: 0.3564 - classification_loss: 0.0474
 3021/10000 [========>.....................] - ETA: 1:43:33 - loss: 0.4039 - regression_loss: 0.3565 - classification_loss: 0.0474
 3022/10000 [========>.....................] - ETA: 1:43:32 - loss: 0.4041 - regression_loss: 0.3566 - classification_loss: 0.0474
 3023/10000 [========>.....................] - ETA: 1:43:31 - loss: 0.4040 - regression_loss: 0.3566 - classification_loss: 0.0474
 3024/10000 [========>.....................] - ETA: 1:43:31 - loss: 0.4041 - regression_loss: 0.3566 - classification_loss: 0.0474
 3025/10000 [========>.....................] - ETA: 1:43:30 - loss: 0.4041 - regression_loss: 0.3567 - classification_loss: 0.0474
 3026/10000 [========>.....................] - ETA: 1:43:29 - loss: 0.4042 - regression_loss: 0.3567 - classification_loss: 0.0474
 3027/10000 [========>.....................] - ETA: 1:43:28 - loss: 0.4041 - regression_loss: 0.3567 - classification_loss: 0.0474
 3028/10000 [========>.....................] - ETA: 1:43:27 - loss: 0.4044 - regression_loss: 0.3569 - classification_loss: 0.0475
 3029/10000 [========>.....................] - ETA: 1:43:26 - loss: 0.4044 - regression_loss: 0.3569 - classification_loss: 0.0475
 3030/10000 [========>.....................] - ETA: 1:43:25 - loss: 0.4043 - regression_loss: 0.3569 - classification_loss: 0.0474
 3031/10000 [========>.....................] - ETA: 1:43:24 - loss: 0.4042 - regression_loss: 0.3567 - classification_loss: 0.0474
 3032/10000 [========>.....................] - ETA: 1:43:23 - loss: 0.4040 - regression_loss: 0.3566 - classification_loss: 0.0474
 3033/10000 [========>.....................] - ETA: 1:43:23 - loss: 0.4040 - regression_loss: 0.3566 - classification_loss: 0.0474
 3034/10000 [========>.....................] - ETA: 1:43:22 - loss: 0.4041 - regression_loss: 0.3567 - classification_loss: 0.0474
 3035/10000 [========>.....................] - ETA: 1:43:21 - loss: 0.4041 - regression_loss: 0.3567 - classification_loss: 0.0474
 3036/10000 [========>.....................] - ETA: 1:43:20 - loss: 0.4041 - regression_loss: 0.3566 - classification_loss: 0.0474
 3037/10000 [========>.....................] - ETA: 1:43:19 - loss: 0.4040 - regression_loss: 0.3566 - classification_loss: 0.0474
 3038/10000 [========>.....................] - ETA: 1:43:18 - loss: 0.4040 - regression_loss: 0.3566 - classification_loss: 0.0474
 3039/10000 [========>.....................] - ETA: 1:43:17 - loss: 0.4040 - regression_loss: 0.3566 - classification_loss: 0.0474
 3040/10000 [========>.....................] - ETA: 1:43:16 - loss: 0.4041 - regression_loss: 0.3567 - classification_loss: 0.0474
 3041/10000 [========>.....................] - ETA: 1:43:15 - loss: 0.4041 - regression_loss: 0.3567 - classification_loss: 0.0474
 3042/10000 [========>.....................] - ETA: 1:43:14 - loss: 0.4041 - regression_loss: 0.3567 - classification_loss: 0.0474
 3043/10000 [========>.....................] - ETA: 1:43:14 - loss: 0.4041 - regression_loss: 0.3567 - classification_loss: 0.0474
 3044/10000 [========>.....................] - ETA: 1:43:13 - loss: 0.4040 - regression_loss: 0.3567 - classification_loss: 0.0474
 3045/10000 [========>.....................] - ETA: 1:43:12 - loss: 0.4039 - regression_loss: 0.3566 - classification_loss: 0.0474
 3046/10000 [========>.....................] - ETA: 1:43:11 - loss: 0.4041 - regression_loss: 0.3566 - classification_loss: 0.0475
 3047/10000 [========>.....................] - ETA: 1:43:10 - loss: 0.4041 - regression_loss: 0.3566 - classification_loss: 0.0475
 3048/10000 [========>.....................] - ETA: 1:43:09 - loss: 0.4040 - regression_loss: 0.3566 - classification_loss: 0.0475
 3049/10000 [========>.....................] - ETA: 1:43:08 - loss: 0.4041 - regression_loss: 0.3566 - classification_loss: 0.0475
 3050/10000 [========>.....................] - ETA: 1:43:07 - loss: 0.4039 - regression_loss: 0.3565 - classification_loss: 0.0475
 3051/10000 [========>.....................] - ETA: 1:43:06 - loss: 0.4038 - regression_loss: 0.3564 - classification_loss: 0.0474
 3052/10000 [========>.....................] - ETA: 1:43:05 - loss: 0.4037 - regression_loss: 0.3563 - classification_loss: 0.0474
 3053/10000 [========>.....................] - ETA: 1:43:05 - loss: 0.4036 - regression_loss: 0.3562 - classification_loss: 0.0474
 3054/10000 [========>.....................] - ETA: 1:43:04 - loss: 0.4037 - regression_loss: 0.3562 - classification_loss: 0.0474
 3055/10000 [========>.....................] - ETA: 1:43:03 - loss: 0.4036 - regression_loss: 0.3562 - classification_loss: 0.0474
 3056/10000 [========>.....................] - ETA: 1:43:02 - loss: 0.4037 - regression_loss: 0.3563 - classification_loss: 0.0474
 3057/10000 [========>.....................] - ETA: 1:43:01 - loss: 0.4037 - regression_loss: 0.3562 - classification_loss: 0.0474
 3058/10000 [========>.....................] - ETA: 1:43:00 - loss: 0.4038 - regression_loss: 0.3563 - classification_loss: 0.0474
 3059/10000 [========>.....................] - ETA: 1:42:59 - loss: 0.4038 - regression_loss: 0.3564 - classification_loss: 0.0474
 3060/10000 [========>.....................] - ETA: 1:42:58 - loss: 0.4038 - regression_loss: 0.3564 - classification_loss: 0.0474
 3061/10000 [========>.....................] - ETA: 1:42:57 - loss: 0.4038 - regression_loss: 0.3563 - classification_loss: 0.0474
 3062/10000 [========>.....................] - ETA: 1:42:57 - loss: 0.4038 - regression_loss: 0.3563 - classification_loss: 0.0474
 3063/10000 [========>.....................] - ETA: 1:42:56 - loss: 0.4037 - regression_loss: 0.3563 - classification_loss: 0.0474
 3064/10000 [========>.....................] - ETA: 1:42:55 - loss: 0.4036 - regression_loss: 0.3562 - classification_loss: 0.0474
 3065/10000 [========>.....................] - ETA: 1:42:54 - loss: 0.4034 - regression_loss: 0.3561 - classification_loss: 0.0474
 3066/10000 [========>.....................] - ETA: 1:42:53 - loss: 0.4034 - regression_loss: 0.3561 - classification_loss: 0.0474
 3067/10000 [========>.....................] - ETA: 1:42:52 - loss: 0.4037 - regression_loss: 0.3562 - classification_loss: 0.0475
 3068/10000 [========>.....................] - ETA: 1:42:51 - loss: 0.4037 - regression_loss: 0.3562 - classification_loss: 0.0475
 3069/10000 [========>.....................] - ETA: 1:42:50 - loss: 0.4037 - regression_loss: 0.3562 - classification_loss: 0.0475
 3070/10000 [========>.....................] - ETA: 1:42:49 - loss: 0.4036 - regression_loss: 0.3561 - classification_loss: 0.0475
 3071/10000 [========>.....................] - ETA: 1:42:48 - loss: 0.4035 - regression_loss: 0.3560 - classification_loss: 0.0475
 3072/10000 [========>.....................] - ETA: 1:42:48 - loss: 0.4034 - regression_loss: 0.3559 - classification_loss: 0.0475
 3073/10000 [========>.....................] - ETA: 1:42:47 - loss: 0.4034 - regression_loss: 0.3559 - classification_loss: 0.0475
 3074/10000 [========>.....................] - ETA: 1:42:46 - loss: 0.4033 - regression_loss: 0.3558 - classification_loss: 0.0475
 3075/10000 [========>.....................] - ETA: 1:42:45 - loss: 0.4033 - regression_loss: 0.3558 - classification_loss: 0.0474
 3076/10000 [========>.....................] - ETA: 1:42:44 - loss: 0.4033 - regression_loss: 0.3558 - classification_loss: 0.0474
 3077/10000 [========>.....................] - ETA: 1:42:43 - loss: 0.4033 - regression_loss: 0.3558 - classification_loss: 0.0475
 3078/10000 [========>.....................] - ETA: 1:42:42 - loss: 0.4032 - regression_loss: 0.3558 - classification_loss: 0.0475
 3079/10000 [========>.....................] - ETA: 1:42:41 - loss: 0.4032 - regression_loss: 0.3558 - classification_loss: 0.0475
 3080/10000 [========>.....................] - ETA: 1:42:40 - loss: 0.4032 - regression_loss: 0.3558 - classification_loss: 0.0474
 3081/10000 [========>.....................] - ETA: 1:42:40 - loss: 0.4033 - regression_loss: 0.3558 - classification_loss: 0.0475
 3082/10000 [========>.....................] - ETA: 1:42:39 - loss: 0.4034 - regression_loss: 0.3559 - classification_loss: 0.0475
 3083/10000 [========>.....................] - ETA: 1:42:38 - loss: 0.4034 - regression_loss: 0.3559 - classification_loss: 0.0475
 3084/10000 [========>.....................] - ETA: 1:42:37 - loss: 0.4034 - regression_loss: 0.3559 - classification_loss: 0.0475
 3085/10000 [========>.....................] - ETA: 1:42:36 - loss: 0.4034 - regression_loss: 0.3559 - classification_loss: 0.0475
 3086/10000 [========>.....................] - ETA: 1:42:35 - loss: 0.4035 - regression_loss: 0.3559 - classification_loss: 0.0476
 3087/10000 [========>.....................] - ETA: 1:42:34 - loss: 0.4035 - regression_loss: 0.3559 - classification_loss: 0.0476
 3088/10000 [========>.....................] - ETA: 1:42:33 - loss: 0.4034 - regression_loss: 0.3559 - classification_loss: 0.0476
 3089/10000 [========>.....................] - ETA: 1:42:32 - loss: 0.4033 - regression_loss: 0.3558 - classification_loss: 0.0476
 3090/10000 [========>.....................] - ETA: 1:42:31 - loss: 0.4032 - regression_loss: 0.3557 - classification_loss: 0.0475
 3091/10000 [========>.....................] - ETA: 1:42:31 - loss: 0.4032 - regression_loss: 0.3557 - classification_loss: 0.0475
 3092/10000 [========>.....................] - ETA: 1:42:30 - loss: 0.4032 - regression_loss: 0.3557 - classification_loss: 0.0475
 3093/10000 [========>.....................] - ETA: 1:42:29 - loss: 0.4033 - regression_loss: 0.3557 - classification_loss: 0.0475
 3094/10000 [========>.....................] - ETA: 1:42:28 - loss: 0.4032 - regression_loss: 0.3557 - classification_loss: 0.0475
 3095/10000 [========>.....................] - ETA: 1:42:27 - loss: 0.4032 - regression_loss: 0.3557 - classification_loss: 0.0475
 3096/10000 [========>.....................] - ETA: 1:42:26 - loss: 0.4031 - regression_loss: 0.3556 - classification_loss: 0.0475
 3097/10000 [========>.....................] - ETA: 1:42:25 - loss: 0.4035 - regression_loss: 0.3559 - classification_loss: 0.0476
 3098/10000 [========>.....................] - ETA: 1:42:24 - loss: 0.4035 - regression_loss: 0.3559 - classification_loss: 0.0476
 3099/10000 [========>.....................] - ETA: 1:42:23 - loss: 0.4036 - regression_loss: 0.3560 - classification_loss: 0.0476
 3100/10000 [========>.....................] - ETA: 1:42:22 - loss: 0.4036 - regression_loss: 0.3560 - classification_loss: 0.0476
 3101/10000 [========>.....................] - ETA: 1:42:22 - loss: 0.4037 - regression_loss: 0.3560 - classification_loss: 0.0476
 3102/10000 [========>.....................] - ETA: 1:42:21 - loss: 0.4036 - regression_loss: 0.3560 - classification_loss: 0.0476
 3103/10000 [========>.....................] - ETA: 1:42:20 - loss: 0.4036 - regression_loss: 0.3560 - classification_loss: 0.0476
 3104/10000 [========>.....................] - ETA: 1:42:19 - loss: 0.4037 - regression_loss: 0.3561 - classification_loss: 0.0476
 3105/10000 [========>.....................] - ETA: 1:42:18 - loss: 0.4038 - regression_loss: 0.3561 - classification_loss: 0.0476
 3106/10000 [========>.....................] - ETA: 1:42:17 - loss: 0.4037 - regression_loss: 0.3561 - classification_loss: 0.0476
 3107/10000 [========>.....................] - ETA: 1:42:16 - loss: 0.4043 - regression_loss: 0.3562 - classification_loss: 0.0481
 3108/10000 [========>.....................] - ETA: 1:42:15 - loss: 0.4043 - regression_loss: 0.3562 - classification_loss: 0.0481
 3109/10000 [========>.....................] - ETA: 1:42:14 - loss: 0.4043 - regression_loss: 0.3563 - classification_loss: 0.0481
 3110/10000 [========>.....................] - ETA: 1:42:14 - loss: 0.4043 - regression_loss: 0.3563 - classification_loss: 0.0481
 3111/10000 [========>.....................] - ETA: 1:42:13 - loss: 0.4043 - regression_loss: 0.3563 - classification_loss: 0.0481
 3112/10000 [========>.....................] - ETA: 1:42:12 - loss: 0.4044 - regression_loss: 0.3563 - classification_loss: 0.0481
 3113/10000 [========>.....................] - ETA: 1:42:11 - loss: 0.4044 - regression_loss: 0.3563 - classification_loss: 0.0481
 3114/10000 [========>.....................] - ETA: 1:42:10 - loss: 0.4043 - regression_loss: 0.3563 - classification_loss: 0.0481
 3115/10000 [========>.....................] - ETA: 1:42:09 - loss: 0.4044 - regression_loss: 0.3563 - classification_loss: 0.0481
 3116/10000 [========>.....................] - ETA: 1:42:08 - loss: 0.4043 - regression_loss: 0.3562 - classification_loss: 0.0481
 3117/10000 [========>.....................] - ETA: 1:42:07 - loss: 0.4042 - regression_loss: 0.3561 - classification_loss: 0.0481
 3118/10000 [========>.....................] - ETA: 1:42:06 - loss: 0.4042 - regression_loss: 0.3561 - classification_loss: 0.0481
 3119/10000 [========>.....................] - ETA: 1:42:05 - loss: 0.4041 - regression_loss: 0.3561 - classification_loss: 0.0481
 3120/10000 [========>.....................] - ETA: 1:42:05 - loss: 0.4042 - regression_loss: 0.3561 - classification_loss: 0.0481
 3121/10000 [========>.....................] - ETA: 1:42:04 - loss: 0.4041 - regression_loss: 0.3561 - classification_loss: 0.0480
 3122/10000 [========>.....................] - ETA: 1:42:03 - loss: 0.4041 - regression_loss: 0.3561 - classification_loss: 0.0480
 3123/10000 [========>.....................] - ETA: 1:42:02 - loss: 0.4041 - regression_loss: 0.3561 - classification_loss: 0.0480
 3124/10000 [========>.....................] - ETA: 1:42:01 - loss: 0.4041 - regression_loss: 0.3561 - classification_loss: 0.0480
 3125/10000 [========>.....................] - ETA: 1:42:00 - loss: 0.4041 - regression_loss: 0.3560 - classification_loss: 0.0480
 3126/10000 [========>.....................] - ETA: 1:41:59 - loss: 0.4041 - regression_loss: 0.3561 - classification_loss: 0.0480
 3127/10000 [========>.....................] - ETA: 1:41:58 - loss: 0.4042 - regression_loss: 0.3561 - classification_loss: 0.0481
 3128/10000 [========>.....................] - ETA: 1:41:57 - loss: 0.4048 - regression_loss: 0.3566 - classification_loss: 0.0482
 3129/10000 [========>.....................] - ETA: 1:41:57 - loss: 0.4048 - regression_loss: 0.3566 - classification_loss: 0.0482
 3130/10000 [========>.....................] - ETA: 1:41:56 - loss: 0.4048 - regression_loss: 0.3567 - classification_loss: 0.0481
 3131/10000 [========>.....................] - ETA: 1:41:55 - loss: 0.4048 - regression_loss: 0.3566 - classification_loss: 0.0481
 3132/10000 [========>.....................] - ETA: 1:41:54 - loss: 0.4047 - regression_loss: 0.3566 - classification_loss: 0.0481
 3133/10000 [========>.....................] - ETA: 1:41:53 - loss: 0.4048 - regression_loss: 0.3567 - classification_loss: 0.0481
 3134/10000 [========>.....................] - ETA: 1:41:52 - loss: 0.4048 - regression_loss: 0.3567 - classification_loss: 0.0481
 3135/10000 [========>.....................] - ETA: 1:41:51 - loss: 0.4048 - regression_loss: 0.3567 - classification_loss: 0.0481
 3136/10000 [========>.....................] - ETA: 1:41:50 - loss: 0.4050 - regression_loss: 0.3569 - classification_loss: 0.0481
 3137/10000 [========>.....................] - ETA: 1:41:49 - loss: 0.4050 - regression_loss: 0.3568 - classification_loss: 0.0481
 3138/10000 [========>.....................] - ETA: 1:41:49 - loss: 0.4050 - regression_loss: 0.3569 - classification_loss: 0.0482
 3139/10000 [========>.....................] - ETA: 1:41:48 - loss: 0.4050 - regression_loss: 0.3568 - classification_loss: 0.0482
 3140/10000 [========>.....................] - ETA: 1:41:47 - loss: 0.4052 - regression_loss: 0.3570 - classification_loss: 0.0481
 3141/10000 [========>.....................] - ETA: 1:41:46 - loss: 0.4051 - regression_loss: 0.3570 - classification_loss: 0.0481
 3142/10000 [========>.....................] - ETA: 1:41:45 - loss: 0.4050 - regression_loss: 0.3569 - classification_loss: 0.0481
 3143/10000 [========>.....................] - ETA: 1:41:44 - loss: 0.4049 - regression_loss: 0.3568 - classification_loss: 0.0481
 3144/10000 [========>.....................] - ETA: 1:41:43 - loss: 0.4050 - regression_loss: 0.3569 - classification_loss: 0.0481
 3145/10000 [========>.....................] - ETA: 1:41:42 - loss: 0.4050 - regression_loss: 0.3569 - classification_loss: 0.0481
 3146/10000 [========>.....................] - ETA: 1:41:41 - loss: 0.4050 - regression_loss: 0.3569 - classification_loss: 0.0481
 3147/10000 [========>.....................] - ETA: 1:41:40 - loss: 0.4050 - regression_loss: 0.3569 - classification_loss: 0.0481
 3148/10000 [========>.....................] - ETA: 1:41:40 - loss: 0.4049 - regression_loss: 0.3569 - classification_loss: 0.0481
 3149/10000 [========>.....................] - ETA: 1:41:39 - loss: 0.4049 - regression_loss: 0.3568 - classification_loss: 0.0481
 3150/10000 [========>.....................] - ETA: 1:41:38 - loss: 0.4048 - regression_loss: 0.3568 - classification_loss: 0.0480
 3151/10000 [========>.....................] - ETA: 1:41:37 - loss: 0.4048 - regression_loss: 0.3567 - classification_loss: 0.0481
 3152/10000 [========>.....................] - ETA: 1:41:36 - loss: 0.4049 - regression_loss: 0.3568 - classification_loss: 0.0481
 3153/10000 [========>.....................] - ETA: 1:41:35 - loss: 0.4049 - regression_loss: 0.3568 - classification_loss: 0.0481
 3154/10000 [========>.....................] - ETA: 1:41:34 - loss: 0.4049 - regression_loss: 0.3568 - classification_loss: 0.0481
 3155/10000 [========>.....................] - ETA: 1:41:33 - loss: 0.4049 - regression_loss: 0.3567 - classification_loss: 0.0481
 3156/10000 [========>.....................] - ETA: 1:41:32 - loss: 0.4048 - regression_loss: 0.3567 - classification_loss: 0.0481
 3157/10000 [========>.....................] - ETA: 1:41:31 - loss: 0.4048 - regression_loss: 0.3567 - classification_loss: 0.0481
 3158/10000 [========>.....................] - ETA: 1:41:31 - loss: 0.4049 - regression_loss: 0.3568 - classification_loss: 0.0481
 3159/10000 [========>.....................] - ETA: 1:41:30 - loss: 0.4048 - regression_loss: 0.3567 - classification_loss: 0.0481
 3160/10000 [========>.....................] - ETA: 1:41:29 - loss: 0.4047 - regression_loss: 0.3566 - classification_loss: 0.0481
 3161/10000 [========>.....................] - ETA: 1:41:28 - loss: 0.4046 - regression_loss: 0.3565 - classification_loss: 0.0481
 3162/10000 [========>.....................] - ETA: 1:41:27 - loss: 0.4046 - regression_loss: 0.3565 - classification_loss: 0.0481
 3163/10000 [========>.....................] - ETA: 1:41:26 - loss: 0.4045 - regression_loss: 0.3564 - classification_loss: 0.0481
 3164/10000 [========>.....................] - ETA: 1:41:25 - loss: 0.4045 - regression_loss: 0.3565 - classification_loss: 0.0481
 3165/10000 [========>.....................] - ETA: 1:41:24 - loss: 0.4045 - regression_loss: 0.3564 - classification_loss: 0.0481
 3166/10000 [========>.....................] - ETA: 1:41:23 - loss: 0.4045 - regression_loss: 0.3564 - classification_loss: 0.0481
 3167/10000 [========>.....................] - ETA: 1:41:23 - loss: 0.4045 - regression_loss: 0.3564 - classification_loss: 0.0480
 3168/10000 [========>.....................] - ETA: 1:41:22 - loss: 0.4044 - regression_loss: 0.3564 - classification_loss: 0.0480
 3169/10000 [========>.....................] - ETA: 1:41:21 - loss: 0.4044 - regression_loss: 0.3564 - classification_loss: 0.0480
 3170/10000 [========>.....................] - ETA: 1:41:20 - loss: 0.4045 - regression_loss: 0.3564 - classification_loss: 0.0481
 3171/10000 [========>.....................] - ETA: 1:41:19 - loss: 0.4045 - regression_loss: 0.3564 - classification_loss: 0.0481
 3172/10000 [========>.....................] - ETA: 1:41:18 - loss: 0.4044 - regression_loss: 0.3563 - classification_loss: 0.0481
 3173/10000 [========>.....................] - ETA: 1:41:17 - loss: 0.4045 - regression_loss: 0.3565 - classification_loss: 0.0481
 3174/10000 [========>.....................] - ETA: 1:41:16 - loss: 0.4045 - regression_loss: 0.3564 - classification_loss: 0.0481
 3175/10000 [========>.....................] - ETA: 1:41:15 - loss: 0.4045 - regression_loss: 0.3564 - classification_loss: 0.0481
 3176/10000 [========>.....................] - ETA: 1:41:15 - loss: 0.4044 - regression_loss: 0.3563 - classification_loss: 0.0480
 3177/10000 [========>.....................] - ETA: 1:41:14 - loss: 0.4044 - regression_loss: 0.3564 - classification_loss: 0.0480
 3178/10000 [========>.....................] - ETA: 1:41:13 - loss: 0.4045 - regression_loss: 0.3564 - classification_loss: 0.0481
 3179/10000 [========>.....................] - ETA: 1:41:12 - loss: 0.4044 - regression_loss: 0.3564 - classification_loss: 0.0480
 3180/10000 [========>.....................] - ETA: 1:41:11 - loss: 0.4044 - regression_loss: 0.3563 - classification_loss: 0.0480
 3181/10000 [========>.....................] - ETA: 1:41:10 - loss: 0.4044 - regression_loss: 0.3563 - classification_loss: 0.0480
 3182/10000 [========>.....................] - ETA: 1:41:09 - loss: 0.4043 - regression_loss: 0.3563 - classification_loss: 0.0480
 3183/10000 [========>.....................] - ETA: 1:41:08 - loss: 0.4041 - regression_loss: 0.3561 - classification_loss: 0.0480
 3184/10000 [========>.....................] - ETA: 1:41:07 - loss: 0.4041 - regression_loss: 0.3561 - classification_loss: 0.0480
 3185/10000 [========>.....................] - ETA: 1:41:07 - loss: 0.4040 - regression_loss: 0.3560 - classification_loss: 0.0480
 3186/10000 [========>.....................] - ETA: 1:41:06 - loss: 0.4039 - regression_loss: 0.3559 - classification_loss: 0.0480
 3187/10000 [========>.....................] - ETA: 1:41:05 - loss: 0.4038 - regression_loss: 0.3558 - classification_loss: 0.0480
 3188/10000 [========>.....................] - ETA: 1:41:04 - loss: 0.4037 - regression_loss: 0.3558 - classification_loss: 0.0479
 3189/10000 [========>.....................] - ETA: 1:41:03 - loss: 0.4037 - regression_loss: 0.3558 - classification_loss: 0.0479
 3190/10000 [========>.....................] - ETA: 1:41:02 - loss: 0.4037 - regression_loss: 0.3558 - classification_loss: 0.0479
 3191/10000 [========>.....................] - ETA: 1:41:01 - loss: 0.4037 - regression_loss: 0.3557 - classification_loss: 0.0479
 3192/10000 [========>.....................] - ETA: 1:41:00 - loss: 0.4037 - regression_loss: 0.3558 - classification_loss: 0.0479
 3193/10000 [========>.....................] - ETA: 1:40:59 - loss: 0.4038 - regression_loss: 0.3558 - classification_loss: 0.0479
 3194/10000 [========>.....................] - ETA: 1:40:58 - loss: 0.4038 - regression_loss: 0.3559 - classification_loss: 0.0479
 3195/10000 [========>.....................] - ETA: 1:40:58 - loss: 0.4038 - regression_loss: 0.3559 - classification_loss: 0.0479
 3196/10000 [========>.....................] - ETA: 1:40:57 - loss: 0.4037 - regression_loss: 0.3558 - classification_loss: 0.0479
 3197/10000 [========>.....................] - ETA: 1:40:56 - loss: 0.4036 - regression_loss: 0.3557 - classification_loss: 0.0479
 3198/10000 [========>.....................] - ETA: 1:40:55 - loss: 0.4037 - regression_loss: 0.3557 - classification_loss: 0.0479
 3199/10000 [========>.....................] - ETA: 1:40:54 - loss: 0.4038 - regression_loss: 0.3558 - classification_loss: 0.0479
 3200/10000 [========>.....................] - ETA: 1:40:53 - loss: 0.4037 - regression_loss: 0.3558 - classification_loss: 0.0479
 3201/10000 [========>.....................] - ETA: 1:40:52 - loss: 0.4037 - regression_loss: 0.3558 - classification_loss: 0.0479
 3202/10000 [========>.....................] - ETA: 1:40:51 - loss: 0.4038 - regression_loss: 0.3559 - classification_loss: 0.0479
 3203/10000 [========>.....................] - ETA: 1:40:50 - loss: 0.4038 - regression_loss: 0.3559 - classification_loss: 0.0479
 3204/10000 [========>.....................] - ETA: 1:40:49 - loss: 0.4039 - regression_loss: 0.3560 - classification_loss: 0.0479
 3205/10000 [========>.....................] - ETA: 1:40:49 - loss: 0.4038 - regression_loss: 0.3560 - classification_loss: 0.0479
 3206/10000 [========>.....................] - ETA: 1:40:48 - loss: 0.4038 - regression_loss: 0.3559 - classification_loss: 0.0479
 3207/10000 [========>.....................] - ETA: 1:40:47 - loss: 0.4038 - regression_loss: 0.3559 - classification_loss: 0.0479
 3208/10000 [========>.....................] - ETA: 1:40:46 - loss: 0.4038 - regression_loss: 0.3559 - classification_loss: 0.0479
 3209/10000 [========>.....................] - ETA: 1:40:45 - loss: 0.4038 - regression_loss: 0.3559 - classification_loss: 0.0479
 3210/10000 [========>.....................] - ETA: 1:40:44 - loss: 0.4038 - regression_loss: 0.3559 - classification_loss: 0.0478
 3211/10000 [========>.....................] - ETA: 1:40:43 - loss: 0.4037 - regression_loss: 0.3559 - classification_loss: 0.0478
 3212/10000 [========>.....................] - ETA: 1:40:42 - loss: 0.4037 - regression_loss: 0.3559 - classification_loss: 0.0478
 3213/10000 [========>.....................] - ETA: 1:40:41 - loss: 0.4037 - regression_loss: 0.3559 - classification_loss: 0.0478
 3214/10000 [========>.....................] - ETA: 1:40:41 - loss: 0.4037 - regression_loss: 0.3558 - classification_loss: 0.0478
 3215/10000 [========>.....................] - ETA: 1:40:40 - loss: 0.4041 - regression_loss: 0.3561 - classification_loss: 0.0480
 3216/10000 [========>.....................] - ETA: 1:40:39 - loss: 0.4040 - regression_loss: 0.3561 - classification_loss: 0.0480
 3217/10000 [========>.....................] - ETA: 1:40:38 - loss: 0.4040 - regression_loss: 0.3561 - classification_loss: 0.0480
 3218/10000 [========>.....................] - ETA: 1:40:37 - loss: 0.4042 - regression_loss: 0.3562 - classification_loss: 0.0480
 3219/10000 [========>.....................] - ETA: 1:40:36 - loss: 0.4042 - regression_loss: 0.3562 - classification_loss: 0.0480
 3220/10000 [========>.....................] - ETA: 1:40:35 - loss: 0.4041 - regression_loss: 0.3562 - classification_loss: 0.0480
 3221/10000 [========>.....................] - ETA: 1:40:34 - loss: 0.4042 - regression_loss: 0.3562 - classification_loss: 0.0480
 3222/10000 [========>.....................] - ETA: 1:40:33 - loss: 0.4042 - regression_loss: 0.3562 - classification_loss: 0.0480
 3223/10000 [========>.....................] - ETA: 1:40:32 - loss: 0.4042 - regression_loss: 0.3562 - classification_loss: 0.0480
 3224/10000 [========>.....................] - ETA: 1:40:32 - loss: 0.4042 - regression_loss: 0.3563 - classification_loss: 0.0480
 3225/10000 [========>.....................] - ETA: 1:40:31 - loss: 0.4045 - regression_loss: 0.3565 - classification_loss: 0.0480
 3226/10000 [========>.....................] - ETA: 1:40:30 - loss: 0.4044 - regression_loss: 0.3564 - classification_loss: 0.0480
 3227/10000 [========>.....................] - ETA: 1:40:29 - loss: 0.4044 - regression_loss: 0.3564 - classification_loss: 0.0480
 3228/10000 [========>.....................] - ETA: 1:40:28 - loss: 0.4043 - regression_loss: 0.3564 - classification_loss: 0.0480
 3229/10000 [========>.....................] - ETA: 1:40:27 - loss: 0.4043 - regression_loss: 0.3563 - classification_loss: 0.0480
 3230/10000 [========>.....................] - ETA: 1:40:26 - loss: 0.4042 - regression_loss: 0.3563 - classification_loss: 0.0479
 3231/10000 [========>.....................] - ETA: 1:40:25 - loss: 0.4041 - regression_loss: 0.3562 - classification_loss: 0.0479
 3232/10000 [========>.....................] - ETA: 1:40:24 - loss: 0.4041 - regression_loss: 0.3562 - classification_loss: 0.0479
 3233/10000 [========>.....................] - ETA: 1:40:24 - loss: 0.4041 - regression_loss: 0.3562 - classification_loss: 0.0479
 3234/10000 [========>.....................] - ETA: 1:40:23 - loss: 0.4041 - regression_loss: 0.3562 - classification_loss: 0.0479
 3235/10000 [========>.....................] - ETA: 1:40:22 - loss: 0.4042 - regression_loss: 0.3563 - classification_loss: 0.0479
 3236/10000 [========>.....................] - ETA: 1:40:21 - loss: 0.4041 - regression_loss: 0.3562 - classification_loss: 0.0479
 3237/10000 [========>.....................] - ETA: 1:40:20 - loss: 0.4042 - regression_loss: 0.3562 - classification_loss: 0.0479
 3238/10000 [========>.....................] - ETA: 1:40:19 - loss: 0.4042 - regression_loss: 0.3563 - classification_loss: 0.0479
 3239/10000 [========>.....................] - ETA: 1:40:18 - loss: 0.4044 - regression_loss: 0.3564 - classification_loss: 0.0480
 3240/10000 [========>.....................] - ETA: 1:40:17 - loss: 0.4045 - regression_loss: 0.3566 - classification_loss: 0.0480
 3241/10000 [========>.....................] - ETA: 1:40:16 - loss: 0.4045 - regression_loss: 0.3565 - classification_loss: 0.0479
 3242/10000 [========>.....................] - ETA: 1:40:15 - loss: 0.4044 - regression_loss: 0.3564 - classification_loss: 0.0479
 3243/10000 [========>.....................] - ETA: 1:40:15 - loss: 0.4043 - regression_loss: 0.3564 - classification_loss: 0.0479
 3244/10000 [========>.....................] - ETA: 1:40:14 - loss: 0.4044 - regression_loss: 0.3564 - classification_loss: 0.0479
 3245/10000 [========>.....................] - ETA: 1:40:13 - loss: 0.4043 - regression_loss: 0.3564 - classification_loss: 0.0479
 3246/10000 [========>.....................] - ETA: 1:40:12 - loss: 0.4043 - regression_loss: 0.3564 - classification_loss: 0.0479
 3247/10000 [========>.....................] - ETA: 1:40:11 - loss: 0.4045 - regression_loss: 0.3565 - classification_loss: 0.0480
 3248/10000 [========>.....................] - ETA: 1:40:10 - loss: 0.4044 - regression_loss: 0.3565 - classification_loss: 0.0480
 3249/10000 [========>.....................] - ETA: 1:40:09 - loss: 0.4044 - regression_loss: 0.3564 - classification_loss: 0.0479
 3250/10000 [========>.....................] - ETA: 1:40:08 - loss: 0.4043 - regression_loss: 0.3563 - classification_loss: 0.0479
 3251/10000 [========>.....................] - ETA: 1:40:07 - loss: 0.4042 - regression_loss: 0.3563 - classification_loss: 0.0479
 3252/10000 [========>.....................] - ETA: 1:40:07 - loss: 0.4043 - regression_loss: 0.3563 - classification_loss: 0.0479
 3253/10000 [========>.....................] - ETA: 1:40:06 - loss: 0.4042 - regression_loss: 0.3563 - classification_loss: 0.0479
 3254/10000 [========>.....................] - ETA: 1:40:05 - loss: 0.4042 - regression_loss: 0.3563 - classification_loss: 0.0479
 3255/10000 [========>.....................] - ETA: 1:40:04 - loss: 0.4043 - regression_loss: 0.3564 - classification_loss: 0.0479
 3256/10000 [========>.....................] - ETA: 1:40:03 - loss: 0.4043 - regression_loss: 0.3564 - classification_loss: 0.0479
 3257/10000 [========>.....................] - ETA: 1:40:02 - loss: 0.4042 - regression_loss: 0.3563 - classification_loss: 0.0479
 3258/10000 [========>.....................] - ETA: 1:40:01 - loss: 0.4042 - regression_loss: 0.3563 - classification_loss: 0.0479
 3259/10000 [========>.....................] - ETA: 1:40:00 - loss: 0.4042 - regression_loss: 0.3563 - classification_loss: 0.0479
 3260/10000 [========>.....................] - ETA: 1:39:59 - loss: 0.4041 - regression_loss: 0.3562 - classification_loss: 0.0479
 3261/10000 [========>.....................] - ETA: 1:39:59 - loss: 0.4041 - regression_loss: 0.3562 - classification_loss: 0.0479
 3262/10000 [========>.....................] - ETA: 1:39:58 - loss: 0.4042 - regression_loss: 0.3563 - classification_loss: 0.0479
 3263/10000 [========>.....................] - ETA: 1:39:57 - loss: 0.4041 - regression_loss: 0.3562 - classification_loss: 0.0479
 3264/10000 [========>.....................] - ETA: 1:39:56 - loss: 0.4042 - regression_loss: 0.3562 - classification_loss: 0.0479
 3265/10000 [========>.....................] - ETA: 1:39:55 - loss: 0.4042 - regression_loss: 0.3563 - classification_loss: 0.0479
 3266/10000 [========>.....................] - ETA: 1:39:54 - loss: 0.4042 - regression_loss: 0.3563 - classification_loss: 0.0479
 3267/10000 [========>.....................] - ETA: 1:39:53 - loss: 0.4041 - regression_loss: 0.3562 - classification_loss: 0.0479
 3268/10000 [========>.....................] - ETA: 1:39:52 - loss: 0.4041 - regression_loss: 0.3562 - classification_loss: 0.0479
 3269/10000 [========>.....................] - ETA: 1:39:51 - loss: 0.4041 - regression_loss: 0.3562 - classification_loss: 0.0479
 3270/10000 [========>.....................] - ETA: 1:39:50 - loss: 0.4040 - regression_loss: 0.3561 - classification_loss: 0.0479
 3271/10000 [========>.....................] - ETA: 1:39:50 - loss: 0.4041 - regression_loss: 0.3562 - classification_loss: 0.0479
 3272/10000 [========>.....................] - ETA: 1:39:49 - loss: 0.4042 - regression_loss: 0.3563 - classification_loss: 0.0479
 3273/10000 [========>.....................] - ETA: 1:39:48 - loss: 0.4041 - regression_loss: 0.3562 - classification_loss: 0.0479
 3274/10000 [========>.....................] - ETA: 1:39:47 - loss: 0.4041 - regression_loss: 0.3562 - classification_loss: 0.0479
 3275/10000 [========>.....................] - ETA: 1:39:46 - loss: 0.4041 - regression_loss: 0.3562 - classification_loss: 0.0479
 3276/10000 [========>.....................] - ETA: 1:39:45 - loss: 0.4041 - regression_loss: 0.3562 - classification_loss: 0.0479
 3277/10000 [========>.....................] - ETA: 1:39:44 - loss: 0.4041 - regression_loss: 0.3562 - classification_loss: 0.0479
 3278/10000 [========>.....................] - ETA: 1:39:43 - loss: 0.4043 - regression_loss: 0.3562 - classification_loss: 0.0481
 3279/10000 [========>.....................] - ETA: 1:39:42 - loss: 0.4045 - regression_loss: 0.3563 - classification_loss: 0.0482
 3280/10000 [========>.....................] - ETA: 1:39:42 - loss: 0.4047 - regression_loss: 0.3565 - classification_loss: 0.0482
 3281/10000 [========>.....................] - ETA: 1:39:41 - loss: 0.4048 - regression_loss: 0.3565 - classification_loss: 0.0482
 3282/10000 [========>.....................] - ETA: 1:39:40 - loss: 0.4048 - regression_loss: 0.3565 - classification_loss: 0.0482
 3283/10000 [========>.....................] - ETA: 1:39:39 - loss: 0.4048 - regression_loss: 0.3566 - classification_loss: 0.0483
 3284/10000 [========>.....................] - ETA: 1:39:38 - loss: 0.4049 - regression_loss: 0.3566 - classification_loss: 0.0483
 3285/10000 [========>.....................] - ETA: 1:39:37 - loss: 0.4048 - regression_loss: 0.3565 - classification_loss: 0.0483
 3286/10000 [========>.....................] - ETA: 1:39:36 - loss: 0.4049 - regression_loss: 0.3566 - classification_loss: 0.0483
 3287/10000 [========>.....................] - ETA: 1:39:35 - loss: 0.4049 - regression_loss: 0.3566 - classification_loss: 0.0483
 3288/10000 [========>.....................] - ETA: 1:39:34 - loss: 0.4049 - regression_loss: 0.3566 - classification_loss: 0.0483
 3289/10000 [========>.....................] - ETA: 1:39:34 - loss: 0.4050 - regression_loss: 0.3567 - classification_loss: 0.0483
 3290/10000 [========>.....................] - ETA: 1:39:33 - loss: 0.4050 - regression_loss: 0.3567 - classification_loss: 0.0483
 3291/10000 [========>.....................] - ETA: 1:39:32 - loss: 0.4049 - regression_loss: 0.3567 - classification_loss: 0.0483
 3292/10000 [========>.....................] - ETA: 1:39:31 - loss: 0.4049 - regression_loss: 0.3566 - classification_loss: 0.0482
 3293/10000 [========>.....................] - ETA: 1:39:30 - loss: 0.4049 - regression_loss: 0.3566 - classification_loss: 0.0483
 3294/10000 [========>.....................] - ETA: 1:39:29 - loss: 0.4049 - regression_loss: 0.3566 - classification_loss: 0.0483
 3295/10000 [========>.....................] - ETA: 1:39:28 - loss: 0.4049 - regression_loss: 0.3566 - classification_loss: 0.0483
 3296/10000 [========>.....................] - ETA: 1:39:27 - loss: 0.4049 - regression_loss: 0.3566 - classification_loss: 0.0483
 3297/10000 [========>.....................] - ETA: 1:39:26 - loss: 0.4049 - regression_loss: 0.3566 - classification_loss: 0.0483
 3298/10000 [========>.....................] - ETA: 1:39:25 - loss: 0.4049 - regression_loss: 0.3567 - classification_loss: 0.0483
 3299/10000 [========>.....................] - ETA: 1:39:25 - loss: 0.4051 - regression_loss: 0.3568 - classification_loss: 0.0483
 3300/10000 [========>.....................] - ETA: 1:39:24 - loss: 0.4050 - regression_loss: 0.3567 - classification_loss: 0.0483
 3301/10000 [========>.....................] - ETA: 1:39:23 - loss: 0.4050 - regression_loss: 0.3567 - classification_loss: 0.0483
 3302/10000 [========>.....................] - ETA: 1:39:22 - loss: 0.4050 - regression_loss: 0.3566 - classification_loss: 0.0483
 3303/10000 [========>.....................] - ETA: 1:39:21 - loss: 0.4049 - regression_loss: 0.3566 - classification_loss: 0.0483
 3304/10000 [========>.....................] - ETA: 1:39:20 - loss: 0.4049 - regression_loss: 0.3566 - classification_loss: 0.0483
 3305/10000 [========>.....................] - ETA: 1:39:19 - loss: 0.4048 - regression_loss: 0.3565 - classification_loss: 0.0483
 3306/10000 [========>.....................] - ETA: 1:39:18 - loss: 0.4048 - regression_loss: 0.3565 - classification_loss: 0.0483
 3307/10000 [========>.....................] - ETA: 1:39:17 - loss: 0.4047 - regression_loss: 0.3564 - classification_loss: 0.0483
 3308/10000 [========>.....................] - ETA: 1:39:16 - loss: 0.4047 - regression_loss: 0.3564 - classification_loss: 0.0483
 3309/10000 [========>.....................] - ETA: 1:39:16 - loss: 0.4047 - regression_loss: 0.3564 - classification_loss: 0.0483
 3310/10000 [========>.....................] - ETA: 1:39:15 - loss: 0.4048 - regression_loss: 0.3565 - classification_loss: 0.0483
 3311/10000 [========>.....................] - ETA: 1:39:14 - loss: 0.4048 - regression_loss: 0.3565 - classification_loss: 0.0483
 3312/10000 [========>.....................] - ETA: 1:39:13 - loss: 0.4048 - regression_loss: 0.3565 - classification_loss: 0.0483
 3313/10000 [========>.....................] - ETA: 1:39:12 - loss: 0.4047 - regression_loss: 0.3564 - classification_loss: 0.0483
 3314/10000 [========>.....................] - ETA: 1:39:11 - loss: 0.4048 - regression_loss: 0.3565 - classification_loss: 0.0483
 3315/10000 [========>.....................] - ETA: 1:39:10 - loss: 0.4048 - regression_loss: 0.3565 - classification_loss: 0.0483
 3316/10000 [========>.....................] - ETA: 1:39:09 - loss: 0.4048 - regression_loss: 0.3565 - classification_loss: 0.0483
 3317/10000 [========>.....................] - ETA: 1:39:08 - loss: 0.4047 - regression_loss: 0.3564 - classification_loss: 0.0483
 3318/10000 [========>.....................] - ETA: 1:39:08 - loss: 0.4047 - regression_loss: 0.3564 - classification_loss: 0.0483
 3319/10000 [========>.....................] - ETA: 1:39:07 - loss: 0.4049 - regression_loss: 0.3566 - classification_loss: 0.0483
 3320/10000 [========>.....................] - ETA: 1:39:06 - loss: 0.4048 - regression_loss: 0.3565 - classification_loss: 0.0483
 3321/10000 [========>.....................] - ETA: 1:39:05 - loss: 0.4047 - regression_loss: 0.3565 - classification_loss: 0.0483
 3322/10000 [========>.....................] - ETA: 1:39:04 - loss: 0.4047 - regression_loss: 0.3564 - classification_loss: 0.0483
 3323/10000 [========>.....................] - ETA: 1:39:03 - loss: 0.4046 - regression_loss: 0.3564 - classification_loss: 0.0482
 3324/10000 [========>.....................] - ETA: 1:39:02 - loss: 0.4047 - regression_loss: 0.3565 - classification_loss: 0.0482
 3325/10000 [========>.....................] - ETA: 1:39:01 - loss: 0.4046 - regression_loss: 0.3564 - classification_loss: 0.0482
 3326/10000 [========>.....................] - ETA: 1:39:00 - loss: 0.4046 - regression_loss: 0.3564 - classification_loss: 0.0482
 3327/10000 [========>.....................] - ETA: 1:39:00 - loss: 0.4046 - regression_loss: 0.3563 - classification_loss: 0.0482
 3328/10000 [========>.....................] - ETA: 1:38:59 - loss: 0.4044 - regression_loss: 0.3562 - classification_loss: 0.0482
 3329/10000 [========>.....................] - ETA: 1:38:58 - loss: 0.4045 - regression_loss: 0.3562 - classification_loss: 0.0482
 3330/10000 [========>.....................] - ETA: 1:38:57 - loss: 0.4044 - regression_loss: 0.3562 - classification_loss: 0.0482
 3331/10000 [========>.....................] - ETA: 1:38:56 - loss: 0.4045 - regression_loss: 0.3562 - classification_loss: 0.0483
 3332/10000 [========>.....................] - ETA: 1:38:55 - loss: 0.4046 - regression_loss: 0.3563 - classification_loss: 0.0483
 3333/10000 [========>.....................] - ETA: 1:38:54 - loss: 0.4046 - regression_loss: 0.3563 - classification_loss: 0.0483
 3334/10000 [=========>....................] - ETA: 1:38:53 - loss: 0.4047 - regression_loss: 0.3564 - classification_loss: 0.0483
 3335/10000 [=========>....................] - ETA: 1:38:52 - loss: 0.4046 - regression_loss: 0.3563 - classification_loss: 0.0483
 3336/10000 [=========>....................] - ETA: 1:38:51 - loss: 0.4046 - regression_loss: 0.3563 - classification_loss: 0.0483
 3337/10000 [=========>....................] - ETA: 1:38:51 - loss: 0.4046 - regression_loss: 0.3563 - classification_loss: 0.0483
 3338/10000 [=========>....................] - ETA: 1:38:50 - loss: 0.4047 - regression_loss: 0.3564 - classification_loss: 0.0483
 3339/10000 [=========>....................] - ETA: 1:38:49 - loss: 0.4046 - regression_loss: 0.3563 - classification_loss: 0.0483
 3340/10000 [=========>....................] - ETA: 1:38:48 - loss: 0.4046 - regression_loss: 0.3563 - classification_loss: 0.0483
 3341/10000 [=========>....................] - ETA: 1:38:47 - loss: 0.4047 - regression_loss: 0.3564 - classification_loss: 0.0483
 3342/10000 [=========>....................] - ETA: 1:38:46 - loss: 0.4047 - regression_loss: 0.3564 - classification_loss: 0.0483
 3343/10000 [=========>....................] - ETA: 1:38:45 - loss: 0.4046 - regression_loss: 0.3563 - classification_loss: 0.0483
 3344/10000 [=========>....................] - ETA: 1:38:44 - loss: 0.4046 - regression_loss: 0.3563 - classification_loss: 0.0483
 3345/10000 [=========>....................] - ETA: 1:38:43 - loss: 0.4046 - regression_loss: 0.3563 - classification_loss: 0.0483
 3346/10000 [=========>....................] - ETA: 1:38:43 - loss: 0.4045 - regression_loss: 0.3563 - classification_loss: 0.0483
 3347/10000 [=========>....................] - ETA: 1:38:42 - loss: 0.4047 - regression_loss: 0.3564 - classification_loss: 0.0483
 3348/10000 [=========>....................] - ETA: 1:38:41 - loss: 0.4047 - regression_loss: 0.3564 - classification_loss: 0.0483
 3349/10000 [=========>....................] - ETA: 1:38:40 - loss: 0.4048 - regression_loss: 0.3565 - classification_loss: 0.0483
 3350/10000 [=========>....................] - ETA: 1:38:39 - loss: 0.4048 - regression_loss: 0.3565 - classification_loss: 0.0483
 3351/10000 [=========>....................] - ETA: 1:38:38 - loss: 0.4049 - regression_loss: 0.3566 - classification_loss: 0.0483
 3352/10000 [=========>....................] - ETA: 1:38:37 - loss: 0.4049 - regression_loss: 0.3566 - classification_loss: 0.0483
 3353/10000 [=========>....................] - ETA: 1:38:36 - loss: 0.4049 - regression_loss: 0.3566 - classification_loss: 0.0483
 3354/10000 [=========>....................] - ETA: 1:38:35 - loss: 0.4049 - regression_loss: 0.3565 - classification_loss: 0.0483
 3355/10000 [=========>....................] - ETA: 1:38:34 - loss: 0.4048 - regression_loss: 0.3565 - classification_loss: 0.0483
 3356/10000 [=========>....................] - ETA: 1:38:34 - loss: 0.4047 - regression_loss: 0.3564 - classification_loss: 0.0483
 3357/10000 [=========>....................] - ETA: 1:38:33 - loss: 0.4048 - regression_loss: 0.3564 - classification_loss: 0.0484
 3358/10000 [=========>....................] - ETA: 1:38:32 - loss: 0.4047 - regression_loss: 0.3563 - classification_loss: 0.0484
 3359/10000 [=========>....................] - ETA: 1:38:31 - loss: 0.4047 - regression_loss: 0.3563 - classification_loss: 0.0484
 3360/10000 [=========>....................] - ETA: 1:38:30 - loss: 0.4047 - regression_loss: 0.3563 - classification_loss: 0.0484
 3361/10000 [=========>....................] - ETA: 1:38:29 - loss: 0.4047 - regression_loss: 0.3563 - classification_loss: 0.0484
 3362/10000 [=========>....................] - ETA: 1:38:28 - loss: 0.4046 - regression_loss: 0.3562 - classification_loss: 0.0484
 3363/10000 [=========>....................] - ETA: 1:38:27 - loss: 0.4045 - regression_loss: 0.3562 - classification_loss: 0.0484
 3364/10000 [=========>....................] - ETA: 1:38:26 - loss: 0.4044 - regression_loss: 0.3560 - classification_loss: 0.0484
 3365/10000 [=========>....................] - ETA: 1:38:26 - loss: 0.4045 - regression_loss: 0.3562 - classification_loss: 0.0484
 3366/10000 [=========>....................] - ETA: 1:38:25 - loss: 0.4045 - regression_loss: 0.3561 - classification_loss: 0.0484
 3367/10000 [=========>....................] - ETA: 1:38:24 - loss: 0.4045 - regression_loss: 0.3561 - classification_loss: 0.0484
 3368/10000 [=========>....................] - ETA: 1:38:23 - loss: 0.4044 - regression_loss: 0.3561 - classification_loss: 0.0484
 3369/10000 [=========>....................] - ETA: 1:38:22 - loss: 0.4044 - regression_loss: 0.3561 - classification_loss: 0.0483
 3370/10000 [=========>....................] - ETA: 1:38:21 - loss: 0.4044 - regression_loss: 0.3560 - classification_loss: 0.0483
 3371/10000 [=========>....................] - ETA: 1:38:20 - loss: 0.4042 - regression_loss: 0.3559 - classification_loss: 0.0483
 3372/10000 [=========>....................] - ETA: 1:38:19 - loss: 0.4044 - regression_loss: 0.3560 - classification_loss: 0.0483
 3373/10000 [=========>....................] - ETA: 1:38:18 - loss: 0.4044 - regression_loss: 0.3560 - classification_loss: 0.0483
 3374/10000 [=========>....................] - ETA: 1:38:17 - loss: 0.4043 - regression_loss: 0.3560 - classification_loss: 0.0483
 3375/10000 [=========>....................] - ETA: 1:38:17 - loss: 0.4043 - regression_loss: 0.3560 - classification_loss: 0.0484
 3376/10000 [=========>....................] - ETA: 1:38:16 - loss: 0.4045 - regression_loss: 0.3561 - classification_loss: 0.0484
 3377/10000 [=========>....................] - ETA: 1:38:15 - loss: 0.4044 - regression_loss: 0.3561 - classification_loss: 0.0484
 3378/10000 [=========>....................] - ETA: 1:38:14 - loss: 0.4044 - regression_loss: 0.3560 - classification_loss: 0.0484
 3379/10000 [=========>....................] - ETA: 1:38:13 - loss: 0.4045 - regression_loss: 0.3561 - classification_loss: 0.0484
 3380/10000 [=========>....................] - ETA: 1:38:12 - loss: 0.4044 - regression_loss: 0.3560 - classification_loss: 0.0484
 3381/10000 [=========>....................] - ETA: 1:38:11 - loss: 0.4044 - regression_loss: 0.3561 - classification_loss: 0.0484
 3382/10000 [=========>....................] - ETA: 1:38:10 - loss: 0.4044 - regression_loss: 0.3561 - classification_loss: 0.0484
 3383/10000 [=========>....................] - ETA: 1:38:09 - loss: 0.4044 - regression_loss: 0.3560 - classification_loss: 0.0484
 3384/10000 [=========>....................] - ETA: 1:38:09 - loss: 0.4043 - regression_loss: 0.3560 - classification_loss: 0.0484
 3385/10000 [=========>....................] - ETA: 1:38:08 - loss: 0.4043 - regression_loss: 0.3559 - classification_loss: 0.0484
 3386/10000 [=========>....................] - ETA: 1:38:07 - loss: 0.4043 - regression_loss: 0.3559 - classification_loss: 0.0484
 3387/10000 [=========>....................] - ETA: 1:38:06 - loss: 0.4046 - regression_loss: 0.3562 - classification_loss: 0.0484
 3388/10000 [=========>....................] - ETA: 1:38:05 - loss: 0.4045 - regression_loss: 0.3561 - classification_loss: 0.0484
 3389/10000 [=========>....................] - ETA: 1:38:04 - loss: 0.4044 - regression_loss: 0.3560 - classification_loss: 0.0484
 3390/10000 [=========>....................] - ETA: 1:38:03 - loss: 0.4044 - regression_loss: 0.3560 - classification_loss: 0.0484
 3391/10000 [=========>....................] - ETA: 1:38:02 - loss: 0.4043 - regression_loss: 0.3559 - classification_loss: 0.0484
 3392/10000 [=========>....................] - ETA: 1:38:01 - loss: 0.4042 - regression_loss: 0.3559 - classification_loss: 0.0484
 3393/10000 [=========>....................] - ETA: 1:38:00 - loss: 0.4042 - regression_loss: 0.3559 - classification_loss: 0.0484
 3394/10000 [=========>....................] - ETA: 1:38:00 - loss: 0.4043 - regression_loss: 0.3559 - classification_loss: 0.0484
 3395/10000 [=========>....................] - ETA: 1:37:59 - loss: 0.4043 - regression_loss: 0.3559 - classification_loss: 0.0484
 3396/10000 [=========>....................] - ETA: 1:37:58 - loss: 0.4043 - regression_loss: 0.3559 - classification_loss: 0.0484
 3397/10000 [=========>....................] - ETA: 1:37:57 - loss: 0.4043 - regression_loss: 0.3559 - classification_loss: 0.0483
 3398/10000 [=========>....................] - ETA: 1:37:56 - loss: 0.4043 - regression_loss: 0.3560 - classification_loss: 0.0483
 3399/10000 [=========>....................] - ETA: 1:37:55 - loss: 0.4043 - regression_loss: 0.3560 - classification_loss: 0.0483
 3400/10000 [=========>....................] - ETA: 1:37:54 - loss: 0.4043 - regression_loss: 0.3559 - classification_loss: 0.0483
 3401/10000 [=========>....................] - ETA: 1:37:53 - loss: 0.4043 - regression_loss: 0.3559 - classification_loss: 0.0483
 3402/10000 [=========>....................] - ETA: 1:37:52 - loss: 0.4042 - regression_loss: 0.3559 - classification_loss: 0.0483
 3403/10000 [=========>....................] - ETA: 1:37:52 - loss: 0.4042 - regression_loss: 0.3559 - classification_loss: 0.0483
 3404/10000 [=========>....................] - ETA: 1:37:51 - loss: 0.4043 - regression_loss: 0.3560 - classification_loss: 0.0483
 3405/10000 [=========>....................] - ETA: 1:37:50 - loss: 0.4043 - regression_loss: 0.3560 - classification_loss: 0.0483
 3406/10000 [=========>....................] - ETA: 1:37:49 - loss: 0.4043 - regression_loss: 0.3560 - classification_loss: 0.0483
 3407/10000 [=========>....................] - ETA: 1:37:48 - loss: 0.4042 - regression_loss: 0.3559 - classification_loss: 0.0483
 3408/10000 [=========>....................] - ETA: 1:37:47 - loss: 0.4042 - regression_loss: 0.3558 - classification_loss: 0.0483
 3409/10000 [=========>....................] - ETA: 1:37:46 - loss: 0.4042 - regression_loss: 0.3559 - classification_loss: 0.0483
 3410/10000 [=========>....................] - ETA: 1:37:45 - loss: 0.4042 - regression_loss: 0.3559 - classification_loss: 0.0483
 3411/10000 [=========>....................] - ETA: 1:37:44 - loss: 0.4043 - regression_loss: 0.3560 - classification_loss: 0.0483
 3412/10000 [=========>....................] - ETA: 1:37:43 - loss: 0.4043 - regression_loss: 0.3559 - classification_loss: 0.0483
 3413/10000 [=========>....................] - ETA: 1:37:43 - loss: 0.4043 - regression_loss: 0.3559 - classification_loss: 0.0483
 3414/10000 [=========>....................] - ETA: 1:37:42 - loss: 0.4042 - regression_loss: 0.3559 - classification_loss: 0.0483
 3415/10000 [=========>....................] - ETA: 1:37:41 - loss: 0.4043 - regression_loss: 0.3560 - classification_loss: 0.0483
 3416/10000 [=========>....................] - ETA: 1:37:40 - loss: 0.4043 - regression_loss: 0.3560 - classification_loss: 0.0483
 3417/10000 [=========>....................] - ETA: 1:37:39 - loss: 0.4043 - regression_loss: 0.3560 - classification_loss: 0.0483
 3418/10000 [=========>....................] - ETA: 1:37:38 - loss: 0.4044 - regression_loss: 0.3561 - classification_loss: 0.0483
 3419/10000 [=========>....................] - ETA: 1:37:37 - loss: 0.4045 - regression_loss: 0.3561 - classification_loss: 0.0483
 3420/10000 [=========>....................] - ETA: 1:37:36 - loss: 0.4044 - regression_loss: 0.3561 - classification_loss: 0.0483
 3421/10000 [=========>....................] - ETA: 1:37:35 - loss: 0.4044 - regression_loss: 0.3561 - classification_loss: 0.0483
 3422/10000 [=========>....................] - ETA: 1:37:35 - loss: 0.4044 - regression_loss: 0.3561 - classification_loss: 0.0483
 3423/10000 [=========>....................] - ETA: 1:37:34 - loss: 0.4044 - regression_loss: 0.3561 - classification_loss: 0.0483
 3424/10000 [=========>....................] - ETA: 1:37:33 - loss: 0.4044 - regression_loss: 0.3561 - classification_loss: 0.0483
 3425/10000 [=========>....................] - ETA: 1:37:32 - loss: 0.4045 - regression_loss: 0.3562 - classification_loss: 0.0483
 3426/10000 [=========>....................] - ETA: 1:37:31 - loss: 0.4046 - regression_loss: 0.3563 - classification_loss: 0.0483
 3427/10000 [=========>....................] - ETA: 1:37:30 - loss: 0.4046 - regression_loss: 0.3563 - classification_loss: 0.0483
 3428/10000 [=========>....................] - ETA: 1:37:29 - loss: 0.4047 - regression_loss: 0.3564 - classification_loss: 0.0483
 3429/10000 [=========>....................] - ETA: 1:37:28 - loss: 0.4050 - regression_loss: 0.3566 - classification_loss: 0.0484
 3430/10000 [=========>....................] - ETA: 1:37:27 - loss: 0.4050 - regression_loss: 0.3566 - classification_loss: 0.0484
 3431/10000 [=========>....................] - ETA: 1:37:27 - loss: 0.4049 - regression_loss: 0.3566 - classification_loss: 0.0483
 3432/10000 [=========>....................] - ETA: 1:37:26 - loss: 0.4048 - regression_loss: 0.3565 - classification_loss: 0.0483
 3433/10000 [=========>....................] - ETA: 1:37:25 - loss: 0.4048 - regression_loss: 0.3565 - classification_loss: 0.0483
 3434/10000 [=========>....................] - ETA: 1:37:24 - loss: 0.4049 - regression_loss: 0.3566 - classification_loss: 0.0483
 3435/10000 [=========>....................] - ETA: 1:37:23 - loss: 0.4050 - regression_loss: 0.3566 - classification_loss: 0.0483
 3436/10000 [=========>....................] - ETA: 1:37:22 - loss: 0.4050 - regression_loss: 0.3566 - classification_loss: 0.0483
 3437/10000 [=========>....................] - ETA: 1:37:21 - loss: 0.4049 - regression_loss: 0.3566 - classification_loss: 0.0483
 3438/10000 [=========>....................] - ETA: 1:37:20 - loss: 0.4048 - regression_loss: 0.3564 - classification_loss: 0.0483
 3439/10000 [=========>....................] - ETA: 1:37:19 - loss: 0.4049 - regression_loss: 0.3566 - classification_loss: 0.0483
 3440/10000 [=========>....................] - ETA: 1:37:18 - loss: 0.4048 - regression_loss: 0.3565 - classification_loss: 0.0483
 3441/10000 [=========>....................] - ETA: 1:37:18 - loss: 0.4048 - regression_loss: 0.3564 - classification_loss: 0.0483
 3442/10000 [=========>....................] - ETA: 1:37:17 - loss: 0.4046 - regression_loss: 0.3563 - classification_loss: 0.0483
 3443/10000 [=========>....................] - ETA: 1:37:16 - loss: 0.4046 - regression_loss: 0.3563 - classification_loss: 0.0483
 3444/10000 [=========>....................] - ETA: 1:37:15 - loss: 0.4046 - regression_loss: 0.3563 - classification_loss: 0.0483
 3445/10000 [=========>....................] - ETA: 1:37:14 - loss: 0.4045 - regression_loss: 0.3563 - classification_loss: 0.0483
 3446/10000 [=========>....................] - ETA: 1:37:13 - loss: 0.4046 - regression_loss: 0.3563 - classification_loss: 0.0483
 3447/10000 [=========>....................] - ETA: 1:37:12 - loss: 0.4046 - regression_loss: 0.3563 - classification_loss: 0.0483
 3448/10000 [=========>....................] - ETA: 1:37:11 - loss: 0.4045 - regression_loss: 0.3563 - classification_loss: 0.0483
 3449/10000 [=========>....................] - ETA: 1:37:10 - loss: 0.4046 - regression_loss: 0.3563 - classification_loss: 0.0483
 3450/10000 [=========>....................] - ETA: 1:37:10 - loss: 0.4045 - regression_loss: 0.3563 - classification_loss: 0.0483
 3451/10000 [=========>....................] - ETA: 1:37:09 - loss: 0.4045 - regression_loss: 0.3562 - classification_loss: 0.0483
 3452/10000 [=========>....................] - ETA: 1:37:08 - loss: 0.4044 - regression_loss: 0.3562 - classification_loss: 0.0482
 3453/10000 [=========>....................] - ETA: 1:37:07 - loss: 0.4044 - regression_loss: 0.3562 - classification_loss: 0.0482
 3454/10000 [=========>....................] - ETA: 1:37:06 - loss: 0.4043 - regression_loss: 0.3561 - classification_loss: 0.0482
 3455/10000 [=========>....................] - ETA: 1:37:05 - loss: 0.4042 - regression_loss: 0.3560 - classification_loss: 0.0482
 3456/10000 [=========>....................] - ETA: 1:37:04 - loss: 0.4041 - regression_loss: 0.3559 - classification_loss: 0.0482
 3457/10000 [=========>....................] - ETA: 1:37:03 - loss: 0.4042 - regression_loss: 0.3560 - classification_loss: 0.0482
 3458/10000 [=========>....................] - ETA: 1:37:02 - loss: 0.4041 - regression_loss: 0.3560 - classification_loss: 0.0482
 3459/10000 [=========>....................] - ETA: 1:37:02 - loss: 0.4042 - regression_loss: 0.3560 - classification_loss: 0.0482
 3460/10000 [=========>....................] - ETA: 1:37:01 - loss: 0.4043 - regression_loss: 0.3561 - classification_loss: 0.0482
 3461/10000 [=========>....................] - ETA: 1:37:00 - loss: 0.4042 - regression_loss: 0.3560 - classification_loss: 0.0482
 3462/10000 [=========>....................] - ETA: 1:36:59 - loss: 0.4043 - regression_loss: 0.3561 - classification_loss: 0.0482
 3463/10000 [=========>....................] - ETA: 1:36:58 - loss: 0.4043 - regression_loss: 0.3561 - classification_loss: 0.0482
 3464/10000 [=========>....................] - ETA: 1:36:57 - loss: 0.4042 - regression_loss: 0.3561 - classification_loss: 0.0481
 3465/10000 [=========>....................] - ETA: 1:36:56 - loss: 0.4041 - regression_loss: 0.3560 - classification_loss: 0.0481
 3466/10000 [=========>....................] - ETA: 1:36:55 - loss: 0.4041 - regression_loss: 0.3560 - classification_loss: 0.0481
 3467/10000 [=========>....................] - ETA: 1:36:54 - loss: 0.4041 - regression_loss: 0.3560 - classification_loss: 0.0481
 3468/10000 [=========>....................] - ETA: 1:36:53 - loss: 0.4040 - regression_loss: 0.3559 - classification_loss: 0.0481
 3469/10000 [=========>....................] - ETA: 1:36:53 - loss: 0.4040 - regression_loss: 0.3560 - classification_loss: 0.0481
 3470/10000 [=========>....................] - ETA: 1:36:52 - loss: 0.4041 - regression_loss: 0.3560 - classification_loss: 0.0481
 3471/10000 [=========>....................] - ETA: 1:36:51 - loss: 0.4041 - regression_loss: 0.3560 - classification_loss: 0.0481
 3472/10000 [=========>....................] - ETA: 1:36:50 - loss: 0.4040 - regression_loss: 0.3559 - classification_loss: 0.0481
 3473/10000 [=========>....................] - ETA: 1:36:49 - loss: 0.4040 - regression_loss: 0.3559 - classification_loss: 0.0481
 3474/10000 [=========>....................] - ETA: 1:36:48 - loss: 0.4042 - regression_loss: 0.3561 - classification_loss: 0.0481
 3475/10000 [=========>....................] - ETA: 1:36:47 - loss: 0.4042 - regression_loss: 0.3561 - classification_loss: 0.0481
 3476/10000 [=========>....................] - ETA: 1:36:46 - loss: 0.4042 - regression_loss: 0.3561 - classification_loss: 0.0481
 3477/10000 [=========>....................] - ETA: 1:36:45 - loss: 0.4042 - regression_loss: 0.3561 - classification_loss: 0.0481
 3478/10000 [=========>....................] - ETA: 1:36:45 - loss: 0.4041 - regression_loss: 0.3560 - classification_loss: 0.0481
 3479/10000 [=========>....................] - ETA: 1:36:44 - loss: 0.4042 - regression_loss: 0.3561 - classification_loss: 0.0481
 3480/10000 [=========>....................] - ETA: 1:36:43 - loss: 0.4041 - regression_loss: 0.3560 - classification_loss: 0.0481
 3481/10000 [=========>....................] - ETA: 1:36:42 - loss: 0.4040 - regression_loss: 0.3559 - classification_loss: 0.0481
 3482/10000 [=========>....................] - ETA: 1:36:41 - loss: 0.4039 - regression_loss: 0.3558 - classification_loss: 0.0481
 3483/10000 [=========>....................] - ETA: 1:36:40 - loss: 0.4039 - regression_loss: 0.3558 - classification_loss: 0.0481
 3484/10000 [=========>....................] - ETA: 1:36:39 - loss: 0.4039 - regression_loss: 0.3559 - classification_loss: 0.0481
 3485/10000 [=========>....................] - ETA: 1:36:38 - loss: 0.4039 - regression_loss: 0.3559 - classification_loss: 0.0481
 3486/10000 [=========>....................] - ETA: 1:36:37 - loss: 0.4040 - regression_loss: 0.3559 - classification_loss: 0.0481
 3487/10000 [=========>....................] - ETA: 1:36:37 - loss: 0.4040 - regression_loss: 0.3559 - classification_loss: 0.0481
 3488/10000 [=========>....................] - ETA: 1:36:36 - loss: 0.4040 - regression_loss: 0.3559 - classification_loss: 0.0480
 3489/10000 [=========>....................] - ETA: 1:36:35 - loss: 0.4040 - regression_loss: 0.3559 - classification_loss: 0.0481
 3490/10000 [=========>....................] - ETA: 1:36:34 - loss: 0.4039 - regression_loss: 0.3559 - classification_loss: 0.0481
 3491/10000 [=========>....................] - ETA: 1:36:33 - loss: 0.4039 - regression_loss: 0.3559 - classification_loss: 0.0480
 3492/10000 [=========>....................] - ETA: 1:36:32 - loss: 0.4040 - regression_loss: 0.3559 - classification_loss: 0.0480
 3493/10000 [=========>....................] - ETA: 1:36:31 - loss: 0.4040 - regression_loss: 0.3559 - classification_loss: 0.0480
 3494/10000 [=========>....................] - ETA: 1:36:30 - loss: 0.4040 - regression_loss: 0.3560 - classification_loss: 0.0481
 3495/10000 [=========>....................] - ETA: 1:36:29 - loss: 0.4040 - regression_loss: 0.3560 - classification_loss: 0.0480
 3496/10000 [=========>....................] - ETA: 1:36:29 - loss: 0.4040 - regression_loss: 0.3560 - classification_loss: 0.0480
 3497/10000 [=========>....................] - ETA: 1:36:28 - loss: 0.4040 - regression_loss: 0.3559 - classification_loss: 0.0480
 3498/10000 [=========>....................] - ETA: 1:36:27 - loss: 0.4039 - regression_loss: 0.3558 - classification_loss: 0.0480
 3499/10000 [=========>....................] - ETA: 1:36:26 - loss: 0.4039 - regression_loss: 0.3559 - classification_loss: 0.0480
 3500/10000 [=========>....................] - ETA: 1:36:25 - loss: 0.4039 - regression_loss: 0.3559 - classification_loss: 0.0480
 3501/10000 [=========>....................] - ETA: 1:36:24 - loss: 0.4040 - regression_loss: 0.3560 - classification_loss: 0.0480
 3502/10000 [=========>....................] - ETA: 1:36:23 - loss: 0.4040 - regression_loss: 0.3560 - classification_loss: 0.0480
 3503/10000 [=========>....................] - ETA: 1:36:22 - loss: 0.4040 - regression_loss: 0.3560 - classification_loss: 0.0480
 3504/10000 [=========>....................] - ETA: 1:36:21 - loss: 0.4040 - regression_loss: 0.3560 - classification_loss: 0.0480
 3505/10000 [=========>....................] - ETA: 1:36:20 - loss: 0.4041 - regression_loss: 0.3561 - classification_loss: 0.0480
 3506/10000 [=========>....................] - ETA: 1:36:20 - loss: 0.4041 - regression_loss: 0.3561 - classification_loss: 0.0480
 3507/10000 [=========>....................] - ETA: 1:36:19 - loss: 0.4041 - regression_loss: 0.3561 - classification_loss: 0.0480
 3508/10000 [=========>....................] - ETA: 1:36:18 - loss: 0.4041 - regression_loss: 0.3561 - classification_loss: 0.0480
 3509/10000 [=========>....................] - ETA: 1:36:17 - loss: 0.4040 - regression_loss: 0.3560 - classification_loss: 0.0480
 3510/10000 [=========>....................] - ETA: 1:36:16 - loss: 0.4040 - regression_loss: 0.3560 - classification_loss: 0.0480
 3511/10000 [=========>....................] - ETA: 1:36:15 - loss: 0.4039 - regression_loss: 0.3560 - classification_loss: 0.0480
 3512/10000 [=========>....................] - ETA: 1:36:14 - loss: 0.4039 - regression_loss: 0.3560 - classification_loss: 0.0479
 3513/10000 [=========>....................] - ETA: 1:36:13 - loss: 0.4039 - regression_loss: 0.3559 - classification_loss: 0.0479
 3514/10000 [=========>....................] - ETA: 1:36:12 - loss: 0.4040 - regression_loss: 0.3560 - classification_loss: 0.0479
 3515/10000 [=========>....................] - ETA: 1:36:12 - loss: 0.4040 - regression_loss: 0.3560 - classification_loss: 0.0479
 3516/10000 [=========>....................] - ETA: 1:36:11 - loss: 0.4039 - regression_loss: 0.3560 - classification_loss: 0.0479
 3517/10000 [=========>....................] - ETA: 1:36:10 - loss: 0.4038 - regression_loss: 0.3559 - classification_loss: 0.0479
 3518/10000 [=========>....................] - ETA: 1:36:09 - loss: 0.4040 - regression_loss: 0.3561 - classification_loss: 0.0480
 3519/10000 [=========>....................] - ETA: 1:36:08 - loss: 0.4041 - regression_loss: 0.3561 - classification_loss: 0.0480
 3520/10000 [=========>....................] - ETA: 1:36:07 - loss: 0.4041 - regression_loss: 0.3561 - classification_loss: 0.0480
 3521/10000 [=========>....................] - ETA: 1:36:06 - loss: 0.4040 - regression_loss: 0.3561 - classification_loss: 0.0480
 3522/10000 [=========>....................] - ETA: 1:36:05 - loss: 0.4041 - regression_loss: 0.3561 - classification_loss: 0.0479
 3523/10000 [=========>....................] - ETA: 1:36:04 - loss: 0.4040 - regression_loss: 0.3560 - classification_loss: 0.0480
 3524/10000 [=========>....................] - ETA: 1:36:04 - loss: 0.4040 - regression_loss: 0.3560 - classification_loss: 0.0479
 3525/10000 [=========>....................] - ETA: 1:36:03 - loss: 0.4039 - regression_loss: 0.3559 - classification_loss: 0.0479
 3526/10000 [=========>....................] - ETA: 1:36:02 - loss: 0.4040 - regression_loss: 0.3560 - classification_loss: 0.0479
 3527/10000 [=========>....................] - ETA: 1:36:01 - loss: 0.4039 - regression_loss: 0.3560 - classification_loss: 0.0479
 3528/10000 [=========>....................] - ETA: 1:36:00 - loss: 0.4039 - regression_loss: 0.3560 - classification_loss: 0.0479
 3529/10000 [=========>....................] - ETA: 1:35:59 - loss: 0.4040 - regression_loss: 0.3560 - classification_loss: 0.0479
 3530/10000 [=========>....................] - ETA: 1:35:58 - loss: 0.4039 - regression_loss: 0.3560 - classification_loss: 0.0479
 3531/10000 [=========>....................] - ETA: 1:35:57 - loss: 0.4039 - regression_loss: 0.3559 - classification_loss: 0.0479
 3532/10000 [=========>....................] - ETA: 1:35:56 - loss: 0.4039 - regression_loss: 0.3560 - classification_loss: 0.0479
 3533/10000 [=========>....................] - ETA: 1:35:56 - loss: 0.4043 - regression_loss: 0.3562 - classification_loss: 0.0481
 3534/10000 [=========>....................] - ETA: 1:35:55 - loss: 0.4042 - regression_loss: 0.3561 - classification_loss: 0.0481
 3535/10000 [=========>....................] - ETA: 1:35:54 - loss: 0.4042 - regression_loss: 0.3562 - classification_loss: 0.0481
 3536/10000 [=========>....................] - ETA: 1:35:53 - loss: 0.4044 - regression_loss: 0.3563 - classification_loss: 0.0481
 3537/10000 [=========>....................] - ETA: 1:35:52 - loss: 0.4044 - regression_loss: 0.3563 - classification_loss: 0.0481
 3538/10000 [=========>....................] - ETA: 1:35:51 - loss: 0.4043 - regression_loss: 0.3562 - classification_loss: 0.0481
 3539/10000 [=========>....................] - ETA: 1:35:50 - loss: 0.4044 - regression_loss: 0.3563 - classification_loss: 0.0481
 3540/10000 [=========>....................] - ETA: 1:35:49 - loss: 0.4044 - regression_loss: 0.3563 - classification_loss: 0.0481
 3541/10000 [=========>....................] - ETA: 1:35:48 - loss: 0.4045 - regression_loss: 0.3564 - classification_loss: 0.0481
 3542/10000 [=========>....................] - ETA: 1:35:47 - loss: 0.4045 - regression_loss: 0.3564 - classification_loss: 0.0481
 3543/10000 [=========>....................] - ETA: 1:35:47 - loss: 0.4045 - regression_loss: 0.3564 - classification_loss: 0.0481
 3544/10000 [=========>....................] - ETA: 1:35:46 - loss: 0.4045 - regression_loss: 0.3564 - classification_loss: 0.0481
 3545/10000 [=========>....................] - ETA: 1:35:45 - loss: 0.4044 - regression_loss: 0.3564 - classification_loss: 0.0481
 3546/10000 [=========>....................] - ETA: 1:35:44 - loss: 0.4044 - regression_loss: 0.3564 - classification_loss: 0.0481
 3547/10000 [=========>....................] - ETA: 1:35:43 - loss: 0.4044 - regression_loss: 0.3563 - classification_loss: 0.0481
 3548/10000 [=========>....................] - ETA: 1:35:42 - loss: 0.4044 - regression_loss: 0.3563 - classification_loss: 0.0481
 3549/10000 [=========>....................] - ETA: 1:35:41 - loss: 0.4044 - regression_loss: 0.3564 - classification_loss: 0.0481
 3550/10000 [=========>....................] - ETA: 1:35:40 - loss: 0.4044 - regression_loss: 0.3563 - classification_loss: 0.0481
 3551/10000 [=========>....................] - ETA: 1:35:39 - loss: 0.4043 - regression_loss: 0.3563 - classification_loss: 0.0481
 3552/10000 [=========>....................] - ETA: 1:35:39 - loss: 0.4043 - regression_loss: 0.3563 - classification_loss: 0.0480
 3553/10000 [=========>....................] - ETA: 1:35:38 - loss: 0.4043 - regression_loss: 0.3562 - classification_loss: 0.0480
 3554/10000 [=========>....................] - ETA: 1:35:37 - loss: 0.4043 - regression_loss: 0.3562 - classification_loss: 0.0480
 3555/10000 [=========>....................] - ETA: 1:35:36 - loss: 0.4042 - regression_loss: 0.3562 - classification_loss: 0.0480
 3556/10000 [=========>....................] - ETA: 1:35:35 - loss: 0.4043 - regression_loss: 0.3563 - classification_loss: 0.0480
 3557/10000 [=========>....................] - ETA: 1:35:34 - loss: 0.4043 - regression_loss: 0.3563 - classification_loss: 0.0480
 3558/10000 [=========>....................] - ETA: 1:35:33 - loss: 0.4042 - regression_loss: 0.3562 - classification_loss: 0.0480
 3559/10000 [=========>....................] - ETA: 1:35:32 - loss: 0.4041 - regression_loss: 0.3561 - classification_loss: 0.0480
 3560/10000 [=========>....................] - ETA: 1:35:31 - loss: 0.4042 - regression_loss: 0.3562 - classification_loss: 0.0480
 3561/10000 [=========>....................] - ETA: 1:35:31 - loss: 0.4042 - regression_loss: 0.3561 - classification_loss: 0.0480
 3562/10000 [=========>....................] - ETA: 1:35:30 - loss: 0.4041 - regression_loss: 0.3561 - classification_loss: 0.0480
 3563/10000 [=========>....................] - ETA: 1:35:29 - loss: 0.4041 - regression_loss: 0.3561 - classification_loss: 0.0480
 3564/10000 [=========>....................] - ETA: 1:35:28 - loss: 0.4041 - regression_loss: 0.3561 - classification_loss: 0.0480
 3565/10000 [=========>....................] - ETA: 1:35:27 - loss: 0.4042 - regression_loss: 0.3562 - classification_loss: 0.0480
 3566/10000 [=========>....................] - ETA: 1:35:26 - loss: 0.4043 - regression_loss: 0.3563 - classification_loss: 0.0480
 3567/10000 [=========>....................] - ETA: 1:35:25 - loss: 0.4043 - regression_loss: 0.3563 - classification_loss: 0.0480
 3568/10000 [=========>....................] - ETA: 1:35:24 - loss: 0.4044 - regression_loss: 0.3564 - classification_loss: 0.0480
 3569/10000 [=========>....................] - ETA: 1:35:23 - loss: 0.4044 - regression_loss: 0.3564 - classification_loss: 0.0480
 3570/10000 [=========>....................] - ETA: 1:35:23 - loss: 0.4044 - regression_loss: 0.3564 - classification_loss: 0.0480
 3571/10000 [=========>....................] - ETA: 1:35:22 - loss: 0.4043 - regression_loss: 0.3563 - classification_loss: 0.0480
 3572/10000 [=========>....................] - ETA: 1:35:21 - loss: 0.4043 - regression_loss: 0.3563 - classification_loss: 0.0480
 3573/10000 [=========>....................] - ETA: 1:35:20 - loss: 0.4045 - regression_loss: 0.3564 - classification_loss: 0.0480
 3574/10000 [=========>....................] - ETA: 1:35:19 - loss: 0.4044 - regression_loss: 0.3564 - classification_loss: 0.0480
 3575/10000 [=========>....................] - ETA: 1:35:18 - loss: 0.4046 - regression_loss: 0.3565 - classification_loss: 0.0481
 3576/10000 [=========>....................] - ETA: 1:35:17 - loss: 0.4046 - regression_loss: 0.3566 - classification_loss: 0.0481
 3577/10000 [=========>....................] - ETA: 1:35:16 - loss: 0.4046 - regression_loss: 0.3566 - classification_loss: 0.0480
 3578/10000 [=========>....................] - ETA: 1:35:15 - loss: 0.4046 - regression_loss: 0.3566 - classification_loss: 0.0480
 3579/10000 [=========>....................] - ETA: 1:35:15 - loss: 0.4045 - regression_loss: 0.3565 - classification_loss: 0.0480
 3580/10000 [=========>....................] - ETA: 1:35:14 - loss: 0.4046 - regression_loss: 0.3566 - classification_loss: 0.0480
 3581/10000 [=========>....................] - ETA: 1:35:13 - loss: 0.4045 - regression_loss: 0.3565 - classification_loss: 0.0480
 3582/10000 [=========>....................] - ETA: 1:35:12 - loss: 0.4047 - regression_loss: 0.3566 - classification_loss: 0.0481
 3583/10000 [=========>....................] - ETA: 1:35:11 - loss: 0.4046 - regression_loss: 0.3566 - classification_loss: 0.0480
 3584/10000 [=========>....................] - ETA: 1:35:10 - loss: 0.4046 - regression_loss: 0.3566 - classification_loss: 0.0480
 3585/10000 [=========>....................] - ETA: 1:35:09 - loss: 0.4049 - regression_loss: 0.3568 - classification_loss: 0.0481
 3586/10000 [=========>....................] - ETA: 1:35:08 - loss: 0.4049 - regression_loss: 0.3568 - classification_loss: 0.0481
 3587/10000 [=========>....................] - ETA: 1:35:07 - loss: 0.4050 - regression_loss: 0.3568 - classification_loss: 0.0482
 3588/10000 [=========>....................] - ETA: 1:35:07 - loss: 0.4050 - regression_loss: 0.3568 - classification_loss: 0.0481
 3589/10000 [=========>....................] - ETA: 1:35:06 - loss: 0.4050 - regression_loss: 0.3568 - classification_loss: 0.0481
 3590/10000 [=========>....................] - ETA: 1:35:05 - loss: 0.4050 - regression_loss: 0.3568 - classification_loss: 0.0481
 3591/10000 [=========>....................] - ETA: 1:35:04 - loss: 0.4050 - regression_loss: 0.3569 - classification_loss: 0.0482
 3592/10000 [=========>....................] - ETA: 1:35:03 - loss: 0.4051 - regression_loss: 0.3569 - classification_loss: 0.0482
 3593/10000 [=========>....................] - ETA: 1:35:02 - loss: 0.4051 - regression_loss: 0.3569 - classification_loss: 0.0482
 3594/10000 [=========>....................] - ETA: 1:35:01 - loss: 0.4054 - regression_loss: 0.3572 - classification_loss: 0.0482
 3595/10000 [=========>....................] - ETA: 1:35:00 - loss: 0.4056 - regression_loss: 0.3572 - classification_loss: 0.0483
 3596/10000 [=========>....................] - ETA: 1:34:59 - loss: 0.4055 - regression_loss: 0.3572 - classification_loss: 0.0483
 3597/10000 [=========>....................] - ETA: 1:34:58 - loss: 0.4056 - regression_loss: 0.3573 - classification_loss: 0.0483
 3598/10000 [=========>....................] - ETA: 1:34:58 - loss: 0.4058 - regression_loss: 0.3574 - classification_loss: 0.0484
 3599/10000 [=========>....................] - ETA: 1:34:57 - loss: 0.4058 - regression_loss: 0.3575 - classification_loss: 0.0484
 3600/10000 [=========>....................] - ETA: 1:34:56 - loss: 0.4059 - regression_loss: 0.3575 - classification_loss: 0.0483
 3601/10000 [=========>....................] - ETA: 1:34:55 - loss: 0.4058 - regression_loss: 0.3574 - classification_loss: 0.0483
 3602/10000 [=========>....................] - ETA: 1:34:54 - loss: 0.4058 - regression_loss: 0.3575 - classification_loss: 0.0483
 3603/10000 [=========>....................] - ETA: 1:34:53 - loss: 0.4057 - regression_loss: 0.3574 - classification_loss: 0.0483
 3604/10000 [=========>....................] - ETA: 1:34:52 - loss: 0.4057 - regression_loss: 0.3574 - classification_loss: 0.0483
 3605/10000 [=========>....................] - ETA: 1:34:51 - loss: 0.4057 - regression_loss: 0.3574 - classification_loss: 0.0483
 3606/10000 [=========>....................] - ETA: 1:34:50 - loss: 0.4057 - regression_loss: 0.3574 - classification_loss: 0.0483
 3607/10000 [=========>....................] - ETA: 1:34:50 - loss: 0.4057 - regression_loss: 0.3574 - classification_loss: 0.0483
 3608/10000 [=========>....................] - ETA: 1:34:49 - loss: 0.4058 - regression_loss: 0.3575 - classification_loss: 0.0483
 3609/10000 [=========>....................] - ETA: 1:34:48 - loss: 0.4057 - regression_loss: 0.3574 - classification_loss: 0.0483
 3610/10000 [=========>....................] - ETA: 1:34:47 - loss: 0.4060 - regression_loss: 0.3575 - classification_loss: 0.0486
 3611/10000 [=========>....................] - ETA: 1:34:46 - loss: 0.4059 - regression_loss: 0.3574 - classification_loss: 0.0485
 3612/10000 [=========>....................] - ETA: 1:34:45 - loss: 0.4058 - regression_loss: 0.3573 - classification_loss: 0.0485
 3613/10000 [=========>....................] - ETA: 1:34:44 - loss: 0.4059 - regression_loss: 0.3574 - classification_loss: 0.0485
 3614/10000 [=========>....................] - ETA: 1:34:43 - loss: 0.4060 - regression_loss: 0.3574 - classification_loss: 0.0486
 3615/10000 [=========>....................] - ETA: 1:34:42 - loss: 0.4061 - regression_loss: 0.3575 - classification_loss: 0.0486
 3616/10000 [=========>....................] - ETA: 1:34:42 - loss: 0.4061 - regression_loss: 0.3575 - classification_loss: 0.0486
 3617/10000 [=========>....................] - ETA: 1:34:41 - loss: 0.4061 - regression_loss: 0.3575 - classification_loss: 0.0486
 3618/10000 [=========>....................] - ETA: 1:34:40 - loss: 0.4061 - regression_loss: 0.3575 - classification_loss: 0.0486
 3619/10000 [=========>....................] - ETA: 1:34:39 - loss: 0.4061 - regression_loss: 0.3575 - classification_loss: 0.0486
 3620/10000 [=========>....................] - ETA: 1:34:38 - loss: 0.4062 - regression_loss: 0.3576 - classification_loss: 0.0486
 3621/10000 [=========>....................] - ETA: 1:34:37 - loss: 0.4062 - regression_loss: 0.3575 - classification_loss: 0.0486
 3622/10000 [=========>....................] - ETA: 1:34:36 - loss: 0.4061 - regression_loss: 0.3574 - classification_loss: 0.0486
 3623/10000 [=========>....................] - ETA: 1:34:35 - loss: 0.4059 - regression_loss: 0.3573 - classification_loss: 0.0486
 3624/10000 [=========>....................] - ETA: 1:34:34 - loss: 0.4060 - regression_loss: 0.3573 - classification_loss: 0.0487
 3625/10000 [=========>....................] - ETA: 1:34:34 - loss: 0.4061 - regression_loss: 0.3574 - classification_loss: 0.0487
 3626/10000 [=========>....................] - ETA: 1:34:33 - loss: 0.4062 - regression_loss: 0.3575 - classification_loss: 0.0487
 3627/10000 [=========>....................] - ETA: 1:34:32 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 3628/10000 [=========>....................] - ETA: 1:34:31 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 3629/10000 [=========>....................] - ETA: 1:34:30 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 3630/10000 [=========>....................] - ETA: 1:34:29 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 3631/10000 [=========>....................] - ETA: 1:34:28 - loss: 0.4062 - regression_loss: 0.3575 - classification_loss: 0.0487
 3632/10000 [=========>....................] - ETA: 1:34:27 - loss: 0.4062 - regression_loss: 0.3575 - classification_loss: 0.0487
 3633/10000 [=========>....................] - ETA: 1:34:26 - loss: 0.4062 - regression_loss: 0.3575 - classification_loss: 0.0487
 3634/10000 [=========>....................] - ETA: 1:34:25 - loss: 0.4061 - regression_loss: 0.3575 - classification_loss: 0.0487
 3635/10000 [=========>....................] - ETA: 1:34:25 - loss: 0.4061 - regression_loss: 0.3574 - classification_loss: 0.0487
 3636/10000 [=========>....................] - ETA: 1:34:24 - loss: 0.4060 - regression_loss: 0.3573 - classification_loss: 0.0487
 3637/10000 [=========>....................] - ETA: 1:34:23 - loss: 0.4060 - regression_loss: 0.3573 - classification_loss: 0.0487
 3638/10000 [=========>....................] - ETA: 1:34:22 - loss: 0.4059 - regression_loss: 0.3572 - classification_loss: 0.0486
 3639/10000 [=========>....................] - ETA: 1:34:21 - loss: 0.4059 - regression_loss: 0.3572 - classification_loss: 0.0486
 3640/10000 [=========>....................] - ETA: 1:34:20 - loss: 0.4060 - regression_loss: 0.3573 - classification_loss: 0.0487
 3641/10000 [=========>....................] - ETA: 1:34:19 - loss: 0.4060 - regression_loss: 0.3574 - classification_loss: 0.0487
 3642/10000 [=========>....................] - ETA: 1:34:18 - loss: 0.4059 - regression_loss: 0.3573 - classification_loss: 0.0487
 3643/10000 [=========>....................] - ETA: 1:34:17 - loss: 0.4059 - regression_loss: 0.3572 - classification_loss: 0.0487
 3644/10000 [=========>....................] - ETA: 1:34:17 - loss: 0.4059 - regression_loss: 0.3572 - classification_loss: 0.0487
 3645/10000 [=========>....................] - ETA: 1:34:16 - loss: 0.4059 - regression_loss: 0.3572 - classification_loss: 0.0487
 3646/10000 [=========>....................] - ETA: 1:34:15 - loss: 0.4059 - regression_loss: 0.3573 - classification_loss: 0.0487
 3647/10000 [=========>....................] - ETA: 1:34:14 - loss: 0.4060 - regression_loss: 0.3573 - classification_loss: 0.0487
 3648/10000 [=========>....................] - ETA: 1:34:13 - loss: 0.4060 - regression_loss: 0.3572 - classification_loss: 0.0487
 3649/10000 [=========>....................] - ETA: 1:34:12 - loss: 0.4059 - regression_loss: 0.3572 - classification_loss: 0.0487
 3650/10000 [=========>....................] - ETA: 1:34:11 - loss: 0.4059 - regression_loss: 0.3572 - classification_loss: 0.0487
 3651/10000 [=========>....................] - ETA: 1:34:10 - loss: 0.4058 - regression_loss: 0.3571 - classification_loss: 0.0487
 3652/10000 [=========>....................] - ETA: 1:34:09 - loss: 0.4058 - regression_loss: 0.3571 - classification_loss: 0.0487
 3653/10000 [=========>....................] - ETA: 1:34:09 - loss: 0.4058 - regression_loss: 0.3571 - classification_loss: 0.0487
 3654/10000 [=========>....................] - ETA: 1:34:08 - loss: 0.4058 - regression_loss: 0.3572 - classification_loss: 0.0487
 3655/10000 [=========>....................] - ETA: 1:34:07 - loss: 0.4058 - regression_loss: 0.3572 - classification_loss: 0.0487
 3656/10000 [=========>....................] - ETA: 1:34:06 - loss: 0.4059 - regression_loss: 0.3572 - classification_loss: 0.0487
 3657/10000 [=========>....................] - ETA: 1:34:05 - loss: 0.4058 - regression_loss: 0.3572 - classification_loss: 0.0487
 3658/10000 [=========>....................] - ETA: 1:34:04 - loss: 0.4059 - regression_loss: 0.3572 - classification_loss: 0.0487
 3659/10000 [=========>....................] - ETA: 1:34:03 - loss: 0.4058 - regression_loss: 0.3572 - classification_loss: 0.0487
 3660/10000 [=========>....................] - ETA: 1:34:02 - loss: 0.4058 - regression_loss: 0.3571 - classification_loss: 0.0487
 3661/10000 [=========>....................] - ETA: 1:34:01 - loss: 0.4058 - regression_loss: 0.3571 - classification_loss: 0.0487
 3662/10000 [=========>....................] - ETA: 1:34:01 - loss: 0.4059 - regression_loss: 0.3572 - classification_loss: 0.0487
 3663/10000 [=========>....................] - ETA: 1:34:00 - loss: 0.4058 - regression_loss: 0.3571 - classification_loss: 0.0487
 3664/10000 [=========>....................] - ETA: 1:33:59 - loss: 0.4057 - regression_loss: 0.3570 - classification_loss: 0.0486
 3665/10000 [=========>....................] - ETA: 1:33:58 - loss: 0.4057 - regression_loss: 0.3571 - classification_loss: 0.0487
 3666/10000 [=========>....................] - ETA: 1:33:57 - loss: 0.4058 - regression_loss: 0.3571 - classification_loss: 0.0486
 3667/10000 [==========>...................] - ETA: 1:33:56 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 3668/10000 [==========>...................] - ETA: 1:33:55 - loss: 0.4057 - regression_loss: 0.3570 - classification_loss: 0.0486
 3669/10000 [==========>...................] - ETA: 1:33:54 - loss: 0.4058 - regression_loss: 0.3571 - classification_loss: 0.0486
 3670/10000 [==========>...................] - ETA: 1:33:53 - loss: 0.4057 - regression_loss: 0.3571 - classification_loss: 0.0486
 3671/10000 [==========>...................] - ETA: 1:33:53 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 3672/10000 [==========>...................] - ETA: 1:33:52 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 3673/10000 [==========>...................] - ETA: 1:33:51 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 3674/10000 [==========>...................] - ETA: 1:33:50 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 3675/10000 [==========>...................] - ETA: 1:33:49 - loss: 0.4053 - regression_loss: 0.3568 - classification_loss: 0.0486
 3676/10000 [==========>...................] - ETA: 1:33:48 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 3677/10000 [==========>...................] - ETA: 1:33:47 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 3678/10000 [==========>...................] - ETA: 1:33:46 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 3679/10000 [==========>...................] - ETA: 1:33:45 - loss: 0.4056 - regression_loss: 0.3569 - classification_loss: 0.0486
 3680/10000 [==========>...................] - ETA: 1:33:45 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 3681/10000 [==========>...................] - ETA: 1:33:44 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 3682/10000 [==========>...................] - ETA: 1:33:43 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 3683/10000 [==========>...................] - ETA: 1:33:42 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 3684/10000 [==========>...................] - ETA: 1:33:41 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 3685/10000 [==========>...................] - ETA: 1:33:40 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 3686/10000 [==========>...................] - ETA: 1:33:39 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 3687/10000 [==========>...................] - ETA: 1:33:38 - loss: 0.4056 - regression_loss: 0.3569 - classification_loss: 0.0487
 3688/10000 [==========>...................] - ETA: 1:33:37 - loss: 0.4056 - regression_loss: 0.3569 - classification_loss: 0.0487
 3689/10000 [==========>...................] - ETA: 1:33:36 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0487
 3690/10000 [==========>...................] - ETA: 1:33:36 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0487
 3691/10000 [==========>...................] - ETA: 1:33:35 - loss: 0.4056 - regression_loss: 0.3569 - classification_loss: 0.0487
 3692/10000 [==========>...................] - ETA: 1:33:34 - loss: 0.4055 - regression_loss: 0.3568 - classification_loss: 0.0487
 3693/10000 [==========>...................] - ETA: 1:33:33 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0487
 3694/10000 [==========>...................] - ETA: 1:33:32 - loss: 0.4055 - regression_loss: 0.3568 - classification_loss: 0.0486
 3695/10000 [==========>...................] - ETA: 1:33:31 - loss: 0.4055 - regression_loss: 0.3568 - classification_loss: 0.0486
 3696/10000 [==========>...................] - ETA: 1:33:30 - loss: 0.4065 - regression_loss: 0.3570 - classification_loss: 0.0495
 3697/10000 [==========>...................] - ETA: 1:33:29 - loss: 0.4065 - regression_loss: 0.3570 - classification_loss: 0.0495
 3698/10000 [==========>...................] - ETA: 1:33:28 - loss: 0.4065 - regression_loss: 0.3569 - classification_loss: 0.0496
 3699/10000 [==========>...................] - ETA: 1:33:28 - loss: 0.4068 - regression_loss: 0.3570 - classification_loss: 0.0498
 3700/10000 [==========>...................] - ETA: 1:33:27 - loss: 0.4067 - regression_loss: 0.3569 - classification_loss: 0.0498
 3701/10000 [==========>...................] - ETA: 1:33:26 - loss: 0.4067 - regression_loss: 0.3570 - classification_loss: 0.0498
 3702/10000 [==========>...................] - ETA: 1:33:25 - loss: 0.4068 - regression_loss: 0.3570 - classification_loss: 0.0498
 3703/10000 [==========>...................] - ETA: 1:33:24 - loss: 0.4067 - regression_loss: 0.3569 - classification_loss: 0.0498
 3704/10000 [==========>...................] - ETA: 1:33:23 - loss: 0.4066 - regression_loss: 0.3569 - classification_loss: 0.0498
 3705/10000 [==========>...................] - ETA: 1:33:22 - loss: 0.4065 - regression_loss: 0.3568 - classification_loss: 0.0498
 3706/10000 [==========>...................] - ETA: 1:33:21 - loss: 0.4065 - regression_loss: 0.3568 - classification_loss: 0.0498
 3707/10000 [==========>...................] - ETA: 1:33:20 - loss: 0.4064 - regression_loss: 0.3567 - classification_loss: 0.0498
 3708/10000 [==========>...................] - ETA: 1:33:20 - loss: 0.4064 - regression_loss: 0.3566 - classification_loss: 0.0498
 3709/10000 [==========>...................] - ETA: 1:33:19 - loss: 0.4063 - regression_loss: 0.3566 - classification_loss: 0.0497
 3710/10000 [==========>...................] - ETA: 1:33:18 - loss: 0.4063 - regression_loss: 0.3566 - classification_loss: 0.0497
 3711/10000 [==========>...................] - ETA: 1:33:17 - loss: 0.4063 - regression_loss: 0.3566 - classification_loss: 0.0497
 3712/10000 [==========>...................] - ETA: 1:33:16 - loss: 0.4062 - regression_loss: 0.3565 - classification_loss: 0.0497
 3713/10000 [==========>...................] - ETA: 1:33:15 - loss: 0.4062 - regression_loss: 0.3565 - classification_loss: 0.0497
 3714/10000 [==========>...................] - ETA: 1:33:14 - loss: 0.4062 - regression_loss: 0.3564 - classification_loss: 0.0497
 3715/10000 [==========>...................] - ETA: 1:33:13 - loss: 0.4061 - regression_loss: 0.3563 - classification_loss: 0.0497
 3716/10000 [==========>...................] - ETA: 1:33:12 - loss: 0.4060 - regression_loss: 0.3563 - classification_loss: 0.0497
 3717/10000 [==========>...................] - ETA: 1:33:12 - loss: 0.4060 - regression_loss: 0.3563 - classification_loss: 0.0497
 3718/10000 [==========>...................] - ETA: 1:33:11 - loss: 0.4063 - regression_loss: 0.3565 - classification_loss: 0.0498
 3719/10000 [==========>...................] - ETA: 1:33:10 - loss: 0.4063 - regression_loss: 0.3565 - classification_loss: 0.0498
 3720/10000 [==========>...................] - ETA: 1:33:09 - loss: 0.4062 - regression_loss: 0.3565 - classification_loss: 0.0498
 3721/10000 [==========>...................] - ETA: 1:33:08 - loss: 0.4061 - regression_loss: 0.3564 - classification_loss: 0.0498
 3722/10000 [==========>...................] - ETA: 1:33:07 - loss: 0.4061 - regression_loss: 0.3564 - classification_loss: 0.0497
 3723/10000 [==========>...................] - ETA: 1:33:06 - loss: 0.4061 - regression_loss: 0.3563 - classification_loss: 0.0497
 3724/10000 [==========>...................] - ETA: 1:33:05 - loss: 0.4060 - regression_loss: 0.3563 - classification_loss: 0.0497
 3725/10000 [==========>...................] - ETA: 1:33:04 - loss: 0.4060 - regression_loss: 0.3563 - classification_loss: 0.0497
 3726/10000 [==========>...................] - ETA: 1:33:03 - loss: 0.4061 - regression_loss: 0.3563 - classification_loss: 0.0497
 3727/10000 [==========>...................] - ETA: 1:33:03 - loss: 0.4060 - regression_loss: 0.3563 - classification_loss: 0.0497
 3728/10000 [==========>...................] - ETA: 1:33:02 - loss: 0.4060 - regression_loss: 0.3563 - classification_loss: 0.0497
 3729/10000 [==========>...................] - ETA: 1:33:01 - loss: 0.4059 - regression_loss: 0.3562 - classification_loss: 0.0497
 3730/10000 [==========>...................] - ETA: 1:33:00 - loss: 0.4061 - regression_loss: 0.3564 - classification_loss: 0.0497
 3731/10000 [==========>...................] - ETA: 1:32:59 - loss: 0.4062 - regression_loss: 0.3565 - classification_loss: 0.0497
 3732/10000 [==========>...................] - ETA: 1:32:58 - loss: 0.4063 - regression_loss: 0.3566 - classification_loss: 0.0497
 3733/10000 [==========>...................] - ETA: 1:32:57 - loss: 0.4062 - regression_loss: 0.3565 - classification_loss: 0.0497
 3734/10000 [==========>...................] - ETA: 1:32:56 - loss: 0.4061 - regression_loss: 0.3564 - classification_loss: 0.0497
 3735/10000 [==========>...................] - ETA: 1:32:55 - loss: 0.4061 - regression_loss: 0.3564 - classification_loss: 0.0497
 3736/10000 [==========>...................] - ETA: 1:32:55 - loss: 0.4061 - regression_loss: 0.3564 - classification_loss: 0.0497
 3737/10000 [==========>...................] - ETA: 1:32:54 - loss: 0.4060 - regression_loss: 0.3563 - classification_loss: 0.0497
 3738/10000 [==========>...................] - ETA: 1:32:53 - loss: 0.4059 - regression_loss: 0.3562 - classification_loss: 0.0497
 3739/10000 [==========>...................] - ETA: 1:32:52 - loss: 0.4059 - regression_loss: 0.3562 - classification_loss: 0.0497
 3740/10000 [==========>...................] - ETA: 1:32:51 - loss: 0.4059 - regression_loss: 0.3563 - classification_loss: 0.0497
 3741/10000 [==========>...................] - ETA: 1:32:50 - loss: 0.4060 - regression_loss: 0.3564 - classification_loss: 0.0497
 3742/10000 [==========>...................] - ETA: 1:32:49 - loss: 0.4059 - regression_loss: 0.3563 - classification_loss: 0.0496
 3743/10000 [==========>...................] - ETA: 1:32:48 - loss: 0.4058 - regression_loss: 0.3562 - classification_loss: 0.0496
 3744/10000 [==========>...................] - ETA: 1:32:47 - loss: 0.4058 - regression_loss: 0.3561 - classification_loss: 0.0496
 3745/10000 [==========>...................] - ETA: 1:32:47 - loss: 0.4057 - regression_loss: 0.3561 - classification_loss: 0.0496
 3746/10000 [==========>...................] - ETA: 1:32:46 - loss: 0.4058 - regression_loss: 0.3562 - classification_loss: 0.0496
 3747/10000 [==========>...................] - ETA: 1:32:45 - loss: 0.4057 - regression_loss: 0.3561 - classification_loss: 0.0496
 3748/10000 [==========>...................] - ETA: 1:32:44 - loss: 0.4057 - regression_loss: 0.3561 - classification_loss: 0.0496
 3749/10000 [==========>...................] - ETA: 1:32:43 - loss: 0.4056 - regression_loss: 0.3560 - classification_loss: 0.0496
 3750/10000 [==========>...................] - ETA: 1:32:42 - loss: 0.4056 - regression_loss: 0.3560 - classification_loss: 0.0496
 3751/10000 [==========>...................] - ETA: 1:32:41 - loss: 0.4056 - regression_loss: 0.3560 - classification_loss: 0.0496
 3752/10000 [==========>...................] - ETA: 1:32:40 - loss: 0.4055 - regression_loss: 0.3560 - classification_loss: 0.0496
 3753/10000 [==========>...................] - ETA: 1:32:39 - loss: 0.4056 - regression_loss: 0.3560 - classification_loss: 0.0496
 3754/10000 [==========>...................] - ETA: 1:32:39 - loss: 0.4057 - regression_loss: 0.3561 - classification_loss: 0.0496
 3755/10000 [==========>...................] - ETA: 1:32:38 - loss: 0.4057 - regression_loss: 0.3561 - classification_loss: 0.0496
 3756/10000 [==========>...................] - ETA: 1:32:37 - loss: 0.4056 - regression_loss: 0.3560 - classification_loss: 0.0496
 3757/10000 [==========>...................] - ETA: 1:32:36 - loss: 0.4055 - regression_loss: 0.3559 - classification_loss: 0.0495
 3758/10000 [==========>...................] - ETA: 1:32:35 - loss: 0.4056 - regression_loss: 0.3560 - classification_loss: 0.0496
 3759/10000 [==========>...................] - ETA: 1:32:34 - loss: 0.4056 - regression_loss: 0.3560 - classification_loss: 0.0496
 3760/10000 [==========>...................] - ETA: 1:32:33 - loss: 0.4056 - regression_loss: 0.3560 - classification_loss: 0.0495
 3761/10000 [==========>...................] - ETA: 1:32:32 - loss: 0.4056 - regression_loss: 0.3561 - classification_loss: 0.0496
 3762/10000 [==========>...................] - ETA: 1:32:31 - loss: 0.4056 - regression_loss: 0.3560 - classification_loss: 0.0495
 3763/10000 [==========>...................] - ETA: 1:32:30 - loss: 0.4056 - regression_loss: 0.3560 - classification_loss: 0.0495
 3764/10000 [==========>...................] - ETA: 1:32:30 - loss: 0.4055 - regression_loss: 0.3559 - classification_loss: 0.0495
 3765/10000 [==========>...................] - ETA: 1:32:29 - loss: 0.4055 - regression_loss: 0.3559 - classification_loss: 0.0495
 3766/10000 [==========>...................] - ETA: 1:32:28 - loss: 0.4055 - regression_loss: 0.3560 - classification_loss: 0.0495
 3767/10000 [==========>...................] - ETA: 1:32:27 - loss: 0.4055 - regression_loss: 0.3560 - classification_loss: 0.0495
 3768/10000 [==========>...................] - ETA: 1:32:26 - loss: 0.4054 - regression_loss: 0.3559 - classification_loss: 0.0495
 3769/10000 [==========>...................] - ETA: 1:32:25 - loss: 0.4054 - regression_loss: 0.3559 - classification_loss: 0.0495
 3770/10000 [==========>...................] - ETA: 1:32:24 - loss: 0.4055 - regression_loss: 0.3559 - classification_loss: 0.0495
 3771/10000 [==========>...................] - ETA: 1:32:23 - loss: 0.4055 - regression_loss: 0.3560 - classification_loss: 0.0495
 3772/10000 [==========>...................] - ETA: 1:32:22 - loss: 0.4055 - regression_loss: 0.3560 - classification_loss: 0.0495
 3773/10000 [==========>...................] - ETA: 1:32:22 - loss: 0.4055 - regression_loss: 0.3560 - classification_loss: 0.0495
 3774/10000 [==========>...................] - ETA: 1:32:21 - loss: 0.4055 - regression_loss: 0.3560 - classification_loss: 0.0495
 3775/10000 [==========>...................] - ETA: 1:32:20 - loss: 0.4056 - regression_loss: 0.3561 - classification_loss: 0.0495
 3776/10000 [==========>...................] - ETA: 1:32:19 - loss: 0.4056 - regression_loss: 0.3560 - classification_loss: 0.0495
 3777/10000 [==========>...................] - ETA: 1:32:18 - loss: 0.4057 - regression_loss: 0.3561 - classification_loss: 0.0495
 3778/10000 [==========>...................] - ETA: 1:32:17 - loss: 0.4057 - regression_loss: 0.3562 - classification_loss: 0.0495
 3779/10000 [==========>...................] - ETA: 1:32:16 - loss: 0.4057 - regression_loss: 0.3562 - classification_loss: 0.0495
 3780/10000 [==========>...................] - ETA: 1:32:15 - loss: 0.4056 - regression_loss: 0.3561 - classification_loss: 0.0495
 3781/10000 [==========>...................] - ETA: 1:32:14 - loss: 0.4055 - regression_loss: 0.3561 - classification_loss: 0.0495
 3782/10000 [==========>...................] - ETA: 1:32:14 - loss: 0.4054 - regression_loss: 0.3560 - classification_loss: 0.0495
 3783/10000 [==========>...................] - ETA: 1:32:13 - loss: 0.4053 - regression_loss: 0.3559 - classification_loss: 0.0495
 3784/10000 [==========>...................] - ETA: 1:32:12 - loss: 0.4053 - regression_loss: 0.3558 - classification_loss: 0.0495
 3785/10000 [==========>...................] - ETA: 1:32:11 - loss: 0.4052 - regression_loss: 0.3558 - classification_loss: 0.0494
 3786/10000 [==========>...................] - ETA: 1:32:10 - loss: 0.4052 - regression_loss: 0.3557 - classification_loss: 0.0494
 3787/10000 [==========>...................] - ETA: 1:32:09 - loss: 0.4051 - regression_loss: 0.3557 - classification_loss: 0.0494
 3788/10000 [==========>...................] - ETA: 1:32:08 - loss: 0.4051 - regression_loss: 0.3557 - classification_loss: 0.0494
 3789/10000 [==========>...................] - ETA: 1:32:07 - loss: 0.4050 - regression_loss: 0.3556 - classification_loss: 0.0494
 3790/10000 [==========>...................] - ETA: 1:32:06 - loss: 0.4052 - regression_loss: 0.3558 - classification_loss: 0.0494
 3791/10000 [==========>...................] - ETA: 1:32:06 - loss: 0.4053 - regression_loss: 0.3559 - classification_loss: 0.0494
 3792/10000 [==========>...................] - ETA: 1:32:05 - loss: 0.4053 - regression_loss: 0.3559 - classification_loss: 0.0494
 3793/10000 [==========>...................] - ETA: 1:32:04 - loss: 0.4053 - regression_loss: 0.3559 - classification_loss: 0.0494
 3794/10000 [==========>...................] - ETA: 1:32:03 - loss: 0.4052 - regression_loss: 0.3558 - classification_loss: 0.0494
 3795/10000 [==========>...................] - ETA: 1:32:02 - loss: 0.4052 - regression_loss: 0.3558 - classification_loss: 0.0494
 3796/10000 [==========>...................] - ETA: 1:32:01 - loss: 0.4052 - regression_loss: 0.3558 - classification_loss: 0.0494
 3797/10000 [==========>...................] - ETA: 1:32:00 - loss: 0.4051 - regression_loss: 0.3558 - classification_loss: 0.0494
 3798/10000 [==========>...................] - ETA: 1:31:59 - loss: 0.4051 - regression_loss: 0.3558 - classification_loss: 0.0494
 3799/10000 [==========>...................] - ETA: 1:31:58 - loss: 0.4051 - regression_loss: 0.3557 - classification_loss: 0.0493
 3800/10000 [==========>...................] - ETA: 1:31:58 - loss: 0.4051 - regression_loss: 0.3557 - classification_loss: 0.0494
 3801/10000 [==========>...................] - ETA: 1:31:57 - loss: 0.4051 - regression_loss: 0.3558 - classification_loss: 0.0494
 3802/10000 [==========>...................] - ETA: 1:31:56 - loss: 0.4050 - regression_loss: 0.3557 - classification_loss: 0.0493
 3803/10000 [==========>...................] - ETA: 1:31:55 - loss: 0.4050 - regression_loss: 0.3557 - classification_loss: 0.0493
 3804/10000 [==========>...................] - ETA: 1:31:54 - loss: 0.4050 - regression_loss: 0.3556 - classification_loss: 0.0493
 3805/10000 [==========>...................] - ETA: 1:31:53 - loss: 0.4049 - regression_loss: 0.3556 - classification_loss: 0.0493
 3806/10000 [==========>...................] - ETA: 1:31:52 - loss: 0.4050 - regression_loss: 0.3556 - classification_loss: 0.0493
 3807/10000 [==========>...................] - ETA: 1:31:51 - loss: 0.4050 - regression_loss: 0.3557 - classification_loss: 0.0493
 3808/10000 [==========>...................] - ETA: 1:31:50 - loss: 0.4050 - regression_loss: 0.3557 - classification_loss: 0.0493
 3809/10000 [==========>...................] - ETA: 1:31:49 - loss: 0.4051 - regression_loss: 0.3558 - classification_loss: 0.0493
 3810/10000 [==========>...................] - ETA: 1:31:49 - loss: 0.4050 - regression_loss: 0.3558 - classification_loss: 0.0493
 3811/10000 [==========>...................] - ETA: 1:31:48 - loss: 0.4050 - regression_loss: 0.3558 - classification_loss: 0.0493
 3812/10000 [==========>...................] - ETA: 1:31:47 - loss: 0.4058 - regression_loss: 0.3557 - classification_loss: 0.0502
 3813/10000 [==========>...................] - ETA: 1:31:46 - loss: 0.4058 - regression_loss: 0.3556 - classification_loss: 0.0502
 3814/10000 [==========>...................] - ETA: 1:31:45 - loss: 0.4058 - regression_loss: 0.3556 - classification_loss: 0.0502
 3815/10000 [==========>...................] - ETA: 1:31:44 - loss: 0.4058 - regression_loss: 0.3557 - classification_loss: 0.0501
 3816/10000 [==========>...................] - ETA: 1:31:43 - loss: 0.4058 - regression_loss: 0.3556 - classification_loss: 0.0501
 3817/10000 [==========>...................] - ETA: 1:31:42 - loss: 0.4057 - regression_loss: 0.3556 - classification_loss: 0.0501
 3818/10000 [==========>...................] - ETA: 1:31:41 - loss: 0.4059 - regression_loss: 0.3557 - classification_loss: 0.0501
 3819/10000 [==========>...................] - ETA: 1:31:41 - loss: 0.4057 - regression_loss: 0.3556 - classification_loss: 0.0501
 3820/10000 [==========>...................] - ETA: 1:31:40 - loss: 0.4062 - regression_loss: 0.3561 - classification_loss: 0.0501
 3821/10000 [==========>...................] - ETA: 1:31:39 - loss: 0.4063 - regression_loss: 0.3561 - classification_loss: 0.0501
 3822/10000 [==========>...................] - ETA: 1:31:38 - loss: 0.4063 - regression_loss: 0.3561 - classification_loss: 0.0501
 3823/10000 [==========>...................] - ETA: 1:31:37 - loss: 0.4063 - regression_loss: 0.3561 - classification_loss: 0.0501
 3824/10000 [==========>...................] - ETA: 1:31:36 - loss: 0.4062 - regression_loss: 0.3561 - classification_loss: 0.0501
 3825/10000 [==========>...................] - ETA: 1:31:35 - loss: 0.4063 - regression_loss: 0.3561 - classification_loss: 0.0501
 3826/10000 [==========>...................] - ETA: 1:31:34 - loss: 0.4063 - regression_loss: 0.3562 - classification_loss: 0.0501
 3827/10000 [==========>...................] - ETA: 1:31:33 - loss: 0.4063 - regression_loss: 0.3562 - classification_loss: 0.0501
 3828/10000 [==========>...................] - ETA: 1:31:33 - loss: 0.4063 - regression_loss: 0.3562 - classification_loss: 0.0501
 3829/10000 [==========>...................] - ETA: 1:31:32 - loss: 0.4062 - regression_loss: 0.3561 - classification_loss: 0.0501
 3830/10000 [==========>...................] - ETA: 1:31:31 - loss: 0.4062 - regression_loss: 0.3561 - classification_loss: 0.0501
 3831/10000 [==========>...................] - ETA: 1:31:30 - loss: 0.4062 - regression_loss: 0.3561 - classification_loss: 0.0501
 3832/10000 [==========>...................] - ETA: 1:31:29 - loss: 0.4062 - regression_loss: 0.3561 - classification_loss: 0.0501
 3833/10000 [==========>...................] - ETA: 1:31:28 - loss: 0.4062 - regression_loss: 0.3561 - classification_loss: 0.0501
 3834/10000 [==========>...................] - ETA: 1:31:27 - loss: 0.4061 - regression_loss: 0.3560 - classification_loss: 0.0501
 3835/10000 [==========>...................] - ETA: 1:31:26 - loss: 0.4061 - regression_loss: 0.3560 - classification_loss: 0.0501
 3836/10000 [==========>...................] - ETA: 1:31:25 - loss: 0.4062 - regression_loss: 0.3560 - classification_loss: 0.0501
 3837/10000 [==========>...................] - ETA: 1:31:25 - loss: 0.4061 - regression_loss: 0.3560 - classification_loss: 0.0501
 3838/10000 [==========>...................] - ETA: 1:31:24 - loss: 0.4061 - regression_loss: 0.3560 - classification_loss: 0.0501
 3839/10000 [==========>...................] - ETA: 1:31:23 - loss: 0.4061 - regression_loss: 0.3560 - classification_loss: 0.0501
 3840/10000 [==========>...................] - ETA: 1:31:22 - loss: 0.4060 - regression_loss: 0.3560 - classification_loss: 0.0501
 3841/10000 [==========>...................] - ETA: 1:31:21 - loss: 0.4059 - regression_loss: 0.3559 - classification_loss: 0.0501
 3842/10000 [==========>...................] - ETA: 1:31:20 - loss: 0.4059 - regression_loss: 0.3558 - classification_loss: 0.0501
 3843/10000 [==========>...................] - ETA: 1:31:19 - loss: 0.4060 - regression_loss: 0.3559 - classification_loss: 0.0501
 3844/10000 [==========>...................] - ETA: 1:31:18 - loss: 0.4060 - regression_loss: 0.3559 - classification_loss: 0.0501
 3845/10000 [==========>...................] - ETA: 1:31:17 - loss: 0.4060 - regression_loss: 0.3560 - classification_loss: 0.0501
 3846/10000 [==========>...................] - ETA: 1:31:16 - loss: 0.4061 - regression_loss: 0.3560 - classification_loss: 0.0501
 3847/10000 [==========>...................] - ETA: 1:31:16 - loss: 0.4061 - regression_loss: 0.3560 - classification_loss: 0.0501
 3848/10000 [==========>...................] - ETA: 1:31:15 - loss: 0.4061 - regression_loss: 0.3560 - classification_loss: 0.0501
 3849/10000 [==========>...................] - ETA: 1:31:14 - loss: 0.4062 - regression_loss: 0.3561 - classification_loss: 0.0501
 3850/10000 [==========>...................] - ETA: 1:31:13 - loss: 0.4062 - regression_loss: 0.3561 - classification_loss: 0.0501
 3851/10000 [==========>...................] - ETA: 1:31:12 - loss: 0.4061 - regression_loss: 0.3560 - classification_loss: 0.0501
 3852/10000 [==========>...................] - ETA: 1:31:11 - loss: 0.4061 - regression_loss: 0.3560 - classification_loss: 0.0501
 3853/10000 [==========>...................] - ETA: 1:31:10 - loss: 0.4061 - regression_loss: 0.3560 - classification_loss: 0.0501
 3854/10000 [==========>...................] - ETA: 1:31:09 - loss: 0.4060 - regression_loss: 0.3559 - classification_loss: 0.0501
 3855/10000 [==========>...................] - ETA: 1:31:08 - loss: 0.4059 - regression_loss: 0.3559 - classification_loss: 0.0501
 3856/10000 [==========>...................] - ETA: 1:31:08 - loss: 0.4059 - regression_loss: 0.3558 - classification_loss: 0.0501
 3857/10000 [==========>...................] - ETA: 1:31:07 - loss: 0.4058 - regression_loss: 0.3558 - classification_loss: 0.0501
 3858/10000 [==========>...................] - ETA: 1:31:06 - loss: 0.4057 - regression_loss: 0.3557 - classification_loss: 0.0500
 3859/10000 [==========>...................] - ETA: 1:31:05 - loss: 0.4057 - regression_loss: 0.3557 - classification_loss: 0.0500
 3860/10000 [==========>...................] - ETA: 1:31:04 - loss: 0.4060 - regression_loss: 0.3559 - classification_loss: 0.0501
 3861/10000 [==========>...................] - ETA: 1:31:03 - loss: 0.4061 - regression_loss: 0.3560 - classification_loss: 0.0501
 3862/10000 [==========>...................] - ETA: 1:31:02 - loss: 0.4060 - regression_loss: 0.3559 - classification_loss: 0.0501
 3863/10000 [==========>...................] - ETA: 1:31:01 - loss: 0.4060 - regression_loss: 0.3559 - classification_loss: 0.0501
 3864/10000 [==========>...................] - ETA: 1:31:00 - loss: 0.4060 - regression_loss: 0.3559 - classification_loss: 0.0501
 3865/10000 [==========>...................] - ETA: 1:31:00 - loss: 0.4060 - regression_loss: 0.3559 - classification_loss: 0.0500
 3866/10000 [==========>...................] - ETA: 1:30:59 - loss: 0.4060 - regression_loss: 0.3559 - classification_loss: 0.0500
 3867/10000 [==========>...................] - ETA: 1:30:58 - loss: 0.4060 - regression_loss: 0.3559 - classification_loss: 0.0500
 3868/10000 [==========>...................] - ETA: 1:30:57 - loss: 0.4059 - regression_loss: 0.3559 - classification_loss: 0.0500
 3869/10000 [==========>...................] - ETA: 1:30:56 - loss: 0.4060 - regression_loss: 0.3559 - classification_loss: 0.0500
 3870/10000 [==========>...................] - ETA: 1:30:55 - loss: 0.4059 - regression_loss: 0.3559 - classification_loss: 0.0500
 3871/10000 [==========>...................] - ETA: 1:30:54 - loss: 0.4059 - regression_loss: 0.3559 - classification_loss: 0.0500
 3872/10000 [==========>...................] - ETA: 1:30:53 - loss: 0.4060 - regression_loss: 0.3560 - classification_loss: 0.0500
 3873/10000 [==========>...................] - ETA: 1:30:52 - loss: 0.4060 - regression_loss: 0.3560 - classification_loss: 0.0500
 3874/10000 [==========>...................] - ETA: 1:30:52 - loss: 0.4060 - regression_loss: 0.3560 - classification_loss: 0.0500
 3875/10000 [==========>...................] - ETA: 1:30:51 - loss: 0.4061 - regression_loss: 0.3561 - classification_loss: 0.0500
 3876/10000 [==========>...................] - ETA: 1:30:50 - loss: 0.4063 - regression_loss: 0.3563 - classification_loss: 0.0500
 3877/10000 [==========>...................] - ETA: 1:30:49 - loss: 0.4064 - regression_loss: 0.3563 - classification_loss: 0.0500
 3878/10000 [==========>...................] - ETA: 1:30:48 - loss: 0.4064 - regression_loss: 0.3563 - classification_loss: 0.0500
 3879/10000 [==========>...................] - ETA: 1:30:47 - loss: 0.4064 - regression_loss: 0.3563 - classification_loss: 0.0501
 3880/10000 [==========>...................] - ETA: 1:30:46 - loss: 0.4064 - regression_loss: 0.3563 - classification_loss: 0.0501
 3881/10000 [==========>...................] - ETA: 1:30:45 - loss: 0.4064 - regression_loss: 0.3564 - classification_loss: 0.0501
 3882/10000 [==========>...................] - ETA: 1:30:44 - loss: 0.4064 - regression_loss: 0.3564 - classification_loss: 0.0501
 3883/10000 [==========>...................] - ETA: 1:30:44 - loss: 0.4064 - regression_loss: 0.3564 - classification_loss: 0.0500
 3884/10000 [==========>...................] - ETA: 1:30:43 - loss: 0.4064 - regression_loss: 0.3564 - classification_loss: 0.0501
 3885/10000 [==========>...................] - ETA: 1:30:42 - loss: 0.4064 - regression_loss: 0.3564 - classification_loss: 0.0500
 3886/10000 [==========>...................] - ETA: 1:30:41 - loss: 0.4063 - regression_loss: 0.3563 - classification_loss: 0.0500
 3887/10000 [==========>...................] - ETA: 1:30:40 - loss: 0.4064 - regression_loss: 0.3563 - classification_loss: 0.0500
 3888/10000 [==========>...................] - ETA: 1:30:39 - loss: 0.4064 - regression_loss: 0.3563 - classification_loss: 0.0500
 3889/10000 [==========>...................] - ETA: 1:30:38 - loss: 0.4064 - regression_loss: 0.3563 - classification_loss: 0.0500
 3890/10000 [==========>...................] - ETA: 1:30:37 - loss: 0.4063 - regression_loss: 0.3563 - classification_loss: 0.0500
 3891/10000 [==========>...................] - ETA: 1:30:36 - loss: 0.4065 - regression_loss: 0.3565 - classification_loss: 0.0500
 3892/10000 [==========>...................] - ETA: 1:30:36 - loss: 0.4065 - regression_loss: 0.3565 - classification_loss: 0.0500
 3893/10000 [==========>...................] - ETA: 1:30:35 - loss: 0.4065 - regression_loss: 0.3565 - classification_loss: 0.0500
 3894/10000 [==========>...................] - ETA: 1:30:34 - loss: 0.4066 - regression_loss: 0.3566 - classification_loss: 0.0500
 3895/10000 [==========>...................] - ETA: 1:30:33 - loss: 0.4067 - regression_loss: 0.3567 - classification_loss: 0.0500
 3896/10000 [==========>...................] - ETA: 1:30:32 - loss: 0.4067 - regression_loss: 0.3566 - classification_loss: 0.0500
 3897/10000 [==========>...................] - ETA: 1:30:31 - loss: 0.4066 - regression_loss: 0.3566 - classification_loss: 0.0500
 3898/10000 [==========>...................] - ETA: 1:30:30 - loss: 0.4066 - regression_loss: 0.3565 - classification_loss: 0.0500
 3899/10000 [==========>...................] - ETA: 1:30:29 - loss: 0.4066 - regression_loss: 0.3565 - classification_loss: 0.0500
 3900/10000 [==========>...................] - ETA: 1:30:28 - loss: 0.4066 - regression_loss: 0.3565 - classification_loss: 0.0500
 3901/10000 [==========>...................] - ETA: 1:30:27 - loss: 0.4065 - regression_loss: 0.3565 - classification_loss: 0.0500
 3902/10000 [==========>...................] - ETA: 1:30:27 - loss: 0.4065 - regression_loss: 0.3564 - classification_loss: 0.0500
 3903/10000 [==========>...................] - ETA: 1:30:26 - loss: 0.4066 - regression_loss: 0.3566 - classification_loss: 0.0500
 3904/10000 [==========>...................] - ETA: 1:30:25 - loss: 0.4065 - regression_loss: 0.3565 - classification_loss: 0.0500
 3905/10000 [==========>...................] - ETA: 1:30:24 - loss: 0.4065 - regression_loss: 0.3565 - classification_loss: 0.0500
 3906/10000 [==========>...................] - ETA: 1:30:23 - loss: 0.4065 - regression_loss: 0.3565 - classification_loss: 0.0500
 3907/10000 [==========>...................] - ETA: 1:30:22 - loss: 0.4066 - regression_loss: 0.3566 - classification_loss: 0.0500
 3908/10000 [==========>...................] - ETA: 1:30:21 - loss: 0.4066 - regression_loss: 0.3566 - classification_loss: 0.0500
 3909/10000 [==========>...................] - ETA: 1:30:20 - loss: 0.4065 - regression_loss: 0.3565 - classification_loss: 0.0500
 3910/10000 [==========>...................] - ETA: 1:30:19 - loss: 0.4065 - regression_loss: 0.3565 - classification_loss: 0.0500
 3911/10000 [==========>...................] - ETA: 1:30:19 - loss: 0.4065 - regression_loss: 0.3565 - classification_loss: 0.0500
 3912/10000 [==========>...................] - ETA: 1:30:18 - loss: 0.4067 - regression_loss: 0.3567 - classification_loss: 0.0500
 3913/10000 [==========>...................] - ETA: 1:30:17 - loss: 0.4068 - regression_loss: 0.3567 - classification_loss: 0.0500
 3914/10000 [==========>...................] - ETA: 1:30:16 - loss: 0.4068 - regression_loss: 0.3567 - classification_loss: 0.0500
 3915/10000 [==========>...................] - ETA: 1:30:15 - loss: 0.4068 - regression_loss: 0.3568 - classification_loss: 0.0500
 3916/10000 [==========>...................] - ETA: 1:30:14 - loss: 0.4067 - regression_loss: 0.3567 - classification_loss: 0.0500
 3917/10000 [==========>...................] - ETA: 1:30:13 - loss: 0.4068 - regression_loss: 0.3568 - classification_loss: 0.0500
 3918/10000 [==========>...................] - ETA: 1:30:12 - loss: 0.4068 - regression_loss: 0.3568 - classification_loss: 0.0500
 3919/10000 [==========>...................] - ETA: 1:30:11 - loss: 0.4068 - regression_loss: 0.3568 - classification_loss: 0.0500
 3920/10000 [==========>...................] - ETA: 1:30:11 - loss: 0.4068 - regression_loss: 0.3568 - classification_loss: 0.0500
 3921/10000 [==========>...................] - ETA: 1:30:10 - loss: 0.4067 - regression_loss: 0.3567 - classification_loss: 0.0500
 3922/10000 [==========>...................] - ETA: 1:30:09 - loss: 0.4068 - regression_loss: 0.3568 - classification_loss: 0.0500
 3923/10000 [==========>...................] - ETA: 1:30:08 - loss: 0.4068 - regression_loss: 0.3568 - classification_loss: 0.0500
 3924/10000 [==========>...................] - ETA: 1:30:07 - loss: 0.4067 - regression_loss: 0.3567 - classification_loss: 0.0500
 3925/10000 [==========>...................] - ETA: 1:30:06 - loss: 0.4067 - regression_loss: 0.3567 - classification_loss: 0.0500
 3926/10000 [==========>...................] - ETA: 1:30:05 - loss: 0.4067 - regression_loss: 0.3567 - classification_loss: 0.0500
 3927/10000 [==========>...................] - ETA: 1:30:04 - loss: 0.4066 - regression_loss: 0.3566 - classification_loss: 0.0500
 3928/10000 [==========>...................] - ETA: 1:30:03 - loss: 0.4067 - regression_loss: 0.3567 - classification_loss: 0.0500
 3929/10000 [==========>...................] - ETA: 1:30:03 - loss: 0.4067 - regression_loss: 0.3567 - classification_loss: 0.0500
 3930/10000 [==========>...................] - ETA: 1:30:02 - loss: 0.4068 - regression_loss: 0.3568 - classification_loss: 0.0500
 3931/10000 [==========>...................] - ETA: 1:30:01 - loss: 0.4068 - regression_loss: 0.3568 - classification_loss: 0.0500
 3932/10000 [==========>...................] - ETA: 1:30:00 - loss: 0.4067 - regression_loss: 0.3567 - classification_loss: 0.0500
 3933/10000 [==========>...................] - ETA: 1:29:59 - loss: 0.4067 - regression_loss: 0.3567 - classification_loss: 0.0500
 3934/10000 [==========>...................] - ETA: 1:29:58 - loss: 0.4066 - regression_loss: 0.3566 - classification_loss: 0.0500
 3935/10000 [==========>...................] - ETA: 1:29:57 - loss: 0.4065 - regression_loss: 0.3565 - classification_loss: 0.0500
 3936/10000 [==========>...................] - ETA: 1:29:56 - loss: 0.4065 - regression_loss: 0.3565 - classification_loss: 0.0500
 3937/10000 [==========>...................] - ETA: 1:29:55 - loss: 0.4064 - regression_loss: 0.3564 - classification_loss: 0.0500
 3938/10000 [==========>...................] - ETA: 1:29:55 - loss: 0.4063 - regression_loss: 0.3564 - classification_loss: 0.0500
 3939/10000 [==========>...................] - ETA: 1:29:54 - loss: 0.4063 - regression_loss: 0.3563 - classification_loss: 0.0500
 3940/10000 [==========>...................] - ETA: 1:29:53 - loss: 0.4064 - regression_loss: 0.3564 - classification_loss: 0.0500
 3941/10000 [==========>...................] - ETA: 1:29:52 - loss: 0.4063 - regression_loss: 0.3563 - classification_loss: 0.0500
 3942/10000 [==========>...................] - ETA: 1:29:51 - loss: 0.4064 - regression_loss: 0.3564 - classification_loss: 0.0500
 3943/10000 [==========>...................] - ETA: 1:29:50 - loss: 0.4063 - regression_loss: 0.3563 - classification_loss: 0.0500
 3944/10000 [==========>...................] - ETA: 1:29:49 - loss: 0.4063 - regression_loss: 0.3563 - classification_loss: 0.0500
 3945/10000 [==========>...................] - ETA: 1:29:48 - loss: 0.4062 - regression_loss: 0.3562 - classification_loss: 0.0500
 3946/10000 [==========>...................] - ETA: 1:29:47 - loss: 0.4063 - regression_loss: 0.3563 - classification_loss: 0.0500
 3947/10000 [==========>...................] - ETA: 1:29:47 - loss: 0.4062 - regression_loss: 0.3562 - classification_loss: 0.0500
 3948/10000 [==========>...................] - ETA: 1:29:46 - loss: 0.4061 - regression_loss: 0.3561 - classification_loss: 0.0500
 3949/10000 [==========>...................] - ETA: 1:29:45 - loss: 0.4061 - regression_loss: 0.3561 - classification_loss: 0.0500
 3950/10000 [==========>...................] - ETA: 1:29:44 - loss: 0.4061 - regression_loss: 0.3561 - classification_loss: 0.0500
 3951/10000 [==========>...................] - ETA: 1:29:43 - loss: 0.4060 - regression_loss: 0.3561 - classification_loss: 0.0500
 3952/10000 [==========>...................] - ETA: 1:29:42 - loss: 0.4060 - regression_loss: 0.3560 - classification_loss: 0.0500
 3953/10000 [==========>...................] - ETA: 1:29:41 - loss: 0.4059 - regression_loss: 0.3559 - classification_loss: 0.0500
 3954/10000 [==========>...................] - ETA: 1:29:40 - loss: 0.4060 - regression_loss: 0.3560 - classification_loss: 0.0499
 3955/10000 [==========>...................] - ETA: 1:29:39 - loss: 0.4060 - regression_loss: 0.3561 - classification_loss: 0.0500
 3956/10000 [==========>...................] - ETA: 1:29:38 - loss: 0.4059 - regression_loss: 0.3560 - classification_loss: 0.0499
 3957/10000 [==========>...................] - ETA: 1:29:38 - loss: 0.4060 - regression_loss: 0.3561 - classification_loss: 0.0499
 3958/10000 [==========>...................] - ETA: 1:29:37 - loss: 0.4060 - regression_loss: 0.3561 - classification_loss: 0.0499
 3959/10000 [==========>...................] - ETA: 1:29:36 - loss: 0.4060 - regression_loss: 0.3561 - classification_loss: 0.0499
 3960/10000 [==========>...................] - ETA: 1:29:35 - loss: 0.4060 - regression_loss: 0.3561 - classification_loss: 0.0499
 3961/10000 [==========>...................] - ETA: 1:29:34 - loss: 0.4061 - regression_loss: 0.3561 - classification_loss: 0.0499
 3962/10000 [==========>...................] - ETA: 1:29:33 - loss: 0.4060 - regression_loss: 0.3561 - classification_loss: 0.0499
 3963/10000 [==========>...................] - ETA: 1:29:32 - loss: 0.4060 - regression_loss: 0.3561 - classification_loss: 0.0499
 3964/10000 [==========>...................] - ETA: 1:29:31 - loss: 0.4060 - regression_loss: 0.3561 - classification_loss: 0.0499
 3965/10000 [==========>...................] - ETA: 1:29:30 - loss: 0.4060 - regression_loss: 0.3560 - classification_loss: 0.0499
 3966/10000 [==========>...................] - ETA: 1:29:30 - loss: 0.4060 - regression_loss: 0.3561 - classification_loss: 0.0499
 3967/10000 [==========>...................] - ETA: 1:29:29 - loss: 0.4059 - regression_loss: 0.3560 - classification_loss: 0.0499
 3968/10000 [==========>...................] - ETA: 1:29:28 - loss: 0.4059 - regression_loss: 0.3560 - classification_loss: 0.0499
 3969/10000 [==========>...................] - ETA: 1:29:27 - loss: 0.4058 - regression_loss: 0.3559 - classification_loss: 0.0499
 3970/10000 [==========>...................] - ETA: 1:29:26 - loss: 0.4059 - regression_loss: 0.3560 - classification_loss: 0.0499
 3971/10000 [==========>...................] - ETA: 1:29:25 - loss: 0.4060 - regression_loss: 0.3560 - classification_loss: 0.0499
 3972/10000 [==========>...................] - ETA: 1:29:24 - loss: 0.4060 - regression_loss: 0.3561 - classification_loss: 0.0499
 3973/10000 [==========>...................] - ETA: 1:29:23 - loss: 0.4061 - regression_loss: 0.3561 - classification_loss: 0.0500
 3974/10000 [==========>...................] - ETA: 1:29:22 - loss: 0.4060 - regression_loss: 0.3561 - classification_loss: 0.0499
 3975/10000 [==========>...................] - ETA: 1:29:22 - loss: 0.4060 - regression_loss: 0.3560 - classification_loss: 0.0499
 3976/10000 [==========>...................] - ETA: 1:29:21 - loss: 0.4060 - regression_loss: 0.3561 - classification_loss: 0.0499
 3977/10000 [==========>...................] - ETA: 1:29:20 - loss: 0.4060 - regression_loss: 0.3561 - classification_loss: 0.0499
 3978/10000 [==========>...................] - ETA: 1:29:19 - loss: 0.4061 - regression_loss: 0.3562 - classification_loss: 0.0499
 3979/10000 [==========>...................] - ETA: 1:29:18 - loss: 0.4061 - regression_loss: 0.3561 - classification_loss: 0.0500
 3980/10000 [==========>...................] - ETA: 1:29:17 - loss: 0.4061 - regression_loss: 0.3562 - classification_loss: 0.0500
 3981/10000 [==========>...................] - ETA: 1:29:16 - loss: 0.4061 - regression_loss: 0.3562 - classification_loss: 0.0500
 3982/10000 [==========>...................] - ETA: 1:29:15 - loss: 0.4063 - regression_loss: 0.3563 - classification_loss: 0.0500
 3983/10000 [==========>...................] - ETA: 1:29:14 - loss: 0.4062 - regression_loss: 0.3562 - classification_loss: 0.0500
 3984/10000 [==========>...................] - ETA: 1:29:14 - loss: 0.4064 - regression_loss: 0.3564 - classification_loss: 0.0500
 3985/10000 [==========>...................] - ETA: 1:29:13 - loss: 0.4064 - regression_loss: 0.3564 - classification_loss: 0.0500
 3986/10000 [==========>...................] - ETA: 1:29:12 - loss: 0.4064 - regression_loss: 0.3564 - classification_loss: 0.0500
 3987/10000 [==========>...................] - ETA: 1:29:11 - loss: 0.4063 - regression_loss: 0.3563 - classification_loss: 0.0500
 3988/10000 [==========>...................] - ETA: 1:29:10 - loss: 0.4062 - regression_loss: 0.3563 - classification_loss: 0.0500
 3989/10000 [==========>...................] - ETA: 1:29:09 - loss: 0.4062 - regression_loss: 0.3563 - classification_loss: 0.0500
 3990/10000 [==========>...................] - ETA: 1:29:08 - loss: 0.4062 - regression_loss: 0.3562 - classification_loss: 0.0500
 3991/10000 [==========>...................] - ETA: 1:29:07 - loss: 0.4061 - regression_loss: 0.3562 - classification_loss: 0.0500
 3992/10000 [==========>...................] - ETA: 1:29:06 - loss: 0.4062 - regression_loss: 0.3563 - classification_loss: 0.0500
 3993/10000 [==========>...................] - ETA: 1:29:05 - loss: 0.4062 - regression_loss: 0.3563 - classification_loss: 0.0499
 3994/10000 [==========>...................] - ETA: 1:29:05 - loss: 0.4062 - regression_loss: 0.3563 - classification_loss: 0.0499
 3995/10000 [==========>...................] - ETA: 1:29:04 - loss: 0.4062 - regression_loss: 0.3563 - classification_loss: 0.0499
 3996/10000 [==========>...................] - ETA: 1:29:03 - loss: 0.4062 - regression_loss: 0.3563 - classification_loss: 0.0499
 3997/10000 [==========>...................] - ETA: 1:29:02 - loss: 0.4062 - regression_loss: 0.3562 - classification_loss: 0.0499
 3998/10000 [==========>...................] - ETA: 1:29:01 - loss: 0.4061 - regression_loss: 0.3562 - classification_loss: 0.0499
 3999/10000 [==========>...................] - ETA: 1:29:00 - loss: 0.4060 - regression_loss: 0.3561 - classification_loss: 0.0499
 4000/10000 [===========>..................] - ETA: 1:28:59 - loss: 0.4060 - regression_loss: 0.3561 - classification_loss: 0.0499
 4001/10000 [===========>..................] - ETA: 1:28:58 - loss: 0.4060 - regression_loss: 0.3561 - classification_loss: 0.0499
 4002/10000 [===========>..................] - ETA: 1:28:57 - loss: 0.4059 - regression_loss: 0.3561 - classification_loss: 0.0499
 4003/10000 [===========>..................] - ETA: 1:28:57 - loss: 0.4059 - regression_loss: 0.3560 - classification_loss: 0.0499
 4004/10000 [===========>..................] - ETA: 1:28:56 - loss: 0.4058 - regression_loss: 0.3560 - classification_loss: 0.0499
 4005/10000 [===========>..................] - ETA: 1:28:55 - loss: 0.4058 - regression_loss: 0.3559 - classification_loss: 0.0499
 4006/10000 [===========>..................] - ETA: 1:28:54 - loss: 0.4058 - regression_loss: 0.3560 - classification_loss: 0.0499
 4007/10000 [===========>..................] - ETA: 1:28:53 - loss: 0.4058 - regression_loss: 0.3559 - classification_loss: 0.0498
 4008/10000 [===========>..................] - ETA: 1:28:52 - loss: 0.4058 - regression_loss: 0.3559 - classification_loss: 0.0498
 4009/10000 [===========>..................] - ETA: 1:28:51 - loss: 0.4058 - regression_loss: 0.3559 - classification_loss: 0.0498
 4010/10000 [===========>..................] - ETA: 1:28:50 - loss: 0.4057 - regression_loss: 0.3559 - classification_loss: 0.0498
 4011/10000 [===========>..................] - ETA: 1:28:49 - loss: 0.4057 - regression_loss: 0.3559 - classification_loss: 0.0498
 4012/10000 [===========>..................] - ETA: 1:28:49 - loss: 0.4058 - regression_loss: 0.3560 - classification_loss: 0.0498
 4013/10000 [===========>..................] - ETA: 1:28:48 - loss: 0.4058 - regression_loss: 0.3560 - classification_loss: 0.0498
 4014/10000 [===========>..................] - ETA: 1:28:47 - loss: 0.4058 - regression_loss: 0.3560 - classification_loss: 0.0498
 4015/10000 [===========>..................] - ETA: 1:28:46 - loss: 0.4058 - regression_loss: 0.3560 - classification_loss: 0.0498
 4016/10000 [===========>..................] - ETA: 1:28:45 - loss: 0.4058 - regression_loss: 0.3560 - classification_loss: 0.0498
 4017/10000 [===========>..................] - ETA: 1:28:44 - loss: 0.4061 - regression_loss: 0.3562 - classification_loss: 0.0499
 4018/10000 [===========>..................] - ETA: 1:28:43 - loss: 0.4064 - regression_loss: 0.3564 - classification_loss: 0.0499
 4019/10000 [===========>..................] - ETA: 1:28:42 - loss: 0.4064 - regression_loss: 0.3565 - classification_loss: 0.0499
 4020/10000 [===========>..................] - ETA: 1:28:41 - loss: 0.4063 - regression_loss: 0.3564 - classification_loss: 0.0499
 4021/10000 [===========>..................] - ETA: 1:28:40 - loss: 0.4063 - regression_loss: 0.3564 - classification_loss: 0.0499
 4022/10000 [===========>..................] - ETA: 1:28:40 - loss: 0.4064 - regression_loss: 0.3564 - classification_loss: 0.0499
 4023/10000 [===========>..................] - ETA: 1:28:39 - loss: 0.4064 - regression_loss: 0.3564 - classification_loss: 0.0499
 4024/10000 [===========>..................] - ETA: 1:28:38 - loss: 0.4063 - regression_loss: 0.3564 - classification_loss: 0.0499
 4025/10000 [===========>..................] - ETA: 1:28:37 - loss: 0.4063 - regression_loss: 0.3564 - classification_loss: 0.0499
 4026/10000 [===========>..................] - ETA: 1:28:36 - loss: 0.4063 - regression_loss: 0.3564 - classification_loss: 0.0499
 4027/10000 [===========>..................] - ETA: 1:28:35 - loss: 0.4063 - regression_loss: 0.3564 - classification_loss: 0.0499
 4028/10000 [===========>..................] - ETA: 1:28:34 - loss: 0.4062 - regression_loss: 0.3563 - classification_loss: 0.0499
 4029/10000 [===========>..................] - ETA: 1:28:33 - loss: 0.4062 - regression_loss: 0.3563 - classification_loss: 0.0499
 4030/10000 [===========>..................] - ETA: 1:28:32 - loss: 0.4062 - regression_loss: 0.3563 - classification_loss: 0.0499
 4031/10000 [===========>..................] - ETA: 1:28:32 - loss: 0.4061 - regression_loss: 0.3562 - classification_loss: 0.0498
 4032/10000 [===========>..................] - ETA: 1:28:31 - loss: 0.4061 - regression_loss: 0.3563 - classification_loss: 0.0498
 4033/10000 [===========>..................] - ETA: 1:28:30 - loss: 0.4061 - regression_loss: 0.3562 - classification_loss: 0.0498
 4034/10000 [===========>..................] - ETA: 1:28:29 - loss: 0.4061 - regression_loss: 0.3562 - classification_loss: 0.0498
 4035/10000 [===========>..................] - ETA: 1:28:28 - loss: 0.4061 - regression_loss: 0.3562 - classification_loss: 0.0498
 4036/10000 [===========>..................] - ETA: 1:28:27 - loss: 0.4060 - regression_loss: 0.3562 - classification_loss: 0.0498
 4037/10000 [===========>..................] - ETA: 1:28:26 - loss: 0.4060 - regression_loss: 0.3562 - classification_loss: 0.0498
 4038/10000 [===========>..................] - ETA: 1:28:25 - loss: 0.4059 - regression_loss: 0.3561 - classification_loss: 0.0498
 4039/10000 [===========>..................] - ETA: 1:28:24 - loss: 0.4059 - regression_loss: 0.3561 - classification_loss: 0.0498
 4040/10000 [===========>..................] - ETA: 1:28:23 - loss: 0.4060 - regression_loss: 0.3562 - classification_loss: 0.0498
 4041/10000 [===========>..................] - ETA: 1:28:23 - loss: 0.4060 - regression_loss: 0.3562 - classification_loss: 0.0498
 4042/10000 [===========>..................] - ETA: 1:28:22 - loss: 0.4059 - regression_loss: 0.3561 - classification_loss: 0.0498
 4043/10000 [===========>..................] - ETA: 1:28:21 - loss: 0.4059 - regression_loss: 0.3561 - classification_loss: 0.0498
 4044/10000 [===========>..................] - ETA: 1:28:20 - loss: 0.4059 - regression_loss: 0.3561 - classification_loss: 0.0498
 4045/10000 [===========>..................] - ETA: 1:28:19 - loss: 0.4059 - regression_loss: 0.3561 - classification_loss: 0.0498
 4046/10000 [===========>..................] - ETA: 1:28:18 - loss: 0.4059 - regression_loss: 0.3561 - classification_loss: 0.0498
 4047/10000 [===========>..................] - ETA: 1:28:17 - loss: 0.4059 - regression_loss: 0.3561 - classification_loss: 0.0498
 4048/10000 [===========>..................] - ETA: 1:28:16 - loss: 0.4059 - regression_loss: 0.3561 - classification_loss: 0.0498
 4049/10000 [===========>..................] - ETA: 1:28:15 - loss: 0.4058 - regression_loss: 0.3560 - classification_loss: 0.0498
 4050/10000 [===========>..................] - ETA: 1:28:15 - loss: 0.4058 - regression_loss: 0.3560 - classification_loss: 0.0498
 4051/10000 [===========>..................] - ETA: 1:28:14 - loss: 0.4058 - regression_loss: 0.3560 - classification_loss: 0.0498
 4052/10000 [===========>..................] - ETA: 1:28:13 - loss: 0.4058 - regression_loss: 0.3560 - classification_loss: 0.0498
 4053/10000 [===========>..................] - ETA: 1:28:12 - loss: 0.4059 - regression_loss: 0.3561 - classification_loss: 0.0498
 4054/10000 [===========>..................] - ETA: 1:28:11 - loss: 0.4058 - regression_loss: 0.3560 - classification_loss: 0.0498
 4055/10000 [===========>..................] - ETA: 1:28:10 - loss: 0.4058 - regression_loss: 0.3560 - classification_loss: 0.0498
 4056/10000 [===========>..................] - ETA: 1:28:09 - loss: 0.4058 - regression_loss: 0.3560 - classification_loss: 0.0498
 4057/10000 [===========>..................] - ETA: 1:28:08 - loss: 0.4058 - regression_loss: 0.3560 - classification_loss: 0.0498
 4058/10000 [===========>..................] - ETA: 1:28:07 - loss: 0.4057 - regression_loss: 0.3559 - classification_loss: 0.0498
 4059/10000 [===========>..................] - ETA: 1:28:07 - loss: 0.4057 - regression_loss: 0.3560 - classification_loss: 0.0498
 4060/10000 [===========>..................] - ETA: 1:28:06 - loss: 0.4058 - regression_loss: 0.3560 - classification_loss: 0.0498
 4061/10000 [===========>..................] - ETA: 1:28:05 - loss: 0.4057 - regression_loss: 0.3559 - classification_loss: 0.0498
 4062/10000 [===========>..................] - ETA: 1:28:04 - loss: 0.4058 - regression_loss: 0.3559 - classification_loss: 0.0498
 4063/10000 [===========>..................] - ETA: 1:28:03 - loss: 0.4057 - regression_loss: 0.3559 - classification_loss: 0.0498
 4064/10000 [===========>..................] - ETA: 1:28:02 - loss: 0.4059 - regression_loss: 0.3560 - classification_loss: 0.0498
 4065/10000 [===========>..................] - ETA: 1:28:01 - loss: 0.4058 - regression_loss: 0.3560 - classification_loss: 0.0498
 4066/10000 [===========>..................] - ETA: 1:28:00 - loss: 0.4057 - regression_loss: 0.3559 - classification_loss: 0.0498
 4067/10000 [===========>..................] - ETA: 1:27:59 - loss: 0.4057 - regression_loss: 0.3559 - classification_loss: 0.0498
 4068/10000 [===========>..................] - ETA: 1:27:58 - loss: 0.4057 - regression_loss: 0.3559 - classification_loss: 0.0498
 4069/10000 [===========>..................] - ETA: 1:27:58 - loss: 0.4057 - regression_loss: 0.3559 - classification_loss: 0.0498
 4070/10000 [===========>..................] - ETA: 1:27:57 - loss: 0.4057 - regression_loss: 0.3558 - classification_loss: 0.0498
 4071/10000 [===========>..................] - ETA: 1:27:56 - loss: 0.4057 - regression_loss: 0.3559 - classification_loss: 0.0498
 4072/10000 [===========>..................] - ETA: 1:27:55 - loss: 0.4057 - regression_loss: 0.3559 - classification_loss: 0.0498
 4073/10000 [===========>..................] - ETA: 1:27:54 - loss: 0.4057 - regression_loss: 0.3559 - classification_loss: 0.0498
 4074/10000 [===========>..................] - ETA: 1:27:53 - loss: 0.4057 - regression_loss: 0.3559 - classification_loss: 0.0498
 4075/10000 [===========>..................] - ETA: 1:27:52 - loss: 0.4057 - regression_loss: 0.3559 - classification_loss: 0.0498
 4076/10000 [===========>..................] - ETA: 1:27:51 - loss: 0.4056 - regression_loss: 0.3558 - classification_loss: 0.0498
 4077/10000 [===========>..................] - ETA: 1:27:50 - loss: 0.4055 - regression_loss: 0.3558 - classification_loss: 0.0498
 4078/10000 [===========>..................] - ETA: 1:27:50 - loss: 0.4055 - regression_loss: 0.3558 - classification_loss: 0.0498
 4079/10000 [===========>..................] - ETA: 1:27:49 - loss: 0.4055 - regression_loss: 0.3558 - classification_loss: 0.0498
 4080/10000 [===========>..................] - ETA: 1:27:48 - loss: 0.4055 - regression_loss: 0.3557 - classification_loss: 0.0498
 4081/10000 [===========>..................] - ETA: 1:27:47 - loss: 0.4054 - regression_loss: 0.3557 - classification_loss: 0.0498
 4082/10000 [===========>..................] - ETA: 1:27:46 - loss: 0.4054 - regression_loss: 0.3556 - classification_loss: 0.0497
 4083/10000 [===========>..................] - ETA: 1:27:45 - loss: 0.4054 - regression_loss: 0.3557 - classification_loss: 0.0497
 4084/10000 [===========>..................] - ETA: 1:27:44 - loss: 0.4054 - regression_loss: 0.3557 - classification_loss: 0.0497
 4085/10000 [===========>..................] - ETA: 1:27:43 - loss: 0.4054 - regression_loss: 0.3557 - classification_loss: 0.0497
 4086/10000 [===========>..................] - ETA: 1:27:42 - loss: 0.4054 - regression_loss: 0.3557 - classification_loss: 0.0497
 4087/10000 [===========>..................] - ETA: 1:27:42 - loss: 0.4055 - regression_loss: 0.3558 - classification_loss: 0.0497
 4088/10000 [===========>..................] - ETA: 1:27:41 - loss: 0.4055 - regression_loss: 0.3557 - classification_loss: 0.0497
 4089/10000 [===========>..................] - ETA: 1:27:40 - loss: 0.4055 - regression_loss: 0.3558 - classification_loss: 0.0497
 4090/10000 [===========>..................] - ETA: 1:27:39 - loss: 0.4055 - regression_loss: 0.3558 - classification_loss: 0.0497
 4091/10000 [===========>..................] - ETA: 1:27:38 - loss: 0.4054 - regression_loss: 0.3557 - classification_loss: 0.0497
 4092/10000 [===========>..................] - ETA: 1:27:37 - loss: 0.4054 - regression_loss: 0.3557 - classification_loss: 0.0497
 4093/10000 [===========>..................] - ETA: 1:27:36 - loss: 0.4053 - regression_loss: 0.3556 - classification_loss: 0.0497
 4094/10000 [===========>..................] - ETA: 1:27:35 - loss: 0.4053 - regression_loss: 0.3556 - classification_loss: 0.0497
 4095/10000 [===========>..................] - ETA: 1:27:34 - loss: 0.4053 - regression_loss: 0.3556 - classification_loss: 0.0497
 4096/10000 [===========>..................] - ETA: 1:27:34 - loss: 0.4052 - regression_loss: 0.3555 - classification_loss: 0.0497
 4097/10000 [===========>..................] - ETA: 1:27:33 - loss: 0.4051 - regression_loss: 0.3555 - classification_loss: 0.0497
 4098/10000 [===========>..................] - ETA: 1:27:32 - loss: 0.4051 - regression_loss: 0.3554 - classification_loss: 0.0497
 4099/10000 [===========>..................] - ETA: 1:27:31 - loss: 0.4051 - regression_loss: 0.3554 - classification_loss: 0.0496
 4100/10000 [===========>..................] - ETA: 1:27:30 - loss: 0.4052 - regression_loss: 0.3555 - classification_loss: 0.0497
 4101/10000 [===========>..................] - ETA: 1:27:29 - loss: 0.4051 - regression_loss: 0.3555 - classification_loss: 0.0497
 4102/10000 [===========>..................] - ETA: 1:27:28 - loss: 0.4055 - regression_loss: 0.3558 - classification_loss: 0.0496
 4103/10000 [===========>..................] - ETA: 1:27:27 - loss: 0.4055 - regression_loss: 0.3558 - classification_loss: 0.0496
 4104/10000 [===========>..................] - ETA: 1:27:26 - loss: 0.4055 - regression_loss: 0.3558 - classification_loss: 0.0497
 4105/10000 [===========>..................] - ETA: 1:27:25 - loss: 0.4056 - regression_loss: 0.3559 - classification_loss: 0.0497
 4106/10000 [===========>..................] - ETA: 1:27:25 - loss: 0.4055 - regression_loss: 0.3558 - classification_loss: 0.0497
 4107/10000 [===========>..................] - ETA: 1:27:24 - loss: 0.4056 - regression_loss: 0.3559 - classification_loss: 0.0497
 4108/10000 [===========>..................] - ETA: 1:27:23 - loss: 0.4057 - regression_loss: 0.3560 - classification_loss: 0.0497
 4109/10000 [===========>..................] - ETA: 1:27:22 - loss: 0.4057 - regression_loss: 0.3560 - classification_loss: 0.0497
 4110/10000 [===========>..................] - ETA: 1:27:21 - loss: 0.4057 - regression_loss: 0.3560 - classification_loss: 0.0497
 4111/10000 [===========>..................] - ETA: 1:27:20 - loss: 0.4056 - regression_loss: 0.3560 - classification_loss: 0.0497
 4112/10000 [===========>..................] - ETA: 1:27:19 - loss: 0.4056 - regression_loss: 0.3560 - classification_loss: 0.0497
 4113/10000 [===========>..................] - ETA: 1:27:18 - loss: 0.4056 - regression_loss: 0.3560 - classification_loss: 0.0496
 4114/10000 [===========>..................] - ETA: 1:27:17 - loss: 0.4058 - regression_loss: 0.3561 - classification_loss: 0.0496
 4115/10000 [===========>..................] - ETA: 1:27:17 - loss: 0.4057 - regression_loss: 0.3560 - classification_loss: 0.0496
 4116/10000 [===========>..................] - ETA: 1:27:16 - loss: 0.4056 - regression_loss: 0.3560 - classification_loss: 0.0496
 4117/10000 [===========>..................] - ETA: 1:27:15 - loss: 0.4056 - regression_loss: 0.3559 - classification_loss: 0.0496
 4118/10000 [===========>..................] - ETA: 1:27:14 - loss: 0.4055 - regression_loss: 0.3559 - classification_loss: 0.0496
 4119/10000 [===========>..................] - ETA: 1:27:13 - loss: 0.4056 - regression_loss: 0.3560 - classification_loss: 0.0496
 4120/10000 [===========>..................] - ETA: 1:27:12 - loss: 0.4057 - regression_loss: 0.3560 - classification_loss: 0.0496
 4121/10000 [===========>..................] - ETA: 1:27:11 - loss: 0.4056 - regression_loss: 0.3560 - classification_loss: 0.0496
 4122/10000 [===========>..................] - ETA: 1:27:10 - loss: 0.4055 - regression_loss: 0.3559 - classification_loss: 0.0496
 4123/10000 [===========>..................] - ETA: 1:27:09 - loss: 0.4057 - regression_loss: 0.3560 - classification_loss: 0.0496
 4124/10000 [===========>..................] - ETA: 1:27:09 - loss: 0.4056 - regression_loss: 0.3560 - classification_loss: 0.0496
 4125/10000 [===========>..................] - ETA: 1:27:08 - loss: 0.4055 - regression_loss: 0.3559 - classification_loss: 0.0496
 4126/10000 [===========>..................] - ETA: 1:27:07 - loss: 0.4057 - regression_loss: 0.3561 - classification_loss: 0.0496
 4127/10000 [===========>..................] - ETA: 1:27:06 - loss: 0.4057 - regression_loss: 0.3561 - classification_loss: 0.0496
 4128/10000 [===========>..................] - ETA: 1:27:05 - loss: 0.4057 - regression_loss: 0.3561 - classification_loss: 0.0496
 4129/10000 [===========>..................] - ETA: 1:27:04 - loss: 0.4057 - regression_loss: 0.3561 - classification_loss: 0.0496
 4130/10000 [===========>..................] - ETA: 1:27:03 - loss: 0.4057 - regression_loss: 0.3561 - classification_loss: 0.0496
 4131/10000 [===========>..................] - ETA: 1:27:02 - loss: 0.4056 - regression_loss: 0.3560 - classification_loss: 0.0496
 4132/10000 [===========>..................] - ETA: 1:27:01 - loss: 0.4056 - regression_loss: 0.3560 - classification_loss: 0.0496
 4133/10000 [===========>..................] - ETA: 1:27:01 - loss: 0.4055 - regression_loss: 0.3559 - classification_loss: 0.0496
 4134/10000 [===========>..................] - ETA: 1:27:00 - loss: 0.4055 - regression_loss: 0.3559 - classification_loss: 0.0496
 4135/10000 [===========>..................] - ETA: 1:26:59 - loss: 0.4054 - regression_loss: 0.3558 - classification_loss: 0.0496
 4136/10000 [===========>..................] - ETA: 1:26:58 - loss: 0.4054 - regression_loss: 0.3559 - classification_loss: 0.0495
 4137/10000 [===========>..................] - ETA: 1:26:57 - loss: 0.4054 - regression_loss: 0.3559 - classification_loss: 0.0495
 4138/10000 [===========>..................] - ETA: 1:26:56 - loss: 0.4053 - regression_loss: 0.3558 - classification_loss: 0.0495
 4139/10000 [===========>..................] - ETA: 1:26:55 - loss: 0.4052 - regression_loss: 0.3557 - classification_loss: 0.0495
 4140/10000 [===========>..................] - ETA: 1:26:54 - loss: 0.4053 - regression_loss: 0.3557 - classification_loss: 0.0495
 4141/10000 [===========>..................] - ETA: 1:26:53 - loss: 0.4053 - regression_loss: 0.3557 - classification_loss: 0.0495
 4142/10000 [===========>..................] - ETA: 1:26:52 - loss: 0.4052 - regression_loss: 0.3557 - classification_loss: 0.0495
 4143/10000 [===========>..................] - ETA: 1:26:52 - loss: 0.4052 - regression_loss: 0.3557 - classification_loss: 0.0495
 4144/10000 [===========>..................] - ETA: 1:26:51 - loss: 0.4051 - regression_loss: 0.3556 - classification_loss: 0.0495
 4145/10000 [===========>..................] - ETA: 1:26:50 - loss: 0.4051 - regression_loss: 0.3556 - classification_loss: 0.0495
 4146/10000 [===========>..................] - ETA: 1:26:49 - loss: 0.4051 - regression_loss: 0.3557 - classification_loss: 0.0495
 4147/10000 [===========>..................] - ETA: 1:26:48 - loss: 0.4052 - regression_loss: 0.3557 - classification_loss: 0.0495
 4148/10000 [===========>..................] - ETA: 1:26:47 - loss: 0.4052 - regression_loss: 0.3557 - classification_loss: 0.0495
 4149/10000 [===========>..................] - ETA: 1:26:46 - loss: 0.4052 - regression_loss: 0.3557 - classification_loss: 0.0495
 4150/10000 [===========>..................] - ETA: 1:26:45 - loss: 0.4052 - regression_loss: 0.3557 - classification_loss: 0.0495
 4151/10000 [===========>..................] - ETA: 1:26:44 - loss: 0.4052 - regression_loss: 0.3557 - classification_loss: 0.0495
 4152/10000 [===========>..................] - ETA: 1:26:44 - loss: 0.4052 - regression_loss: 0.3557 - classification_loss: 0.0495
 4153/10000 [===========>..................] - ETA: 1:26:43 - loss: 0.4052 - regression_loss: 0.3558 - classification_loss: 0.0495
 4154/10000 [===========>..................] - ETA: 1:26:42 - loss: 0.4052 - regression_loss: 0.3557 - classification_loss: 0.0495
 4155/10000 [===========>..................] - ETA: 1:26:41 - loss: 0.4052 - regression_loss: 0.3557 - classification_loss: 0.0495
 4156/10000 [===========>..................] - ETA: 1:26:40 - loss: 0.4052 - regression_loss: 0.3557 - classification_loss: 0.0494
 4157/10000 [===========>..................] - ETA: 1:26:39 - loss: 0.4052 - regression_loss: 0.3557 - classification_loss: 0.0495
 4158/10000 [===========>..................] - ETA: 1:26:38 - loss: 0.4053 - regression_loss: 0.3558 - classification_loss: 0.0495
 4159/10000 [===========>..................] - ETA: 1:26:37 - loss: 0.4053 - regression_loss: 0.3558 - classification_loss: 0.0495
 4160/10000 [===========>..................] - ETA: 1:26:36 - loss: 0.4052 - regression_loss: 0.3558 - classification_loss: 0.0494
 4161/10000 [===========>..................] - ETA: 1:26:36 - loss: 0.4052 - regression_loss: 0.3558 - classification_loss: 0.0494
 4162/10000 [===========>..................] - ETA: 1:26:35 - loss: 0.4052 - regression_loss: 0.3558 - classification_loss: 0.0494
 4163/10000 [===========>..................] - ETA: 1:26:34 - loss: 0.4052 - regression_loss: 0.3558 - classification_loss: 0.0495
 4164/10000 [===========>..................] - ETA: 1:26:33 - loss: 0.4052 - regression_loss: 0.3557 - classification_loss: 0.0495
 4165/10000 [===========>..................] - ETA: 1:26:32 - loss: 0.4051 - regression_loss: 0.3557 - classification_loss: 0.0495
 4166/10000 [===========>..................] - ETA: 1:26:31 - loss: 0.4051 - regression_loss: 0.3557 - classification_loss: 0.0495
 4167/10000 [===========>..................] - ETA: 1:26:30 - loss: 0.4051 - regression_loss: 0.3557 - classification_loss: 0.0494
 4168/10000 [===========>..................] - ETA: 1:26:29 - loss: 0.4051 - regression_loss: 0.3556 - classification_loss: 0.0494
 4169/10000 [===========>..................] - ETA: 1:26:28 - loss: 0.4051 - regression_loss: 0.3556 - classification_loss: 0.0494
 4170/10000 [===========>..................] - ETA: 1:26:28 - loss: 0.4050 - regression_loss: 0.3556 - classification_loss: 0.0494
 4171/10000 [===========>..................] - ETA: 1:26:27 - loss: 0.4051 - regression_loss: 0.3556 - classification_loss: 0.0494
 4172/10000 [===========>..................] - ETA: 1:26:26 - loss: 0.4051 - regression_loss: 0.3556 - classification_loss: 0.0494
 4173/10000 [===========>..................] - ETA: 1:26:25 - loss: 0.4051 - regression_loss: 0.3557 - classification_loss: 0.0494
 4174/10000 [===========>..................] - ETA: 1:26:24 - loss: 0.4051 - regression_loss: 0.3557 - classification_loss: 0.0494
 4175/10000 [===========>..................] - ETA: 1:26:23 - loss: 0.4052 - regression_loss: 0.3558 - classification_loss: 0.0494
 4176/10000 [===========>..................] - ETA: 1:26:22 - loss: 0.4052 - regression_loss: 0.3558 - classification_loss: 0.0494
 4177/10000 [===========>..................] - ETA: 1:26:21 - loss: 0.4052 - regression_loss: 0.3558 - classification_loss: 0.0494
 4178/10000 [===========>..................] - ETA: 1:26:20 - loss: 0.4052 - regression_loss: 0.3558 - classification_loss: 0.0494
 4179/10000 [===========>..................] - ETA: 1:26:20 - loss: 0.4052 - regression_loss: 0.3558 - classification_loss: 0.0494
 4180/10000 [===========>..................] - ETA: 1:26:19 - loss: 0.4051 - regression_loss: 0.3557 - classification_loss: 0.0494
 4181/10000 [===========>..................] - ETA: 1:26:18 - loss: 0.4051 - regression_loss: 0.3557 - classification_loss: 0.0494
 4182/10000 [===========>..................] - ETA: 1:26:17 - loss: 0.4053 - regression_loss: 0.3558 - classification_loss: 0.0495
 4183/10000 [===========>..................] - ETA: 1:26:16 - loss: 0.4054 - regression_loss: 0.3559 - classification_loss: 0.0495
 4184/10000 [===========>..................] - ETA: 1:26:15 - loss: 0.4054 - regression_loss: 0.3560 - classification_loss: 0.0495
 4185/10000 [===========>..................] - ETA: 1:26:14 - loss: 0.4054 - regression_loss: 0.3559 - classification_loss: 0.0495
 4186/10000 [===========>..................] - ETA: 1:26:13 - loss: 0.4054 - regression_loss: 0.3559 - classification_loss: 0.0496
 4187/10000 [===========>..................] - ETA: 1:26:12 - loss: 0.4054 - regression_loss: 0.3559 - classification_loss: 0.0495
 4188/10000 [===========>..................] - ETA: 1:26:12 - loss: 0.4054 - regression_loss: 0.3558 - classification_loss: 0.0495
 4189/10000 [===========>..................] - ETA: 1:26:11 - loss: 0.4054 - regression_loss: 0.3559 - classification_loss: 0.0495
 4190/10000 [===========>..................] - ETA: 1:26:10 - loss: 0.4055 - regression_loss: 0.3559 - classification_loss: 0.0495
 4191/10000 [===========>..................] - ETA: 1:26:09 - loss: 0.4055 - regression_loss: 0.3560 - classification_loss: 0.0496
 4192/10000 [===========>..................] - ETA: 1:26:08 - loss: 0.4056 - regression_loss: 0.3560 - classification_loss: 0.0495
 4193/10000 [===========>..................] - ETA: 1:26:07 - loss: 0.4057 - regression_loss: 0.3562 - classification_loss: 0.0495
 4194/10000 [===========>..................] - ETA: 1:26:06 - loss: 0.4057 - regression_loss: 0.3561 - classification_loss: 0.0495
 4195/10000 [===========>..................] - ETA: 1:26:05 - loss: 0.4056 - regression_loss: 0.3561 - classification_loss: 0.0495
 4196/10000 [===========>..................] - ETA: 1:26:04 - loss: 0.4058 - regression_loss: 0.3562 - classification_loss: 0.0495
 4197/10000 [===========>..................] - ETA: 1:26:03 - loss: 0.4058 - regression_loss: 0.3562 - classification_loss: 0.0495
 4198/10000 [===========>..................] - ETA: 1:26:03 - loss: 0.4057 - regression_loss: 0.3562 - classification_loss: 0.0495
 4199/10000 [===========>..................] - ETA: 1:26:02 - loss: 0.4058 - regression_loss: 0.3562 - classification_loss: 0.0495
 4200/10000 [===========>..................] - ETA: 1:26:01 - loss: 0.4057 - regression_loss: 0.3562 - classification_loss: 0.0495
 4201/10000 [===========>..................] - ETA: 1:26:00 - loss: 0.4067 - regression_loss: 0.3569 - classification_loss: 0.0498
 4202/10000 [===========>..................] - ETA: 1:25:59 - loss: 0.4068 - regression_loss: 0.3570 - classification_loss: 0.0498
 4203/10000 [===========>..................] - ETA: 1:25:58 - loss: 0.4068 - regression_loss: 0.3570 - classification_loss: 0.0498
 4204/10000 [===========>..................] - ETA: 1:25:57 - loss: 0.4067 - regression_loss: 0.3569 - classification_loss: 0.0498
 4205/10000 [===========>..................] - ETA: 1:25:56 - loss: 0.4066 - regression_loss: 0.3569 - classification_loss: 0.0498
 4206/10000 [===========>..................] - ETA: 1:25:55 - loss: 0.4066 - regression_loss: 0.3568 - classification_loss: 0.0498
 4207/10000 [===========>..................] - ETA: 1:25:55 - loss: 0.4069 - regression_loss: 0.3571 - classification_loss: 0.0498
 4208/10000 [===========>..................] - ETA: 1:25:54 - loss: 0.4069 - regression_loss: 0.3571 - classification_loss: 0.0498
 4209/10000 [===========>..................] - ETA: 1:25:54 - loss: 0.4068 - regression_loss: 0.3570 - classification_loss: 0.0498
 4210/10000 [===========>..................] - ETA: 1:25:53 - loss: 0.4068 - regression_loss: 0.3570 - classification_loss: 0.0498
 4211/10000 [===========>..................] - ETA: 1:25:52 - loss: 0.4067 - regression_loss: 0.3569 - classification_loss: 0.0498
 4212/10000 [===========>..................] - ETA: 1:25:51 - loss: 0.4067 - regression_loss: 0.3569 - classification_loss: 0.0498
 4213/10000 [===========>..................] - ETA: 1:25:50 - loss: 0.4067 - regression_loss: 0.3569 - classification_loss: 0.0498
 4214/10000 [===========>..................] - ETA: 1:25:49 - loss: 0.4066 - regression_loss: 0.3569 - classification_loss: 0.0498
 4215/10000 [===========>..................] - ETA: 1:25:48 - loss: 0.4066 - regression_loss: 0.3568 - classification_loss: 0.0498
 4216/10000 [===========>..................] - ETA: 1:25:47 - loss: 0.4065 - regression_loss: 0.3568 - classification_loss: 0.0498
 4217/10000 [===========>..................] - ETA: 1:25:46 - loss: 0.4065 - regression_loss: 0.3567 - classification_loss: 0.0498
 4218/10000 [===========>..................] - ETA: 1:25:46 - loss: 0.4065 - regression_loss: 0.3567 - classification_loss: 0.0498
 4219/10000 [===========>..................] - ETA: 1:25:45 - loss: 0.4065 - regression_loss: 0.3567 - classification_loss: 0.0498
 4220/10000 [===========>..................] - ETA: 1:25:44 - loss: 0.4064 - regression_loss: 0.3567 - classification_loss: 0.0498
 4221/10000 [===========>..................] - ETA: 1:25:43 - loss: 0.4064 - regression_loss: 0.3566 - classification_loss: 0.0498
 4222/10000 [===========>..................] - ETA: 1:25:42 - loss: 0.4064 - regression_loss: 0.3566 - classification_loss: 0.0498
 4223/10000 [===========>..................] - ETA: 1:25:41 - loss: 0.4064 - regression_loss: 0.3566 - classification_loss: 0.0498
 4224/10000 [===========>..................] - ETA: 1:25:40 - loss: 0.4064 - regression_loss: 0.3566 - classification_loss: 0.0498
 4225/10000 [===========>..................] - ETA: 1:25:39 - loss: 0.4064 - regression_loss: 0.3566 - classification_loss: 0.0498
 4226/10000 [===========>..................] - ETA: 1:25:38 - loss: 0.4063 - regression_loss: 0.3565 - classification_loss: 0.0498
 4227/10000 [===========>..................] - ETA: 1:25:38 - loss: 0.4063 - regression_loss: 0.3566 - classification_loss: 0.0498
 4228/10000 [===========>..................] - ETA: 1:25:37 - loss: 0.4064 - regression_loss: 0.3566 - classification_loss: 0.0498
 4229/10000 [===========>..................] - ETA: 1:25:36 - loss: 0.4063 - regression_loss: 0.3566 - classification_loss: 0.0498
 4230/10000 [===========>..................] - ETA: 1:25:35 - loss: 0.4064 - regression_loss: 0.3566 - classification_loss: 0.0498
 4231/10000 [===========>..................] - ETA: 1:25:34 - loss: 0.4064 - regression_loss: 0.3566 - classification_loss: 0.0498
 4232/10000 [===========>..................] - ETA: 1:25:33 - loss: 0.4064 - regression_loss: 0.3566 - classification_loss: 0.0498
 4233/10000 [===========>..................] - ETA: 1:25:32 - loss: 0.4067 - regression_loss: 0.3569 - classification_loss: 0.0498
 4234/10000 [===========>..................] - ETA: 1:25:31 - loss: 0.4068 - regression_loss: 0.3569 - classification_loss: 0.0498
 4235/10000 [===========>..................] - ETA: 1:25:30 - loss: 0.4067 - regression_loss: 0.3569 - classification_loss: 0.0498
 4236/10000 [===========>..................] - ETA: 1:25:30 - loss: 0.4067 - regression_loss: 0.3569 - classification_loss: 0.0498
 4237/10000 [===========>..................] - ETA: 1:25:29 - loss: 0.4067 - regression_loss: 0.3569 - classification_loss: 0.0498
 4238/10000 [===========>..................] - ETA: 1:25:28 - loss: 0.4068 - regression_loss: 0.3570 - classification_loss: 0.0498
 4239/10000 [===========>..................] - ETA: 1:25:27 - loss: 0.4069 - regression_loss: 0.3570 - classification_loss: 0.0498
 4240/10000 [===========>..................] - ETA: 1:25:26 - loss: 0.4069 - regression_loss: 0.3570 - classification_loss: 0.0498
 4241/10000 [===========>..................] - ETA: 1:25:25 - loss: 0.4069 - regression_loss: 0.3570 - classification_loss: 0.0498
 4242/10000 [===========>..................] - ETA: 1:25:24 - loss: 0.4068 - regression_loss: 0.3570 - classification_loss: 0.0498
 4243/10000 [===========>..................] - ETA: 1:25:23 - loss: 0.4068 - regression_loss: 0.3570 - classification_loss: 0.0498
 4244/10000 [===========>..................] - ETA: 1:25:22 - loss: 0.4067 - regression_loss: 0.3569 - classification_loss: 0.0498
 4245/10000 [===========>..................] - ETA: 1:25:21 - loss: 0.4067 - regression_loss: 0.3569 - classification_loss: 0.0498
 4246/10000 [===========>..................] - ETA: 1:25:21 - loss: 0.4067 - regression_loss: 0.3569 - classification_loss: 0.0498
 4247/10000 [===========>..................] - ETA: 1:25:20 - loss: 0.4067 - regression_loss: 0.3569 - classification_loss: 0.0498
 4248/10000 [===========>..................] - ETA: 1:25:19 - loss: 0.4069 - regression_loss: 0.3571 - classification_loss: 0.0498
 4249/10000 [===========>..................] - ETA: 1:25:18 - loss: 0.4070 - regression_loss: 0.3571 - classification_loss: 0.0498
 4250/10000 [===========>..................] - ETA: 1:25:17 - loss: 0.4070 - regression_loss: 0.3572 - classification_loss: 0.0498
 4251/10000 [===========>..................] - ETA: 1:25:16 - loss: 0.4069 - regression_loss: 0.3571 - classification_loss: 0.0498
 4252/10000 [===========>..................] - ETA: 1:25:15 - loss: 0.4069 - regression_loss: 0.3571 - classification_loss: 0.0498
 4253/10000 [===========>..................] - ETA: 1:25:14 - loss: 0.4068 - regression_loss: 0.3570 - classification_loss: 0.0498
 4254/10000 [===========>..................] - ETA: 1:25:13 - loss: 0.4068 - regression_loss: 0.3570 - classification_loss: 0.0498
 4255/10000 [===========>..................] - ETA: 1:25:13 - loss: 0.4068 - regression_loss: 0.3570 - classification_loss: 0.0498
 4256/10000 [===========>..................] - ETA: 1:25:12 - loss: 0.4069 - regression_loss: 0.3571 - classification_loss: 0.0498
 4257/10000 [===========>..................] - ETA: 1:25:11 - loss: 0.4068 - regression_loss: 0.3570 - classification_loss: 0.0498
 4258/10000 [===========>..................] - ETA: 1:25:10 - loss: 0.4068 - regression_loss: 0.3570 - classification_loss: 0.0498
 4259/10000 [===========>..................] - ETA: 1:25:09 - loss: 0.4067 - regression_loss: 0.3570 - classification_loss: 0.0498
 4260/10000 [===========>..................] - ETA: 1:25:08 - loss: 0.4067 - regression_loss: 0.3569 - classification_loss: 0.0498
 4261/10000 [===========>..................] - ETA: 1:25:07 - loss: 0.4067 - regression_loss: 0.3569 - classification_loss: 0.0498
 4262/10000 [===========>..................] - ETA: 1:25:06 - loss: 0.4067 - regression_loss: 0.3569 - classification_loss: 0.0498
 4263/10000 [===========>..................] - ETA: 1:25:05 - loss: 0.4066 - regression_loss: 0.3568 - classification_loss: 0.0498
 4264/10000 [===========>..................] - ETA: 1:25:04 - loss: 0.4066 - regression_loss: 0.3568 - classification_loss: 0.0498
 4265/10000 [===========>..................] - ETA: 1:25:04 - loss: 0.4066 - regression_loss: 0.3568 - classification_loss: 0.0498
 4266/10000 [===========>..................] - ETA: 1:25:03 - loss: 0.4066 - regression_loss: 0.3568 - classification_loss: 0.0498
 4267/10000 [===========>..................] - ETA: 1:25:02 - loss: 0.4066 - regression_loss: 0.3568 - classification_loss: 0.0498
 4268/10000 [===========>..................] - ETA: 1:25:01 - loss: 0.4066 - regression_loss: 0.3568 - classification_loss: 0.0498
 4269/10000 [===========>..................] - ETA: 1:25:00 - loss: 0.4065 - regression_loss: 0.3568 - classification_loss: 0.0498
 4270/10000 [===========>..................] - ETA: 1:24:59 - loss: 0.4065 - regression_loss: 0.3567 - classification_loss: 0.0498
 4271/10000 [===========>..................] - ETA: 1:24:58 - loss: 0.4064 - regression_loss: 0.3567 - classification_loss: 0.0498
 4272/10000 [===========>..................] - ETA: 1:24:57 - loss: 0.4066 - regression_loss: 0.3568 - classification_loss: 0.0498
 4273/10000 [===========>..................] - ETA: 1:24:56 - loss: 0.4066 - regression_loss: 0.3568 - classification_loss: 0.0498
 4274/10000 [===========>..................] - ETA: 1:24:56 - loss: 0.4066 - regression_loss: 0.3568 - classification_loss: 0.0498
 4275/10000 [===========>..................] - ETA: 1:24:55 - loss: 0.4066 - regression_loss: 0.3568 - classification_loss: 0.0498
 4276/10000 [===========>..................] - ETA: 1:24:54 - loss: 0.4066 - regression_loss: 0.3569 - classification_loss: 0.0498
 4277/10000 [===========>..................] - ETA: 1:24:53 - loss: 0.4066 - regression_loss: 0.3569 - classification_loss: 0.0498
 4278/10000 [===========>..................] - ETA: 1:24:52 - loss: 0.4066 - regression_loss: 0.3569 - classification_loss: 0.0497
 4279/10000 [===========>..................] - ETA: 1:24:51 - loss: 0.4066 - regression_loss: 0.3569 - classification_loss: 0.0497
 4280/10000 [===========>..................] - ETA: 1:24:50 - loss: 0.4067 - regression_loss: 0.3569 - classification_loss: 0.0498
 4281/10000 [===========>..................] - ETA: 1:24:49 - loss: 0.4067 - regression_loss: 0.3569 - classification_loss: 0.0498
 4282/10000 [===========>..................] - ETA: 1:24:48 - loss: 0.4067 - regression_loss: 0.3569 - classification_loss: 0.0498
 4283/10000 [===========>..................] - ETA: 1:24:47 - loss: 0.4067 - regression_loss: 0.3569 - classification_loss: 0.0498
 4284/10000 [===========>..................] - ETA: 1:24:47 - loss: 0.4067 - regression_loss: 0.3569 - classification_loss: 0.0498
 4285/10000 [===========>..................] - ETA: 1:24:46 - loss: 0.4067 - regression_loss: 0.3569 - classification_loss: 0.0498
 4286/10000 [===========>..................] - ETA: 1:24:45 - loss: 0.4067 - regression_loss: 0.3569 - classification_loss: 0.0498
 4287/10000 [===========>..................] - ETA: 1:24:44 - loss: 0.4066 - regression_loss: 0.3568 - classification_loss: 0.0498
 4288/10000 [===========>..................] - ETA: 1:24:43 - loss: 0.4067 - regression_loss: 0.3568 - classification_loss: 0.0499
 4289/10000 [===========>..................] - ETA: 1:24:42 - loss: 0.4066 - regression_loss: 0.3568 - classification_loss: 0.0499
 4290/10000 [===========>..................] - ETA: 1:24:41 - loss: 0.4067 - regression_loss: 0.3568 - classification_loss: 0.0499
 4291/10000 [===========>..................] - ETA: 1:24:40 - loss: 0.4068 - regression_loss: 0.3569 - classification_loss: 0.0499
 4292/10000 [===========>..................] - ETA: 1:24:39 - loss: 0.4068 - regression_loss: 0.3569 - classification_loss: 0.0499
 4293/10000 [===========>..................] - ETA: 1:24:39 - loss: 0.4068 - regression_loss: 0.3569 - classification_loss: 0.0499
 4294/10000 [===========>..................] - ETA: 1:24:38 - loss: 0.4069 - regression_loss: 0.3570 - classification_loss: 0.0499
 4295/10000 [===========>..................] - ETA: 1:24:37 - loss: 0.4070 - regression_loss: 0.3572 - classification_loss: 0.0499
 4296/10000 [===========>..................] - ETA: 1:24:36 - loss: 0.4071 - regression_loss: 0.3572 - classification_loss: 0.0499
 4297/10000 [===========>..................] - ETA: 1:24:35 - loss: 0.4071 - regression_loss: 0.3572 - classification_loss: 0.0499
 4298/10000 [===========>..................] - ETA: 1:24:34 - loss: 0.4070 - regression_loss: 0.3571 - classification_loss: 0.0499
 4299/10000 [===========>..................] - ETA: 1:24:33 - loss: 0.4070 - regression_loss: 0.3571 - classification_loss: 0.0499
 4300/10000 [===========>..................] - ETA: 1:24:32 - loss: 0.4070 - regression_loss: 0.3571 - classification_loss: 0.0499
 4301/10000 [===========>..................] - ETA: 1:24:31 - loss: 0.4070 - regression_loss: 0.3571 - classification_loss: 0.0499
 4302/10000 [===========>..................] - ETA: 1:24:31 - loss: 0.4069 - regression_loss: 0.3571 - classification_loss: 0.0499
 4303/10000 [===========>..................] - ETA: 1:24:30 - loss: 0.4069 - regression_loss: 0.3570 - classification_loss: 0.0499
 4304/10000 [===========>..................] - ETA: 1:24:29 - loss: 0.4069 - regression_loss: 0.3571 - classification_loss: 0.0498
 4305/10000 [===========>..................] - ETA: 1:24:28 - loss: 0.4069 - regression_loss: 0.3571 - classification_loss: 0.0498
 4306/10000 [===========>..................] - ETA: 1:24:27 - loss: 0.4069 - regression_loss: 0.3571 - classification_loss: 0.0498
 4307/10000 [===========>..................] - ETA: 1:24:26 - loss: 0.4071 - regression_loss: 0.3572 - classification_loss: 0.0499
 4308/10000 [===========>..................] - ETA: 1:24:25 - loss: 0.4071 - regression_loss: 0.3572 - classification_loss: 0.0499
 4309/10000 [===========>..................] - ETA: 1:24:24 - loss: 0.4071 - regression_loss: 0.3572 - classification_loss: 0.0499
 4310/10000 [===========>..................] - ETA: 1:24:23 - loss: 0.4070 - regression_loss: 0.3572 - classification_loss: 0.0499
 4311/10000 [===========>..................] - ETA: 1:24:23 - loss: 0.4070 - regression_loss: 0.3572 - classification_loss: 0.0499
 4312/10000 [===========>..................] - ETA: 1:24:22 - loss: 0.4070 - regression_loss: 0.3572 - classification_loss: 0.0499
 4313/10000 [===========>..................] - ETA: 1:24:21 - loss: 0.4070 - regression_loss: 0.3572 - classification_loss: 0.0499
 4314/10000 [===========>..................] - ETA: 1:24:20 - loss: 0.4069 - regression_loss: 0.3571 - classification_loss: 0.0499
 4315/10000 [===========>..................] - ETA: 1:24:19 - loss: 0.4069 - regression_loss: 0.3571 - classification_loss: 0.0499
 4316/10000 [===========>..................] - ETA: 1:24:18 - loss: 0.4070 - regression_loss: 0.3571 - classification_loss: 0.0499
 4317/10000 [===========>..................] - ETA: 1:24:17 - loss: 0.4070 - regression_loss: 0.3571 - classification_loss: 0.0499
 4318/10000 [===========>..................] - ETA: 1:24:16 - loss: 0.4069 - regression_loss: 0.3570 - classification_loss: 0.0499
 4319/10000 [===========>..................] - ETA: 1:24:15 - loss: 0.4070 - regression_loss: 0.3571 - classification_loss: 0.0499
 4320/10000 [===========>..................] - ETA: 1:24:14 - loss: 0.4069 - regression_loss: 0.3571 - classification_loss: 0.0499
 4321/10000 [===========>..................] - ETA: 1:24:14 - loss: 0.4069 - regression_loss: 0.3571 - classification_loss: 0.0499
 4322/10000 [===========>..................] - ETA: 1:24:13 - loss: 0.4070 - regression_loss: 0.3571 - classification_loss: 0.0499
 4323/10000 [===========>..................] - ETA: 1:24:12 - loss: 0.4070 - regression_loss: 0.3572 - classification_loss: 0.0499
 4324/10000 [===========>..................] - ETA: 1:24:11 - loss: 0.4070 - regression_loss: 0.3571 - classification_loss: 0.0499
 4325/10000 [===========>..................] - ETA: 1:24:10 - loss: 0.4070 - regression_loss: 0.3571 - classification_loss: 0.0498
 4326/10000 [===========>..................] - ETA: 1:24:09 - loss: 0.4071 - regression_loss: 0.3572 - classification_loss: 0.0498
 4327/10000 [===========>..................] - ETA: 1:24:08 - loss: 0.4071 - regression_loss: 0.3572 - classification_loss: 0.0498
 4328/10000 [===========>..................] - ETA: 1:24:07 - loss: 0.4071 - regression_loss: 0.3573 - classification_loss: 0.0498
 4329/10000 [===========>..................] - ETA: 1:24:06 - loss: 0.4071 - regression_loss: 0.3573 - classification_loss: 0.0498
 4330/10000 [===========>..................] - ETA: 1:24:06 - loss: 0.4072 - regression_loss: 0.3574 - classification_loss: 0.0499
 4331/10000 [===========>..................] - ETA: 1:24:05 - loss: 0.4072 - regression_loss: 0.3573 - classification_loss: 0.0498
 4332/10000 [===========>..................] - ETA: 1:24:04 - loss: 0.4072 - regression_loss: 0.3573 - classification_loss: 0.0498
 4333/10000 [===========>..................] - ETA: 1:24:03 - loss: 0.4072 - regression_loss: 0.3573 - classification_loss: 0.0498
 4334/10000 [============>.................] - ETA: 1:24:02 - loss: 0.4072 - regression_loss: 0.3574 - classification_loss: 0.0498
 4335/10000 [============>.................] - ETA: 1:24:01 - loss: 0.4072 - regression_loss: 0.3574 - classification_loss: 0.0498
 4336/10000 [============>.................] - ETA: 1:24:00 - loss: 0.4072 - regression_loss: 0.3573 - classification_loss: 0.0498
 4337/10000 [============>.................] - ETA: 1:23:59 - loss: 0.4072 - regression_loss: 0.3573 - classification_loss: 0.0498
 4338/10000 [============>.................] - ETA: 1:23:58 - loss: 0.4072 - regression_loss: 0.3574 - classification_loss: 0.0498
 4339/10000 [============>.................] - ETA: 1:23:58 - loss: 0.4073 - regression_loss: 0.3574 - classification_loss: 0.0498
 4340/10000 [============>.................] - ETA: 1:23:57 - loss: 0.4073 - regression_loss: 0.3575 - classification_loss: 0.0498
 4341/10000 [============>.................] - ETA: 1:23:56 - loss: 0.4073 - regression_loss: 0.3575 - classification_loss: 0.0498
 4342/10000 [============>.................] - ETA: 1:23:55 - loss: 0.4073 - regression_loss: 0.3575 - classification_loss: 0.0498
 4343/10000 [============>.................] - ETA: 1:23:54 - loss: 0.4073 - regression_loss: 0.3575 - classification_loss: 0.0498
 4344/10000 [============>.................] - ETA: 1:23:53 - loss: 0.4072 - regression_loss: 0.3574 - classification_loss: 0.0498
 4345/10000 [============>.................] - ETA: 1:23:52 - loss: 0.4073 - regression_loss: 0.3574 - classification_loss: 0.0498
 4346/10000 [============>.................] - ETA: 1:23:51 - loss: 0.4073 - regression_loss: 0.3574 - classification_loss: 0.0498
 4347/10000 [============>.................] - ETA: 1:23:50 - loss: 0.4073 - regression_loss: 0.3575 - classification_loss: 0.0498
 4348/10000 [============>.................] - ETA: 1:23:49 - loss: 0.4073 - regression_loss: 0.3574 - classification_loss: 0.0498
 4349/10000 [============>.................] - ETA: 1:23:49 - loss: 0.4074 - regression_loss: 0.3574 - classification_loss: 0.0499
 4350/10000 [============>.................] - ETA: 1:23:48 - loss: 0.4073 - regression_loss: 0.3574 - classification_loss: 0.0499
 4351/10000 [============>.................] - ETA: 1:23:47 - loss: 0.4072 - regression_loss: 0.3573 - classification_loss: 0.0499
 4352/10000 [============>.................] - ETA: 1:23:46 - loss: 0.4072 - regression_loss: 0.3573 - classification_loss: 0.0499
 4353/10000 [============>.................] - ETA: 1:23:45 - loss: 0.4072 - regression_loss: 0.3573 - classification_loss: 0.0499
 4354/10000 [============>.................] - ETA: 1:23:44 - loss: 0.4072 - regression_loss: 0.3573 - classification_loss: 0.0499
 4355/10000 [============>.................] - ETA: 1:23:43 - loss: 0.4072 - regression_loss: 0.3573 - classification_loss: 0.0499
 4356/10000 [============>.................] - ETA: 1:23:42 - loss: 0.4072 - regression_loss: 0.3573 - classification_loss: 0.0499
 4357/10000 [============>.................] - ETA: 1:23:41 - loss: 0.4072 - regression_loss: 0.3573 - classification_loss: 0.0499
 4358/10000 [============>.................] - ETA: 1:23:41 - loss: 0.4072 - regression_loss: 0.3573 - classification_loss: 0.0499
 4359/10000 [============>.................] - ETA: 1:23:40 - loss: 0.4072 - regression_loss: 0.3574 - classification_loss: 0.0499
 4360/10000 [============>.................] - ETA: 1:23:39 - loss: 0.4072 - regression_loss: 0.3574 - classification_loss: 0.0499
 4361/10000 [============>.................] - ETA: 1:23:38 - loss: 0.4073 - regression_loss: 0.3574 - classification_loss: 0.0499
 4362/10000 [============>.................] - ETA: 1:23:37 - loss: 0.4073 - regression_loss: 0.3574 - classification_loss: 0.0499
 4363/10000 [============>.................] - ETA: 1:23:36 - loss: 0.4072 - regression_loss: 0.3574 - classification_loss: 0.0499
 4364/10000 [============>.................] - ETA: 1:23:35 - loss: 0.4073 - regression_loss: 0.3574 - classification_loss: 0.0499
 4365/10000 [============>.................] - ETA: 1:23:34 - loss: 0.4072 - regression_loss: 0.3573 - classification_loss: 0.0499
 4366/10000 [============>.................] - ETA: 1:23:33 - loss: 0.4071 - regression_loss: 0.3573 - classification_loss: 0.0498
 4367/10000 [============>.................] - ETA: 1:23:32 - loss: 0.4071 - regression_loss: 0.3573 - classification_loss: 0.0498
 4368/10000 [============>.................] - ETA: 1:23:32 - loss: 0.4071 - regression_loss: 0.3573 - classification_loss: 0.0498
 4369/10000 [============>.................] - ETA: 1:23:31 - loss: 0.4071 - regression_loss: 0.3572 - classification_loss: 0.0498
 4370/10000 [============>.................] - ETA: 1:23:30 - loss: 0.4071 - regression_loss: 0.3573 - classification_loss: 0.0498
 4371/10000 [============>.................] - ETA: 1:23:29 - loss: 0.4071 - regression_loss: 0.3573 - classification_loss: 0.0498
 4372/10000 [============>.................] - ETA: 1:23:28 - loss: 0.4070 - regression_loss: 0.3572 - classification_loss: 0.0498
 4373/10000 [============>.................] - ETA: 1:23:27 - loss: 0.4072 - regression_loss: 0.3574 - classification_loss: 0.0498
 4374/10000 [============>.................] - ETA: 1:23:26 - loss: 0.4072 - regression_loss: 0.3574 - classification_loss: 0.0498
 4375/10000 [============>.................] - ETA: 1:23:25 - loss: 0.4072 - regression_loss: 0.3573 - classification_loss: 0.0498
 4376/10000 [============>.................] - ETA: 1:23:24 - loss: 0.4073 - regression_loss: 0.3574 - classification_loss: 0.0498
 4377/10000 [============>.................] - ETA: 1:23:24 - loss: 0.4073 - regression_loss: 0.3574 - classification_loss: 0.0499
 4378/10000 [============>.................] - ETA: 1:23:23 - loss: 0.4073 - regression_loss: 0.3575 - classification_loss: 0.0499
 4379/10000 [============>.................] - ETA: 1:23:22 - loss: 0.4073 - regression_loss: 0.3574 - classification_loss: 0.0498
 4380/10000 [============>.................] - ETA: 1:23:21 - loss: 0.4072 - regression_loss: 0.3574 - classification_loss: 0.0498
 4381/10000 [============>.................] - ETA: 1:23:20 - loss: 0.4072 - regression_loss: 0.3574 - classification_loss: 0.0498
 4382/10000 [============>.................] - ETA: 1:23:19 - loss: 0.4072 - regression_loss: 0.3574 - classification_loss: 0.0498
 4383/10000 [============>.................] - ETA: 1:23:18 - loss: 0.4072 - regression_loss: 0.3574 - classification_loss: 0.0498
 4384/10000 [============>.................] - ETA: 1:23:17 - loss: 0.4073 - regression_loss: 0.3573 - classification_loss: 0.0499
 4385/10000 [============>.................] - ETA: 1:23:16 - loss: 0.4073 - regression_loss: 0.3574 - classification_loss: 0.0499
 4386/10000 [============>.................] - ETA: 1:23:16 - loss: 0.4073 - regression_loss: 0.3574 - classification_loss: 0.0499
 4387/10000 [============>.................] - ETA: 1:23:15 - loss: 0.4072 - regression_loss: 0.3573 - classification_loss: 0.0499
 4388/10000 [============>.................] - ETA: 1:23:14 - loss: 0.4071 - regression_loss: 0.3573 - classification_loss: 0.0499
 4389/10000 [============>.................] - ETA: 1:23:13 - loss: 0.4072 - regression_loss: 0.3573 - classification_loss: 0.0499
 4390/10000 [============>.................] - ETA: 1:23:12 - loss: 0.4073 - regression_loss: 0.3574 - classification_loss: 0.0499
 4391/10000 [============>.................] - ETA: 1:23:11 - loss: 0.4073 - regression_loss: 0.3574 - classification_loss: 0.0499
 4392/10000 [============>.................] - ETA: 1:23:10 - loss: 0.4073 - regression_loss: 0.3574 - classification_loss: 0.0499
 4393/10000 [============>.................] - ETA: 1:23:09 - loss: 0.4073 - regression_loss: 0.3574 - classification_loss: 0.0499
 4394/10000 [============>.................] - ETA: 1:23:08 - loss: 0.4072 - regression_loss: 0.3573 - classification_loss: 0.0499
 4395/10000 [============>.................] - ETA: 1:23:07 - loss: 0.4071 - regression_loss: 0.3572 - classification_loss: 0.0499
 4396/10000 [============>.................] - ETA: 1:23:07 - loss: 0.4072 - regression_loss: 0.3573 - classification_loss: 0.0499
 4397/10000 [============>.................] - ETA: 1:23:06 - loss: 0.4072 - regression_loss: 0.3573 - classification_loss: 0.0499
 4398/10000 [============>.................] - ETA: 1:23:05 - loss: 0.4072 - regression_loss: 0.3573 - classification_loss: 0.0499
 4399/10000 [============>.................] - ETA: 1:23:04 - loss: 0.4072 - regression_loss: 0.3573 - classification_loss: 0.0499
 4400/10000 [============>.................] - ETA: 1:23:03 - loss: 0.4072 - regression_loss: 0.3573 - classification_loss: 0.0499
 4401/10000 [============>.................] - ETA: 1:23:02 - loss: 0.4072 - regression_loss: 0.3573 - classification_loss: 0.0499
 4402/10000 [============>.................] - ETA: 1:23:01 - loss: 0.4074 - regression_loss: 0.3575 - classification_loss: 0.0499
 4403/10000 [============>.................] - ETA: 1:23:00 - loss: 0.4074 - regression_loss: 0.3575 - classification_loss: 0.0499
 4404/10000 [============>.................] - ETA: 1:22:59 - loss: 0.4074 - regression_loss: 0.3575 - classification_loss: 0.0498
 4405/10000 [============>.................] - ETA: 1:22:59 - loss: 0.4073 - regression_loss: 0.3575 - classification_loss: 0.0498
 4406/10000 [============>.................] - ETA: 1:22:58 - loss: 0.4073 - regression_loss: 0.3575 - classification_loss: 0.0498
 4407/10000 [============>.................] - ETA: 1:22:57 - loss: 0.4072 - regression_loss: 0.3574 - classification_loss: 0.0498
 4408/10000 [============>.................] - ETA: 1:22:56 - loss: 0.4073 - regression_loss: 0.3575 - classification_loss: 0.0498
 4409/10000 [============>.................] - ETA: 1:22:55 - loss: 0.4073 - regression_loss: 0.3574 - classification_loss: 0.0498
 4410/10000 [============>.................] - ETA: 1:22:54 - loss: 0.4073 - regression_loss: 0.3574 - classification_loss: 0.0498
 4411/10000 [============>.................] - ETA: 1:22:53 - loss: 0.4073 - regression_loss: 0.3574 - classification_loss: 0.0498
 4412/10000 [============>.................] - ETA: 1:22:52 - loss: 0.4072 - regression_loss: 0.3574 - classification_loss: 0.0498
 4413/10000 [============>.................] - ETA: 1:22:51 - loss: 0.4072 - regression_loss: 0.3574 - classification_loss: 0.0498
 4414/10000 [============>.................] - ETA: 1:22:51 - loss: 0.4074 - regression_loss: 0.3575 - classification_loss: 0.0498
 4415/10000 [============>.................] - ETA: 1:22:50 - loss: 0.4074 - regression_loss: 0.3576 - classification_loss: 0.0498
 4416/10000 [============>.................] - ETA: 1:22:49 - loss: 0.4074 - regression_loss: 0.3576 - classification_loss: 0.0498
 4417/10000 [============>.................] - ETA: 1:22:48 - loss: 0.4074 - regression_loss: 0.3576 - classification_loss: 0.0498
 4418/10000 [============>.................] - ETA: 1:22:47 - loss: 0.4073 - regression_loss: 0.3576 - classification_loss: 0.0498
 4419/10000 [============>.................] - ETA: 1:22:46 - loss: 0.4074 - regression_loss: 0.3576 - classification_loss: 0.0498
 4420/10000 [============>.................] - ETA: 1:22:45 - loss: 0.4074 - regression_loss: 0.3576 - classification_loss: 0.0498
 4421/10000 [============>.................] - ETA: 1:22:44 - loss: 0.4075 - regression_loss: 0.3577 - classification_loss: 0.0498
 4422/10000 [============>.................] - ETA: 1:22:43 - loss: 0.4075 - regression_loss: 0.3577 - classification_loss: 0.0498
 4423/10000 [============>.................] - ETA: 1:22:42 - loss: 0.4078 - regression_loss: 0.3580 - classification_loss: 0.0498
 4424/10000 [============>.................] - ETA: 1:22:42 - loss: 0.4077 - regression_loss: 0.3579 - classification_loss: 0.0498
 4425/10000 [============>.................] - ETA: 1:22:41 - loss: 0.4078 - regression_loss: 0.3580 - classification_loss: 0.0498
 4426/10000 [============>.................] - ETA: 1:22:40 - loss: 0.4079 - regression_loss: 0.3581 - classification_loss: 0.0498
 4427/10000 [============>.................] - ETA: 1:22:39 - loss: 0.4078 - regression_loss: 0.3580 - classification_loss: 0.0498
 4428/10000 [============>.................] - ETA: 1:22:38 - loss: 0.4078 - regression_loss: 0.3580 - classification_loss: 0.0498
 4429/10000 [============>.................] - ETA: 1:22:37 - loss: 0.4078 - regression_loss: 0.3580 - classification_loss: 0.0498
 4430/10000 [============>.................] - ETA: 1:22:36 - loss: 0.4077 - regression_loss: 0.3579 - classification_loss: 0.0498
 4431/10000 [============>.................] - ETA: 1:22:35 - loss: 0.4077 - regression_loss: 0.3579 - classification_loss: 0.0498
 4432/10000 [============>.................] - ETA: 1:22:34 - loss: 0.4077 - regression_loss: 0.3579 - classification_loss: 0.0498
 4433/10000 [============>.................] - ETA: 1:22:34 - loss: 0.4076 - regression_loss: 0.3578 - classification_loss: 0.0498
 4434/10000 [============>.................] - ETA: 1:22:33 - loss: 0.4076 - regression_loss: 0.3578 - classification_loss: 0.0498
 4435/10000 [============>.................] - ETA: 1:22:32 - loss: 0.4076 - regression_loss: 0.3578 - classification_loss: 0.0498
 4436/10000 [============>.................] - ETA: 1:22:31 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0497
 4437/10000 [============>.................] - ETA: 1:22:30 - loss: 0.4075 - regression_loss: 0.3577 - classification_loss: 0.0497
 4438/10000 [============>.................] - ETA: 1:22:29 - loss: 0.4075 - regression_loss: 0.3577 - classification_loss: 0.0497
 4439/10000 [============>.................] - ETA: 1:22:28 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0497
 4440/10000 [============>.................] - ETA: 1:22:27 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0497
 4441/10000 [============>.................] - ETA: 1:22:26 - loss: 0.4076 - regression_loss: 0.3579 - classification_loss: 0.0497
 4442/10000 [============>.................] - ETA: 1:22:26 - loss: 0.4077 - regression_loss: 0.3580 - classification_loss: 0.0497
 4443/10000 [============>.................] - ETA: 1:22:25 - loss: 0.4077 - regression_loss: 0.3580 - classification_loss: 0.0497
 4444/10000 [============>.................] - ETA: 1:22:24 - loss: 0.4078 - regression_loss: 0.3580 - classification_loss: 0.0497
 4445/10000 [============>.................] - ETA: 1:22:23 - loss: 0.4077 - regression_loss: 0.3580 - classification_loss: 0.0497
 4446/10000 [============>.................] - ETA: 1:22:22 - loss: 0.4078 - regression_loss: 0.3580 - classification_loss: 0.0497
 4447/10000 [============>.................] - ETA: 1:22:21 - loss: 0.4079 - regression_loss: 0.3581 - classification_loss: 0.0497
 4448/10000 [============>.................] - ETA: 1:22:20 - loss: 0.4079 - regression_loss: 0.3581 - classification_loss: 0.0497
 4449/10000 [============>.................] - ETA: 1:22:19 - loss: 0.4080 - regression_loss: 0.3583 - classification_loss: 0.0497
 4450/10000 [============>.................] - ETA: 1:22:18 - loss: 0.4081 - regression_loss: 0.3583 - classification_loss: 0.0497
 4451/10000 [============>.................] - ETA: 1:22:17 - loss: 0.4081 - regression_loss: 0.3584 - classification_loss: 0.0497
 4452/10000 [============>.................] - ETA: 1:22:17 - loss: 0.4082 - regression_loss: 0.3585 - classification_loss: 0.0497
 4453/10000 [============>.................] - ETA: 1:22:16 - loss: 0.4081 - regression_loss: 0.3584 - classification_loss: 0.0497
 4454/10000 [============>.................] - ETA: 1:22:15 - loss: 0.4080 - regression_loss: 0.3583 - classification_loss: 0.0497
 4455/10000 [============>.................] - ETA: 1:22:14 - loss: 0.4079 - regression_loss: 0.3582 - classification_loss: 0.0497
 4456/10000 [============>.................] - ETA: 1:22:13 - loss: 0.4079 - regression_loss: 0.3582 - classification_loss: 0.0497
 4457/10000 [============>.................] - ETA: 1:22:12 - loss: 0.4079 - regression_loss: 0.3582 - classification_loss: 0.0497
 4458/10000 [============>.................] - ETA: 1:22:11 - loss: 0.4079 - regression_loss: 0.3582 - classification_loss: 0.0497
 4459/10000 [============>.................] - ETA: 1:22:10 - loss: 0.4078 - regression_loss: 0.3581 - classification_loss: 0.0497
 4460/10000 [============>.................] - ETA: 1:22:09 - loss: 0.4078 - regression_loss: 0.3581 - classification_loss: 0.0497
 4461/10000 [============>.................] - ETA: 1:22:09 - loss: 0.4078 - regression_loss: 0.3581 - classification_loss: 0.0497
 4462/10000 [============>.................] - ETA: 1:22:08 - loss: 0.4078 - regression_loss: 0.3581 - classification_loss: 0.0497
 4463/10000 [============>.................] - ETA: 1:22:07 - loss: 0.4078 - regression_loss: 0.3582 - classification_loss: 0.0497
 4464/10000 [============>.................] - ETA: 1:22:06 - loss: 0.4078 - regression_loss: 0.3582 - classification_loss: 0.0497
 4465/10000 [============>.................] - ETA: 1:22:05 - loss: 0.4078 - regression_loss: 0.3581 - classification_loss: 0.0497
 4466/10000 [============>.................] - ETA: 1:22:04 - loss: 0.4078 - regression_loss: 0.3581 - classification_loss: 0.0497
 4467/10000 [============>.................] - ETA: 1:22:03 - loss: 0.4077 - regression_loss: 0.3580 - classification_loss: 0.0497
 4468/10000 [============>.................] - ETA: 1:22:02 - loss: 0.4077 - regression_loss: 0.3580 - classification_loss: 0.0497
 4469/10000 [============>.................] - ETA: 1:22:01 - loss: 0.4077 - regression_loss: 0.3580 - classification_loss: 0.0497
 4470/10000 [============>.................] - ETA: 1:22:01 - loss: 0.4076 - regression_loss: 0.3580 - classification_loss: 0.0497
 4471/10000 [============>.................] - ETA: 1:22:00 - loss: 0.4075 - regression_loss: 0.3579 - classification_loss: 0.0497
 4472/10000 [============>.................] - ETA: 1:21:59 - loss: 0.4075 - regression_loss: 0.3579 - classification_loss: 0.0496
 4473/10000 [============>.................] - ETA: 1:21:58 - loss: 0.4075 - regression_loss: 0.3579 - classification_loss: 0.0496
 4474/10000 [============>.................] - ETA: 1:21:57 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0496
 4475/10000 [============>.................] - ETA: 1:21:56 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0496
 4476/10000 [============>.................] - ETA: 1:21:55 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0496
 4477/10000 [============>.................] - ETA: 1:21:54 - loss: 0.4074 - regression_loss: 0.3578 - classification_loss: 0.0496
 4478/10000 [============>.................] - ETA: 1:21:53 - loss: 0.4074 - regression_loss: 0.3578 - classification_loss: 0.0496
 4479/10000 [============>.................] - ETA: 1:21:52 - loss: 0.4073 - regression_loss: 0.3577 - classification_loss: 0.0496
 4480/10000 [============>.................] - ETA: 1:21:52 - loss: 0.4073 - regression_loss: 0.3577 - classification_loss: 0.0496
 4481/10000 [============>.................] - ETA: 1:21:51 - loss: 0.4072 - regression_loss: 0.3576 - classification_loss: 0.0496
 4482/10000 [============>.................] - ETA: 1:21:50 - loss: 0.4072 - regression_loss: 0.3576 - classification_loss: 0.0496
 4483/10000 [============>.................] - ETA: 1:21:49 - loss: 0.4073 - regression_loss: 0.3577 - classification_loss: 0.0496
 4484/10000 [============>.................] - ETA: 1:21:48 - loss: 0.4072 - regression_loss: 0.3576 - classification_loss: 0.0496
 4485/10000 [============>.................] - ETA: 1:21:47 - loss: 0.4071 - regression_loss: 0.3575 - classification_loss: 0.0496
 4486/10000 [============>.................] - ETA: 1:21:46 - loss: 0.4071 - regression_loss: 0.3575 - classification_loss: 0.0496
 4487/10000 [============>.................] - ETA: 1:21:45 - loss: 0.4071 - regression_loss: 0.3575 - classification_loss: 0.0496
 4488/10000 [============>.................] - ETA: 1:21:44 - loss: 0.4071 - regression_loss: 0.3575 - classification_loss: 0.0496
 4489/10000 [============>.................] - ETA: 1:21:44 - loss: 0.4071 - regression_loss: 0.3575 - classification_loss: 0.0496
 4490/10000 [============>.................] - ETA: 1:21:43 - loss: 0.4071 - regression_loss: 0.3575 - classification_loss: 0.0496
 4491/10000 [============>.................] - ETA: 1:21:42 - loss: 0.4071 - regression_loss: 0.3575 - classification_loss: 0.0496
 4492/10000 [============>.................] - ETA: 1:21:41 - loss: 0.4070 - regression_loss: 0.3574 - classification_loss: 0.0496
 4493/10000 [============>.................] - ETA: 1:21:40 - loss: 0.4070 - regression_loss: 0.3574 - classification_loss: 0.0496
 4494/10000 [============>.................] - ETA: 1:21:39 - loss: 0.4070 - regression_loss: 0.3574 - classification_loss: 0.0496
 4495/10000 [============>.................] - ETA: 1:21:38 - loss: 0.4070 - regression_loss: 0.3575 - classification_loss: 0.0496
 4496/10000 [============>.................] - ETA: 1:21:37 - loss: 0.4070 - regression_loss: 0.3574 - classification_loss: 0.0496
 4497/10000 [============>.................] - ETA: 1:21:36 - loss: 0.4071 - regression_loss: 0.3575 - classification_loss: 0.0496
 4498/10000 [============>.................] - ETA: 1:21:36 - loss: 0.4070 - regression_loss: 0.3575 - classification_loss: 0.0496
 4499/10000 [============>.................] - ETA: 1:21:35 - loss: 0.4072 - regression_loss: 0.3575 - classification_loss: 0.0496
 4500/10000 [============>.................] - ETA: 1:21:34 - loss: 0.4072 - regression_loss: 0.3575 - classification_loss: 0.0496
 4501/10000 [============>.................] - ETA: 1:21:33 - loss: 0.4071 - regression_loss: 0.3575 - classification_loss: 0.0496
 4502/10000 [============>.................] - ETA: 1:21:32 - loss: 0.4071 - regression_loss: 0.3575 - classification_loss: 0.0496
 4503/10000 [============>.................] - ETA: 1:21:31 - loss: 0.4071 - regression_loss: 0.3575 - classification_loss: 0.0496
 4504/10000 [============>.................] - ETA: 1:21:30 - loss: 0.4071 - regression_loss: 0.3575 - classification_loss: 0.0496
 4505/10000 [============>.................] - ETA: 1:21:29 - loss: 0.4071 - regression_loss: 0.3575 - classification_loss: 0.0496
 4506/10000 [============>.................] - ETA: 1:21:28 - loss: 0.4072 - regression_loss: 0.3575 - classification_loss: 0.0497
 4507/10000 [============>.................] - ETA: 1:21:27 - loss: 0.4071 - regression_loss: 0.3574 - classification_loss: 0.0497
 4508/10000 [============>.................] - ETA: 1:21:27 - loss: 0.4071 - regression_loss: 0.3574 - classification_loss: 0.0497
 4509/10000 [============>.................] - ETA: 1:21:26 - loss: 0.4072 - regression_loss: 0.3575 - classification_loss: 0.0497
 4510/10000 [============>.................] - ETA: 1:21:25 - loss: 0.4072 - regression_loss: 0.3575 - classification_loss: 0.0497
 4511/10000 [============>.................] - ETA: 1:21:24 - loss: 0.4072 - regression_loss: 0.3576 - classification_loss: 0.0497
 4512/10000 [============>.................] - ETA: 1:21:23 - loss: 0.4072 - regression_loss: 0.3575 - classification_loss: 0.0497
 4513/10000 [============>.................] - ETA: 1:21:22 - loss: 0.4072 - regression_loss: 0.3575 - classification_loss: 0.0497
 4514/10000 [============>.................] - ETA: 1:21:21 - loss: 0.4071 - regression_loss: 0.3574 - classification_loss: 0.0497
 4515/10000 [============>.................] - ETA: 1:21:20 - loss: 0.4072 - regression_loss: 0.3575 - classification_loss: 0.0497
 4516/10000 [============>.................] - ETA: 1:21:19 - loss: 0.4073 - regression_loss: 0.3576 - classification_loss: 0.0497
 4517/10000 [============>.................] - ETA: 1:21:19 - loss: 0.4073 - regression_loss: 0.3576 - classification_loss: 0.0497
 4518/10000 [============>.................] - ETA: 1:21:18 - loss: 0.4072 - regression_loss: 0.3576 - classification_loss: 0.0497
 4519/10000 [============>.................] - ETA: 1:21:17 - loss: 0.4073 - regression_loss: 0.3576 - classification_loss: 0.0497
 4520/10000 [============>.................] - ETA: 1:21:16 - loss: 0.4073 - regression_loss: 0.3577 - classification_loss: 0.0497
 4521/10000 [============>.................] - ETA: 1:21:15 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0497
 4522/10000 [============>.................] - ETA: 1:21:14 - loss: 0.4076 - regression_loss: 0.3578 - classification_loss: 0.0497
 4523/10000 [============>.................] - ETA: 1:21:13 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0497
 4524/10000 [============>.................] - ETA: 1:21:12 - loss: 0.4074 - regression_loss: 0.3577 - classification_loss: 0.0497
 4525/10000 [============>.................] - ETA: 1:21:11 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0497
 4526/10000 [============>.................] - ETA: 1:21:11 - loss: 0.4074 - regression_loss: 0.3577 - classification_loss: 0.0497
 4527/10000 [============>.................] - ETA: 1:21:10 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0497
 4528/10000 [============>.................] - ETA: 1:21:09 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0497
 4529/10000 [============>.................] - ETA: 1:21:08 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0497
 4530/10000 [============>.................] - ETA: 1:21:07 - loss: 0.4075 - regression_loss: 0.3577 - classification_loss: 0.0497
 4531/10000 [============>.................] - ETA: 1:21:06 - loss: 0.4075 - regression_loss: 0.3577 - classification_loss: 0.0498
 4532/10000 [============>.................] - ETA: 1:21:05 - loss: 0.4076 - regression_loss: 0.3578 - classification_loss: 0.0498
 4533/10000 [============>.................] - ETA: 1:21:04 - loss: 0.4076 - regression_loss: 0.3578 - classification_loss: 0.0498
 4534/10000 [============>.................] - ETA: 1:21:03 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0498
 4535/10000 [============>.................] - ETA: 1:21:03 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0498
 4536/10000 [============>.................] - ETA: 1:21:02 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0498
 4537/10000 [============>.................] - ETA: 1:21:01 - loss: 0.4075 - regression_loss: 0.3577 - classification_loss: 0.0497
 4538/10000 [============>.................] - ETA: 1:21:00 - loss: 0.4074 - regression_loss: 0.3577 - classification_loss: 0.0497
 4539/10000 [============>.................] - ETA: 1:20:59 - loss: 0.4074 - regression_loss: 0.3576 - classification_loss: 0.0497
 4540/10000 [============>.................] - ETA: 1:20:58 - loss: 0.4074 - regression_loss: 0.3577 - classification_loss: 0.0497
 4541/10000 [============>.................] - ETA: 1:20:57 - loss: 0.4074 - regression_loss: 0.3577 - classification_loss: 0.0497
 4542/10000 [============>.................] - ETA: 1:20:56 - loss: 0.4074 - regression_loss: 0.3577 - classification_loss: 0.0497
 4543/10000 [============>.................] - ETA: 1:20:55 - loss: 0.4074 - regression_loss: 0.3577 - classification_loss: 0.0497
 4544/10000 [============>.................] - ETA: 1:20:54 - loss: 0.4074 - regression_loss: 0.3577 - classification_loss: 0.0498
 4545/10000 [============>.................] - ETA: 1:20:54 - loss: 0.4074 - regression_loss: 0.3576 - classification_loss: 0.0498
 4546/10000 [============>.................] - ETA: 1:20:53 - loss: 0.4074 - regression_loss: 0.3576 - classification_loss: 0.0498
 4547/10000 [============>.................] - ETA: 1:20:52 - loss: 0.4074 - regression_loss: 0.3576 - classification_loss: 0.0498
 4548/10000 [============>.................] - ETA: 1:20:51 - loss: 0.4074 - regression_loss: 0.3576 - classification_loss: 0.0498
 4549/10000 [============>.................] - ETA: 1:20:50 - loss: 0.4074 - regression_loss: 0.3576 - classification_loss: 0.0498
 4550/10000 [============>.................] - ETA: 1:20:49 - loss: 0.4074 - regression_loss: 0.3576 - classification_loss: 0.0498
 4551/10000 [============>.................] - ETA: 1:20:48 - loss: 0.4075 - regression_loss: 0.3577 - classification_loss: 0.0498
 4552/10000 [============>.................] - ETA: 1:20:47 - loss: 0.4074 - regression_loss: 0.3576 - classification_loss: 0.0498
 4553/10000 [============>.................] - ETA: 1:20:46 - loss: 0.4074 - regression_loss: 0.3576 - classification_loss: 0.0498
 4554/10000 [============>.................] - ETA: 1:20:46 - loss: 0.4074 - regression_loss: 0.3576 - classification_loss: 0.0498
 4555/10000 [============>.................] - ETA: 1:20:45 - loss: 0.4073 - regression_loss: 0.3576 - classification_loss: 0.0498
 4556/10000 [============>.................] - ETA: 1:20:44 - loss: 0.4073 - regression_loss: 0.3575 - classification_loss: 0.0498
 4557/10000 [============>.................] - ETA: 1:20:43 - loss: 0.4073 - regression_loss: 0.3575 - classification_loss: 0.0498
 4558/10000 [============>.................] - ETA: 1:20:42 - loss: 0.4072 - regression_loss: 0.3575 - classification_loss: 0.0498
 4559/10000 [============>.................] - ETA: 1:20:41 - loss: 0.4072 - regression_loss: 0.3575 - classification_loss: 0.0497
 4560/10000 [============>.................] - ETA: 1:20:40 - loss: 0.4072 - regression_loss: 0.3574 - classification_loss: 0.0497
 4561/10000 [============>.................] - ETA: 1:20:39 - loss: 0.4072 - regression_loss: 0.3574 - classification_loss: 0.0497
 4562/10000 [============>.................] - ETA: 1:20:38 - loss: 0.4072 - regression_loss: 0.3575 - classification_loss: 0.0497
 4563/10000 [============>.................] - ETA: 1:20:38 - loss: 0.4073 - regression_loss: 0.3575 - classification_loss: 0.0498
 4564/10000 [============>.................] - ETA: 1:20:37 - loss: 0.4072 - regression_loss: 0.3575 - classification_loss: 0.0497
 4565/10000 [============>.................] - ETA: 1:20:36 - loss: 0.4072 - regression_loss: 0.3575 - classification_loss: 0.0497
 4566/10000 [============>.................] - ETA: 1:20:35 - loss: 0.4073 - regression_loss: 0.3576 - classification_loss: 0.0498
 4567/10000 [============>.................] - ETA: 1:20:34 - loss: 0.4072 - regression_loss: 0.3575 - classification_loss: 0.0497
 4568/10000 [============>.................] - ETA: 1:20:33 - loss: 0.4073 - regression_loss: 0.3575 - classification_loss: 0.0497
 4569/10000 [============>.................] - ETA: 1:20:32 - loss: 0.4073 - regression_loss: 0.3575 - classification_loss: 0.0497
 4570/10000 [============>.................] - ETA: 1:20:31 - loss: 0.4072 - regression_loss: 0.3574 - classification_loss: 0.0497
 4571/10000 [============>.................] - ETA: 1:20:30 - loss: 0.4071 - regression_loss: 0.3574 - classification_loss: 0.0498
 4572/10000 [============>.................] - ETA: 1:20:30 - loss: 0.4071 - regression_loss: 0.3574 - classification_loss: 0.0497
 4573/10000 [============>.................] - ETA: 1:20:29 - loss: 0.4071 - regression_loss: 0.3574 - classification_loss: 0.0497
 4574/10000 [============>.................] - ETA: 1:20:28 - loss: 0.4072 - regression_loss: 0.3574 - classification_loss: 0.0497
 4575/10000 [============>.................] - ETA: 1:20:27 - loss: 0.4071 - regression_loss: 0.3574 - classification_loss: 0.0497
 4576/10000 [============>.................] - ETA: 1:20:26 - loss: 0.4071 - regression_loss: 0.3574 - classification_loss: 0.0497
 4577/10000 [============>.................] - ETA: 1:20:25 - loss: 0.4073 - regression_loss: 0.3575 - classification_loss: 0.0498
 4578/10000 [============>.................] - ETA: 1:20:24 - loss: 0.4073 - regression_loss: 0.3575 - classification_loss: 0.0497
 4579/10000 [============>.................] - ETA: 1:20:23 - loss: 0.4073 - regression_loss: 0.3575 - classification_loss: 0.0497
 4580/10000 [============>.................] - ETA: 1:20:22 - loss: 0.4073 - regression_loss: 0.3576 - classification_loss: 0.0497
 4581/10000 [============>.................] - ETA: 1:20:21 - loss: 0.4073 - regression_loss: 0.3576 - classification_loss: 0.0497
 4582/10000 [============>.................] - ETA: 1:20:21 - loss: 0.4073 - regression_loss: 0.3575 - classification_loss: 0.0497
 4583/10000 [============>.................] - ETA: 1:20:20 - loss: 0.4073 - regression_loss: 0.3575 - classification_loss: 0.0497
 4584/10000 [============>.................] - ETA: 1:20:19 - loss: 0.4073 - regression_loss: 0.3576 - classification_loss: 0.0497
 4585/10000 [============>.................] - ETA: 1:20:18 - loss: 0.4073 - regression_loss: 0.3576 - classification_loss: 0.0497
 4586/10000 [============>.................] - ETA: 1:20:17 - loss: 0.4073 - regression_loss: 0.3576 - classification_loss: 0.0497
 4587/10000 [============>.................] - ETA: 1:20:16 - loss: 0.4073 - regression_loss: 0.3576 - classification_loss: 0.0497
 4588/10000 [============>.................] - ETA: 1:20:15 - loss: 0.4073 - regression_loss: 0.3576 - classification_loss: 0.0497
 4589/10000 [============>.................] - ETA: 1:20:14 - loss: 0.4073 - regression_loss: 0.3576 - classification_loss: 0.0497
 4590/10000 [============>.................] - ETA: 1:20:13 - loss: 0.4072 - regression_loss: 0.3575 - classification_loss: 0.0497
 4591/10000 [============>.................] - ETA: 1:20:13 - loss: 0.4072 - regression_loss: 0.3576 - classification_loss: 0.0497
 4592/10000 [============>.................] - ETA: 1:20:12 - loss: 0.4072 - regression_loss: 0.3575 - classification_loss: 0.0497
 4593/10000 [============>.................] - ETA: 1:20:11 - loss: 0.4072 - regression_loss: 0.3575 - classification_loss: 0.0497
 4594/10000 [============>.................] - ETA: 1:20:10 - loss: 0.4072 - regression_loss: 0.3575 - classification_loss: 0.0497
 4595/10000 [============>.................] - ETA: 1:20:09 - loss: 0.4072 - regression_loss: 0.3575 - classification_loss: 0.0497
 4596/10000 [============>.................] - ETA: 1:20:08 - loss: 0.4073 - regression_loss: 0.3576 - classification_loss: 0.0497
 4597/10000 [============>.................] - ETA: 1:20:07 - loss: 0.4073 - regression_loss: 0.3576 - classification_loss: 0.0497
 4598/10000 [============>.................] - ETA: 1:20:06 - loss: 0.4077 - regression_loss: 0.3579 - classification_loss: 0.0498
 4599/10000 [============>.................] - ETA: 1:20:05 - loss: 0.4076 - regression_loss: 0.3579 - classification_loss: 0.0497
 4600/10000 [============>.................] - ETA: 1:20:05 - loss: 0.4076 - regression_loss: 0.3578 - classification_loss: 0.0497
 4601/10000 [============>.................] - ETA: 1:20:04 - loss: 0.4076 - regression_loss: 0.3578 - classification_loss: 0.0497
 4602/10000 [============>.................] - ETA: 1:20:03 - loss: 0.4076 - regression_loss: 0.3578 - classification_loss: 0.0497
 4603/10000 [============>.................] - ETA: 1:20:02 - loss: 0.4076 - regression_loss: 0.3578 - classification_loss: 0.0498
 4604/10000 [============>.................] - ETA: 1:20:01 - loss: 0.4076 - regression_loss: 0.3579 - classification_loss: 0.0498
 4605/10000 [============>.................] - ETA: 1:20:00 - loss: 0.4078 - regression_loss: 0.3580 - classification_loss: 0.0498
 4606/10000 [============>.................] - ETA: 1:19:59 - loss: 0.4077 - regression_loss: 0.3579 - classification_loss: 0.0498
 4607/10000 [============>.................] - ETA: 1:19:58 - loss: 0.4076 - regression_loss: 0.3579 - classification_loss: 0.0497
 4608/10000 [============>.................] - ETA: 1:19:57 - loss: 0.4077 - regression_loss: 0.3580 - classification_loss: 0.0498
 4609/10000 [============>.................] - ETA: 1:19:56 - loss: 0.4077 - regression_loss: 0.3579 - classification_loss: 0.0498
 4610/10000 [============>.................] - ETA: 1:19:56 - loss: 0.4077 - regression_loss: 0.3579 - classification_loss: 0.0498
 4611/10000 [============>.................] - ETA: 1:19:55 - loss: 0.4076 - regression_loss: 0.3579 - classification_loss: 0.0498
 4612/10000 [============>.................] - ETA: 1:19:54 - loss: 0.4076 - regression_loss: 0.3579 - classification_loss: 0.0498
 4613/10000 [============>.................] - ETA: 1:19:53 - loss: 0.4077 - regression_loss: 0.3579 - classification_loss: 0.0498
 4614/10000 [============>.................] - ETA: 1:19:52 - loss: 0.4077 - regression_loss: 0.3579 - classification_loss: 0.0498
 4615/10000 [============>.................] - ETA: 1:19:51 - loss: 0.4078 - regression_loss: 0.3580 - classification_loss: 0.0498
 4616/10000 [============>.................] - ETA: 1:19:50 - loss: 0.4077 - regression_loss: 0.3579 - classification_loss: 0.0498
 4617/10000 [============>.................] - ETA: 1:19:49 - loss: 0.4076 - regression_loss: 0.3578 - classification_loss: 0.0498
 4618/10000 [============>.................] - ETA: 1:19:48 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0497
 4619/10000 [============>.................] - ETA: 1:19:48 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0497
 4620/10000 [============>.................] - ETA: 1:19:47 - loss: 0.4076 - regression_loss: 0.3578 - classification_loss: 0.0497
 4621/10000 [============>.................] - ETA: 1:19:46 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0497
 4622/10000 [============>.................] - ETA: 1:19:45 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0497
 4623/10000 [============>.................] - ETA: 1:19:44 - loss: 0.4075 - regression_loss: 0.3577 - classification_loss: 0.0497
 4624/10000 [============>.................] - ETA: 1:19:43 - loss: 0.4075 - regression_loss: 0.3577 - classification_loss: 0.0497
 4625/10000 [============>.................] - ETA: 1:19:42 - loss: 0.4075 - regression_loss: 0.3577 - classification_loss: 0.0497
 4626/10000 [============>.................] - ETA: 1:19:41 - loss: 0.4074 - regression_loss: 0.3577 - classification_loss: 0.0497
 4627/10000 [============>.................] - ETA: 1:19:40 - loss: 0.4074 - regression_loss: 0.3577 - classification_loss: 0.0497
 4628/10000 [============>.................] - ETA: 1:19:40 - loss: 0.4074 - regression_loss: 0.3577 - classification_loss: 0.0497
 4629/10000 [============>.................] - ETA: 1:19:39 - loss: 0.4074 - regression_loss: 0.3577 - classification_loss: 0.0497
 4630/10000 [============>.................] - ETA: 1:19:38 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0497
 4631/10000 [============>.................] - ETA: 1:19:37 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0497
 4632/10000 [============>.................] - ETA: 1:19:36 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0497
 4633/10000 [============>.................] - ETA: 1:19:35 - loss: 0.4074 - regression_loss: 0.3578 - classification_loss: 0.0497
 4634/10000 [============>.................] - ETA: 1:19:34 - loss: 0.4074 - regression_loss: 0.3577 - classification_loss: 0.0497
 4635/10000 [============>.................] - ETA: 1:19:33 - loss: 0.4074 - regression_loss: 0.3577 - classification_loss: 0.0497
 4636/10000 [============>.................] - ETA: 1:19:32 - loss: 0.4074 - regression_loss: 0.3578 - classification_loss: 0.0497
 4637/10000 [============>.................] - ETA: 1:19:32 - loss: 0.4074 - regression_loss: 0.3577 - classification_loss: 0.0496
 4638/10000 [============>.................] - ETA: 1:19:31 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0497
 4639/10000 [============>.................] - ETA: 1:19:30 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0497
 4640/10000 [============>.................] - ETA: 1:19:29 - loss: 0.4076 - regression_loss: 0.3579 - classification_loss: 0.0497
 4641/10000 [============>.................] - ETA: 1:19:28 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0497
 4642/10000 [============>.................] - ETA: 1:19:27 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0497
 4643/10000 [============>.................] - ETA: 1:19:26 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0497
 4644/10000 [============>.................] - ETA: 1:19:25 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0497
 4645/10000 [============>.................] - ETA: 1:19:24 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0497
 4646/10000 [============>.................] - ETA: 1:19:23 - loss: 0.4074 - regression_loss: 0.3578 - classification_loss: 0.0497
 4647/10000 [============>.................] - ETA: 1:19:23 - loss: 0.4074 - regression_loss: 0.3577 - classification_loss: 0.0497
 4648/10000 [============>.................] - ETA: 1:19:22 - loss: 0.4074 - regression_loss: 0.3577 - classification_loss: 0.0497
 4649/10000 [============>.................] - ETA: 1:19:21 - loss: 0.4073 - regression_loss: 0.3577 - classification_loss: 0.0497
 4650/10000 [============>.................] - ETA: 1:19:20 - loss: 0.4073 - regression_loss: 0.3577 - classification_loss: 0.0497
 4651/10000 [============>.................] - ETA: 1:19:19 - loss: 0.4072 - regression_loss: 0.3576 - classification_loss: 0.0496
 4652/10000 [============>.................] - ETA: 1:19:18 - loss: 0.4073 - regression_loss: 0.3577 - classification_loss: 0.0497
 4653/10000 [============>.................] - ETA: 1:19:17 - loss: 0.4074 - regression_loss: 0.3577 - classification_loss: 0.0497
 4654/10000 [============>.................] - ETA: 1:19:16 - loss: 0.4073 - regression_loss: 0.3576 - classification_loss: 0.0497
 4655/10000 [============>.................] - ETA: 1:19:15 - loss: 0.4074 - regression_loss: 0.3577 - classification_loss: 0.0497
 4656/10000 [============>.................] - ETA: 1:19:15 - loss: 0.4074 - regression_loss: 0.3577 - classification_loss: 0.0496
 4657/10000 [============>.................] - ETA: 1:19:14 - loss: 0.4074 - regression_loss: 0.3577 - classification_loss: 0.0497
 4658/10000 [============>.................] - ETA: 1:19:13 - loss: 0.4074 - regression_loss: 0.3578 - classification_loss: 0.0497
 4659/10000 [============>.................] - ETA: 1:19:12 - loss: 0.4074 - regression_loss: 0.3577 - classification_loss: 0.0497
 4660/10000 [============>.................] - ETA: 1:19:11 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0497
 4661/10000 [============>.................] - ETA: 1:19:10 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0497
 4662/10000 [============>.................] - ETA: 1:19:09 - loss: 0.4074 - regression_loss: 0.3577 - classification_loss: 0.0497
 4663/10000 [============>.................] - ETA: 1:19:08 - loss: 0.4074 - regression_loss: 0.3577 - classification_loss: 0.0496
 4664/10000 [============>.................] - ETA: 1:19:07 - loss: 0.4074 - regression_loss: 0.3578 - classification_loss: 0.0496
 4665/10000 [============>.................] - ETA: 1:19:07 - loss: 0.4074 - regression_loss: 0.3577 - classification_loss: 0.0496
 4666/10000 [============>.................] - ETA: 1:19:06 - loss: 0.4074 - regression_loss: 0.3578 - classification_loss: 0.0496
 4667/10000 [=============>................] - ETA: 1:19:05 - loss: 0.4074 - regression_loss: 0.3578 - classification_loss: 0.0496
 4668/10000 [=============>................] - ETA: 1:19:04 - loss: 0.4074 - regression_loss: 0.3577 - classification_loss: 0.0496
 4669/10000 [=============>................] - ETA: 1:19:03 - loss: 0.4074 - regression_loss: 0.3578 - classification_loss: 0.0496
 4670/10000 [=============>................] - ETA: 1:19:02 - loss: 0.4073 - regression_loss: 0.3577 - classification_loss: 0.0496
 4671/10000 [=============>................] - ETA: 1:19:01 - loss: 0.4072 - regression_loss: 0.3577 - classification_loss: 0.0496
 4672/10000 [=============>................] - ETA: 1:19:00 - loss: 0.4073 - regression_loss: 0.3577 - classification_loss: 0.0496
 4673/10000 [=============>................] - ETA: 1:18:59 - loss: 0.4073 - regression_loss: 0.3577 - classification_loss: 0.0496
 4674/10000 [=============>................] - ETA: 1:18:59 - loss: 0.4074 - regression_loss: 0.3577 - classification_loss: 0.0497
 4675/10000 [=============>................] - ETA: 1:18:58 - loss: 0.4074 - regression_loss: 0.3578 - classification_loss: 0.0497
 4676/10000 [=============>................] - ETA: 1:18:57 - loss: 0.4075 - regression_loss: 0.3578 - classification_loss: 0.0496
 4677/10000 [=============>................] - ETA: 1:18:56 - loss: 0.4074 - regression_loss: 0.3578 - classification_loss: 0.0496
 4678/10000 [=============>................] - ETA: 1:18:55 - loss: 0.4073 - regression_loss: 0.3577 - classification_loss: 0.0496
 4679/10000 [=============>................] - ETA: 1:18:54 - loss: 0.4073 - regression_loss: 0.3577 - classification_loss: 0.0496
 4680/10000 [=============>................] - ETA: 1:18:53 - loss: 0.4073 - regression_loss: 0.3577 - classification_loss: 0.0496
 4681/10000 [=============>................] - ETA: 1:18:52 - loss: 0.4072 - regression_loss: 0.3576 - classification_loss: 0.0496
 4682/10000 [=============>................] - ETA: 1:18:51 - loss: 0.4071 - regression_loss: 0.3575 - classification_loss: 0.0496
 4683/10000 [=============>................] - ETA: 1:18:50 - loss: 0.4073 - regression_loss: 0.3576 - classification_loss: 0.0496
 4684/10000 [=============>................] - ETA: 1:18:50 - loss: 0.4079 - regression_loss: 0.3580 - classification_loss: 0.0499
 4685/10000 [=============>................] - ETA: 1:18:49 - loss: 0.4079 - regression_loss: 0.3580 - classification_loss: 0.0499
 4686/10000 [=============>................] - ETA: 1:18:48 - loss: 0.4078 - regression_loss: 0.3579 - classification_loss: 0.0499
 4687/10000 [=============>................] - ETA: 1:18:47 - loss: 0.4079 - regression_loss: 0.3580 - classification_loss: 0.0499
 4688/10000 [=============>................] - ETA: 1:18:46 - loss: 0.4080 - regression_loss: 0.3581 - classification_loss: 0.0499
 4689/10000 [=============>................] - ETA: 1:18:45 - loss: 0.4079 - regression_loss: 0.3580 - classification_loss: 0.0499
 4690/10000 [=============>................] - ETA: 1:18:44 - loss: 0.4080 - regression_loss: 0.3581 - classification_loss: 0.0499
 4691/10000 [=============>................] - ETA: 1:18:43 - loss: 0.4080 - regression_loss: 0.3581 - classification_loss: 0.0499
 4692/10000 [=============>................] - ETA: 1:18:42 - loss: 0.4081 - regression_loss: 0.3582 - classification_loss: 0.0499
 4693/10000 [=============>................] - ETA: 1:18:42 - loss: 0.4080 - regression_loss: 0.3581 - classification_loss: 0.0499
 4694/10000 [=============>................] - ETA: 1:18:41 - loss: 0.4080 - regression_loss: 0.3581 - classification_loss: 0.0499
 4695/10000 [=============>................] - ETA: 1:18:40 - loss: 0.4079 - regression_loss: 0.3580 - classification_loss: 0.0499
 4696/10000 [=============>................] - ETA: 1:18:39 - loss: 0.4079 - regression_loss: 0.3580 - classification_loss: 0.0499
 4697/10000 [=============>................] - ETA: 1:18:38 - loss: 0.4079 - regression_loss: 0.3580 - classification_loss: 0.0499
 4698/10000 [=============>................] - ETA: 1:18:37 - loss: 0.4080 - regression_loss: 0.3580 - classification_loss: 0.0499
 4699/10000 [=============>................] - ETA: 1:18:36 - loss: 0.4079 - regression_loss: 0.3580 - classification_loss: 0.0499
 4700/10000 [=============>................] - ETA: 1:18:35 - loss: 0.4079 - regression_loss: 0.3580 - classification_loss: 0.0499
 4701/10000 [=============>................] - ETA: 1:18:34 - loss: 0.4079 - regression_loss: 0.3580 - classification_loss: 0.0499
 4702/10000 [=============>................] - ETA: 1:18:34 - loss: 0.4079 - regression_loss: 0.3580 - classification_loss: 0.0499
 4703/10000 [=============>................] - ETA: 1:18:33 - loss: 0.4081 - regression_loss: 0.3581 - classification_loss: 0.0500
 4704/10000 [=============>................] - ETA: 1:18:32 - loss: 0.4080 - regression_loss: 0.3580 - classification_loss: 0.0500
 4705/10000 [=============>................] - ETA: 1:18:31 - loss: 0.4080 - regression_loss: 0.3580 - classification_loss: 0.0500
 4706/10000 [=============>................] - ETA: 1:18:30 - loss: 0.4079 - regression_loss: 0.3580 - classification_loss: 0.0500
 4707/10000 [=============>................] - ETA: 1:18:29 - loss: 0.4079 - regression_loss: 0.3579 - classification_loss: 0.0500
 4708/10000 [=============>................] - ETA: 1:18:28 - loss: 0.4080 - regression_loss: 0.3579 - classification_loss: 0.0500
 4709/10000 [=============>................] - ETA: 1:18:27 - loss: 0.4080 - regression_loss: 0.3580 - classification_loss: 0.0501
 4710/10000 [=============>................] - ETA: 1:18:26 - loss: 0.4080 - regression_loss: 0.3579 - classification_loss: 0.0501
 4711/10000 [=============>................] - ETA: 1:18:25 - loss: 0.4080 - regression_loss: 0.3579 - classification_loss: 0.0501
 4712/10000 [=============>................] - ETA: 1:18:25 - loss: 0.4081 - regression_loss: 0.3580 - classification_loss: 0.0501
 4713/10000 [=============>................] - ETA: 1:18:24 - loss: 0.4081 - regression_loss: 0.3580 - classification_loss: 0.0501
 4714/10000 [=============>................] - ETA: 1:18:23 - loss: 0.4082 - regression_loss: 0.3581 - classification_loss: 0.0501
 4715/10000 [=============>................] - ETA: 1:18:22 - loss: 0.4082 - regression_loss: 0.3581 - classification_loss: 0.0501
 4716/10000 [=============>................] - ETA: 1:18:21 - loss: 0.4081 - regression_loss: 0.3580 - classification_loss: 0.0501
 4717/10000 [=============>................] - ETA: 1:18:20 - loss: 0.4081 - regression_loss: 0.3580 - classification_loss: 0.0501
 4718/10000 [=============>................] - ETA: 1:18:19 - loss: 0.4081 - regression_loss: 0.3580 - classification_loss: 0.0501
 4719/10000 [=============>................] - ETA: 1:18:18 - loss: 0.4080 - regression_loss: 0.3580 - classification_loss: 0.0501
 4720/10000 [=============>................] - ETA: 1:18:17 - loss: 0.4080 - regression_loss: 0.3580 - classification_loss: 0.0501
 4721/10000 [=============>................] - ETA: 1:18:17 - loss: 0.4079 - regression_loss: 0.3579 - classification_loss: 0.0501
 4722/10000 [=============>................] - ETA: 1:18:16 - loss: 0.4079 - regression_loss: 0.3579 - classification_loss: 0.0500
 4723/10000 [=============>................] - ETA: 1:18:15 - loss: 0.4079 - regression_loss: 0.3578 - classification_loss: 0.0500
 4724/10000 [=============>................] - ETA: 1:18:14 - loss: 0.4078 - regression_loss: 0.3578 - classification_loss: 0.0500
 4725/10000 [=============>................] - ETA: 1:18:13 - loss: 0.4077 - regression_loss: 0.3577 - classification_loss: 0.0500
 4726/10000 [=============>................] - ETA: 1:18:12 - loss: 0.4077 - regression_loss: 0.3577 - classification_loss: 0.0500
 4727/10000 [=============>................] - ETA: 1:18:11 - loss: 0.4078 - regression_loss: 0.3577 - classification_loss: 0.0500
 4728/10000 [=============>................] - ETA: 1:18:10 - loss: 0.4078 - regression_loss: 0.3578 - classification_loss: 0.0500
 4729/10000 [=============>................] - ETA: 1:18:09 - loss: 0.4078 - regression_loss: 0.3578 - classification_loss: 0.0500
 4730/10000 [=============>................] - ETA: 1:18:09 - loss: 0.4077 - regression_loss: 0.3577 - classification_loss: 0.0500
 4731/10000 [=============>................] - ETA: 1:18:08 - loss: 0.4077 - regression_loss: 0.3577 - classification_loss: 0.0500
 4732/10000 [=============>................] - ETA: 1:18:07 - loss: 0.4076 - regression_loss: 0.3576 - classification_loss: 0.0500
 4733/10000 [=============>................] - ETA: 1:18:06 - loss: 0.4076 - regression_loss: 0.3576 - classification_loss: 0.0500
 4734/10000 [=============>................] - ETA: 1:18:05 - loss: 0.4077 - regression_loss: 0.3577 - classification_loss: 0.0500
 4735/10000 [=============>................] - ETA: 1:18:04 - loss: 0.4076 - regression_loss: 0.3576 - classification_loss: 0.0500
 4736/10000 [=============>................] - ETA: 1:18:03 - loss: 0.4077 - regression_loss: 0.3577 - classification_loss: 0.0500
 4737/10000 [=============>................] - ETA: 1:18:02 - loss: 0.4076 - regression_loss: 0.3577 - classification_loss: 0.0500
 4738/10000 [=============>................] - ETA: 1:18:01 - loss: 0.4077 - regression_loss: 0.3577 - classification_loss: 0.0500
 4739/10000 [=============>................] - ETA: 1:18:01 - loss: 0.4077 - regression_loss: 0.3577 - classification_loss: 0.0500
 4740/10000 [=============>................] - ETA: 1:18:00 - loss: 0.4077 - regression_loss: 0.3577 - classification_loss: 0.0500
 4741/10000 [=============>................] - ETA: 1:17:59 - loss: 0.4078 - regression_loss: 0.3578 - classification_loss: 0.0500
 4742/10000 [=============>................] - ETA: 1:17:58 - loss: 0.4077 - regression_loss: 0.3577 - classification_loss: 0.0500
 4743/10000 [=============>................] - ETA: 1:17:57 - loss: 0.4079 - regression_loss: 0.3579 - classification_loss: 0.0500
 4744/10000 [=============>................] - ETA: 1:17:56 - loss: 0.4079 - regression_loss: 0.3579 - classification_loss: 0.0500
 4745/10000 [=============>................] - ETA: 1:17:55 - loss: 0.4079 - regression_loss: 0.3579 - classification_loss: 0.0500
 4746/10000 [=============>................] - ETA: 1:17:54 - loss: 0.4078 - regression_loss: 0.3578 - classification_loss: 0.0500
 4747/10000 [=============>................] - ETA: 1:17:53 - loss: 0.4079 - regression_loss: 0.3578 - classification_loss: 0.0500
 4748/10000 [=============>................] - ETA: 1:17:53 - loss: 0.4080 - regression_loss: 0.3579 - classification_loss: 0.0501
 4749/10000 [=============>................] - ETA: 1:17:52 - loss: 0.4080 - regression_loss: 0.3579 - classification_loss: 0.0501
 4750/10000 [=============>................] - ETA: 1:17:51 - loss: 0.4079 - regression_loss: 0.3579 - classification_loss: 0.0500
 4751/10000 [=============>................] - ETA: 1:17:50 - loss: 0.4079 - regression_loss: 0.3579 - classification_loss: 0.0500
 4752/10000 [=============>................] - ETA: 1:17:49 - loss: 0.4080 - regression_loss: 0.3580 - classification_loss: 0.0500
 4753/10000 [=============>................] - ETA: 1:17:48 - loss: 0.4079 - regression_loss: 0.3579 - classification_loss: 0.0500
 4754/10000 [=============>................] - ETA: 1:17:47 - loss: 0.4079 - regression_loss: 0.3579 - classification_loss: 0.0500
 4755/10000 [=============>................] - ETA: 1:17:46 - loss: 0.4079 - regression_loss: 0.3579 - classification_loss: 0.0500
 4756/10000 [=============>................] - ETA: 1:17:45 - loss: 0.4079 - regression_loss: 0.3579 - classification_loss: 0.0500
 4757/10000 [=============>................] - ETA: 1:17:44 - loss: 0.4080 - regression_loss: 0.3580 - classification_loss: 0.0500
 4758/10000 [=============>................] - ETA: 1:17:44 - loss: 0.4079 - regression_loss: 0.3579 - classification_loss: 0.0500
 4759/10000 [=============>................] - ETA: 1:17:43 - loss: 0.4078 - regression_loss: 0.3578 - classification_loss: 0.0500
 4760/10000 [=============>................] - ETA: 1:17:42 - loss: 0.4078 - regression_loss: 0.3578 - classification_loss: 0.0500
 4761/10000 [=============>................] - ETA: 1:17:41 - loss: 0.4078 - regression_loss: 0.3578 - classification_loss: 0.0500
 4762/10000 [=============>................] - ETA: 1:17:40 - loss: 0.4078 - regression_loss: 0.3577 - classification_loss: 0.0500
 4763/10000 [=============>................] - ETA: 1:17:39 - loss: 0.4077 - regression_loss: 0.3577 - classification_loss: 0.0500
 4764/10000 [=============>................] - ETA: 1:17:38 - loss: 0.4076 - regression_loss: 0.3577 - classification_loss: 0.0500
 4765/10000 [=============>................] - ETA: 1:17:37 - loss: 0.4076 - regression_loss: 0.3576 - classification_loss: 0.0500
 4766/10000 [=============>................] - ETA: 1:17:36 - loss: 0.4076 - regression_loss: 0.3576 - classification_loss: 0.0500
 4767/10000 [=============>................] - ETA: 1:17:36 - loss: 0.4076 - regression_loss: 0.3575 - classification_loss: 0.0500
 4768/10000 [=============>................] - ETA: 1:17:35 - loss: 0.4076 - regression_loss: 0.3576 - classification_loss: 0.0500
 4769/10000 [=============>................] - ETA: 1:17:34 - loss: 0.4076 - regression_loss: 0.3575 - classification_loss: 0.0500
 4770/10000 [=============>................] - ETA: 1:17:33 - loss: 0.4075 - regression_loss: 0.3575 - classification_loss: 0.0500
 4771/10000 [=============>................] - ETA: 1:17:32 - loss: 0.4075 - regression_loss: 0.3575 - classification_loss: 0.0500
 4772/10000 [=============>................] - ETA: 1:17:31 - loss: 0.4076 - regression_loss: 0.3576 - classification_loss: 0.0500
 4773/10000 [=============>................] - ETA: 1:17:30 - loss: 0.4076 - regression_loss: 0.3576 - classification_loss: 0.0500
 4774/10000 [=============>................] - ETA: 1:17:29 - loss: 0.4076 - regression_loss: 0.3576 - classification_loss: 0.0500
 4775/10000 [=============>................] - ETA: 1:17:28 - loss: 0.4076 - regression_loss: 0.3576 - classification_loss: 0.0500
 4776/10000 [=============>................] - ETA: 1:17:28 - loss: 0.4075 - regression_loss: 0.3575 - classification_loss: 0.0500
 4777/10000 [=============>................] - ETA: 1:17:27 - loss: 0.4076 - regression_loss: 0.3576 - classification_loss: 0.0500
 4778/10000 [=============>................] - ETA: 1:17:26 - loss: 0.4077 - regression_loss: 0.3577 - classification_loss: 0.0500
 4779/10000 [=============>................] - ETA: 1:17:25 - loss: 0.4078 - regression_loss: 0.3577 - classification_loss: 0.0501
 4780/10000 [=============>................] - ETA: 1:17:24 - loss: 0.4078 - regression_loss: 0.3577 - classification_loss: 0.0501
 4781/10000 [=============>................] - ETA: 1:17:23 - loss: 0.4079 - regression_loss: 0.3578 - classification_loss: 0.0501
 4782/10000 [=============>................] - ETA: 1:17:22 - loss: 0.4079 - regression_loss: 0.3578 - classification_loss: 0.0501
 4783/10000 [=============>................] - ETA: 1:17:21 - loss: 0.4081 - regression_loss: 0.3579 - classification_loss: 0.0502
 4784/10000 [=============>................] - ETA: 1:17:20 - loss: 0.4081 - regression_loss: 0.3579 - classification_loss: 0.0502
 4785/10000 [=============>................] - ETA: 1:17:20 - loss: 0.4080 - regression_loss: 0.3579 - classification_loss: 0.0502
 4786/10000 [=============>................] - ETA: 1:17:19 - loss: 0.4080 - regression_loss: 0.3579 - classification_loss: 0.0502
 4787/10000 [=============>................] - ETA: 1:17:18 - loss: 0.4080 - regression_loss: 0.3578 - classification_loss: 0.0502
 4788/10000 [=============>................] - ETA: 1:17:17 - loss: 0.4079 - regression_loss: 0.3578 - classification_loss: 0.0501
 4789/10000 [=============>................] - ETA: 1:17:16 - loss: 0.4080 - regression_loss: 0.3578 - classification_loss: 0.0501
 4790/10000 [=============>................] - ETA: 1:17:15 - loss: 0.4080 - regression_loss: 0.3579 - classification_loss: 0.0501
 4791/10000 [=============>................] - ETA: 1:17:14 - loss: 0.4080 - regression_loss: 0.3578 - classification_loss: 0.0501
 4792/10000 [=============>................] - ETA: 1:17:13 - loss: 0.4080 - regression_loss: 0.3578 - classification_loss: 0.0501
 4793/10000 [=============>................] - ETA: 1:17:12 - loss: 0.4079 - regression_loss: 0.3578 - classification_loss: 0.0501
 4794/10000 [=============>................] - ETA: 1:17:12 - loss: 0.4081 - regression_loss: 0.3580 - classification_loss: 0.0502
 4795/10000 [=============>................] - ETA: 1:17:11 - loss: 0.4082 - regression_loss: 0.3580 - classification_loss: 0.0502
 4796/10000 [=============>................] - ETA: 1:17:10 - loss: 0.4082 - regression_loss: 0.3580 - classification_loss: 0.0502
 4797/10000 [=============>................] - ETA: 1:17:09 - loss: 0.4081 - regression_loss: 0.3580 - classification_loss: 0.0502
 4798/10000 [=============>................] - ETA: 1:17:08 - loss: 0.4081 - regression_loss: 0.3580 - classification_loss: 0.0502
 4799/10000 [=============>................] - ETA: 1:17:07 - loss: 0.4081 - regression_loss: 0.3580 - classification_loss: 0.0502
 4800/10000 [=============>................] - ETA: 1:17:06 - loss: 0.4080 - regression_loss: 0.3579 - classification_loss: 0.0502
 4801/10000 [=============>................] - ETA: 1:17:05 - loss: 0.4080 - regression_loss: 0.3579 - classification_loss: 0.0502
 4802/10000 [=============>................] - ETA: 1:17:04 - loss: 0.4081 - regression_loss: 0.3579 - classification_loss: 0.0502
 4803/10000 [=============>................] - ETA: 1:17:04 - loss: 0.4081 - regression_loss: 0.3580 - classification_loss: 0.0502
 4804/10000 [=============>................] - ETA: 1:17:03 - loss: 0.4083 - regression_loss: 0.3581 - classification_loss: 0.0502
 4805/10000 [=============>................] - ETA: 1:17:02 - loss: 0.4084 - regression_loss: 0.3582 - classification_loss: 0.0502
 4806/10000 [=============>................] - ETA: 1:17:01 - loss: 0.4085 - regression_loss: 0.3583 - classification_loss: 0.0502
 4807/10000 [=============>................] - ETA: 1:17:00 - loss: 0.4085 - regression_loss: 0.3583 - classification_loss: 0.0502
 4808/10000 [=============>................] - ETA: 1:16:59 - loss: 0.4084 - regression_loss: 0.3582 - classification_loss: 0.0502
 4809/10000 [=============>................] - ETA: 1:16:58 - loss: 0.4085 - regression_loss: 0.3583 - classification_loss: 0.0502
 4810/10000 [=============>................] - ETA: 1:16:57 - loss: 0.4085 - regression_loss: 0.3583 - classification_loss: 0.0502
 4811/10000 [=============>................] - ETA: 1:16:56 - loss: 0.4085 - regression_loss: 0.3583 - classification_loss: 0.0502
 4812/10000 [=============>................] - ETA: 1:16:55 - loss: 0.4086 - regression_loss: 0.3584 - classification_loss: 0.0502
 4813/10000 [=============>................] - ETA: 1:16:55 - loss: 0.4085 - regression_loss: 0.3583 - classification_loss: 0.0502
 4814/10000 [=============>................] - ETA: 1:16:54 - loss: 0.4085 - regression_loss: 0.3583 - classification_loss: 0.0502
 4815/10000 [=============>................] - ETA: 1:16:53 - loss: 0.4084 - regression_loss: 0.3582 - classification_loss: 0.0502
 4816/10000 [=============>................] - ETA: 1:16:52 - loss: 0.4084 - regression_loss: 0.3582 - classification_loss: 0.0502
 4817/10000 [=============>................] - ETA: 1:16:51 - loss: 0.4083 - regression_loss: 0.3582 - classification_loss: 0.0502
 4818/10000 [=============>................] - ETA: 1:16:50 - loss: 0.4083 - regression_loss: 0.3581 - classification_loss: 0.0502
 4819/10000 [=============>................] - ETA: 1:16:49 - loss: 0.4083 - regression_loss: 0.3581 - classification_loss: 0.0502
 4820/10000 [=============>................] - ETA: 1:16:48 - loss: 0.4082 - regression_loss: 0.3581 - classification_loss: 0.0502
 4821/10000 [=============>................] - ETA: 1:16:47 - loss: 0.4082 - regression_loss: 0.3581 - classification_loss: 0.0502
 4822/10000 [=============>................] - ETA: 1:16:47 - loss: 0.4082 - regression_loss: 0.3580 - classification_loss: 0.0502
 4823/10000 [=============>................] - ETA: 1:16:46 - loss: 0.4081 - regression_loss: 0.3580 - classification_loss: 0.0502
 4824/10000 [=============>................] - ETA: 1:16:45 - loss: 0.4081 - regression_loss: 0.3580 - classification_loss: 0.0502
 4825/10000 [=============>................] - ETA: 1:16:44 - loss: 0.4082 - regression_loss: 0.3580 - classification_loss: 0.0502
 4826/10000 [=============>................] - ETA: 1:16:43 - loss: 0.4081 - regression_loss: 0.3580 - classification_loss: 0.0502
 4827/10000 [=============>................] - ETA: 1:16:42 - loss: 0.4081 - regression_loss: 0.3579 - classification_loss: 0.0501
 4828/10000 [=============>................] - ETA: 1:16:41 - loss: 0.4080 - regression_loss: 0.3579 - classification_loss: 0.0501
 4829/10000 [=============>................] - ETA: 1:16:40 - loss: 0.4080 - regression_loss: 0.3579 - classification_loss: 0.0501
 4830/10000 [=============>................] - ETA: 1:16:39 - loss: 0.4079 - regression_loss: 0.3578 - classification_loss: 0.0501
 4831/10000 [=============>................] - ETA: 1:16:39 - loss: 0.4080 - regression_loss: 0.3579 - classification_loss: 0.0501
 4832/10000 [=============>................] - ETA: 1:16:38 - loss: 0.4081 - regression_loss: 0.3580 - classification_loss: 0.0502
 4833/10000 [=============>................] - ETA: 1:16:37 - loss: 0.4081 - regression_loss: 0.3580 - classification_loss: 0.0502
 4834/10000 [=============>................] - ETA: 1:16:36 - loss: 0.4082 - regression_loss: 0.3580 - classification_loss: 0.0502
 4835/10000 [=============>................] - ETA: 1:16:35 - loss: 0.4082 - regression_loss: 0.3580 - classification_loss: 0.0502
 4836/10000 [=============>................] - ETA: 1:16:34 - loss: 0.4081 - regression_loss: 0.3580 - classification_loss: 0.0502
 4837/10000 [=============>................] - ETA: 1:16:33 - loss: 0.4082 - regression_loss: 0.3581 - classification_loss: 0.0502
 4838/10000 [=============>................] - ETA: 1:16:32 - loss: 0.4083 - regression_loss: 0.3581 - classification_loss: 0.0502
 4839/10000 [=============>................] - ETA: 1:16:31 - loss: 0.4083 - regression_loss: 0.3581 - classification_loss: 0.0502
 4840/10000 [=============>................] - ETA: 1:16:30 - loss: 0.4082 - regression_loss: 0.3581 - classification_loss: 0.0502
 4841/10000 [=============>................] - ETA: 1:16:30 - loss: 0.4082 - regression_loss: 0.3581 - classification_loss: 0.0502
 4842/10000 [=============>................] - ETA: 1:16:29 - loss: 0.4083 - regression_loss: 0.3581 - classification_loss: 0.0502
 4843/10000 [=============>................] - ETA: 1:16:28 - loss: 0.4082 - regression_loss: 0.3581 - classification_loss: 0.0501
 4844/10000 [=============>................] - ETA: 1:16:27 - loss: 0.4082 - regression_loss: 0.3580 - classification_loss: 0.0501
 4845/10000 [=============>................] - ETA: 1:16:26 - loss: 0.4081 - regression_loss: 0.3580 - classification_loss: 0.0501
 4846/10000 [=============>................] - ETA: 1:16:25 - loss: 0.4081 - regression_loss: 0.3580 - classification_loss: 0.0501
 4847/10000 [=============>................] - ETA: 1:16:24 - loss: 0.4080 - regression_loss: 0.3579 - classification_loss: 0.0501
 4848/10000 [=============>................] - ETA: 1:16:23 - loss: 0.4080 - regression_loss: 0.3579 - classification_loss: 0.0501
 4849/10000 [=============>................] - ETA: 1:16:22 - loss: 0.4080 - regression_loss: 0.3579 - classification_loss: 0.0501
 4850/10000 [=============>................] - ETA: 1:16:22 - loss: 0.4083 - regression_loss: 0.3579 - classification_loss: 0.0504
 4851/10000 [=============>................] - ETA: 1:16:21 - loss: 0.4084 - regression_loss: 0.3579 - classification_loss: 0.0504
 4852/10000 [=============>................] - ETA: 1:16:20 - loss: 0.4083 - regression_loss: 0.3579 - classification_loss: 0.0504
 4853/10000 [=============>................] - ETA: 1:16:19 - loss: 0.4084 - regression_loss: 0.3579 - classification_loss: 0.0504
 4854/10000 [=============>................] - ETA: 1:16:18 - loss: 0.4083 - regression_loss: 0.3579 - classification_loss: 0.0504
 4855/10000 [=============>................] - ETA: 1:16:17 - loss: 0.4084 - regression_loss: 0.3579 - classification_loss: 0.0504
 4856/10000 [=============>................] - ETA: 1:16:16 - loss: 0.4083 - regression_loss: 0.3579 - classification_loss: 0.0504
 4857/10000 [=============>................] - ETA: 1:16:15 - loss: 0.4083 - regression_loss: 0.3579 - classification_loss: 0.0504
 4858/10000 [=============>................] - ETA: 1:16:14 - loss: 0.4083 - regression_loss: 0.3579 - classification_loss: 0.0504
 4859/10000 [=============>................] - ETA: 1:16:14 - loss: 0.4083 - regression_loss: 0.3579 - classification_loss: 0.0504
 4860/10000 [=============>................] - ETA: 1:16:13 - loss: 0.4083 - regression_loss: 0.3578 - classification_loss: 0.0504
 4861/10000 [=============>................] - ETA: 1:16:12 - loss: 0.4085 - regression_loss: 0.3581 - classification_loss: 0.0504
 4862/10000 [=============>................] - ETA: 1:16:11 - loss: 0.4085 - regression_loss: 0.3581 - classification_loss: 0.0504
 4863/10000 [=============>................] - ETA: 1:16:10 - loss: 0.4085 - regression_loss: 0.3581 - classification_loss: 0.0504
 4864/10000 [=============>................] - ETA: 1:16:09 - loss: 0.4084 - regression_loss: 0.3580 - classification_loss: 0.0504
 4865/10000 [=============>................] - ETA: 1:16:08 - loss: 0.4084 - regression_loss: 0.3580 - classification_loss: 0.0504
 4866/10000 [=============>................] - ETA: 1:16:07 - loss: 0.4085 - regression_loss: 0.3581 - classification_loss: 0.0504
 4867/10000 [=============>................] - ETA: 1:16:06 - loss: 0.4085 - regression_loss: 0.3581 - classification_loss: 0.0504
 4868/10000 [=============>................] - ETA: 1:16:06 - loss: 0.4085 - regression_loss: 0.3581 - classification_loss: 0.0504
 4869/10000 [=============>................] - ETA: 1:16:05 - loss: 0.4085 - regression_loss: 0.3581 - classification_loss: 0.0504
 4870/10000 [=============>................] - ETA: 1:16:04 - loss: 0.4086 - regression_loss: 0.3582 - classification_loss: 0.0504
 4871/10000 [=============>................] - ETA: 1:16:03 - loss: 0.4086 - regression_loss: 0.3582 - classification_loss: 0.0504
 4872/10000 [=============>................] - ETA: 1:16:02 - loss: 0.4086 - regression_loss: 0.3582 - classification_loss: 0.0504
 4873/10000 [=============>................] - ETA: 1:16:01 - loss: 0.4085 - regression_loss: 0.3581 - classification_loss: 0.0504
 4874/10000 [=============>................] - ETA: 1:16:00 - loss: 0.4085 - regression_loss: 0.3581 - classification_loss: 0.0504
 4875/10000 [=============>................] - ETA: 1:15:59 - loss: 0.4087 - regression_loss: 0.3583 - classification_loss: 0.0504
 4876/10000 [=============>................] - ETA: 1:15:58 - loss: 0.4089 - regression_loss: 0.3585 - classification_loss: 0.0504
 4877/10000 [=============>................] - ETA: 1:15:58 - loss: 0.4089 - regression_loss: 0.3584 - classification_loss: 0.0504
 4878/10000 [=============>................] - ETA: 1:15:57 - loss: 0.4088 - regression_loss: 0.3584 - classification_loss: 0.0504
 4879/10000 [=============>................] - ETA: 1:15:56 - loss: 0.4088 - regression_loss: 0.3584 - classification_loss: 0.0504
 4880/10000 [=============>................] - ETA: 1:15:55 - loss: 0.4088 - regression_loss: 0.3584 - classification_loss: 0.0504
 4881/10000 [=============>................] - ETA: 1:15:54 - loss: 0.4088 - regression_loss: 0.3584 - classification_loss: 0.0504
 4882/10000 [=============>................] - ETA: 1:15:53 - loss: 0.4087 - regression_loss: 0.3583 - classification_loss: 0.0504
 4883/10000 [=============>................] - ETA: 1:15:52 - loss: 0.4088 - regression_loss: 0.3584 - classification_loss: 0.0504
 4884/10000 [=============>................] - ETA: 1:15:51 - loss: 0.4087 - regression_loss: 0.3583 - classification_loss: 0.0504
 4885/10000 [=============>................] - ETA: 1:15:50 - loss: 0.4086 - regression_loss: 0.3583 - classification_loss: 0.0504
 4886/10000 [=============>................] - ETA: 1:15:49 - loss: 0.4086 - regression_loss: 0.3582 - classification_loss: 0.0504
 4887/10000 [=============>................] - ETA: 1:15:49 - loss: 0.4086 - regression_loss: 0.3582 - classification_loss: 0.0504
 4888/10000 [=============>................] - ETA: 1:15:48 - loss: 0.4085 - regression_loss: 0.3581 - classification_loss: 0.0503
 4889/10000 [=============>................] - ETA: 1:15:47 - loss: 0.4085 - regression_loss: 0.3581 - classification_loss: 0.0503
 4890/10000 [=============>................] - ETA: 1:15:46 - loss: 0.4084 - regression_loss: 0.3581 - classification_loss: 0.0503
 4891/10000 [=============>................] - ETA: 1:15:45 - loss: 0.4084 - regression_loss: 0.3581 - classification_loss: 0.0503
 4892/10000 [=============>................] - ETA: 1:15:44 - loss: 0.4084 - regression_loss: 0.3581 - classification_loss: 0.0503
 4893/10000 [=============>................] - ETA: 1:15:43 - loss: 0.4084 - regression_loss: 0.3581 - classification_loss: 0.0503
 4894/10000 [=============>................] - ETA: 1:15:42 - loss: 0.4085 - regression_loss: 0.3581 - classification_loss: 0.0504
 4895/10000 [=============>................] - ETA: 1:15:41 - loss: 0.4084 - regression_loss: 0.3581 - classification_loss: 0.0504
 4896/10000 [=============>................] - ETA: 1:15:41 - loss: 0.4084 - regression_loss: 0.3581 - classification_loss: 0.0504
 4897/10000 [=============>................] - ETA: 1:15:40 - loss: 0.4084 - regression_loss: 0.3581 - classification_loss: 0.0504
 4898/10000 [=============>................] - ETA: 1:15:39 - loss: 0.4084 - regression_loss: 0.3580 - classification_loss: 0.0504
 4899/10000 [=============>................] - ETA: 1:15:38 - loss: 0.4084 - regression_loss: 0.3580 - classification_loss: 0.0504
 4900/10000 [=============>................] - ETA: 1:15:37 - loss: 0.4089 - regression_loss: 0.3583 - classification_loss: 0.0506
 4901/10000 [=============>................] - ETA: 1:15:36 - loss: 0.4089 - regression_loss: 0.3583 - classification_loss: 0.0506
 4902/10000 [=============>................] - ETA: 1:15:35 - loss: 0.4088 - regression_loss: 0.3582 - classification_loss: 0.0506
 4903/10000 [=============>................] - ETA: 1:15:34 - loss: 0.4088 - regression_loss: 0.3582 - classification_loss: 0.0505
 4904/10000 [=============>................] - ETA: 1:15:33 - loss: 0.4087 - regression_loss: 0.3582 - classification_loss: 0.0506
 4905/10000 [=============>................] - ETA: 1:15:33 - loss: 0.4087 - regression_loss: 0.3582 - classification_loss: 0.0506
 4906/10000 [=============>................] - ETA: 1:15:32 - loss: 0.4088 - regression_loss: 0.3582 - classification_loss: 0.0506
 4907/10000 [=============>................] - ETA: 1:15:31 - loss: 0.4088 - regression_loss: 0.3582 - classification_loss: 0.0506
 4908/10000 [=============>................] - ETA: 1:15:30 - loss: 0.4088 - regression_loss: 0.3582 - classification_loss: 0.0506
 4909/10000 [=============>................] - ETA: 1:15:29 - loss: 0.4088 - regression_loss: 0.3582 - classification_loss: 0.0506
 4910/10000 [=============>................] - ETA: 1:15:28 - loss: 0.4087 - regression_loss: 0.3582 - classification_loss: 0.0506
 4911/10000 [=============>................] - ETA: 1:15:27 - loss: 0.4087 - regression_loss: 0.3581 - classification_loss: 0.0506
 4912/10000 [=============>................] - ETA: 1:15:26 - loss: 0.4087 - regression_loss: 0.3581 - classification_loss: 0.0506
 4913/10000 [=============>................] - ETA: 1:15:25 - loss: 0.4087 - regression_loss: 0.3581 - classification_loss: 0.0506
 4914/10000 [=============>................] - ETA: 1:15:25 - loss: 0.4087 - regression_loss: 0.3581 - classification_loss: 0.0506
 4915/10000 [=============>................] - ETA: 1:15:24 - loss: 0.4088 - regression_loss: 0.3582 - classification_loss: 0.0506
 4916/10000 [=============>................] - ETA: 1:15:23 - loss: 0.4087 - regression_loss: 0.3582 - classification_loss: 0.0505
 4917/10000 [=============>................] - ETA: 1:15:22 - loss: 0.4088 - regression_loss: 0.3582 - classification_loss: 0.0505
 4918/10000 [=============>................] - ETA: 1:15:21 - loss: 0.4088 - regression_loss: 0.3582 - classification_loss: 0.0505
 4919/10000 [=============>................] - ETA: 1:15:20 - loss: 0.4087 - regression_loss: 0.3582 - classification_loss: 0.0505
 4920/10000 [=============>................] - ETA: 1:15:19 - loss: 0.4088 - regression_loss: 0.3582 - classification_loss: 0.0505
 4921/10000 [=============>................] - ETA: 1:15:18 - loss: 0.4088 - regression_loss: 0.3582 - classification_loss: 0.0505
 4922/10000 [=============>................] - ETA: 1:15:17 - loss: 0.4088 - regression_loss: 0.3582 - classification_loss: 0.0505
 4923/10000 [=============>................] - ETA: 1:15:16 - loss: 0.4087 - regression_loss: 0.3582 - classification_loss: 0.0505
 4924/10000 [=============>................] - ETA: 1:15:16 - loss: 0.4087 - regression_loss: 0.3582 - classification_loss: 0.0505
 4925/10000 [=============>................] - ETA: 1:15:15 - loss: 0.4091 - regression_loss: 0.3585 - classification_loss: 0.0506
 4926/10000 [=============>................] - ETA: 1:15:14 - loss: 0.4091 - regression_loss: 0.3585 - classification_loss: 0.0505
 4927/10000 [=============>................] - ETA: 1:15:13 - loss: 0.4090 - regression_loss: 0.3585 - classification_loss: 0.0505
 4928/10000 [=============>................] - ETA: 1:15:12 - loss: 0.4091 - regression_loss: 0.3585 - classification_loss: 0.0505
 4929/10000 [=============>................] - ETA: 1:15:11 - loss: 0.4091 - regression_loss: 0.3585 - classification_loss: 0.0505
 4930/10000 [=============>................] - ETA: 1:15:10 - loss: 0.4091 - regression_loss: 0.3585 - classification_loss: 0.0505
 4931/10000 [=============>................] - ETA: 1:15:09 - loss: 0.4090 - regression_loss: 0.3585 - classification_loss: 0.0505
 4932/10000 [=============>................] - ETA: 1:15:08 - loss: 0.4091 - regression_loss: 0.3585 - classification_loss: 0.0505
 4933/10000 [=============>................] - ETA: 1:15:08 - loss: 0.4091 - regression_loss: 0.3585 - classification_loss: 0.0505
 4934/10000 [=============>................] - ETA: 1:15:07 - loss: 0.4090 - regression_loss: 0.3585 - classification_loss: 0.0505
 4935/10000 [=============>................] - ETA: 1:15:06 - loss: 0.4093 - regression_loss: 0.3586 - classification_loss: 0.0507
 4936/10000 [=============>................] - ETA: 1:15:05 - loss: 0.4094 - regression_loss: 0.3586 - classification_loss: 0.0508
 4937/10000 [=============>................] - ETA: 1:15:04 - loss: 0.4094 - regression_loss: 0.3586 - classification_loss: 0.0508
 4938/10000 [=============>................] - ETA: 1:15:03 - loss: 0.4094 - regression_loss: 0.3586 - classification_loss: 0.0508
 4939/10000 [=============>................] - ETA: 1:15:02 - loss: 0.4093 - regression_loss: 0.3586 - classification_loss: 0.0507
 4940/10000 [=============>................] - ETA: 1:15:01 - loss: 0.4093 - regression_loss: 0.3585 - classification_loss: 0.0507
 4941/10000 [=============>................] - ETA: 1:15:00 - loss: 0.4093 - regression_loss: 0.3585 - classification_loss: 0.0507
 4942/10000 [=============>................] - ETA: 1:15:00 - loss: 0.4093 - regression_loss: 0.3585 - classification_loss: 0.0507
 4943/10000 [=============>................] - ETA: 1:14:59 - loss: 0.4093 - regression_loss: 0.3585 - classification_loss: 0.0507
 4944/10000 [=============>................] - ETA: 1:14:58 - loss: 0.4092 - regression_loss: 0.3585 - classification_loss: 0.0507
 4945/10000 [=============>................] - ETA: 1:14:57 - loss: 0.4092 - regression_loss: 0.3585 - classification_loss: 0.0507
 4946/10000 [=============>................] - ETA: 1:14:56 - loss: 0.4093 - regression_loss: 0.3586 - classification_loss: 0.0507
 4947/10000 [=============>................] - ETA: 1:14:55 - loss: 0.4094 - regression_loss: 0.3586 - classification_loss: 0.0507
 4948/10000 [=============>................] - ETA: 1:14:54 - loss: 0.4094 - regression_loss: 0.3587 - classification_loss: 0.0507
 4949/10000 [=============>................] - ETA: 1:14:53 - loss: 0.4094 - regression_loss: 0.3587 - classification_loss: 0.0507
 4950/10000 [=============>................] - ETA: 1:14:52 - loss: 0.4096 - regression_loss: 0.3589 - classification_loss: 0.0507
 4951/10000 [=============>................] - ETA: 1:14:52 - loss: 0.4096 - regression_loss: 0.3588 - classification_loss: 0.0507
 4952/10000 [=============>................] - ETA: 1:14:51 - loss: 0.4096 - regression_loss: 0.3588 - classification_loss: 0.0507
 4953/10000 [=============>................] - ETA: 1:14:50 - loss: 0.4095 - regression_loss: 0.3588 - classification_loss: 0.0507
 4954/10000 [=============>................] - ETA: 1:14:49 - loss: 0.4095 - regression_loss: 0.3588 - classification_loss: 0.0507
 4955/10000 [=============>................] - ETA: 1:14:48 - loss: 0.4094 - regression_loss: 0.3587 - classification_loss: 0.0507
 4956/10000 [=============>................] - ETA: 1:14:47 - loss: 0.4094 - regression_loss: 0.3587 - classification_loss: 0.0507
 4957/10000 [=============>................] - ETA: 1:14:46 - loss: 0.4094 - regression_loss: 0.3587 - classification_loss: 0.0507
 4958/10000 [=============>................] - ETA: 1:14:45 - loss: 0.4094 - regression_loss: 0.3587 - classification_loss: 0.0507
 4959/10000 [=============>................] - ETA: 1:14:44 - loss: 0.4093 - regression_loss: 0.3586 - classification_loss: 0.0507
 4960/10000 [=============>................] - ETA: 1:14:43 - loss: 0.4093 - regression_loss: 0.3586 - classification_loss: 0.0507
 4961/10000 [=============>................] - ETA: 1:14:43 - loss: 0.4094 - regression_loss: 0.3587 - classification_loss: 0.0507
 4962/10000 [=============>................] - ETA: 1:14:42 - loss: 0.4094 - regression_loss: 0.3587 - classification_loss: 0.0507
 4963/10000 [=============>................] - ETA: 1:14:41 - loss: 0.4094 - regression_loss: 0.3587 - classification_loss: 0.0507
 4964/10000 [=============>................] - ETA: 1:14:40 - loss: 0.4094 - regression_loss: 0.3587 - classification_loss: 0.0507
 4965/10000 [=============>................] - ETA: 1:14:39 - loss: 0.4094 - regression_loss: 0.3587 - classification_loss: 0.0507
 4966/10000 [=============>................] - ETA: 1:14:38 - loss: 0.4094 - regression_loss: 0.3587 - classification_loss: 0.0507
 4967/10000 [=============>................] - ETA: 1:14:37 - loss: 0.4094 - regression_loss: 0.3587 - classification_loss: 0.0507
 4968/10000 [=============>................] - ETA: 1:14:36 - loss: 0.4094 - regression_loss: 0.3587 - classification_loss: 0.0507
 4969/10000 [=============>................] - ETA: 1:14:35 - loss: 0.4094 - regression_loss: 0.3587 - classification_loss: 0.0507
 4970/10000 [=============>................] - ETA: 1:14:35 - loss: 0.4094 - regression_loss: 0.3587 - classification_loss: 0.0507
 4971/10000 [=============>................] - ETA: 1:14:34 - loss: 0.4093 - regression_loss: 0.3587 - classification_loss: 0.0507
 4972/10000 [=============>................] - ETA: 1:14:33 - loss: 0.4094 - regression_loss: 0.3587 - classification_loss: 0.0507
 4973/10000 [=============>................] - ETA: 1:14:32 - loss: 0.4093 - regression_loss: 0.3586 - classification_loss: 0.0507
 4974/10000 [=============>................] - ETA: 1:14:31 - loss: 0.4093 - regression_loss: 0.3586 - classification_loss: 0.0507
 4975/10000 [=============>................] - ETA: 1:14:30 - loss: 0.4092 - regression_loss: 0.3586 - classification_loss: 0.0507
 4976/10000 [=============>................] - ETA: 1:14:29 - loss: 0.4092 - regression_loss: 0.3585 - classification_loss: 0.0507
 4977/10000 [=============>................] - ETA: 1:14:28 - loss: 0.4091 - regression_loss: 0.3585 - classification_loss: 0.0506
 4978/10000 [=============>................] - ETA: 1:14:27 - loss: 0.4091 - regression_loss: 0.3584 - classification_loss: 0.0506
 4979/10000 [=============>................] - ETA: 1:14:27 - loss: 0.4091 - regression_loss: 0.3584 - classification_loss: 0.0506
 4980/10000 [=============>................] - ETA: 1:14:26 - loss: 0.4090 - regression_loss: 0.3584 - classification_loss: 0.0506
 4981/10000 [=============>................] - ETA: 1:14:25 - loss: 0.4091 - regression_loss: 0.3585 - classification_loss: 0.0506
 4982/10000 [=============>................] - ETA: 1:14:24 - loss: 0.4091 - regression_loss: 0.3585 - classification_loss: 0.0507
 4983/10000 [=============>................] - ETA: 1:14:23 - loss: 0.4091 - regression_loss: 0.3584 - classification_loss: 0.0506
 4984/10000 [=============>................] - ETA: 1:14:22 - loss: 0.4090 - regression_loss: 0.3584 - classification_loss: 0.0506
 4985/10000 [=============>................] - ETA: 1:14:21 - loss: 0.4089 - regression_loss: 0.3583 - classification_loss: 0.0506
 4986/10000 [=============>................] - ETA: 1:14:20 - loss: 0.4089 - regression_loss: 0.3583 - classification_loss: 0.0506
 4987/10000 [=============>................] - ETA: 1:14:19 - loss: 0.4089 - regression_loss: 0.3583 - classification_loss: 0.0506
 4988/10000 [=============>................] - ETA: 1:14:19 - loss: 0.4088 - regression_loss: 0.3582 - classification_loss: 0.0506
 4989/10000 [=============>................] - ETA: 1:14:18 - loss: 0.4088 - regression_loss: 0.3582 - classification_loss: 0.0506
 4990/10000 [=============>................] - ETA: 1:14:17 - loss: 0.4087 - regression_loss: 0.3582 - classification_loss: 0.0506
 4991/10000 [=============>................] - ETA: 1:14:16 - loss: 0.4087 - regression_loss: 0.3581 - classification_loss: 0.0506
 4992/10000 [=============>................] - ETA: 1:14:15 - loss: 0.4087 - regression_loss: 0.3581 - classification_loss: 0.0506
 4993/10000 [=============>................] - ETA: 1:14:14 - loss: 0.4087 - regression_loss: 0.3581 - classification_loss: 0.0506
 4994/10000 [=============>................] - ETA: 1:14:13 - loss: 0.4087 - regression_loss: 0.3581 - classification_loss: 0.0506
 4995/10000 [=============>................] - ETA: 1:14:12 - loss: 0.4087 - regression_loss: 0.3581 - classification_loss: 0.0506
 4996/10000 [=============>................] - ETA: 1:14:11 - loss: 0.4086 - regression_loss: 0.3580 - classification_loss: 0.0506
 4997/10000 [=============>................] - ETA: 1:14:11 - loss: 0.4086 - regression_loss: 0.3580 - classification_loss: 0.0506
 4998/10000 [=============>................] - ETA: 1:14:10 - loss: 0.4085 - regression_loss: 0.3580 - classification_loss: 0.0506
 4999/10000 [=============>................] - ETA: 1:14:09 - loss: 0.4085 - regression_loss: 0.3580 - classification_loss: 0.0505
 5000/10000 [==============>...............] - ETA: 1:14:08 - loss: 0.4085 - regression_loss: 0.3579 - classification_loss: 0.0505
 5001/10000 [==============>...............] - ETA: 1:14:07 - loss: 0.4085 - regression_loss: 0.3579 - classification_loss: 0.0505
 5002/10000 [==============>...............] - ETA: 1:14:06 - loss: 0.4084 - regression_loss: 0.3579 - classification_loss: 0.0505
 5003/10000 [==============>...............] - ETA: 1:14:05 - loss: 0.4084 - regression_loss: 0.3579 - classification_loss: 0.0505
 5004/10000 [==============>...............] - ETA: 1:14:04 - loss: 0.4084 - regression_loss: 0.3579 - classification_loss: 0.0505
 5005/10000 [==============>...............] - ETA: 1:14:03 - loss: 0.4084 - regression_loss: 0.3579 - classification_loss: 0.0505
 5006/10000 [==============>...............] - ETA: 1:14:02 - loss: 0.4084 - regression_loss: 0.3579 - classification_loss: 0.0505
 5007/10000 [==============>...............] - ETA: 1:14:02 - loss: 0.4084 - regression_loss: 0.3579 - classification_loss: 0.0505
 5008/10000 [==============>...............] - ETA: 1:14:01 - loss: 0.4084 - regression_loss: 0.3579 - classification_loss: 0.0505
 5009/10000 [==============>...............] - ETA: 1:14:00 - loss: 0.4083 - regression_loss: 0.3578 - classification_loss: 0.0505
 5010/10000 [==============>...............] - ETA: 1:13:59 - loss: 0.4082 - regression_loss: 0.3578 - classification_loss: 0.0505
 5011/10000 [==============>...............] - ETA: 1:13:58 - loss: 0.4082 - regression_loss: 0.3577 - classification_loss: 0.0505
 5012/10000 [==============>...............] - ETA: 1:13:57 - loss: 0.4081 - regression_loss: 0.3577 - classification_loss: 0.0505
 5013/10000 [==============>...............] - ETA: 1:13:56 - loss: 0.4081 - regression_loss: 0.3576 - classification_loss: 0.0505
 5014/10000 [==============>...............] - ETA: 1:13:55 - loss: 0.4081 - regression_loss: 0.3576 - classification_loss: 0.0505
 5015/10000 [==============>...............] - ETA: 1:13:54 - loss: 0.4081 - regression_loss: 0.3576 - classification_loss: 0.0505
 5016/10000 [==============>...............] - ETA: 1:13:54 - loss: 0.4082 - regression_loss: 0.3577 - classification_loss: 0.0505
 5017/10000 [==============>...............] - ETA: 1:13:53 - loss: 0.4082 - regression_loss: 0.3577 - classification_loss: 0.0505
 5018/10000 [==============>...............] - ETA: 1:13:52 - loss: 0.4082 - regression_loss: 0.3577 - classification_loss: 0.0505
 5019/10000 [==============>...............] - ETA: 1:13:51 - loss: 0.4082 - regression_loss: 0.3577 - classification_loss: 0.0505
 5020/10000 [==============>...............] - ETA: 1:13:50 - loss: 0.4082 - regression_loss: 0.3577 - classification_loss: 0.0505
 5021/10000 [==============>...............] - ETA: 1:13:49 - loss: 0.4081 - regression_loss: 0.3577 - classification_loss: 0.0505
 5022/10000 [==============>...............] - ETA: 1:13:48 - loss: 0.4080 - regression_loss: 0.3576 - classification_loss: 0.0504
 5023/10000 [==============>...............] - ETA: 1:13:47 - loss: 0.4080 - regression_loss: 0.3576 - classification_loss: 0.0504
 5024/10000 [==============>...............] - ETA: 1:13:46 - loss: 0.4081 - regression_loss: 0.3576 - classification_loss: 0.0504
 5025/10000 [==============>...............] - ETA: 1:13:46 - loss: 0.4080 - regression_loss: 0.3576 - classification_loss: 0.0504
 5026/10000 [==============>...............] - ETA: 1:13:45 - loss: 0.4081 - regression_loss: 0.3576 - classification_loss: 0.0504
 5027/10000 [==============>...............] - ETA: 1:13:44 - loss: 0.4081 - regression_loss: 0.3577 - classification_loss: 0.0504
 5028/10000 [==============>...............] - ETA: 1:13:43 - loss: 0.4081 - regression_loss: 0.3577 - classification_loss: 0.0504
 5029/10000 [==============>...............] - ETA: 1:13:42 - loss: 0.4081 - regression_loss: 0.3576 - classification_loss: 0.0504
 5030/10000 [==============>...............] - ETA: 1:13:41 - loss: 0.4080 - regression_loss: 0.3576 - classification_loss: 0.0504
 5031/10000 [==============>...............] - ETA: 1:13:40 - loss: 0.4080 - regression_loss: 0.3576 - classification_loss: 0.0504
 5032/10000 [==============>...............] - ETA: 1:13:39 - loss: 0.4081 - regression_loss: 0.3577 - classification_loss: 0.0504
 5033/10000 [==============>...............] - ETA: 1:13:38 - loss: 0.4082 - regression_loss: 0.3578 - classification_loss: 0.0504
 5034/10000 [==============>...............] - ETA: 1:13:38 - loss: 0.4081 - regression_loss: 0.3577 - classification_loss: 0.0504
 5035/10000 [==============>...............] - ETA: 1:13:37 - loss: 0.4081 - regression_loss: 0.3577 - classification_loss: 0.0504
 5036/10000 [==============>...............] - ETA: 1:13:36 - loss: 0.4080 - regression_loss: 0.3576 - classification_loss: 0.0504
 5037/10000 [==============>...............] - ETA: 1:13:35 - loss: 0.4080 - regression_loss: 0.3576 - classification_loss: 0.0504
 5038/10000 [==============>...............] - ETA: 1:13:34 - loss: 0.4081 - regression_loss: 0.3577 - classification_loss: 0.0504
 5039/10000 [==============>...............] - ETA: 1:13:33 - loss: 0.4080 - regression_loss: 0.3576 - classification_loss: 0.0504
 5040/10000 [==============>...............] - ETA: 1:13:32 - loss: 0.4080 - regression_loss: 0.3576 - classification_loss: 0.0504
 5041/10000 [==============>...............] - ETA: 1:13:31 - loss: 0.4080 - regression_loss: 0.3576 - classification_loss: 0.0504
 5042/10000 [==============>...............] - ETA: 1:13:30 - loss: 0.4079 - regression_loss: 0.3575 - classification_loss: 0.0504
 5043/10000 [==============>...............] - ETA: 1:13:30 - loss: 0.4080 - regression_loss: 0.3576 - classification_loss: 0.0504
 5044/10000 [==============>...............] - ETA: 1:13:29 - loss: 0.4081 - regression_loss: 0.3577 - classification_loss: 0.0504
 5045/10000 [==============>...............] - ETA: 1:13:28 - loss: 0.4080 - regression_loss: 0.3577 - classification_loss: 0.0504
 5046/10000 [==============>...............] - ETA: 1:13:27 - loss: 0.4080 - regression_loss: 0.3577 - classification_loss: 0.0504
 5047/10000 [==============>...............] - ETA: 1:13:26 - loss: 0.4081 - regression_loss: 0.3577 - classification_loss: 0.0504
 5048/10000 [==============>...............] - ETA: 1:13:25 - loss: 0.4081 - regression_loss: 0.3577 - classification_loss: 0.0503
 5049/10000 [==============>...............] - ETA: 1:13:24 - loss: 0.4081 - regression_loss: 0.3578 - classification_loss: 0.0503
 5050/10000 [==============>...............] - ETA: 1:13:23 - loss: 0.4081 - regression_loss: 0.3577 - classification_loss: 0.0503
 5051/10000 [==============>...............] - ETA: 1:13:22 - loss: 0.4081 - regression_loss: 0.3577 - classification_loss: 0.0503
 5052/10000 [==============>...............] - ETA: 1:13:21 - loss: 0.4081 - regression_loss: 0.3578 - classification_loss: 0.0503
 5053/10000 [==============>...............] - ETA: 1:13:21 - loss: 0.4081 - regression_loss: 0.3577 - classification_loss: 0.0503
 5054/10000 [==============>...............] - ETA: 1:13:20 - loss: 0.4081 - regression_loss: 0.3578 - classification_loss: 0.0503
 5055/10000 [==============>...............] - ETA: 1:13:19 - loss: 0.4081 - regression_loss: 0.3578 - classification_loss: 0.0503
 5056/10000 [==============>...............] - ETA: 1:13:18 - loss: 0.4081 - regression_loss: 0.3578 - classification_loss: 0.0503
 5057/10000 [==============>...............] - ETA: 1:13:17 - loss: 0.4082 - regression_loss: 0.3578 - classification_loss: 0.0503
 5058/10000 [==============>...............] - ETA: 1:13:16 - loss: 0.4082 - regression_loss: 0.3578 - classification_loss: 0.0503
 5059/10000 [==============>...............] - ETA: 1:13:15 - loss: 0.4081 - regression_loss: 0.3578 - classification_loss: 0.0503
 5060/10000 [==============>...............] - ETA: 1:13:14 - loss: 0.4081 - regression_loss: 0.3577 - classification_loss: 0.0503
 5061/10000 [==============>...............] - ETA: 1:13:13 - loss: 0.4080 - regression_loss: 0.3577 - classification_loss: 0.0503
 5062/10000 [==============>...............] - ETA: 1:13:13 - loss: 0.4080 - regression_loss: 0.3577 - classification_loss: 0.0503
 5063/10000 [==============>...............] - ETA: 1:13:12 - loss: 0.4081 - regression_loss: 0.3578 - classification_loss: 0.0503
 5064/10000 [==============>...............] - ETA: 1:13:11 - loss: 0.4080 - regression_loss: 0.3577 - classification_loss: 0.0503
 5065/10000 [==============>...............] - ETA: 1:13:10 - loss: 0.4080 - regression_loss: 0.3577 - classification_loss: 0.0503
 5066/10000 [==============>...............] - ETA: 1:13:09 - loss: 0.4080 - regression_loss: 0.3577 - classification_loss: 0.0503
 5067/10000 [==============>...............] - ETA: 1:13:08 - loss: 0.4080 - regression_loss: 0.3577 - classification_loss: 0.0503
 5068/10000 [==============>...............] - ETA: 1:13:07 - loss: 0.4080 - regression_loss: 0.3577 - classification_loss: 0.0503
 5069/10000 [==============>...............] - ETA: 1:13:06 - loss: 0.4080 - regression_loss: 0.3577 - classification_loss: 0.0503
 5070/10000 [==============>...............] - ETA: 1:13:05 - loss: 0.4080 - regression_loss: 0.3577 - classification_loss: 0.0503
 5071/10000 [==============>...............] - ETA: 1:13:05 - loss: 0.4080 - regression_loss: 0.3577 - classification_loss: 0.0503
 5072/10000 [==============>...............] - ETA: 1:13:04 - loss: 0.4079 - regression_loss: 0.3577 - classification_loss: 0.0503
 5073/10000 [==============>...............] - ETA: 1:13:03 - loss: 0.4078 - regression_loss: 0.3576 - classification_loss: 0.0503
 5074/10000 [==============>...............] - ETA: 1:13:02 - loss: 0.4078 - regression_loss: 0.3576 - classification_loss: 0.0503
 5075/10000 [==============>...............] - ETA: 1:13:01 - loss: 0.4078 - regression_loss: 0.3575 - classification_loss: 0.0502
 5076/10000 [==============>...............] - ETA: 1:13:00 - loss: 0.4077 - regression_loss: 0.3575 - classification_loss: 0.0502
 5077/10000 [==============>...............] - ETA: 1:12:59 - loss: 0.4078 - regression_loss: 0.3575 - classification_loss: 0.0502
 5078/10000 [==============>...............] - ETA: 1:12:58 - loss: 0.4078 - regression_loss: 0.3575 - classification_loss: 0.0502
 5079/10000 [==============>...............] - ETA: 1:12:57 - loss: 0.4077 - regression_loss: 0.3575 - classification_loss: 0.0502
 5080/10000 [==============>...............] - ETA: 1:12:57 - loss: 0.4078 - regression_loss: 0.3575 - classification_loss: 0.0502
 5081/10000 [==============>...............] - ETA: 1:12:56 - loss: 0.4078 - regression_loss: 0.3576 - classification_loss: 0.0502
 5082/10000 [==============>...............] - ETA: 1:12:55 - loss: 0.4079 - regression_loss: 0.3576 - classification_loss: 0.0502
 5083/10000 [==============>...............] - ETA: 1:12:54 - loss: 0.4078 - regression_loss: 0.3576 - classification_loss: 0.0502
 5084/10000 [==============>...............] - ETA: 1:12:53 - loss: 0.4079 - regression_loss: 0.3576 - classification_loss: 0.0502
 5085/10000 [==============>...............] - ETA: 1:12:52 - loss: 0.4078 - regression_loss: 0.3576 - classification_loss: 0.0502
 5086/10000 [==============>...............] - ETA: 1:12:51 - loss: 0.4078 - regression_loss: 0.3576 - classification_loss: 0.0502
 5087/10000 [==============>...............] - ETA: 1:12:50 - loss: 0.4077 - regression_loss: 0.3575 - classification_loss: 0.0502
 5088/10000 [==============>...............] - ETA: 1:12:49 - loss: 0.4077 - regression_loss: 0.3574 - classification_loss: 0.0502
 5089/10000 [==============>...............] - ETA: 1:12:48 - loss: 0.4076 - regression_loss: 0.3574 - classification_loss: 0.0502
 5090/10000 [==============>...............] - ETA: 1:12:48 - loss: 0.4076 - regression_loss: 0.3574 - classification_loss: 0.0502
 5091/10000 [==============>...............] - ETA: 1:12:47 - loss: 0.4076 - regression_loss: 0.3574 - classification_loss: 0.0502
 5092/10000 [==============>...............] - ETA: 1:12:46 - loss: 0.4076 - regression_loss: 0.3574 - classification_loss: 0.0502
 5093/10000 [==============>...............] - ETA: 1:12:45 - loss: 0.4075 - regression_loss: 0.3573 - classification_loss: 0.0502
 5094/10000 [==============>...............] - ETA: 1:12:44 - loss: 0.4075 - regression_loss: 0.3574 - classification_loss: 0.0502
 5095/10000 [==============>...............] - ETA: 1:12:43 - loss: 0.4075 - regression_loss: 0.3573 - classification_loss: 0.0502
 5096/10000 [==============>...............] - ETA: 1:12:42 - loss: 0.4075 - regression_loss: 0.3573 - classification_loss: 0.0502
 5097/10000 [==============>...............] - ETA: 1:12:41 - loss: 0.4074 - regression_loss: 0.3573 - classification_loss: 0.0502
 5098/10000 [==============>...............] - ETA: 1:12:40 - loss: 0.4074 - regression_loss: 0.3573 - classification_loss: 0.0502
 5099/10000 [==============>...............] - ETA: 1:12:40 - loss: 0.4074 - regression_loss: 0.3573 - classification_loss: 0.0502
 5100/10000 [==============>...............] - ETA: 1:12:39 - loss: 0.4074 - regression_loss: 0.3573 - classification_loss: 0.0502
 5101/10000 [==============>...............] - ETA: 1:12:38 - loss: 0.4074 - regression_loss: 0.3573 - classification_loss: 0.0502
 5102/10000 [==============>...............] - ETA: 1:12:37 - loss: 0.4074 - regression_loss: 0.3572 - classification_loss: 0.0501
 5103/10000 [==============>...............] - ETA: 1:12:36 - loss: 0.4073 - regression_loss: 0.3572 - classification_loss: 0.0501
 5104/10000 [==============>...............] - ETA: 1:12:35 - loss: 0.4073 - regression_loss: 0.3572 - classification_loss: 0.0501
 5105/10000 [==============>...............] - ETA: 1:12:34 - loss: 0.4074 - regression_loss: 0.3572 - classification_loss: 0.0501
 5106/10000 [==============>...............] - ETA: 1:12:33 - loss: 0.4074 - regression_loss: 0.3572 - classification_loss: 0.0501
 5107/10000 [==============>...............] - ETA: 1:12:32 - loss: 0.4073 - regression_loss: 0.3572 - classification_loss: 0.0501
 5108/10000 [==============>...............] - ETA: 1:12:32 - loss: 0.4074 - regression_loss: 0.3572 - classification_loss: 0.0501
 5109/10000 [==============>...............] - ETA: 1:12:31 - loss: 0.4074 - regression_loss: 0.3572 - classification_loss: 0.0501
 5110/10000 [==============>...............] - ETA: 1:12:30 - loss: 0.4073 - regression_loss: 0.3572 - classification_loss: 0.0501
 5111/10000 [==============>...............] - ETA: 1:12:29 - loss: 0.4073 - regression_loss: 0.3572 - classification_loss: 0.0501
 5112/10000 [==============>...............] - ETA: 1:12:28 - loss: 0.4073 - regression_loss: 0.3572 - classification_loss: 0.0501
 5113/10000 [==============>...............] - ETA: 1:12:27 - loss: 0.4073 - regression_loss: 0.3572 - classification_loss: 0.0501
 5114/10000 [==============>...............] - ETA: 1:12:26 - loss: 0.4073 - regression_loss: 0.3571 - classification_loss: 0.0501
 5115/10000 [==============>...............] - ETA: 1:12:25 - loss: 0.4072 - regression_loss: 0.3571 - classification_loss: 0.0501
 5116/10000 [==============>...............] - ETA: 1:12:24 - loss: 0.4072 - regression_loss: 0.3571 - classification_loss: 0.0501
 5117/10000 [==============>...............] - ETA: 1:12:24 - loss: 0.4072 - regression_loss: 0.3571 - classification_loss: 0.0501
 5118/10000 [==============>...............] - ETA: 1:12:23 - loss: 0.4072 - regression_loss: 0.3571 - classification_loss: 0.0501
 5119/10000 [==============>...............] - ETA: 1:12:22 - loss: 0.4071 - regression_loss: 0.3570 - classification_loss: 0.0501
 5120/10000 [==============>...............] - ETA: 1:12:21 - loss: 0.4071 - regression_loss: 0.3570 - classification_loss: 0.0501
 5121/10000 [==============>...............] - ETA: 1:12:20 - loss: 0.4072 - regression_loss: 0.3571 - classification_loss: 0.0501
 5122/10000 [==============>...............] - ETA: 1:12:19 - loss: 0.4072 - regression_loss: 0.3571 - classification_loss: 0.0501
 5123/10000 [==============>...............] - ETA: 1:12:18 - loss: 0.4074 - regression_loss: 0.3573 - classification_loss: 0.0501
 5124/10000 [==============>...............] - ETA: 1:12:17 - loss: 0.4075 - regression_loss: 0.3574 - classification_loss: 0.0501
 5125/10000 [==============>...............] - ETA: 1:12:16 - loss: 0.4075 - regression_loss: 0.3574 - classification_loss: 0.0501
 5126/10000 [==============>...............] - ETA: 1:12:16 - loss: 0.4075 - regression_loss: 0.3574 - classification_loss: 0.0501
 5127/10000 [==============>...............] - ETA: 1:12:15 - loss: 0.4074 - regression_loss: 0.3573 - classification_loss: 0.0501
 5128/10000 [==============>...............] - ETA: 1:12:14 - loss: 0.4074 - regression_loss: 0.3573 - classification_loss: 0.0501
 5129/10000 [==============>...............] - ETA: 1:12:13 - loss: 0.4074 - regression_loss: 0.3573 - classification_loss: 0.0501
 5130/10000 [==============>...............] - ETA: 1:12:12 - loss: 0.4075 - regression_loss: 0.3574 - classification_loss: 0.0501
 5131/10000 [==============>...............] - ETA: 1:12:11 - loss: 0.4074 - regression_loss: 0.3574 - classification_loss: 0.0501
 5132/10000 [==============>...............] - ETA: 1:12:10 - loss: 0.4074 - regression_loss: 0.3574 - classification_loss: 0.0501
 5133/10000 [==============>...............] - ETA: 1:12:09 - loss: 0.4074 - regression_loss: 0.3573 - classification_loss: 0.0501
 5134/10000 [==============>...............] - ETA: 1:12:08 - loss: 0.4074 - regression_loss: 0.3573 - classification_loss: 0.0501
 5135/10000 [==============>...............] - ETA: 1:12:08 - loss: 0.4074 - regression_loss: 0.3573 - classification_loss: 0.0501
 5136/10000 [==============>...............] - ETA: 1:12:07 - loss: 0.4073 - regression_loss: 0.3573 - classification_loss: 0.0501
 5137/10000 [==============>...............] - ETA: 1:12:06 - loss: 0.4073 - regression_loss: 0.3573 - classification_loss: 0.0501
 5138/10000 [==============>...............] - ETA: 1:12:05 - loss: 0.4073 - regression_loss: 0.3573 - classification_loss: 0.0500
 5139/10000 [==============>...............] - ETA: 1:12:04 - loss: 0.4074 - regression_loss: 0.3573 - classification_loss: 0.0500
 5140/10000 [==============>...............] - ETA: 1:12:03 - loss: 0.4073 - regression_loss: 0.3573 - classification_loss: 0.0500
 5141/10000 [==============>...............] - ETA: 1:12:02 - loss: 0.4074 - regression_loss: 0.3573 - classification_loss: 0.0501
 5142/10000 [==============>...............] - ETA: 1:12:01 - loss: 0.4074 - regression_loss: 0.3573 - classification_loss: 0.0501
 5143/10000 [==============>...............] - ETA: 1:12:00 - loss: 0.4074 - regression_loss: 0.3573 - classification_loss: 0.0500
 5144/10000 [==============>...............] - ETA: 1:11:59 - loss: 0.4074 - regression_loss: 0.3573 - classification_loss: 0.0500
 5145/10000 [==============>...............] - ETA: 1:11:59 - loss: 0.4073 - regression_loss: 0.3573 - classification_loss: 0.0500
 5146/10000 [==============>...............] - ETA: 1:11:58 - loss: 0.4074 - regression_loss: 0.3573 - classification_loss: 0.0500
 5147/10000 [==============>...............] - ETA: 1:11:57 - loss: 0.4074 - regression_loss: 0.3573 - classification_loss: 0.0500
 5148/10000 [==============>...............] - ETA: 1:11:56 - loss: 0.4073 - regression_loss: 0.3573 - classification_loss: 0.0500
 5149/10000 [==============>...............] - ETA: 1:11:55 - loss: 0.4073 - regression_loss: 0.3573 - classification_loss: 0.0500
 5150/10000 [==============>...............] - ETA: 1:11:54 - loss: 0.4073 - regression_loss: 0.3573 - classification_loss: 0.0500
 5151/10000 [==============>...............] - ETA: 1:11:53 - loss: 0.4073 - regression_loss: 0.3573 - classification_loss: 0.0500
 5152/10000 [==============>...............] - ETA: 1:11:52 - loss: 0.4073 - regression_loss: 0.3573 - classification_loss: 0.0500
 5153/10000 [==============>...............] - ETA: 1:11:51 - loss: 0.4074 - regression_loss: 0.3573 - classification_loss: 0.0500
 5154/10000 [==============>...............] - ETA: 1:11:51 - loss: 0.4073 - regression_loss: 0.3573 - classification_loss: 0.0500
 5155/10000 [==============>...............] - ETA: 1:11:50 - loss: 0.4074 - regression_loss: 0.3573 - classification_loss: 0.0500
 5156/10000 [==============>...............] - ETA: 1:11:49 - loss: 0.4074 - regression_loss: 0.3573 - classification_loss: 0.0500
 5157/10000 [==============>...............] - ETA: 1:11:48 - loss: 0.4074 - regression_loss: 0.3574 - classification_loss: 0.0500
 5158/10000 [==============>...............] - ETA: 1:11:47 - loss: 0.4075 - regression_loss: 0.3574 - classification_loss: 0.0500
 5159/10000 [==============>...............] - ETA: 1:11:46 - loss: 0.4075 - regression_loss: 0.3575 - classification_loss: 0.0500
 5160/10000 [==============>...............] - ETA: 1:11:45 - loss: 0.4075 - regression_loss: 0.3574 - classification_loss: 0.0500
 5161/10000 [==============>...............] - ETA: 1:11:44 - loss: 0.4074 - regression_loss: 0.3574 - classification_loss: 0.0500
 5162/10000 [==============>...............] - ETA: 1:11:43 - loss: 0.4073 - regression_loss: 0.3573 - classification_loss: 0.0500
 5163/10000 [==============>...............] - ETA: 1:11:43 - loss: 0.4073 - regression_loss: 0.3573 - classification_loss: 0.0500
 5164/10000 [==============>...............] - ETA: 1:11:42 - loss: 0.4073 - regression_loss: 0.3573 - classification_loss: 0.0500
 5165/10000 [==============>...............] - ETA: 1:11:41 - loss: 0.4072 - regression_loss: 0.3572 - classification_loss: 0.0500
 5166/10000 [==============>...............] - ETA: 1:11:40 - loss: 0.4072 - regression_loss: 0.3573 - classification_loss: 0.0500
 5167/10000 [==============>...............] - ETA: 1:11:39 - loss: 0.4072 - regression_loss: 0.3572 - classification_loss: 0.0500
 5168/10000 [==============>...............] - ETA: 1:11:38 - loss: 0.4072 - regression_loss: 0.3572 - classification_loss: 0.0500
 5169/10000 [==============>...............] - ETA: 1:11:37 - loss: 0.4072 - regression_loss: 0.3572 - classification_loss: 0.0500
 5170/10000 [==============>...............] - ETA: 1:11:36 - loss: 0.4072 - regression_loss: 0.3572 - classification_loss: 0.0500
 5171/10000 [==============>...............] - ETA: 1:11:35 - loss: 0.4072 - regression_loss: 0.3572 - classification_loss: 0.0500
 5172/10000 [==============>...............] - ETA: 1:11:35 - loss: 0.4074 - regression_loss: 0.3574 - classification_loss: 0.0500
 5173/10000 [==============>...............] - ETA: 1:11:34 - loss: 0.4075 - regression_loss: 0.3575 - classification_loss: 0.0500
 5174/10000 [==============>...............] - ETA: 1:11:33 - loss: 0.4074 - regression_loss: 0.3574 - classification_loss: 0.0500
 5175/10000 [==============>...............] - ETA: 1:11:32 - loss: 0.4075 - regression_loss: 0.3574 - classification_loss: 0.0500
 5176/10000 [==============>...............] - ETA: 1:11:31 - loss: 0.4074 - regression_loss: 0.3574 - classification_loss: 0.0500
 5177/10000 [==============>...............] - ETA: 1:11:30 - loss: 0.4073 - regression_loss: 0.3573 - classification_loss: 0.0500
 5178/10000 [==============>...............] - ETA: 1:11:29 - loss: 0.4073 - regression_loss: 0.3573 - classification_loss: 0.0500
 5179/10000 [==============>...............] - ETA: 1:11:28 - loss: 0.4073 - regression_loss: 0.3573 - classification_loss: 0.0500
 5180/10000 [==============>...............] - ETA: 1:11:27 - loss: 0.4073 - regression_loss: 0.3573 - classification_loss: 0.0500
 5181/10000 [==============>...............] - ETA: 1:11:26 - loss: 0.4072 - regression_loss: 0.3573 - classification_loss: 0.0500
 5182/10000 [==============>...............] - ETA: 1:11:26 - loss: 0.4073 - regression_loss: 0.3573 - classification_loss: 0.0500
 5183/10000 [==============>...............] - ETA: 1:11:25 - loss: 0.4072 - regression_loss: 0.3573 - classification_loss: 0.0500
 5184/10000 [==============>...............] - ETA: 1:11:24 - loss: 0.4072 - regression_loss: 0.3573 - classification_loss: 0.0500
 5185/10000 [==============>...............] - ETA: 1:11:23 - loss: 0.4074 - regression_loss: 0.3574 - classification_loss: 0.0500
 5186/10000 [==============>...............] - ETA: 1:11:22 - loss: 0.4074 - regression_loss: 0.3574 - classification_loss: 0.0500
 5187/10000 [==============>...............] - ETA: 1:11:21 - loss: 0.4074 - regression_loss: 0.3574 - classification_loss: 0.0499
 5188/10000 [==============>...............] - ETA: 1:11:20 - loss: 0.4074 - regression_loss: 0.3574 - classification_loss: 0.0500
 5189/10000 [==============>...............] - ETA: 1:11:19 - loss: 0.4074 - regression_loss: 0.3575 - classification_loss: 0.0500
 5190/10000 [==============>...............] - ETA: 1:11:18 - loss: 0.4074 - regression_loss: 0.3574 - classification_loss: 0.0500
 5191/10000 [==============>...............] - ETA: 1:11:18 - loss: 0.4074 - regression_loss: 0.3575 - classification_loss: 0.0500
 5192/10000 [==============>...............] - ETA: 1:11:17 - loss: 0.4074 - regression_loss: 0.3574 - classification_loss: 0.0499
 5193/10000 [==============>...............] - ETA: 1:11:16 - loss: 0.4075 - regression_loss: 0.3575 - classification_loss: 0.0499
 5194/10000 [==============>...............] - ETA: 1:11:15 - loss: 0.4075 - regression_loss: 0.3575 - classification_loss: 0.0499
 5195/10000 [==============>...............] - ETA: 1:11:14 - loss: 0.4075 - regression_loss: 0.3575 - classification_loss: 0.0499
 5196/10000 [==============>...............] - ETA: 1:11:13 - loss: 0.4075 - regression_loss: 0.3576 - classification_loss: 0.0499
 5197/10000 [==============>...............] - ETA: 1:11:12 - loss: 0.4074 - regression_loss: 0.3575 - classification_loss: 0.0499
 5198/10000 [==============>...............] - ETA: 1:11:11 - loss: 0.4075 - regression_loss: 0.3574 - classification_loss: 0.0500
 5199/10000 [==============>...............] - ETA: 1:11:10 - loss: 0.4074 - regression_loss: 0.3574 - classification_loss: 0.0500
 5200/10000 [==============>...............] - ETA: 1:11:10 - loss: 0.4075 - regression_loss: 0.3575 - classification_loss: 0.0500
 5201/10000 [==============>...............] - ETA: 1:11:09 - loss: 0.4075 - regression_loss: 0.3575 - classification_loss: 0.0500
 5202/10000 [==============>...............] - ETA: 1:11:08 - loss: 0.4075 - regression_loss: 0.3575 - classification_loss: 0.0500
 5203/10000 [==============>...............] - ETA: 1:11:07 - loss: 0.4075 - regression_loss: 0.3575 - classification_loss: 0.0500
 5204/10000 [==============>...............] - ETA: 1:11:06 - loss: 0.4076 - regression_loss: 0.3575 - classification_loss: 0.0501
 5205/10000 [==============>...............] - ETA: 1:11:05 - loss: 0.4076 - regression_loss: 0.3575 - classification_loss: 0.0501
 5206/10000 [==============>...............] - ETA: 1:11:04 - loss: 0.4077 - regression_loss: 0.3576 - classification_loss: 0.0501
 5207/10000 [==============>...............] - ETA: 1:11:03 - loss: 0.4077 - regression_loss: 0.3576 - classification_loss: 0.0501
 5208/10000 [==============>...............] - ETA: 1:11:02 - loss: 0.4077 - regression_loss: 0.3576 - classification_loss: 0.0501
 5209/10000 [==============>...............] - ETA: 1:11:02 - loss: 0.4077 - regression_loss: 0.3576 - classification_loss: 0.0501
 5210/10000 [==============>...............] - ETA: 1:11:01 - loss: 0.4077 - regression_loss: 0.3576 - classification_loss: 0.0500
 5211/10000 [==============>...............] - ETA: 1:11:00 - loss: 0.4077 - regression_loss: 0.3577 - classification_loss: 0.0501
 5212/10000 [==============>...............] - ETA: 1:10:59 - loss: 0.4079 - regression_loss: 0.3579 - classification_loss: 0.0501
 5213/10000 [==============>...............] - ETA: 1:10:58 - loss: 0.4079 - regression_loss: 0.3579 - classification_loss: 0.0501
 5214/10000 [==============>...............] - ETA: 1:10:57 - loss: 0.4079 - regression_loss: 0.3579 - classification_loss: 0.0501
 5215/10000 [==============>...............] - ETA: 1:10:56 - loss: 0.4079 - regression_loss: 0.3579 - classification_loss: 0.0501
 5216/10000 [==============>...............] - ETA: 1:10:55 - loss: 0.4079 - regression_loss: 0.3579 - classification_loss: 0.0500
 5217/10000 [==============>...............] - ETA: 1:10:54 - loss: 0.4079 - regression_loss: 0.3578 - classification_loss: 0.0500
 5218/10000 [==============>...............] - ETA: 1:10:54 - loss: 0.4079 - regression_loss: 0.3578 - classification_loss: 0.0500
 5219/10000 [==============>...............] - ETA: 1:10:53 - loss: 0.4079 - regression_loss: 0.3579 - classification_loss: 0.0500
 5220/10000 [==============>...............] - ETA: 1:10:52 - loss: 0.4079 - regression_loss: 0.3579 - classification_loss: 0.0500
 5221/10000 [==============>...............] - ETA: 1:10:51 - loss: 0.4079 - regression_loss: 0.3579 - classification_loss: 0.0500
 5222/10000 [==============>...............] - ETA: 1:10:50 - loss: 0.4079 - regression_loss: 0.3579 - classification_loss: 0.0500
 5223/10000 [==============>...............] - ETA: 1:10:49 - loss: 0.4078 - regression_loss: 0.3578 - classification_loss: 0.0500
 5224/10000 [==============>...............] - ETA: 1:10:48 - loss: 0.4079 - regression_loss: 0.3578 - classification_loss: 0.0500
 5225/10000 [==============>...............] - ETA: 1:10:47 - loss: 0.4078 - regression_loss: 0.3578 - classification_loss: 0.0500
 5226/10000 [==============>...............] - ETA: 1:10:46 - loss: 0.4078 - regression_loss: 0.3578 - classification_loss: 0.0500
 5227/10000 [==============>...............] - ETA: 1:10:46 - loss: 0.4078 - regression_loss: 0.3578 - classification_loss: 0.0500
 5228/10000 [==============>...............] - ETA: 1:10:45 - loss: 0.4078 - regression_loss: 0.3577 - classification_loss: 0.0500
 5229/10000 [==============>...............] - ETA: 1:10:44 - loss: 0.4077 - regression_loss: 0.3577 - classification_loss: 0.0500
 5230/10000 [==============>...............] - ETA: 1:10:43 - loss: 0.4077 - regression_loss: 0.3577 - classification_loss: 0.0500
 5231/10000 [==============>...............] - ETA: 1:10:42 - loss: 0.4077 - regression_loss: 0.3577 - classification_loss: 0.0500
 5232/10000 [==============>...............] - ETA: 1:10:41 - loss: 0.4077 - regression_loss: 0.3577 - classification_loss: 0.0500
 5233/10000 [==============>...............] - ETA: 1:10:40 - loss: 0.4076 - regression_loss: 0.3576 - classification_loss: 0.0500
 5234/10000 [==============>...............] - ETA: 1:10:39 - loss: 0.4076 - regression_loss: 0.3576 - classification_loss: 0.0500
 5235/10000 [==============>...............] - ETA: 1:10:38 - loss: 0.4076 - regression_loss: 0.3576 - classification_loss: 0.0500
 5236/10000 [==============>...............] - ETA: 1:10:37 - loss: 0.4077 - regression_loss: 0.3577 - classification_loss: 0.0500
 5237/10000 [==============>...............] - ETA: 1:10:37 - loss: 0.4078 - regression_loss: 0.3578 - classification_loss: 0.0500
 5238/10000 [==============>...............] - ETA: 1:10:36 - loss: 0.4079 - regression_loss: 0.3578 - classification_loss: 0.0500
 5239/10000 [==============>...............] - ETA: 1:10:35 - loss: 0.4078 - regression_loss: 0.3578 - classification_loss: 0.0500
 5240/10000 [==============>...............] - ETA: 1:10:34 - loss: 0.4081 - regression_loss: 0.3581 - classification_loss: 0.0501
 5241/10000 [==============>...............] - ETA: 1:10:33 - loss: 0.4080 - regression_loss: 0.3580 - classification_loss: 0.0500
 5242/10000 [==============>...............] - ETA: 1:10:32 - loss: 0.4080 - regression_loss: 0.3580 - classification_loss: 0.0500
 5243/10000 [==============>...............] - ETA: 1:10:31 - loss: 0.4080 - regression_loss: 0.3580 - classification_loss: 0.0500
 5244/10000 [==============>...............] - ETA: 1:10:30 - loss: 0.4080 - regression_loss: 0.3579 - classification_loss: 0.0500
 5245/10000 [==============>...............] - ETA: 1:10:29 - loss: 0.4080 - regression_loss: 0.3579 - classification_loss: 0.0500
 5246/10000 [==============>...............] - ETA: 1:10:29 - loss: 0.4079 - regression_loss: 0.3579 - classification_loss: 0.0500
 5247/10000 [==============>...............] - ETA: 1:10:28 - loss: 0.4079 - regression_loss: 0.3579 - classification_loss: 0.0500
 5248/10000 [==============>...............] - ETA: 1:10:27 - loss: 0.4079 - regression_loss: 0.3579 - classification_loss: 0.0500
 5249/10000 [==============>...............] - ETA: 1:10:26 - loss: 0.4079 - regression_loss: 0.3579 - classification_loss: 0.0500
 5250/10000 [==============>...............] - ETA: 1:10:25 - loss: 0.4078 - regression_loss: 0.3578 - classification_loss: 0.0500
 5251/10000 [==============>...............] - ETA: 1:10:24 - loss: 0.4078 - regression_loss: 0.3578 - classification_loss: 0.0500
 5252/10000 [==============>...............] - ETA: 1:10:23 - loss: 0.4078 - regression_loss: 0.3578 - classification_loss: 0.0500
 5253/10000 [==============>...............] - ETA: 1:10:22 - loss: 0.4078 - regression_loss: 0.3578 - classification_loss: 0.0500
 5254/10000 [==============>...............] - ETA: 1:10:21 - loss: 0.4078 - regression_loss: 0.3578 - classification_loss: 0.0500
 5255/10000 [==============>...............] - ETA: 1:10:21 - loss: 0.4077 - regression_loss: 0.3578 - classification_loss: 0.0500
 5256/10000 [==============>...............] - ETA: 1:10:20 - loss: 0.4078 - regression_loss: 0.3578 - classification_loss: 0.0500
 5257/10000 [==============>...............] - ETA: 1:10:19 - loss: 0.4077 - regression_loss: 0.3577 - classification_loss: 0.0500
 5258/10000 [==============>...............] - ETA: 1:10:18 - loss: 0.4077 - regression_loss: 0.3577 - classification_loss: 0.0500
 5259/10000 [==============>...............] - ETA: 1:10:17 - loss: 0.4077 - regression_loss: 0.3578 - classification_loss: 0.0500
 5260/10000 [==============>...............] - ETA: 1:10:16 - loss: 0.4077 - regression_loss: 0.3578 - classification_loss: 0.0500
 5261/10000 [==============>...............] - ETA: 1:10:15 - loss: 0.4077 - regression_loss: 0.3577 - classification_loss: 0.0500
 5262/10000 [==============>...............] - ETA: 1:10:14 - loss: 0.4077 - regression_loss: 0.3577 - classification_loss: 0.0500
 5263/10000 [==============>...............] - ETA: 1:10:13 - loss: 0.4076 - regression_loss: 0.3577 - classification_loss: 0.0499
 5264/10000 [==============>...............] - ETA: 1:10:13 - loss: 0.4076 - regression_loss: 0.3577 - classification_loss: 0.0499
 5265/10000 [==============>...............] - ETA: 1:10:12 - loss: 0.4076 - regression_loss: 0.3576 - classification_loss: 0.0499
 5266/10000 [==============>...............] - ETA: 1:10:11 - loss: 0.4076 - regression_loss: 0.3576 - classification_loss: 0.0499
 5267/10000 [==============>...............] - ETA: 1:10:10 - loss: 0.4075 - regression_loss: 0.3576 - classification_loss: 0.0499
 5268/10000 [==============>...............] - ETA: 1:10:09 - loss: 0.4075 - regression_loss: 0.3576 - classification_loss: 0.0499
 5269/10000 [==============>...............] - ETA: 1:10:08 - loss: 0.4075 - regression_loss: 0.3576 - classification_loss: 0.0499
 5270/10000 [==============>...............] - ETA: 1:10:07 - loss: 0.4075 - regression_loss: 0.3576 - classification_loss: 0.0499
 5271/10000 [==============>...............] - ETA: 1:10:06 - loss: 0.4076 - regression_loss: 0.3577 - classification_loss: 0.0499
 5272/10000 [==============>...............] - ETA: 1:10:05 - loss: 0.4076 - regression_loss: 0.3577 - classification_loss: 0.0499
 5273/10000 [==============>...............] - ETA: 1:10:05 - loss: 0.4077 - regression_loss: 0.3577 - classification_loss: 0.0499
 5274/10000 [==============>...............] - ETA: 1:10:04 - loss: 0.4076 - regression_loss: 0.3577 - classification_loss: 0.0499
 5275/10000 [==============>...............] - ETA: 1:10:03 - loss: 0.4077 - regression_loss: 0.3577 - classification_loss: 0.0499
 5276/10000 [==============>...............] - ETA: 1:10:02 - loss: 0.4076 - regression_loss: 0.3577 - classification_loss: 0.0499
 5277/10000 [==============>...............] - ETA: 1:10:01 - loss: 0.4076 - regression_loss: 0.3577 - classification_loss: 0.0499
 5278/10000 [==============>...............] - ETA: 1:10:00 - loss: 0.4076 - regression_loss: 0.3577 - classification_loss: 0.0499
 5279/10000 [==============>...............] - ETA: 1:09:59 - loss: 0.4076 - regression_loss: 0.3577 - classification_loss: 0.0499
 5280/10000 [==============>...............] - ETA: 1:09:58 - loss: 0.4076 - regression_loss: 0.3577 - classification_loss: 0.0499
 5281/10000 [==============>...............] - ETA: 1:09:57 - loss: 0.4077 - regression_loss: 0.3578 - classification_loss: 0.0499
 5282/10000 [==============>...............] - ETA: 1:09:57 - loss: 0.4077 - regression_loss: 0.3578 - classification_loss: 0.0499
 5283/10000 [==============>...............] - ETA: 1:09:56 - loss: 0.4077 - regression_loss: 0.3578 - classification_loss: 0.0499
 5284/10000 [==============>...............] - ETA: 1:09:55 - loss: 0.4077 - regression_loss: 0.3578 - classification_loss: 0.0499
 5285/10000 [==============>...............] - ETA: 1:09:54 - loss: 0.4077 - regression_loss: 0.3578 - classification_loss: 0.0499
 5286/10000 [==============>...............] - ETA: 1:09:53 - loss: 0.4077 - regression_loss: 0.3578 - classification_loss: 0.0499
 5287/10000 [==============>...............] - ETA: 1:09:52 - loss: 0.4077 - regression_loss: 0.3578 - classification_loss: 0.0499
 5288/10000 [==============>...............] - ETA: 1:09:51 - loss: 0.4077 - regression_loss: 0.3578 - classification_loss: 0.0499
 5289/10000 [==============>...............] - ETA: 1:09:50 - loss: 0.4077 - regression_loss: 0.3578 - classification_loss: 0.0499
 5290/10000 [==============>...............] - ETA: 1:09:49 - loss: 0.4077 - regression_loss: 0.3578 - classification_loss: 0.0499
 5291/10000 [==============>...............] - ETA: 1:09:48 - loss: 0.4077 - regression_loss: 0.3578 - classification_loss: 0.0499
 5292/10000 [==============>...............] - ETA: 1:09:48 - loss: 0.4077 - regression_loss: 0.3578 - classification_loss: 0.0499
 5293/10000 [==============>...............] - ETA: 1:09:47 - loss: 0.4077 - regression_loss: 0.3578 - classification_loss: 0.0499
 5294/10000 [==============>...............] - ETA: 1:09:46 - loss: 0.4077 - regression_loss: 0.3578 - classification_loss: 0.0499
 5295/10000 [==============>...............] - ETA: 1:09:45 - loss: 0.4077 - regression_loss: 0.3578 - classification_loss: 0.0499
 5296/10000 [==============>...............] - ETA: 1:09:44 - loss: 0.4078 - regression_loss: 0.3579 - classification_loss: 0.0499
 5297/10000 [==============>...............] - ETA: 1:09:43 - loss: 0.4078 - regression_loss: 0.3579 - classification_loss: 0.0499
 5298/10000 [==============>...............] - ETA: 1:09:42 - loss: 0.4078 - regression_loss: 0.3579 - classification_loss: 0.0499
 5299/10000 [==============>...............] - ETA: 1:09:41 - loss: 0.4078 - regression_loss: 0.3579 - classification_loss: 0.0499
 5300/10000 [==============>...............] - ETA: 1:09:40 - loss: 0.4077 - regression_loss: 0.3578 - classification_loss: 0.0499
 5301/10000 [==============>...............] - ETA: 1:09:40 - loss: 0.4077 - regression_loss: 0.3578 - classification_loss: 0.0499
 5302/10000 [==============>...............] - ETA: 1:09:39 - loss: 0.4077 - regression_loss: 0.3579 - classification_loss: 0.0499
 5303/10000 [==============>...............] - ETA: 1:09:38 - loss: 0.4077 - regression_loss: 0.3578 - classification_loss: 0.0499
 5304/10000 [==============>...............] - ETA: 1:09:37 - loss: 0.4077 - regression_loss: 0.3578 - classification_loss: 0.0499
 5305/10000 [==============>...............] - ETA: 1:09:36 - loss: 0.4076 - regression_loss: 0.3577 - classification_loss: 0.0499
 5306/10000 [==============>...............] - ETA: 1:09:35 - loss: 0.4076 - regression_loss: 0.3577 - classification_loss: 0.0499
 5307/10000 [==============>...............] - ETA: 1:09:34 - loss: 0.4076 - regression_loss: 0.3577 - classification_loss: 0.0499
 5308/10000 [==============>...............] - ETA: 1:09:33 - loss: 0.4076 - regression_loss: 0.3577 - classification_loss: 0.0499
 5309/10000 [==============>...............] - ETA: 1:09:32 - loss: 0.4076 - regression_loss: 0.3577 - classification_loss: 0.0499
 5310/10000 [==============>...............] - ETA: 1:09:32 - loss: 0.4076 - regression_loss: 0.3577 - classification_loss: 0.0499
 5311/10000 [==============>...............] - ETA: 1:09:31 - loss: 0.4076 - regression_loss: 0.3577 - classification_loss: 0.0499
 5312/10000 [==============>...............] - ETA: 1:09:30 - loss: 0.4076 - regression_loss: 0.3577 - classification_loss: 0.0499
 5313/10000 [==============>...............] - ETA: 1:09:29 - loss: 0.4075 - regression_loss: 0.3577 - classification_loss: 0.0498
 5314/10000 [==============>...............] - ETA: 1:09:28 - loss: 0.4075 - regression_loss: 0.3577 - classification_loss: 0.0498
 5315/10000 [==============>...............] - ETA: 1:09:27 - loss: 0.4076 - regression_loss: 0.3577 - classification_loss: 0.0499
 5316/10000 [==============>...............] - ETA: 1:09:26 - loss: 0.4075 - regression_loss: 0.3577 - classification_loss: 0.0498
 5317/10000 [==============>...............] - ETA: 1:09:25 - loss: 0.4077 - regression_loss: 0.3579 - classification_loss: 0.0499
 5318/10000 [==============>...............] - ETA: 1:09:24 - loss: 0.4077 - regression_loss: 0.3579 - classification_loss: 0.0498
 5319/10000 [==============>...............] - ETA: 1:09:24 - loss: 0.4077 - regression_loss: 0.3579 - classification_loss: 0.0498
 5320/10000 [==============>...............] - ETA: 1:09:23 - loss: 0.4077 - regression_loss: 0.3578 - classification_loss: 0.0498
 5321/10000 [==============>...............] - ETA: 1:09:22 - loss: 0.4077 - regression_loss: 0.3579 - classification_loss: 0.0498
 5322/10000 [==============>...............] - ETA: 1:09:21 - loss: 0.4078 - regression_loss: 0.3579 - classification_loss: 0.0498
 5323/10000 [==============>...............] - ETA: 1:09:20 - loss: 0.4077 - regression_loss: 0.3579 - classification_loss: 0.0498
 5324/10000 [==============>...............] - ETA: 1:09:19 - loss: 0.4078 - regression_loss: 0.3579 - classification_loss: 0.0498
 5325/10000 [==============>...............] - ETA: 1:09:18 - loss: 0.4078 - regression_loss: 0.3579 - classification_loss: 0.0498
 5326/10000 [==============>...............] - ETA: 1:09:17 - loss: 0.4078 - regression_loss: 0.3579 - classification_loss: 0.0498
 5327/10000 [==============>...............] - ETA: 1:09:16 - loss: 0.4078 - regression_loss: 0.3579 - classification_loss: 0.0498
 5328/10000 [==============>...............] - ETA: 1:09:16 - loss: 0.4077 - regression_loss: 0.3579 - classification_loss: 0.0498
 5329/10000 [==============>...............] - ETA: 1:09:15 - loss: 0.4076 - regression_loss: 0.3578 - classification_loss: 0.0498
 5330/10000 [==============>...............] - ETA: 1:09:14 - loss: 0.4077 - regression_loss: 0.3578 - classification_loss: 0.0498
 5331/10000 [==============>...............] - ETA: 1:09:13 - loss: 0.4078 - regression_loss: 0.3580 - classification_loss: 0.0499
 5332/10000 [==============>...............] - ETA: 1:09:12 - loss: 0.4079 - regression_loss: 0.3580 - classification_loss: 0.0499
 5333/10000 [==============>...............] - ETA: 1:09:11 - loss: 0.4079 - regression_loss: 0.3580 - classification_loss: 0.0499
 5334/10000 [===============>..............] - ETA: 1:09:10 - loss: 0.4079 - regression_loss: 0.3580 - classification_loss: 0.0499
 5335/10000 [===============>..............] - ETA: 1:09:09 - loss: 0.4079 - regression_loss: 0.3581 - classification_loss: 0.0499
 5336/10000 [===============>..............] - ETA: 1:09:08 - loss: 0.4079 - regression_loss: 0.3580 - classification_loss: 0.0499
 5337/10000 [===============>..............] - ETA: 1:09:08 - loss: 0.4080 - regression_loss: 0.3581 - classification_loss: 0.0499
 5338/10000 [===============>..............] - ETA: 1:09:07 - loss: 0.4081 - regression_loss: 0.3582 - classification_loss: 0.0499
 5339/10000 [===============>..............] - ETA: 1:09:06 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0499
 5340/10000 [===============>..............] - ETA: 1:09:05 - loss: 0.4081 - regression_loss: 0.3582 - classification_loss: 0.0499
 5341/10000 [===============>..............] - ETA: 1:09:04 - loss: 0.4080 - regression_loss: 0.3581 - classification_loss: 0.0499
 5342/10000 [===============>..............] - ETA: 1:09:03 - loss: 0.4080 - regression_loss: 0.3581 - classification_loss: 0.0499
 5343/10000 [===============>..............] - ETA: 1:09:02 - loss: 0.4081 - regression_loss: 0.3582 - classification_loss: 0.0499
 5344/10000 [===============>..............] - ETA: 1:09:01 - loss: 0.4080 - regression_loss: 0.3581 - classification_loss: 0.0499
 5345/10000 [===============>..............] - ETA: 1:09:00 - loss: 0.4080 - regression_loss: 0.3581 - classification_loss: 0.0499
 5346/10000 [===============>..............] - ETA: 1:08:59 - loss: 0.4080 - regression_loss: 0.3581 - classification_loss: 0.0499
 5347/10000 [===============>..............] - ETA: 1:08:59 - loss: 0.4080 - regression_loss: 0.3581 - classification_loss: 0.0499
 5348/10000 [===============>..............] - ETA: 1:08:58 - loss: 0.4079 - regression_loss: 0.3581 - classification_loss: 0.0499
 5349/10000 [===============>..............] - ETA: 1:08:57 - loss: 0.4079 - regression_loss: 0.3580 - classification_loss: 0.0499
 5350/10000 [===============>..............] - ETA: 1:08:56 - loss: 0.4079 - regression_loss: 0.3580 - classification_loss: 0.0499
 5351/10000 [===============>..............] - ETA: 1:08:55 - loss: 0.4078 - regression_loss: 0.3580 - classification_loss: 0.0498
 5352/10000 [===============>..............] - ETA: 1:08:54 - loss: 0.4078 - regression_loss: 0.3580 - classification_loss: 0.0498
 5353/10000 [===============>..............] - ETA: 1:08:53 - loss: 0.4079 - regression_loss: 0.3580 - classification_loss: 0.0498
 5354/10000 [===============>..............] - ETA: 1:08:52 - loss: 0.4080 - regression_loss: 0.3581 - classification_loss: 0.0498
 5355/10000 [===============>..............] - ETA: 1:08:51 - loss: 0.4079 - regression_loss: 0.3581 - classification_loss: 0.0499
 5356/10000 [===============>..............] - ETA: 1:08:51 - loss: 0.4079 - regression_loss: 0.3580 - classification_loss: 0.0498
 5357/10000 [===============>..............] - ETA: 1:08:50 - loss: 0.4078 - regression_loss: 0.3580 - classification_loss: 0.0498
 5358/10000 [===============>..............] - ETA: 1:08:49 - loss: 0.4079 - regression_loss: 0.3580 - classification_loss: 0.0498
 5359/10000 [===============>..............] - ETA: 1:08:48 - loss: 0.4078 - regression_loss: 0.3580 - classification_loss: 0.0498
 5360/10000 [===============>..............] - ETA: 1:08:47 - loss: 0.4079 - regression_loss: 0.3580 - classification_loss: 0.0498
 5361/10000 [===============>..............] - ETA: 1:08:46 - loss: 0.4079 - regression_loss: 0.3580 - classification_loss: 0.0498
 5362/10000 [===============>..............] - ETA: 1:08:45 - loss: 0.4079 - regression_loss: 0.3581 - classification_loss: 0.0498
 5363/10000 [===============>..............] - ETA: 1:08:44 - loss: 0.4079 - regression_loss: 0.3581 - classification_loss: 0.0498
 5364/10000 [===============>..............] - ETA: 1:08:43 - loss: 0.4079 - regression_loss: 0.3581 - classification_loss: 0.0498
 5365/10000 [===============>..............] - ETA: 1:08:43 - loss: 0.4079 - regression_loss: 0.3581 - classification_loss: 0.0498
 5366/10000 [===============>..............] - ETA: 1:08:42 - loss: 0.4078 - regression_loss: 0.3580 - classification_loss: 0.0498
 5367/10000 [===============>..............] - ETA: 1:08:41 - loss: 0.4078 - regression_loss: 0.3580 - classification_loss: 0.0498
 5368/10000 [===============>..............] - ETA: 1:08:40 - loss: 0.4077 - regression_loss: 0.3579 - classification_loss: 0.0498
 5369/10000 [===============>..............] - ETA: 1:08:39 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0498
 5370/10000 [===============>..............] - ETA: 1:08:38 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0498
 5371/10000 [===============>..............] - ETA: 1:08:37 - loss: 0.4081 - regression_loss: 0.3582 - classification_loss: 0.0498
 5372/10000 [===============>..............] - ETA: 1:08:36 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0498
 5373/10000 [===============>..............] - ETA: 1:08:35 - loss: 0.4081 - regression_loss: 0.3582 - classification_loss: 0.0498
 5374/10000 [===============>..............] - ETA: 1:08:35 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0498
 5375/10000 [===============>..............] - ETA: 1:08:34 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0498
 5376/10000 [===============>..............] - ETA: 1:08:33 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0498
 5377/10000 [===============>..............] - ETA: 1:08:32 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0498
 5378/10000 [===============>..............] - ETA: 1:08:31 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0498
 5379/10000 [===============>..............] - ETA: 1:08:30 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0498
 5380/10000 [===============>..............] - ETA: 1:08:29 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0498
 5381/10000 [===============>..............] - ETA: 1:08:28 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0498
 5382/10000 [===============>..............] - ETA: 1:08:27 - loss: 0.4081 - regression_loss: 0.3583 - classification_loss: 0.0498
 5383/10000 [===============>..............] - ETA: 1:08:26 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0498
 5384/10000 [===============>..............] - ETA: 1:08:26 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0498
 5385/10000 [===============>..............] - ETA: 1:08:25 - loss: 0.4079 - regression_loss: 0.3582 - classification_loss: 0.0498
 5386/10000 [===============>..............] - ETA: 1:08:24 - loss: 0.4079 - regression_loss: 0.3581 - classification_loss: 0.0498
 5387/10000 [===============>..............] - ETA: 1:08:23 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0498
 5388/10000 [===============>..............] - ETA: 1:08:22 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0498
 5389/10000 [===============>..............] - ETA: 1:08:21 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0498
 5390/10000 [===============>..............] - ETA: 1:08:20 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0498
 5391/10000 [===============>..............] - ETA: 1:08:19 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0498
 5392/10000 [===============>..............] - ETA: 1:08:18 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0498
 5393/10000 [===============>..............] - ETA: 1:08:18 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0498
 5394/10000 [===============>..............] - ETA: 1:08:17 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0498
 5395/10000 [===============>..............] - ETA: 1:08:16 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0498
 5396/10000 [===============>..............] - ETA: 1:08:15 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0498
 5397/10000 [===============>..............] - ETA: 1:08:14 - loss: 0.4079 - regression_loss: 0.3582 - classification_loss: 0.0498
 5398/10000 [===============>..............] - ETA: 1:08:13 - loss: 0.4079 - regression_loss: 0.3582 - classification_loss: 0.0498
 5399/10000 [===============>..............] - ETA: 1:08:12 - loss: 0.4079 - regression_loss: 0.3581 - classification_loss: 0.0498
 5400/10000 [===============>..............] - ETA: 1:08:11 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0498
 5401/10000 [===============>..............] - ETA: 1:08:10 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0498
 5402/10000 [===============>..............] - ETA: 1:08:10 - loss: 0.4081 - regression_loss: 0.3583 - classification_loss: 0.0498
 5403/10000 [===============>..............] - ETA: 1:08:09 - loss: 0.4081 - regression_loss: 0.3583 - classification_loss: 0.0498
 5404/10000 [===============>..............] - ETA: 1:08:08 - loss: 0.4081 - regression_loss: 0.3583 - classification_loss: 0.0498
 5405/10000 [===============>..............] - ETA: 1:08:07 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0498
 5406/10000 [===============>..............] - ETA: 1:08:06 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0498
 5407/10000 [===============>..............] - ETA: 1:08:05 - loss: 0.4080 - regression_loss: 0.3583 - classification_loss: 0.0498
 5408/10000 [===============>..............] - ETA: 1:08:04 - loss: 0.4081 - regression_loss: 0.3583 - classification_loss: 0.0498
 5409/10000 [===============>..............] - ETA: 1:08:03 - loss: 0.4081 - regression_loss: 0.3583 - classification_loss: 0.0498
 5410/10000 [===============>..............] - ETA: 1:08:02 - loss: 0.4081 - regression_loss: 0.3583 - classification_loss: 0.0498
 5411/10000 [===============>..............] - ETA: 1:08:02 - loss: 0.4081 - regression_loss: 0.3583 - classification_loss: 0.0498
 5412/10000 [===============>..............] - ETA: 1:08:01 - loss: 0.4081 - regression_loss: 0.3583 - classification_loss: 0.0498
 5413/10000 [===============>..............] - ETA: 1:08:00 - loss: 0.4081 - regression_loss: 0.3583 - classification_loss: 0.0498
 5414/10000 [===============>..............] - ETA: 1:07:59 - loss: 0.4082 - regression_loss: 0.3584 - classification_loss: 0.0498
 5415/10000 [===============>..............] - ETA: 1:07:58 - loss: 0.4081 - regression_loss: 0.3584 - classification_loss: 0.0498
 5416/10000 [===============>..............] - ETA: 1:07:57 - loss: 0.4081 - regression_loss: 0.3583 - classification_loss: 0.0498
 5417/10000 [===============>..............] - ETA: 1:07:56 - loss: 0.4081 - regression_loss: 0.3583 - classification_loss: 0.0498
 5418/10000 [===============>..............] - ETA: 1:07:55 - loss: 0.4081 - regression_loss: 0.3583 - classification_loss: 0.0498
 5419/10000 [===============>..............] - ETA: 1:07:54 - loss: 0.4081 - regression_loss: 0.3583 - classification_loss: 0.0498
 5420/10000 [===============>..............] - ETA: 1:07:54 - loss: 0.4080 - regression_loss: 0.3582 - classification_loss: 0.0498
 5421/10000 [===============>..............] - ETA: 1:07:53 - loss: 0.4081 - regression_loss: 0.3583 - classification_loss: 0.0498
 5422/10000 [===============>..............] - ETA: 1:07:52 - loss: 0.4081 - regression_loss: 0.3583 - classification_loss: 0.0498
 5423/10000 [===============>..............] - ETA: 1:07:51 - loss: 0.4081 - regression_loss: 0.3583 - classification_loss: 0.0498
 5424/10000 [===============>..............] - ETA: 1:07:50 - loss: 0.4081 - regression_loss: 0.3583 - classification_loss: 0.0498
 5425/10000 [===============>..............] - ETA: 1:07:49 - loss: 0.4080 - regression_loss: 0.3583 - classification_loss: 0.0498
 5426/10000 [===============>..............] - ETA: 1:07:48 - loss: 0.4082 - regression_loss: 0.3584 - classification_loss: 0.0498
 5427/10000 [===============>..............] - ETA: 1:07:47 - loss: 0.4082 - regression_loss: 0.3584 - classification_loss: 0.0498
 5428/10000 [===============>..............] - ETA: 1:07:46 - loss: 0.4082 - regression_loss: 0.3584 - classification_loss: 0.0498
 5429/10000 [===============>..............] - ETA: 1:07:46 - loss: 0.4082 - regression_loss: 0.3584 - classification_loss: 0.0498
 5430/10000 [===============>..............] - ETA: 1:07:45 - loss: 0.4081 - regression_loss: 0.3584 - classification_loss: 0.0498
 5431/10000 [===============>..............] - ETA: 1:07:44 - loss: 0.4081 - regression_loss: 0.3584 - classification_loss: 0.0498
 5432/10000 [===============>..............] - ETA: 1:07:43 - loss: 0.4082 - regression_loss: 0.3584 - classification_loss: 0.0498
 5433/10000 [===============>..............] - ETA: 1:07:42 - loss: 0.4082 - regression_loss: 0.3584 - classification_loss: 0.0498
 5434/10000 [===============>..............] - ETA: 1:07:41 - loss: 0.4081 - regression_loss: 0.3584 - classification_loss: 0.0497
 5435/10000 [===============>..............] - ETA: 1:07:40 - loss: 0.4081 - regression_loss: 0.3584 - classification_loss: 0.0497
 5436/10000 [===============>..............] - ETA: 1:07:39 - loss: 0.4081 - regression_loss: 0.3583 - classification_loss: 0.0497
 5437/10000 [===============>..............] - ETA: 1:07:38 - loss: 0.4081 - regression_loss: 0.3583 - classification_loss: 0.0497
 5438/10000 [===============>..............] - ETA: 1:07:37 - loss: 0.4081 - regression_loss: 0.3584 - classification_loss: 0.0497
 5439/10000 [===============>..............] - ETA: 1:07:37 - loss: 0.4081 - regression_loss: 0.3584 - classification_loss: 0.0497
 5440/10000 [===============>..............] - ETA: 1:07:36 - loss: 0.4081 - regression_loss: 0.3584 - classification_loss: 0.0497
 5441/10000 [===============>..............] - ETA: 1:07:35 - loss: 0.4082 - regression_loss: 0.3585 - classification_loss: 0.0497
 5442/10000 [===============>..............] - ETA: 1:07:34 - loss: 0.4083 - regression_loss: 0.3586 - classification_loss: 0.0497
 5443/10000 [===============>..............] - ETA: 1:07:33 - loss: 0.4084 - regression_loss: 0.3587 - classification_loss: 0.0497
 5444/10000 [===============>..............] - ETA: 1:07:32 - loss: 0.4084 - regression_loss: 0.3587 - classification_loss: 0.0497
 5445/10000 [===============>..............] - ETA: 1:07:31 - loss: 0.4084 - regression_loss: 0.3587 - classification_loss: 0.0497
 5446/10000 [===============>..............] - ETA: 1:07:30 - loss: 0.4085 - regression_loss: 0.3588 - classification_loss: 0.0497
 5447/10000 [===============>..............] - ETA: 1:07:29 - loss: 0.4085 - regression_loss: 0.3588 - classification_loss: 0.0497
 5448/10000 [===============>..............] - ETA: 1:07:29 - loss: 0.4085 - regression_loss: 0.3588 - classification_loss: 0.0497
 5449/10000 [===============>..............] - ETA: 1:07:28 - loss: 0.4087 - regression_loss: 0.3589 - classification_loss: 0.0497
 5450/10000 [===============>..............] - ETA: 1:07:27 - loss: 0.4087 - regression_loss: 0.3589 - classification_loss: 0.0498
 5451/10000 [===============>..............] - ETA: 1:07:26 - loss: 0.4086 - regression_loss: 0.3589 - classification_loss: 0.0498
 5452/10000 [===============>..............] - ETA: 1:07:25 - loss: 0.4087 - regression_loss: 0.3589 - classification_loss: 0.0498
 5453/10000 [===============>..............] - ETA: 1:07:24 - loss: 0.4086 - regression_loss: 0.3589 - classification_loss: 0.0497
 5454/10000 [===============>..............] - ETA: 1:07:23 - loss: 0.4086 - regression_loss: 0.3588 - classification_loss: 0.0497
 5455/10000 [===============>..............] - ETA: 1:07:22 - loss: 0.4086 - regression_loss: 0.3588 - classification_loss: 0.0497
 5456/10000 [===============>..............] - ETA: 1:07:21 - loss: 0.4085 - regression_loss: 0.3588 - classification_loss: 0.0497
 5457/10000 [===============>..............] - ETA: 1:07:21 - loss: 0.4085 - regression_loss: 0.3587 - classification_loss: 0.0497
 5458/10000 [===============>..............] - ETA: 1:07:20 - loss: 0.4087 - regression_loss: 0.3590 - classification_loss: 0.0497
 5459/10000 [===============>..............] - ETA: 1:07:19 - loss: 0.4087 - regression_loss: 0.3590 - classification_loss: 0.0497
 5460/10000 [===============>..............] - ETA: 1:07:18 - loss: 0.4087 - regression_loss: 0.3589 - classification_loss: 0.0497
 5461/10000 [===============>..............] - ETA: 1:07:17 - loss: 0.4087 - regression_loss: 0.3589 - classification_loss: 0.0497
 5462/10000 [===============>..............] - ETA: 1:07:16 - loss: 0.4086 - regression_loss: 0.3589 - classification_loss: 0.0497
 5463/10000 [===============>..............] - ETA: 1:07:15 - loss: 0.4086 - regression_loss: 0.3589 - classification_loss: 0.0497
 5464/10000 [===============>..............] - ETA: 1:07:14 - loss: 0.4085 - regression_loss: 0.3588 - classification_loss: 0.0497
 5465/10000 [===============>..............] - ETA: 1:07:13 - loss: 0.4085 - regression_loss: 0.3588 - classification_loss: 0.0497
 5466/10000 [===============>..............] - ETA: 1:07:13 - loss: 0.4086 - regression_loss: 0.3589 - classification_loss: 0.0497
 5467/10000 [===============>..............] - ETA: 1:07:12 - loss: 0.4086 - regression_loss: 0.3589 - classification_loss: 0.0497
 5468/10000 [===============>..............] - ETA: 1:07:11 - loss: 0.4086 - regression_loss: 0.3589 - classification_loss: 0.0497
 5469/10000 [===============>..............] - ETA: 1:07:10 - loss: 0.4086 - regression_loss: 0.3589 - classification_loss: 0.0497
 5470/10000 [===============>..............] - ETA: 1:07:09 - loss: 0.4086 - regression_loss: 0.3589 - classification_loss: 0.0497
 5471/10000 [===============>..............] - ETA: 1:07:08 - loss: 0.4086 - regression_loss: 0.3589 - classification_loss: 0.0497
 5472/10000 [===============>..............] - ETA: 1:07:07 - loss: 0.4086 - regression_loss: 0.3589 - classification_loss: 0.0497
 5473/10000 [===============>..............] - ETA: 1:07:06 - loss: 0.4085 - regression_loss: 0.3589 - classification_loss: 0.0497
 5474/10000 [===============>..............] - ETA: 1:07:05 - loss: 0.4085 - regression_loss: 0.3588 - classification_loss: 0.0496
 5475/10000 [===============>..............] - ETA: 1:07:05 - loss: 0.4086 - regression_loss: 0.3589 - classification_loss: 0.0497
 5476/10000 [===============>..............] - ETA: 1:07:04 - loss: 0.4086 - regression_loss: 0.3589 - classification_loss: 0.0496
 5477/10000 [===============>..............] - ETA: 1:07:03 - loss: 0.4086 - regression_loss: 0.3589 - classification_loss: 0.0496
 5478/10000 [===============>..............] - ETA: 1:07:02 - loss: 0.4086 - regression_loss: 0.3589 - classification_loss: 0.0496
 5479/10000 [===============>..............] - ETA: 1:07:01 - loss: 0.4085 - regression_loss: 0.3589 - classification_loss: 0.0496
 5480/10000 [===============>..............] - ETA: 1:07:00 - loss: 0.4086 - regression_loss: 0.3589 - classification_loss: 0.0496
 5481/10000 [===============>..............] - ETA: 1:06:59 - loss: 0.4085 - regression_loss: 0.3589 - classification_loss: 0.0496
 5482/10000 [===============>..............] - ETA: 1:06:58 - loss: 0.4085 - regression_loss: 0.3588 - classification_loss: 0.0496
 5483/10000 [===============>..............] - ETA: 1:06:57 - loss: 0.4085 - regression_loss: 0.3589 - classification_loss: 0.0496
 5484/10000 [===============>..............] - ETA: 1:06:57 - loss: 0.4086 - regression_loss: 0.3590 - classification_loss: 0.0496
 5485/10000 [===============>..............] - ETA: 1:06:56 - loss: 0.4086 - regression_loss: 0.3590 - classification_loss: 0.0496
 5486/10000 [===============>..............] - ETA: 1:06:55 - loss: 0.4085 - regression_loss: 0.3589 - classification_loss: 0.0496
 5487/10000 [===============>..............] - ETA: 1:06:54 - loss: 0.4085 - regression_loss: 0.3589 - classification_loss: 0.0496
 5488/10000 [===============>..............] - ETA: 1:06:53 - loss: 0.4086 - regression_loss: 0.3590 - classification_loss: 0.0496
 5489/10000 [===============>..............] - ETA: 1:06:52 - loss: 0.4087 - regression_loss: 0.3590 - classification_loss: 0.0497
 5490/10000 [===============>..............] - ETA: 1:06:51 - loss: 0.4088 - regression_loss: 0.3591 - classification_loss: 0.0497
 5491/10000 [===============>..............] - ETA: 1:06:50 - loss: 0.4087 - regression_loss: 0.3590 - classification_loss: 0.0497
 5492/10000 [===============>..............] - ETA: 1:06:49 - loss: 0.4087 - regression_loss: 0.3590 - classification_loss: 0.0497
 5493/10000 [===============>..............] - ETA: 1:06:48 - loss: 0.4087 - regression_loss: 0.3590 - classification_loss: 0.0497
 5494/10000 [===============>..............] - ETA: 1:06:48 - loss: 0.4087 - regression_loss: 0.3590 - classification_loss: 0.0497
 5495/10000 [===============>..............] - ETA: 1:06:47 - loss: 0.4087 - regression_loss: 0.3590 - classification_loss: 0.0497
 5496/10000 [===============>..............] - ETA: 1:06:46 - loss: 0.4087 - regression_loss: 0.3590 - classification_loss: 0.0497
 5497/10000 [===============>..............] - ETA: 1:06:45 - loss: 0.4087 - regression_loss: 0.3590 - classification_loss: 0.0497
 5498/10000 [===============>..............] - ETA: 1:06:44 - loss: 0.4087 - regression_loss: 0.3590 - classification_loss: 0.0496
 5499/10000 [===============>..............] - ETA: 1:06:43 - loss: 0.4086 - regression_loss: 0.3590 - classification_loss: 0.0496
 5500/10000 [===============>..............] - ETA: 1:06:42 - loss: 0.4087 - regression_loss: 0.3590 - classification_loss: 0.0497
 5501/10000 [===============>..............] - ETA: 1:06:41 - loss: 0.4087 - regression_loss: 0.3590 - classification_loss: 0.0497
 5502/10000 [===============>..............] - ETA: 1:06:40 - loss: 0.4087 - regression_loss: 0.3590 - classification_loss: 0.0497
 5503/10000 [===============>..............] - ETA: 1:06:40 - loss: 0.4087 - regression_loss: 0.3590 - classification_loss: 0.0497
 5504/10000 [===============>..............] - ETA: 1:06:39 - loss: 0.4087 - regression_loss: 0.3590 - classification_loss: 0.0497
 5505/10000 [===============>..............] - ETA: 1:06:38 - loss: 0.4086 - regression_loss: 0.3590 - classification_loss: 0.0497
 5506/10000 [===============>..............] - ETA: 1:06:37 - loss: 0.4086 - regression_loss: 0.3590 - classification_loss: 0.0497
 5507/10000 [===============>..............] - ETA: 1:06:36 - loss: 0.4086 - regression_loss: 0.3590 - classification_loss: 0.0497
 5508/10000 [===============>..............] - ETA: 1:06:35 - loss: 0.4087 - regression_loss: 0.3591 - classification_loss: 0.0497
 5509/10000 [===============>..............] - ETA: 1:06:34 - loss: 0.4087 - regression_loss: 0.3591 - classification_loss: 0.0497
 5510/10000 [===============>..............] - ETA: 1:06:33 - loss: 0.4087 - regression_loss: 0.3590 - classification_loss: 0.0497
 5511/10000 [===============>..............] - ETA: 1:06:32 - loss: 0.4087 - regression_loss: 0.3590 - classification_loss: 0.0497
 5512/10000 [===============>..............] - ETA: 1:06:32 - loss: 0.4086 - regression_loss: 0.3590 - classification_loss: 0.0497
 5513/10000 [===============>..............] - ETA: 1:06:31 - loss: 0.4086 - regression_loss: 0.3589 - classification_loss: 0.0497
 5514/10000 [===============>..............] - ETA: 1:06:30 - loss: 0.4086 - regression_loss: 0.3589 - classification_loss: 0.0497
 5515/10000 [===============>..............] - ETA: 1:06:29 - loss: 0.4085 - regression_loss: 0.3589 - classification_loss: 0.0497
 5516/10000 [===============>..............] - ETA: 1:06:28 - loss: 0.4086 - regression_loss: 0.3589 - classification_loss: 0.0497
 5517/10000 [===============>..............] - ETA: 1:06:27 - loss: 0.4086 - regression_loss: 0.3590 - classification_loss: 0.0496
 5518/10000 [===============>..............] - ETA: 1:06:26 - loss: 0.4086 - regression_loss: 0.3589 - classification_loss: 0.0496
 5519/10000 [===============>..............] - ETA: 1:06:25 - loss: 0.4086 - regression_loss: 0.3589 - classification_loss: 0.0496
 5520/10000 [===============>..............] - ETA: 1:06:24 - loss: 0.4085 - regression_loss: 0.3589 - classification_loss: 0.0496
 5521/10000 [===============>..............] - ETA: 1:06:24 - loss: 0.4085 - regression_loss: 0.3588 - classification_loss: 0.0496
 5522/10000 [===============>..............] - ETA: 1:06:23 - loss: 0.4084 - regression_loss: 0.3588 - classification_loss: 0.0496
 5523/10000 [===============>..............] - ETA: 1:06:22 - loss: 0.4085 - regression_loss: 0.3588 - classification_loss: 0.0496
 5524/10000 [===============>..............] - ETA: 1:06:21 - loss: 0.4084 - regression_loss: 0.3588 - classification_loss: 0.0496
 5525/10000 [===============>..............] - ETA: 1:06:20 - loss: 0.4084 - regression_loss: 0.3588 - classification_loss: 0.0496
 5526/10000 [===============>..............] - ETA: 1:06:19 - loss: 0.4084 - regression_loss: 0.3587 - classification_loss: 0.0496
 5527/10000 [===============>..............] - ETA: 1:06:18 - loss: 0.4084 - regression_loss: 0.3587 - classification_loss: 0.0496
 5528/10000 [===============>..............] - ETA: 1:06:17 - loss: 0.4084 - regression_loss: 0.3587 - classification_loss: 0.0496
 5529/10000 [===============>..............] - ETA: 1:06:16 - loss: 0.4083 - regression_loss: 0.3587 - classification_loss: 0.0496
 5530/10000 [===============>..............] - ETA: 1:06:16 - loss: 0.4084 - regression_loss: 0.3587 - classification_loss: 0.0496
 5531/10000 [===============>..............] - ETA: 1:06:15 - loss: 0.4083 - regression_loss: 0.3587 - classification_loss: 0.0496
 5532/10000 [===============>..............] - ETA: 1:06:14 - loss: 0.4083 - regression_loss: 0.3587 - classification_loss: 0.0496
 5533/10000 [===============>..............] - ETA: 1:06:13 - loss: 0.4083 - regression_loss: 0.3587 - classification_loss: 0.0496
 5534/10000 [===============>..............] - ETA: 1:06:12 - loss: 0.4083 - regression_loss: 0.3587 - classification_loss: 0.0496
 5535/10000 [===============>..............] - ETA: 1:06:11 - loss: 0.4084 - regression_loss: 0.3588 - classification_loss: 0.0496
 5536/10000 [===============>..............] - ETA: 1:06:10 - loss: 0.4084 - regression_loss: 0.3588 - classification_loss: 0.0496
 5537/10000 [===============>..............] - ETA: 1:06:09 - loss: 0.4084 - regression_loss: 0.3588 - classification_loss: 0.0496
 5538/10000 [===============>..............] - ETA: 1:06:08 - loss: 0.4084 - regression_loss: 0.3588 - classification_loss: 0.0496
 5539/10000 [===============>..............] - ETA: 1:06:08 - loss: 0.4083 - regression_loss: 0.3588 - classification_loss: 0.0496
 5540/10000 [===============>..............] - ETA: 1:06:07 - loss: 0.4084 - regression_loss: 0.3588 - classification_loss: 0.0496
 5541/10000 [===============>..............] - ETA: 1:06:06 - loss: 0.4083 - regression_loss: 0.3588 - classification_loss: 0.0496
 5542/10000 [===============>..............] - ETA: 1:06:05 - loss: 0.4084 - regression_loss: 0.3588 - classification_loss: 0.0496
 5543/10000 [===============>..............] - ETA: 1:06:04 - loss: 0.4084 - regression_loss: 0.3588 - classification_loss: 0.0496
 5544/10000 [===============>..............] - ETA: 1:06:03 - loss: 0.4084 - regression_loss: 0.3588 - classification_loss: 0.0496
 5545/10000 [===============>..............] - ETA: 1:06:02 - loss: 0.4084 - regression_loss: 0.3588 - classification_loss: 0.0496
 5546/10000 [===============>..............] - ETA: 1:06:01 - loss: 0.4084 - regression_loss: 0.3589 - classification_loss: 0.0496
 5547/10000 [===============>..............] - ETA: 1:06:00 - loss: 0.4084 - regression_loss: 0.3588 - classification_loss: 0.0496
 5548/10000 [===============>..............] - ETA: 1:05:59 - loss: 0.4084 - regression_loss: 0.3589 - classification_loss: 0.0496
 5549/10000 [===============>..............] - ETA: 1:05:59 - loss: 0.4084 - regression_loss: 0.3589 - classification_loss: 0.0496
 5550/10000 [===============>..............] - ETA: 1:05:58 - loss: 0.4085 - regression_loss: 0.3589 - classification_loss: 0.0496
 5551/10000 [===============>..............] - ETA: 1:05:57 - loss: 0.4086 - regression_loss: 0.3590 - classification_loss: 0.0496
 5552/10000 [===============>..............] - ETA: 1:05:56 - loss: 0.4086 - regression_loss: 0.3590 - classification_loss: 0.0496
 5553/10000 [===============>..............] - ETA: 1:05:55 - loss: 0.4086 - regression_loss: 0.3590 - classification_loss: 0.0496
 5554/10000 [===============>..............] - ETA: 1:05:54 - loss: 0.4086 - regression_loss: 0.3590 - classification_loss: 0.0496
 5555/10000 [===============>..............] - ETA: 1:05:53 - loss: 0.4086 - regression_loss: 0.3591 - classification_loss: 0.0496
 5556/10000 [===============>..............] - ETA: 1:05:52 - loss: 0.4086 - regression_loss: 0.3590 - classification_loss: 0.0496
 5557/10000 [===============>..............] - ETA: 1:05:51 - loss: 0.4085 - regression_loss: 0.3590 - classification_loss: 0.0496
 5558/10000 [===============>..............] - ETA: 1:05:51 - loss: 0.4085 - regression_loss: 0.3590 - classification_loss: 0.0495
 5559/10000 [===============>..............] - ETA: 1:05:50 - loss: 0.4086 - regression_loss: 0.3590 - classification_loss: 0.0496
 5560/10000 [===============>..............] - ETA: 1:05:49 - loss: 0.4086 - regression_loss: 0.3591 - classification_loss: 0.0495
 5561/10000 [===============>..............] - ETA: 1:05:48 - loss: 0.4086 - regression_loss: 0.3591 - classification_loss: 0.0495
 5562/10000 [===============>..............] - ETA: 1:05:47 - loss: 0.4087 - regression_loss: 0.3591 - classification_loss: 0.0495
 5563/10000 [===============>..............] - ETA: 1:05:46 - loss: 0.4086 - regression_loss: 0.3591 - classification_loss: 0.0495
 5564/10000 [===============>..............] - ETA: 1:05:45 - loss: 0.4087 - regression_loss: 0.3592 - classification_loss: 0.0495
 5565/10000 [===============>..............] - ETA: 1:05:44 - loss: 0.4087 - regression_loss: 0.3592 - classification_loss: 0.0495
 5566/10000 [===============>..............] - ETA: 1:05:43 - loss: 0.4087 - regression_loss: 0.3592 - classification_loss: 0.0495
 5567/10000 [===============>..............] - ETA: 1:05:43 - loss: 0.4087 - regression_loss: 0.3591 - classification_loss: 0.0495
 5568/10000 [===============>..............] - ETA: 1:05:42 - loss: 0.4087 - regression_loss: 0.3591 - classification_loss: 0.0495
 5569/10000 [===============>..............] - ETA: 1:05:41 - loss: 0.4086 - regression_loss: 0.3591 - classification_loss: 0.0495
 5570/10000 [===============>..............] - ETA: 1:05:40 - loss: 0.4086 - regression_loss: 0.3591 - classification_loss: 0.0495
 5571/10000 [===============>..............] - ETA: 1:05:39 - loss: 0.4087 - regression_loss: 0.3592 - classification_loss: 0.0495
 5572/10000 [===============>..............] - ETA: 1:05:38 - loss: 0.4087 - regression_loss: 0.3592 - classification_loss: 0.0495
 5573/10000 [===============>..............] - ETA: 1:05:37 - loss: 0.4087 - regression_loss: 0.3592 - classification_loss: 0.0495
 5574/10000 [===============>..............] - ETA: 1:05:36 - loss: 0.4087 - regression_loss: 0.3592 - classification_loss: 0.0495
 5575/10000 [===============>..............] - ETA: 1:05:35 - loss: 0.4087 - regression_loss: 0.3592 - classification_loss: 0.0495
 5576/10000 [===============>..............] - ETA: 1:05:35 - loss: 0.4087 - regression_loss: 0.3592 - classification_loss: 0.0495
 5577/10000 [===============>..............] - ETA: 1:05:34 - loss: 0.4087 - regression_loss: 0.3592 - classification_loss: 0.0495
 5578/10000 [===============>..............] - ETA: 1:05:33 - loss: 0.4087 - regression_loss: 0.3592 - classification_loss: 0.0495
 5579/10000 [===============>..............] - ETA: 1:05:32 - loss: 0.4086 - regression_loss: 0.3591 - classification_loss: 0.0495
 5580/10000 [===============>..............] - ETA: 1:05:31 - loss: 0.4087 - regression_loss: 0.3592 - classification_loss: 0.0495
 5581/10000 [===============>..............] - ETA: 1:05:30 - loss: 0.4087 - regression_loss: 0.3592 - classification_loss: 0.0495
 5582/10000 [===============>..............] - ETA: 1:05:29 - loss: 0.4086 - regression_loss: 0.3592 - classification_loss: 0.0495
 5583/10000 [===============>..............] - ETA: 1:05:28 - loss: 0.4086 - regression_loss: 0.3591 - classification_loss: 0.0495
 5584/10000 [===============>..............] - ETA: 1:05:27 - loss: 0.4087 - regression_loss: 0.3592 - classification_loss: 0.0495
 5585/10000 [===============>..............] - ETA: 1:05:27 - loss: 0.4086 - regression_loss: 0.3592 - classification_loss: 0.0495
 5586/10000 [===============>..............] - ETA: 1:05:26 - loss: 0.4086 - regression_loss: 0.3592 - classification_loss: 0.0495
 5587/10000 [===============>..............] - ETA: 1:05:25 - loss: 0.4086 - regression_loss: 0.3591 - classification_loss: 0.0495
 5588/10000 [===============>..............] - ETA: 1:05:24 - loss: 0.4085 - regression_loss: 0.3591 - classification_loss: 0.0495
 5589/10000 [===============>..............] - ETA: 1:05:23 - loss: 0.4085 - regression_loss: 0.3591 - classification_loss: 0.0495
 5590/10000 [===============>..............] - ETA: 1:05:22 - loss: 0.4085 - regression_loss: 0.3591 - classification_loss: 0.0495
 5591/10000 [===============>..............] - ETA: 1:05:21 - loss: 0.4086 - regression_loss: 0.3591 - classification_loss: 0.0494
 5592/10000 [===============>..............] - ETA: 1:05:20 - loss: 0.4086 - regression_loss: 0.3591 - classification_loss: 0.0494
 5593/10000 [===============>..............] - ETA: 1:05:19 - loss: 0.4087 - regression_loss: 0.3592 - classification_loss: 0.0495
 5594/10000 [===============>..............] - ETA: 1:05:19 - loss: 0.4087 - regression_loss: 0.3592 - classification_loss: 0.0494
 5595/10000 [===============>..............] - ETA: 1:05:18 - loss: 0.4086 - regression_loss: 0.3591 - classification_loss: 0.0494
 5596/10000 [===============>..............] - ETA: 1:05:17 - loss: 0.4086 - regression_loss: 0.3592 - classification_loss: 0.0494
 5597/10000 [===============>..............] - ETA: 1:05:16 - loss: 0.4086 - regression_loss: 0.3592 - classification_loss: 0.0494
 5598/10000 [===============>..............] - ETA: 1:05:15 - loss: 0.4086 - regression_loss: 0.3592 - classification_loss: 0.0494
 5599/10000 [===============>..............] - ETA: 1:05:14 - loss: 0.4086 - regression_loss: 0.3591 - classification_loss: 0.0494
 5600/10000 [===============>..............] - ETA: 1:05:13 - loss: 0.4085 - regression_loss: 0.3591 - classification_loss: 0.0494
 5601/10000 [===============>..............] - ETA: 1:05:12 - loss: 0.4085 - regression_loss: 0.3591 - classification_loss: 0.0494
 5602/10000 [===============>..............] - ETA: 1:05:11 - loss: 0.4085 - regression_loss: 0.3591 - classification_loss: 0.0494
 5603/10000 [===============>..............] - ETA: 1:05:11 - loss: 0.4085 - regression_loss: 0.3591 - classification_loss: 0.0494
 5604/10000 [===============>..............] - ETA: 1:05:10 - loss: 0.4085 - regression_loss: 0.3591 - classification_loss: 0.0494
 5605/10000 [===============>..............] - ETA: 1:05:09 - loss: 0.4084 - regression_loss: 0.3591 - classification_loss: 0.0494
 5606/10000 [===============>..............] - ETA: 1:05:08 - loss: 0.4084 - regression_loss: 0.3591 - classification_loss: 0.0494
 5607/10000 [===============>..............] - ETA: 1:05:07 - loss: 0.4085 - regression_loss: 0.3591 - classification_loss: 0.0494
 5608/10000 [===============>..............] - ETA: 1:05:06 - loss: 0.4085 - regression_loss: 0.3591 - classification_loss: 0.0494
 5609/10000 [===============>..............] - ETA: 1:05:05 - loss: 0.4085 - regression_loss: 0.3591 - classification_loss: 0.0494
 5610/10000 [===============>..............] - ETA: 1:05:04 - loss: 0.4085 - regression_loss: 0.3591 - classification_loss: 0.0494
 5611/10000 [===============>..............] - ETA: 1:05:03 - loss: 0.4087 - regression_loss: 0.3593 - classification_loss: 0.0494
 5612/10000 [===============>..............] - ETA: 1:05:03 - loss: 0.4086 - regression_loss: 0.3592 - classification_loss: 0.0494
 5613/10000 [===============>..............] - ETA: 1:05:02 - loss: 0.4086 - regression_loss: 0.3592 - classification_loss: 0.0494
 5614/10000 [===============>..............] - ETA: 1:05:01 - loss: 0.4085 - regression_loss: 0.3591 - classification_loss: 0.0494
 5615/10000 [===============>..............] - ETA: 1:05:00 - loss: 0.4085 - regression_loss: 0.3591 - classification_loss: 0.0494
 5616/10000 [===============>..............] - ETA: 1:04:59 - loss: 0.4084 - regression_loss: 0.3591 - classification_loss: 0.0494
 5617/10000 [===============>..............] - ETA: 1:04:58 - loss: 0.4084 - regression_loss: 0.3591 - classification_loss: 0.0494
 5618/10000 [===============>..............] - ETA: 1:04:57 - loss: 0.4086 - regression_loss: 0.3592 - classification_loss: 0.0494
 5619/10000 [===============>..............] - ETA: 1:04:56 - loss: 0.4085 - regression_loss: 0.3592 - classification_loss: 0.0494
 5620/10000 [===============>..............] - ETA: 1:04:55 - loss: 0.4085 - regression_loss: 0.3591 - classification_loss: 0.0494
 5621/10000 [===============>..............] - ETA: 1:04:54 - loss: 0.4084 - regression_loss: 0.3591 - classification_loss: 0.0493
 5622/10000 [===============>..............] - ETA: 1:04:54 - loss: 0.4084 - regression_loss: 0.3590 - classification_loss: 0.0493
 5623/10000 [===============>..............] - ETA: 1:04:53 - loss: 0.4084 - regression_loss: 0.3591 - classification_loss: 0.0493
 5624/10000 [===============>..............] - ETA: 1:04:52 - loss: 0.4084 - regression_loss: 0.3591 - classification_loss: 0.0493
 5625/10000 [===============>..............] - ETA: 1:04:51 - loss: 0.4084 - regression_loss: 0.3591 - classification_loss: 0.0493
 5626/10000 [===============>..............] - ETA: 1:04:50 - loss: 0.4085 - regression_loss: 0.3591 - classification_loss: 0.0493
 5627/10000 [===============>..............] - ETA: 1:04:49 - loss: 0.4085 - regression_loss: 0.3591 - classification_loss: 0.0493
 5628/10000 [===============>..............] - ETA: 1:04:48 - loss: 0.4084 - regression_loss: 0.3591 - classification_loss: 0.0493
 5629/10000 [===============>..............] - ETA: 1:04:47 - loss: 0.4084 - regression_loss: 0.3591 - classification_loss: 0.0493
 5630/10000 [===============>..............] - ETA: 1:04:46 - loss: 0.4084 - regression_loss: 0.3591 - classification_loss: 0.0493
 5631/10000 [===============>..............] - ETA: 1:04:46 - loss: 0.4084 - regression_loss: 0.3591 - classification_loss: 0.0493
 5632/10000 [===============>..............] - ETA: 1:04:45 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0493
 5633/10000 [===============>..............] - ETA: 1:04:44 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0493
 5634/10000 [===============>..............] - ETA: 1:04:43 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0493
 5635/10000 [===============>..............] - ETA: 1:04:42 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0493
 5636/10000 [===============>..............] - ETA: 1:04:41 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0493
 5637/10000 [===============>..............] - ETA: 1:04:40 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0493
 5638/10000 [===============>..............] - ETA: 1:04:39 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0493
 5639/10000 [===============>..............] - ETA: 1:04:38 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0493
 5640/10000 [===============>..............] - ETA: 1:04:38 - loss: 0.4082 - regression_loss: 0.3589 - classification_loss: 0.0493
 5641/10000 [===============>..............] - ETA: 1:04:37 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0493
 5642/10000 [===============>..............] - ETA: 1:04:36 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0493
 5643/10000 [===============>..............] - ETA: 1:04:35 - loss: 0.4083 - regression_loss: 0.3591 - classification_loss: 0.0493
 5644/10000 [===============>..............] - ETA: 1:04:34 - loss: 0.4083 - regression_loss: 0.3591 - classification_loss: 0.0493
 5645/10000 [===============>..............] - ETA: 1:04:33 - loss: 0.4083 - regression_loss: 0.3591 - classification_loss: 0.0493
 5646/10000 [===============>..............] - ETA: 1:04:32 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0493
 5647/10000 [===============>..............] - ETA: 1:04:31 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0493
 5648/10000 [===============>..............] - ETA: 1:04:30 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0492
 5649/10000 [===============>..............] - ETA: 1:04:30 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0492
 5650/10000 [===============>..............] - ETA: 1:04:29 - loss: 0.4082 - regression_loss: 0.3590 - classification_loss: 0.0492
 5651/10000 [===============>..............] - ETA: 1:04:28 - loss: 0.4083 - regression_loss: 0.3591 - classification_loss: 0.0492
 5652/10000 [===============>..............] - ETA: 1:04:27 - loss: 0.4085 - regression_loss: 0.3592 - classification_loss: 0.0492
 5653/10000 [===============>..............] - ETA: 1:04:26 - loss: 0.4085 - regression_loss: 0.3592 - classification_loss: 0.0492
 5654/10000 [===============>..............] - ETA: 1:04:25 - loss: 0.4085 - regression_loss: 0.3592 - classification_loss: 0.0492
 5655/10000 [===============>..............] - ETA: 1:04:24 - loss: 0.4085 - regression_loss: 0.3593 - classification_loss: 0.0492
 5656/10000 [===============>..............] - ETA: 1:04:23 - loss: 0.4085 - regression_loss: 0.3593 - classification_loss: 0.0492
 5657/10000 [===============>..............] - ETA: 1:04:22 - loss: 0.4087 - regression_loss: 0.3594 - classification_loss: 0.0493
 5658/10000 [===============>..............] - ETA: 1:04:22 - loss: 0.4086 - regression_loss: 0.3594 - classification_loss: 0.0493
 5659/10000 [===============>..............] - ETA: 1:04:21 - loss: 0.4087 - regression_loss: 0.3594 - classification_loss: 0.0493
 5660/10000 [===============>..............] - ETA: 1:04:20 - loss: 0.4086 - regression_loss: 0.3593 - classification_loss: 0.0493
 5661/10000 [===============>..............] - ETA: 1:04:19 - loss: 0.4086 - regression_loss: 0.3593 - classification_loss: 0.0493
 5662/10000 [===============>..............] - ETA: 1:04:18 - loss: 0.4086 - regression_loss: 0.3593 - classification_loss: 0.0493
 5663/10000 [===============>..............] - ETA: 1:04:17 - loss: 0.4086 - regression_loss: 0.3593 - classification_loss: 0.0493
 5664/10000 [===============>..............] - ETA: 1:04:16 - loss: 0.4086 - regression_loss: 0.3593 - classification_loss: 0.0493
 5665/10000 [===============>..............] - ETA: 1:04:15 - loss: 0.4086 - regression_loss: 0.3593 - classification_loss: 0.0493
 5666/10000 [===============>..............] - ETA: 1:04:14 - loss: 0.4086 - regression_loss: 0.3593 - classification_loss: 0.0493
 5667/10000 [================>.............] - ETA: 1:04:14 - loss: 0.4085 - regression_loss: 0.3592 - classification_loss: 0.0493
 5668/10000 [================>.............] - ETA: 1:04:13 - loss: 0.4085 - regression_loss: 0.3592 - classification_loss: 0.0493
 5669/10000 [================>.............] - ETA: 1:04:12 - loss: 0.4085 - regression_loss: 0.3593 - classification_loss: 0.0493
 5670/10000 [================>.............] - ETA: 1:04:11 - loss: 0.4085 - regression_loss: 0.3592 - classification_loss: 0.0493
 5671/10000 [================>.............] - ETA: 1:04:10 - loss: 0.4085 - regression_loss: 0.3593 - classification_loss: 0.0493
 5672/10000 [================>.............] - ETA: 1:04:09 - loss: 0.4085 - regression_loss: 0.3592 - classification_loss: 0.0493
 5673/10000 [================>.............] - ETA: 1:04:08 - loss: 0.4084 - regression_loss: 0.3592 - classification_loss: 0.0492
 5674/10000 [================>.............] - ETA: 1:04:07 - loss: 0.4085 - regression_loss: 0.3593 - classification_loss: 0.0492
 5675/10000 [================>.............] - ETA: 1:04:06 - loss: 0.4085 - regression_loss: 0.3593 - classification_loss: 0.0492
 5676/10000 [================>.............] - ETA: 1:04:06 - loss: 0.4085 - regression_loss: 0.3593 - classification_loss: 0.0492
 5677/10000 [================>.............] - ETA: 1:04:05 - loss: 0.4085 - regression_loss: 0.3592 - classification_loss: 0.0492
 5678/10000 [================>.............] - ETA: 1:04:04 - loss: 0.4085 - regression_loss: 0.3593 - classification_loss: 0.0492
 5679/10000 [================>.............] - ETA: 1:04:03 - loss: 0.4086 - regression_loss: 0.3593 - classification_loss: 0.0492
 5680/10000 [================>.............] - ETA: 1:04:02 - loss: 0.4085 - regression_loss: 0.3593 - classification_loss: 0.0492
 5681/10000 [================>.............] - ETA: 1:04:01 - loss: 0.4085 - regression_loss: 0.3593 - classification_loss: 0.0492
 5682/10000 [================>.............] - ETA: 1:04:00 - loss: 0.4085 - regression_loss: 0.3593 - classification_loss: 0.0492
 5683/10000 [================>.............] - ETA: 1:03:59 - loss: 0.4085 - regression_loss: 0.3592 - classification_loss: 0.0493
 5684/10000 [================>.............] - ETA: 1:03:58 - loss: 0.4084 - regression_loss: 0.3592 - classification_loss: 0.0492
 5685/10000 [================>.............] - ETA: 1:03:57 - loss: 0.4085 - regression_loss: 0.3592 - classification_loss: 0.0493
 5686/10000 [================>.............] - ETA: 1:03:57 - loss: 0.4085 - regression_loss: 0.3592 - classification_loss: 0.0493
 5687/10000 [================>.............] - ETA: 1:03:56 - loss: 0.4084 - regression_loss: 0.3592 - classification_loss: 0.0492
 5688/10000 [================>.............] - ETA: 1:03:55 - loss: 0.4084 - regression_loss: 0.3592 - classification_loss: 0.0492
 5689/10000 [================>.............] - ETA: 1:03:54 - loss: 0.4083 - regression_loss: 0.3591 - classification_loss: 0.0492
 5690/10000 [================>.............] - ETA: 1:03:53 - loss: 0.4083 - regression_loss: 0.3591 - classification_loss: 0.0492
 5691/10000 [================>.............] - ETA: 1:03:52 - loss: 0.4083 - regression_loss: 0.3591 - classification_loss: 0.0492
 5692/10000 [================>.............] - ETA: 1:03:51 - loss: 0.4083 - regression_loss: 0.3591 - classification_loss: 0.0492
 5693/10000 [================>.............] - ETA: 1:03:50 - loss: 0.4083 - regression_loss: 0.3591 - classification_loss: 0.0492
 5694/10000 [================>.............] - ETA: 1:03:49 - loss: 0.4084 - regression_loss: 0.3592 - classification_loss: 0.0492
 5695/10000 [================>.............] - ETA: 1:03:49 - loss: 0.4084 - regression_loss: 0.3592 - classification_loss: 0.0492
 5696/10000 [================>.............] - ETA: 1:03:48 - loss: 0.4084 - regression_loss: 0.3591 - classification_loss: 0.0492
 5697/10000 [================>.............] - ETA: 1:03:47 - loss: 0.4084 - regression_loss: 0.3591 - classification_loss: 0.0492
 5698/10000 [================>.............] - ETA: 1:03:46 - loss: 0.4083 - regression_loss: 0.3591 - classification_loss: 0.0492
 5699/10000 [================>.............] - ETA: 1:03:45 - loss: 0.4084 - regression_loss: 0.3592 - classification_loss: 0.0493
 5700/10000 [================>.............] - ETA: 1:03:44 - loss: 0.4084 - regression_loss: 0.3592 - classification_loss: 0.0493
 5701/10000 [================>.............] - ETA: 1:03:43 - loss: 0.4084 - regression_loss: 0.3592 - classification_loss: 0.0492
 5702/10000 [================>.............] - ETA: 1:03:42 - loss: 0.4084 - regression_loss: 0.3592 - classification_loss: 0.0492
 5703/10000 [================>.............] - ETA: 1:03:41 - loss: 0.4084 - regression_loss: 0.3592 - classification_loss: 0.0492
 5704/10000 [================>.............] - ETA: 1:03:41 - loss: 0.4084 - regression_loss: 0.3592 - classification_loss: 0.0492
 5705/10000 [================>.............] - ETA: 1:03:40 - loss: 0.4084 - regression_loss: 0.3592 - classification_loss: 0.0492
 5706/10000 [================>.............] - ETA: 1:03:39 - loss: 0.4084 - regression_loss: 0.3592 - classification_loss: 0.0492
 5707/10000 [================>.............] - ETA: 1:03:38 - loss: 0.4084 - regression_loss: 0.3592 - classification_loss: 0.0492
 5708/10000 [================>.............] - ETA: 1:03:37 - loss: 0.4084 - regression_loss: 0.3592 - classification_loss: 0.0492
 5709/10000 [================>.............] - ETA: 1:03:36 - loss: 0.4084 - regression_loss: 0.3592 - classification_loss: 0.0492
 5710/10000 [================>.............] - ETA: 1:03:35 - loss: 0.4084 - regression_loss: 0.3592 - classification_loss: 0.0492
 5711/10000 [================>.............] - ETA: 1:03:34 - loss: 0.4084 - regression_loss: 0.3591 - classification_loss: 0.0492
 5712/10000 [================>.............] - ETA: 1:03:33 - loss: 0.4083 - regression_loss: 0.3591 - classification_loss: 0.0492
 5713/10000 [================>.............] - ETA: 1:03:33 - loss: 0.4083 - regression_loss: 0.3591 - classification_loss: 0.0492
 5714/10000 [================>.............] - ETA: 1:03:32 - loss: 0.4082 - regression_loss: 0.3590 - classification_loss: 0.0492
 5715/10000 [================>.............] - ETA: 1:03:31 - loss: 0.4083 - regression_loss: 0.3591 - classification_loss: 0.0492
 5716/10000 [================>.............] - ETA: 1:03:30 - loss: 0.4083 - regression_loss: 0.3591 - classification_loss: 0.0492
 5717/10000 [================>.............] - ETA: 1:03:29 - loss: 0.4083 - regression_loss: 0.3591 - classification_loss: 0.0492
 5718/10000 [================>.............] - ETA: 1:03:28 - loss: 0.4082 - regression_loss: 0.3590 - classification_loss: 0.0492
 5719/10000 [================>.............] - ETA: 1:03:27 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0493
 5720/10000 [================>.............] - ETA: 1:03:26 - loss: 0.4082 - regression_loss: 0.3590 - classification_loss: 0.0493
 5721/10000 [================>.............] - ETA: 1:03:25 - loss: 0.4082 - regression_loss: 0.3590 - classification_loss: 0.0493
 5722/10000 [================>.............] - ETA: 1:03:25 - loss: 0.4082 - regression_loss: 0.3589 - classification_loss: 0.0492
 5723/10000 [================>.............] - ETA: 1:03:24 - loss: 0.4081 - regression_loss: 0.3589 - classification_loss: 0.0492
 5724/10000 [================>.............] - ETA: 1:03:23 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0493
 5725/10000 [================>.............] - ETA: 1:03:22 - loss: 0.4082 - regression_loss: 0.3590 - classification_loss: 0.0493
 5726/10000 [================>.............] - ETA: 1:03:21 - loss: 0.4082 - regression_loss: 0.3589 - classification_loss: 0.0493
 5727/10000 [================>.............] - ETA: 1:03:20 - loss: 0.4081 - regression_loss: 0.3589 - classification_loss: 0.0492
 5728/10000 [================>.............] - ETA: 1:03:19 - loss: 0.4081 - regression_loss: 0.3589 - classification_loss: 0.0493
 5729/10000 [================>.............] - ETA: 1:03:18 - loss: 0.4082 - regression_loss: 0.3590 - classification_loss: 0.0493
 5730/10000 [================>.............] - ETA: 1:03:17 - loss: 0.4082 - regression_loss: 0.3589 - classification_loss: 0.0493
 5731/10000 [================>.............] - ETA: 1:03:17 - loss: 0.4082 - regression_loss: 0.3590 - classification_loss: 0.0493
 5732/10000 [================>.............] - ETA: 1:03:16 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0493
 5733/10000 [================>.............] - ETA: 1:03:15 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0493
 5734/10000 [================>.............] - ETA: 1:03:14 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0493
 5735/10000 [================>.............] - ETA: 1:03:13 - loss: 0.4084 - regression_loss: 0.3591 - classification_loss: 0.0493
 5736/10000 [================>.............] - ETA: 1:03:12 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0493
 5737/10000 [================>.............] - ETA: 1:03:11 - loss: 0.4083 - regression_loss: 0.3591 - classification_loss: 0.0493
 5738/10000 [================>.............] - ETA: 1:03:10 - loss: 0.4084 - regression_loss: 0.3591 - classification_loss: 0.0493
 5739/10000 [================>.............] - ETA: 1:03:09 - loss: 0.4084 - regression_loss: 0.3591 - classification_loss: 0.0493
 5740/10000 [================>.............] - ETA: 1:03:09 - loss: 0.4083 - regression_loss: 0.3591 - classification_loss: 0.0493
 5741/10000 [================>.............] - ETA: 1:03:08 - loss: 0.4083 - regression_loss: 0.3591 - classification_loss: 0.0493
 5742/10000 [================>.............] - ETA: 1:03:07 - loss: 0.4084 - regression_loss: 0.3590 - classification_loss: 0.0494
 5743/10000 [================>.............] - ETA: 1:03:06 - loss: 0.4085 - regression_loss: 0.3590 - classification_loss: 0.0494
 5744/10000 [================>.............] - ETA: 1:03:05 - loss: 0.4085 - regression_loss: 0.3591 - classification_loss: 0.0494
 5745/10000 [================>.............] - ETA: 1:03:04 - loss: 0.4084 - regression_loss: 0.3590 - classification_loss: 0.0494
 5746/10000 [================>.............] - ETA: 1:03:03 - loss: 0.4085 - regression_loss: 0.3591 - classification_loss: 0.0494
 5747/10000 [================>.............] - ETA: 1:03:02 - loss: 0.4085 - regression_loss: 0.3591 - classification_loss: 0.0494
 5748/10000 [================>.............] - ETA: 1:03:01 - loss: 0.4085 - regression_loss: 0.3591 - classification_loss: 0.0494
 5749/10000 [================>.............] - ETA: 1:03:01 - loss: 0.4085 - regression_loss: 0.3591 - classification_loss: 0.0494
 5750/10000 [================>.............] - ETA: 1:03:00 - loss: 0.4085 - regression_loss: 0.3591 - classification_loss: 0.0494
 5751/10000 [================>.............] - ETA: 1:02:59 - loss: 0.4085 - regression_loss: 0.3591 - classification_loss: 0.0494
 5752/10000 [================>.............] - ETA: 1:02:58 - loss: 0.4085 - regression_loss: 0.3591 - classification_loss: 0.0494
 5753/10000 [================>.............] - ETA: 1:02:57 - loss: 0.4085 - regression_loss: 0.3591 - classification_loss: 0.0494
 5754/10000 [================>.............] - ETA: 1:02:56 - loss: 0.4085 - regression_loss: 0.3591 - classification_loss: 0.0494
 5755/10000 [================>.............] - ETA: 1:02:55 - loss: 0.4084 - regression_loss: 0.3590 - classification_loss: 0.0494
 5756/10000 [================>.............] - ETA: 1:02:54 - loss: 0.4084 - regression_loss: 0.3590 - classification_loss: 0.0494
 5757/10000 [================>.............] - ETA: 1:02:53 - loss: 0.4084 - regression_loss: 0.3590 - classification_loss: 0.0494
 5758/10000 [================>.............] - ETA: 1:02:52 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0494
 5759/10000 [================>.............] - ETA: 1:02:52 - loss: 0.4083 - regression_loss: 0.3589 - classification_loss: 0.0494
 5760/10000 [================>.............] - ETA: 1:02:51 - loss: 0.4082 - regression_loss: 0.3589 - classification_loss: 0.0494
 5761/10000 [================>.............] - ETA: 1:02:50 - loss: 0.4082 - regression_loss: 0.3588 - classification_loss: 0.0493
 5762/10000 [================>.............] - ETA: 1:02:49 - loss: 0.4082 - regression_loss: 0.3588 - classification_loss: 0.0493
 5763/10000 [================>.............] - ETA: 1:02:48 - loss: 0.4082 - regression_loss: 0.3589 - classification_loss: 0.0493
 5764/10000 [================>.............] - ETA: 1:02:47 - loss: 0.4082 - regression_loss: 0.3588 - classification_loss: 0.0493
 5765/10000 [================>.............] - ETA: 1:02:46 - loss: 0.4082 - regression_loss: 0.3588 - classification_loss: 0.0493
 5766/10000 [================>.............] - ETA: 1:02:45 - loss: 0.4082 - regression_loss: 0.3589 - classification_loss: 0.0493
 5767/10000 [================>.............] - ETA: 1:02:44 - loss: 0.4082 - regression_loss: 0.3589 - classification_loss: 0.0493
 5768/10000 [================>.............] - ETA: 1:02:44 - loss: 0.4082 - regression_loss: 0.3589 - classification_loss: 0.0493
 5769/10000 [================>.............] - ETA: 1:02:43 - loss: 0.4082 - regression_loss: 0.3589 - classification_loss: 0.0493
 5770/10000 [================>.............] - ETA: 1:02:42 - loss: 0.4081 - regression_loss: 0.3588 - classification_loss: 0.0493
 5771/10000 [================>.............] - ETA: 1:02:41 - loss: 0.4081 - regression_loss: 0.3588 - classification_loss: 0.0493
 5772/10000 [================>.............] - ETA: 1:02:40 - loss: 0.4081 - regression_loss: 0.3588 - classification_loss: 0.0493
 5773/10000 [================>.............] - ETA: 1:02:39 - loss: 0.4081 - regression_loss: 0.3589 - classification_loss: 0.0493
 5774/10000 [================>.............] - ETA: 1:02:38 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0493
 5775/10000 [================>.............] - ETA: 1:02:37 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0493
 5776/10000 [================>.............] - ETA: 1:02:36 - loss: 0.4083 - regression_loss: 0.3589 - classification_loss: 0.0493
 5777/10000 [================>.............] - ETA: 1:02:36 - loss: 0.4083 - regression_loss: 0.3589 - classification_loss: 0.0493
 5778/10000 [================>.............] - ETA: 1:02:35 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0493
 5779/10000 [================>.............] - ETA: 1:02:34 - loss: 0.4083 - regression_loss: 0.3589 - classification_loss: 0.0493
 5780/10000 [================>.............] - ETA: 1:02:33 - loss: 0.4082 - regression_loss: 0.3589 - classification_loss: 0.0493
 5781/10000 [================>.............] - ETA: 1:02:32 - loss: 0.4082 - regression_loss: 0.3589 - classification_loss: 0.0493
 5782/10000 [================>.............] - ETA: 1:02:31 - loss: 0.4082 - regression_loss: 0.3588 - classification_loss: 0.0493
 5783/10000 [================>.............] - ETA: 1:02:30 - loss: 0.4081 - regression_loss: 0.3588 - classification_loss: 0.0493
 5784/10000 [================>.............] - ETA: 1:02:29 - loss: 0.4082 - regression_loss: 0.3589 - classification_loss: 0.0493
 5785/10000 [================>.............] - ETA: 1:02:28 - loss: 0.4082 - regression_loss: 0.3589 - classification_loss: 0.0493
 5786/10000 [================>.............] - ETA: 1:02:28 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0493
 5787/10000 [================>.............] - ETA: 1:02:27 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0493
 5788/10000 [================>.............] - ETA: 1:02:26 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0493
 5789/10000 [================>.............] - ETA: 1:02:25 - loss: 0.4084 - regression_loss: 0.3591 - classification_loss: 0.0493
 5790/10000 [================>.............] - ETA: 1:02:24 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0493
 5791/10000 [================>.............] - ETA: 1:02:23 - loss: 0.4083 - regression_loss: 0.3590 - classification_loss: 0.0493
 5792/10000 [================>.............] - ETA: 1:02:22 - loss: 0.4082 - regression_loss: 0.3589 - classification_loss: 0.0493
 5793/10000 [================>.............] - ETA: 1:02:21 - loss: 0.4082 - regression_loss: 0.3589 - classification_loss: 0.0493
 5794/10000 [================>.............] - ETA: 1:02:20 - loss: 0.4082 - regression_loss: 0.3589 - classification_loss: 0.0493
 5795/10000 [================>.............] - ETA: 1:02:20 - loss: 0.4082 - regression_loss: 0.3589 - classification_loss: 0.0493
 5796/10000 [================>.............] - ETA: 1:02:19 - loss: 0.4081 - regression_loss: 0.3588 - classification_loss: 0.0493
 5797/10000 [================>.............] - ETA: 1:02:18 - loss: 0.4082 - regression_loss: 0.3589 - classification_loss: 0.0493
 5798/10000 [================>.............] - ETA: 1:02:17 - loss: 0.4082 - regression_loss: 0.3589 - classification_loss: 0.0493
 5799/10000 [================>.............] - ETA: 1:02:16 - loss: 0.4081 - regression_loss: 0.3589 - classification_loss: 0.0493
 5800/10000 [================>.............] - ETA: 1:02:15 - loss: 0.4082 - regression_loss: 0.3589 - classification_loss: 0.0493
 5801/10000 [================>.............] - ETA: 1:02:14 - loss: 0.4081 - regression_loss: 0.3588 - classification_loss: 0.0493
 5802/10000 [================>.............] - ETA: 1:02:13 - loss: 0.4081 - regression_loss: 0.3588 - classification_loss: 0.0493
 5803/10000 [================>.............] - ETA: 1:02:12 - loss: 0.4081 - regression_loss: 0.3588 - classification_loss: 0.0493
 5804/10000 [================>.............] - ETA: 1:02:12 - loss: 0.4080 - regression_loss: 0.3588 - classification_loss: 0.0493
 5805/10000 [================>.............] - ETA: 1:02:11 - loss: 0.4080 - regression_loss: 0.3588 - classification_loss: 0.0492
 5806/10000 [================>.............] - ETA: 1:02:10 - loss: 0.4081 - regression_loss: 0.3588 - classification_loss: 0.0492
 5807/10000 [================>.............] - ETA: 1:02:09 - loss: 0.4080 - regression_loss: 0.3588 - classification_loss: 0.0492
 5808/10000 [================>.............] - ETA: 1:02:08 - loss: 0.4080 - regression_loss: 0.3588 - classification_loss: 0.0492
 5809/10000 [================>.............] - ETA: 1:02:07 - loss: 0.4081 - regression_loss: 0.3588 - classification_loss: 0.0492
 5810/10000 [================>.............] - ETA: 1:02:06 - loss: 0.4080 - regression_loss: 0.3588 - classification_loss: 0.0492
 5811/10000 [================>.............] - ETA: 1:02:05 - loss: 0.4080 - regression_loss: 0.3588 - classification_loss: 0.0492
 5812/10000 [================>.............] - ETA: 1:02:04 - loss: 0.4080 - regression_loss: 0.3587 - classification_loss: 0.0492
 5813/10000 [================>.............] - ETA: 1:02:04 - loss: 0.4079 - regression_loss: 0.3587 - classification_loss: 0.0492
 5814/10000 [================>.............] - ETA: 1:02:03 - loss: 0.4079 - regression_loss: 0.3587 - classification_loss: 0.0492
 5815/10000 [================>.............] - ETA: 1:02:02 - loss: 0.4079 - regression_loss: 0.3587 - classification_loss: 0.0492
 5816/10000 [================>.............] - ETA: 1:02:01 - loss: 0.4079 - regression_loss: 0.3587 - classification_loss: 0.0492
 5817/10000 [================>.............] - ETA: 1:02:00 - loss: 0.4079 - regression_loss: 0.3587 - classification_loss: 0.0492
 5818/10000 [================>.............] - ETA: 1:01:59 - loss: 0.4079 - regression_loss: 0.3587 - classification_loss: 0.0492
 5819/10000 [================>.............] - ETA: 1:01:58 - loss: 0.4079 - regression_loss: 0.3587 - classification_loss: 0.0492
 5820/10000 [================>.............] - ETA: 1:01:57 - loss: 0.4079 - regression_loss: 0.3587 - classification_loss: 0.0492
 5821/10000 [================>.............] - ETA: 1:01:56 - loss: 0.4078 - regression_loss: 0.3586 - classification_loss: 0.0492
 5822/10000 [================>.............] - ETA: 1:01:56 - loss: 0.4078 - regression_loss: 0.3586 - classification_loss: 0.0492
 5823/10000 [================>.............] - ETA: 1:01:55 - loss: 0.4078 - regression_loss: 0.3586 - classification_loss: 0.0492
 5824/10000 [================>.............] - ETA: 1:01:54 - loss: 0.4078 - regression_loss: 0.3586 - classification_loss: 0.0492
 5825/10000 [================>.............] - ETA: 1:01:53 - loss: 0.4078 - regression_loss: 0.3586 - classification_loss: 0.0492
 5826/10000 [================>.............] - ETA: 1:01:52 - loss: 0.4078 - regression_loss: 0.3586 - classification_loss: 0.0492
 5827/10000 [================>.............] - ETA: 1:01:51 - loss: 0.4079 - regression_loss: 0.3587 - classification_loss: 0.0493
 5828/10000 [================>.............] - ETA: 1:01:50 - loss: 0.4080 - regression_loss: 0.3587 - classification_loss: 0.0493
 5829/10000 [================>.............] - ETA: 1:01:49 - loss: 0.4080 - regression_loss: 0.3587 - classification_loss: 0.0493
 5830/10000 [================>.............] - ETA: 1:01:48 - loss: 0.4079 - regression_loss: 0.3586 - classification_loss: 0.0493
 5831/10000 [================>.............] - ETA: 1:01:48 - loss: 0.4079 - regression_loss: 0.3586 - classification_loss: 0.0493
 5832/10000 [================>.............] - ETA: 1:01:47 - loss: 0.4079 - regression_loss: 0.3586 - classification_loss: 0.0493
 5833/10000 [================>.............] - ETA: 1:01:46 - loss: 0.4079 - regression_loss: 0.3586 - classification_loss: 0.0493
 5834/10000 [================>.............] - ETA: 1:01:45 - loss: 0.4078 - regression_loss: 0.3586 - classification_loss: 0.0493
 5835/10000 [================>.............] - ETA: 1:01:44 - loss: 0.4078 - regression_loss: 0.3585 - classification_loss: 0.0493
 5836/10000 [================>.............] - ETA: 1:01:43 - loss: 0.4078 - regression_loss: 0.3585 - classification_loss: 0.0493
 5837/10000 [================>.............] - ETA: 1:01:42 - loss: 0.4078 - regression_loss: 0.3585 - classification_loss: 0.0493
 5838/10000 [================>.............] - ETA: 1:01:41 - loss: 0.4078 - regression_loss: 0.3585 - classification_loss: 0.0493
 5839/10000 [================>.............] - ETA: 1:01:40 - loss: 0.4078 - regression_loss: 0.3586 - classification_loss: 0.0493
 5840/10000 [================>.............] - ETA: 1:01:40 - loss: 0.4078 - regression_loss: 0.3586 - classification_loss: 0.0493
 5841/10000 [================>.............] - ETA: 1:01:39 - loss: 0.4078 - regression_loss: 0.3586 - classification_loss: 0.0493
 5842/10000 [================>.............] - ETA: 1:01:38 - loss: 0.4078 - regression_loss: 0.3586 - classification_loss: 0.0493
 5843/10000 [================>.............] - ETA: 1:01:37 - loss: 0.4078 - regression_loss: 0.3586 - classification_loss: 0.0493
 5844/10000 [================>.............] - ETA: 1:01:36 - loss: 0.4078 - regression_loss: 0.3586 - classification_loss: 0.0493
 5845/10000 [================>.............] - ETA: 1:01:35 - loss: 0.4078 - regression_loss: 0.3586 - classification_loss: 0.0492
 5846/10000 [================>.............] - ETA: 1:01:34 - loss: 0.4078 - regression_loss: 0.3585 - classification_loss: 0.0492
 5847/10000 [================>.............] - ETA: 1:01:33 - loss: 0.4078 - regression_loss: 0.3586 - classification_loss: 0.0492
 5848/10000 [================>.............] - ETA: 1:01:32 - loss: 0.4078 - regression_loss: 0.3586 - classification_loss: 0.0492
 5849/10000 [================>.............] - ETA: 1:01:32 - loss: 0.4079 - regression_loss: 0.3586 - classification_loss: 0.0492
 5850/10000 [================>.............] - ETA: 1:01:31 - loss: 0.4078 - regression_loss: 0.3586 - classification_loss: 0.0492
 5851/10000 [================>.............] - ETA: 1:01:30 - loss: 0.4078 - regression_loss: 0.3586 - classification_loss: 0.0492
 5852/10000 [================>.............] - ETA: 1:01:29 - loss: 0.4078 - regression_loss: 0.3586 - classification_loss: 0.0492
 5853/10000 [================>.............] - ETA: 1:01:28 - loss: 0.4078 - regression_loss: 0.3586 - classification_loss: 0.0492
 5854/10000 [================>.............] - ETA: 1:01:27 - loss: 0.4078 - regression_loss: 0.3586 - classification_loss: 0.0492
 5855/10000 [================>.............] - ETA: 1:01:26 - loss: 0.4078 - regression_loss: 0.3586 - classification_loss: 0.0492
 5856/10000 [================>.............] - ETA: 1:01:25 - loss: 0.4078 - regression_loss: 0.3586 - classification_loss: 0.0492
 5857/10000 [================>.............] - ETA: 1:01:24 - loss: 0.4078 - regression_loss: 0.3586 - classification_loss: 0.0492
 5858/10000 [================>.............] - ETA: 1:01:24 - loss: 0.4077 - regression_loss: 0.3585 - classification_loss: 0.0492
 5859/10000 [================>.............] - ETA: 1:01:23 - loss: 0.4077 - regression_loss: 0.3586 - classification_loss: 0.0492
 5860/10000 [================>.............] - ETA: 1:01:22 - loss: 0.4077 - regression_loss: 0.3585 - classification_loss: 0.0492
 5861/10000 [================>.............] - ETA: 1:01:21 - loss: 0.4078 - regression_loss: 0.3586 - classification_loss: 0.0492
 5862/10000 [================>.............] - ETA: 1:01:20 - loss: 0.4077 - regression_loss: 0.3586 - classification_loss: 0.0492
 5863/10000 [================>.............] - ETA: 1:01:19 - loss: 0.4077 - regression_loss: 0.3586 - classification_loss: 0.0492
 5864/10000 [================>.............] - ETA: 1:01:18 - loss: 0.4077 - regression_loss: 0.3585 - classification_loss: 0.0492
 5865/10000 [================>.............] - ETA: 1:01:17 - loss: 0.4077 - regression_loss: 0.3585 - classification_loss: 0.0492
 5866/10000 [================>.............] - ETA: 1:01:16 - loss: 0.4077 - regression_loss: 0.3585 - classification_loss: 0.0492
 5867/10000 [================>.............] - ETA: 1:01:15 - loss: 0.4077 - regression_loss: 0.3585 - classification_loss: 0.0492
 5868/10000 [================>.............] - ETA: 1:01:15 - loss: 0.4076 - regression_loss: 0.3585 - classification_loss: 0.0492
 5869/10000 [================>.............] - ETA: 1:01:14 - loss: 0.4076 - regression_loss: 0.3584 - classification_loss: 0.0492
 5870/10000 [================>.............] - ETA: 1:01:13 - loss: 0.4076 - regression_loss: 0.3584 - classification_loss: 0.0492
 5871/10000 [================>.............] - ETA: 1:01:12 - loss: 0.4076 - regression_loss: 0.3584 - classification_loss: 0.0492
 5872/10000 [================>.............] - ETA: 1:01:11 - loss: 0.4075 - regression_loss: 0.3584 - classification_loss: 0.0492
 5873/10000 [================>.............] - ETA: 1:01:10 - loss: 0.4076 - regression_loss: 0.3585 - classification_loss: 0.0492
 5874/10000 [================>.............] - ETA: 1:01:09 - loss: 0.4076 - regression_loss: 0.3585 - classification_loss: 0.0491
 5875/10000 [================>.............] - ETA: 1:01:08 - loss: 0.4076 - regression_loss: 0.3584 - classification_loss: 0.0491
 5876/10000 [================>.............] - ETA: 1:01:07 - loss: 0.4076 - regression_loss: 0.3584 - classification_loss: 0.0491
 5877/10000 [================>.............] - ETA: 1:01:07 - loss: 0.4075 - regression_loss: 0.3584 - classification_loss: 0.0491
 5878/10000 [================>.............] - ETA: 1:01:06 - loss: 0.4075 - regression_loss: 0.3584 - classification_loss: 0.0491
 5879/10000 [================>.............] - ETA: 1:01:05 - loss: 0.4075 - regression_loss: 0.3584 - classification_loss: 0.0491
 5880/10000 [================>.............] - ETA: 1:01:04 - loss: 0.4075 - regression_loss: 0.3584 - classification_loss: 0.0491
 5881/10000 [================>.............] - ETA: 1:01:03 - loss: 0.4074 - regression_loss: 0.3583 - classification_loss: 0.0491
 5882/10000 [================>.............] - ETA: 1:01:02 - loss: 0.4074 - regression_loss: 0.3583 - classification_loss: 0.0491
 5883/10000 [================>.............] - ETA: 1:01:01 - loss: 0.4073 - regression_loss: 0.3582 - classification_loss: 0.0491
 5884/10000 [================>.............] - ETA: 1:01:00 - loss: 0.4073 - regression_loss: 0.3582 - classification_loss: 0.0491
 5885/10000 [================>.............] - ETA: 1:00:59 - loss: 0.4072 - regression_loss: 0.3581 - classification_loss: 0.0491
 5886/10000 [================>.............] - ETA: 1:00:59 - loss: 0.4072 - regression_loss: 0.3581 - classification_loss: 0.0491
 5887/10000 [================>.............] - ETA: 1:00:58 - loss: 0.4072 - regression_loss: 0.3581 - classification_loss: 0.0491
 5888/10000 [================>.............] - ETA: 1:00:57 - loss: 0.4072 - regression_loss: 0.3581 - classification_loss: 0.0491
 5889/10000 [================>.............] - ETA: 1:00:56 - loss: 0.4072 - regression_loss: 0.3581 - classification_loss: 0.0491
 5890/10000 [================>.............] - ETA: 1:00:55 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0491
 5891/10000 [================>.............] - ETA: 1:00:54 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0491
 5892/10000 [================>.............] - ETA: 1:00:53 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0491
 5893/10000 [================>.............] - ETA: 1:00:52 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0491
 5894/10000 [================>.............] - ETA: 1:00:51 - loss: 0.4073 - regression_loss: 0.3582 - classification_loss: 0.0491
 5895/10000 [================>.............] - ETA: 1:00:51 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0491
 5896/10000 [================>.............] - ETA: 1:00:50 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0491
 5897/10000 [================>.............] - ETA: 1:00:49 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0491
 5898/10000 [================>.............] - ETA: 1:00:48 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0491
 5899/10000 [================>.............] - ETA: 1:00:47 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0491
 5900/10000 [================>.............] - ETA: 1:00:46 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0490
 5901/10000 [================>.............] - ETA: 1:00:45 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0490
 5902/10000 [================>.............] - ETA: 1:00:44 - loss: 0.4072 - regression_loss: 0.3581 - classification_loss: 0.0490
 5903/10000 [================>.............] - ETA: 1:00:43 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 5904/10000 [================>.............] - ETA: 1:00:43 - loss: 0.4072 - regression_loss: 0.3581 - classification_loss: 0.0490
 5905/10000 [================>.............] - ETA: 1:00:42 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 5906/10000 [================>.............] - ETA: 1:00:41 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 5907/10000 [================>.............] - ETA: 1:00:40 - loss: 0.4072 - regression_loss: 0.3581 - classification_loss: 0.0490
 5908/10000 [================>.............] - ETA: 1:00:39 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 5909/10000 [================>.............] - ETA: 1:00:38 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 5910/10000 [================>.............] - ETA: 1:00:37 - loss: 0.4072 - regression_loss: 0.3581 - classification_loss: 0.0490
 5911/10000 [================>.............] - ETA: 1:00:36 - loss: 0.4072 - regression_loss: 0.3581 - classification_loss: 0.0490
 5912/10000 [================>.............] - ETA: 1:00:35 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 5913/10000 [================>.............] - ETA: 1:00:35 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 5914/10000 [================>.............] - ETA: 1:00:34 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 5915/10000 [================>.............] - ETA: 1:00:33 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 5916/10000 [================>.............] - ETA: 1:00:32 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 5917/10000 [================>.............] - ETA: 1:00:31 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 5918/10000 [================>.............] - ETA: 1:00:30 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 5919/10000 [================>.............] - ETA: 1:00:29 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 5920/10000 [================>.............] - ETA: 1:00:28 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 5921/10000 [================>.............] - ETA: 1:00:27 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 5922/10000 [================>.............] - ETA: 1:00:27 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 5923/10000 [================>.............] - ETA: 1:00:26 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 5924/10000 [================>.............] - ETA: 1:00:25 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 5925/10000 [================>.............] - ETA: 1:00:24 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 5926/10000 [================>.............] - ETA: 1:00:23 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 5927/10000 [================>.............] - ETA: 1:00:22 - loss: 0.4071 - regression_loss: 0.3580 - classification_loss: 0.0490
 5928/10000 [================>.............] - ETA: 1:00:21 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 5929/10000 [================>.............] - ETA: 1:00:20 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 5930/10000 [================>.............] - ETA: 1:00:19 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0490
 5931/10000 [================>.............] - ETA: 1:00:19 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0490
 5932/10000 [================>.............] - ETA: 1:00:18 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0490
 5933/10000 [================>.............] - ETA: 1:00:17 - loss: 0.4073 - regression_loss: 0.3582 - classification_loss: 0.0490
 5934/10000 [================>.............] - ETA: 1:00:16 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5935/10000 [================>.............] - ETA: 1:00:15 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5936/10000 [================>.............] - ETA: 1:00:14 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0491
 5937/10000 [================>.............] - ETA: 1:00:13 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5938/10000 [================>.............] - ETA: 1:00:12 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5939/10000 [================>.............] - ETA: 1:00:11 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5940/10000 [================>.............] - ETA: 1:00:11 - loss: 0.4073 - regression_loss: 0.3582 - classification_loss: 0.0490
 5941/10000 [================>.............] - ETA: 1:00:10 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0490
 5942/10000 [================>.............] - ETA: 1:00:09 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0490
 5943/10000 [================>.............] - ETA: 1:00:08 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0490
 5944/10000 [================>.............] - ETA: 1:00:07 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0490
 5945/10000 [================>.............] - ETA: 1:00:06 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0490
 5946/10000 [================>.............] - ETA: 1:00:05 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0490
 5947/10000 [================>.............] - ETA: 1:00:04 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0490
 5948/10000 [================>.............] - ETA: 1:00:03 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0490
 5949/10000 [================>.............] - ETA: 1:00:02 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5950/10000 [================>.............] - ETA: 1:00:02 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5951/10000 [================>.............] - ETA: 1:00:01 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5952/10000 [================>.............] - ETA: 1:00:00 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0490
 5953/10000 [================>.............] - ETA: 59:59 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0490  
 5954/10000 [================>.............] - ETA: 59:58 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0490
 5955/10000 [================>.............] - ETA: 59:57 - loss: 0.4074 - regression_loss: 0.3583 - classification_loss: 0.0490
 5956/10000 [================>.............] - ETA: 59:56 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5957/10000 [================>.............] - ETA: 59:55 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5958/10000 [================>.............] - ETA: 59:54 - loss: 0.4073 - regression_loss: 0.3582 - classification_loss: 0.0490
 5959/10000 [================>.............] - ETA: 59:54 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5960/10000 [================>.............] - ETA: 59:53 - loss: 0.4074 - regression_loss: 0.3584 - classification_loss: 0.0490
 5961/10000 [================>.............] - ETA: 59:52 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5962/10000 [================>.............] - ETA: 59:51 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5963/10000 [================>.............] - ETA: 59:50 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5964/10000 [================>.............] - ETA: 59:49 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5965/10000 [================>.............] - ETA: 59:48 - loss: 0.4074 - regression_loss: 0.3583 - classification_loss: 0.0490
 5966/10000 [================>.............] - ETA: 59:47 - loss: 0.4074 - regression_loss: 0.3584 - classification_loss: 0.0490
 5967/10000 [================>.............] - ETA: 59:46 - loss: 0.4074 - regression_loss: 0.3583 - classification_loss: 0.0490
 5968/10000 [================>.............] - ETA: 59:46 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5969/10000 [================>.............] - ETA: 59:45 - loss: 0.4074 - regression_loss: 0.3584 - classification_loss: 0.0490
 5970/10000 [================>.............] - ETA: 59:44 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5971/10000 [================>.............] - ETA: 59:43 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5972/10000 [================>.............] - ETA: 59:42 - loss: 0.4074 - regression_loss: 0.3584 - classification_loss: 0.0490
 5973/10000 [================>.............] - ETA: 59:41 - loss: 0.4074 - regression_loss: 0.3584 - classification_loss: 0.0490
 5974/10000 [================>.............] - ETA: 59:40 - loss: 0.4075 - regression_loss: 0.3584 - classification_loss: 0.0490
 5975/10000 [================>.............] - ETA: 59:39 - loss: 0.4075 - regression_loss: 0.3584 - classification_loss: 0.0490
 5976/10000 [================>.............] - ETA: 59:38 - loss: 0.4074 - regression_loss: 0.3584 - classification_loss: 0.0490
 5977/10000 [================>.............] - ETA: 59:38 - loss: 0.4074 - regression_loss: 0.3584 - classification_loss: 0.0490
 5978/10000 [================>.............] - ETA: 59:37 - loss: 0.4074 - regression_loss: 0.3584 - classification_loss: 0.0490
 5979/10000 [================>.............] - ETA: 59:36 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5980/10000 [================>.............] - ETA: 59:35 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5981/10000 [================>.............] - ETA: 59:34 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5982/10000 [================>.............] - ETA: 59:33 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5983/10000 [================>.............] - ETA: 59:32 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5984/10000 [================>.............] - ETA: 59:31 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5985/10000 [================>.............] - ETA: 59:30 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5986/10000 [================>.............] - ETA: 59:30 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5987/10000 [================>.............] - ETA: 59:29 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5988/10000 [================>.............] - ETA: 59:28 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5989/10000 [================>.............] - ETA: 59:27 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 5990/10000 [================>.............] - ETA: 59:26 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0490
 5991/10000 [================>.............] - ETA: 59:25 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0490
 5992/10000 [================>.............] - ETA: 59:24 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0490
 5993/10000 [================>.............] - ETA: 59:23 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 5994/10000 [================>.............] - ETA: 59:22 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 5995/10000 [================>.............] - ETA: 59:22 - loss: 0.4070 - regression_loss: 0.3581 - classification_loss: 0.0490
 5996/10000 [================>.............] - ETA: 59:21 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 5997/10000 [================>.............] - ETA: 59:20 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 5998/10000 [================>.............] - ETA: 59:19 - loss: 0.4069 - regression_loss: 0.3580 - classification_loss: 0.0490
 5999/10000 [================>.............] - ETA: 59:18 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 6000/10000 [=================>............] - ETA: 59:17 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 6001/10000 [=================>............] - ETA: 59:16 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 6002/10000 [=================>............] - ETA: 59:15 - loss: 0.4069 - regression_loss: 0.3580 - classification_loss: 0.0490
 6003/10000 [=================>............] - ETA: 59:14 - loss: 0.4069 - regression_loss: 0.3580 - classification_loss: 0.0490
 6004/10000 [=================>............] - ETA: 59:14 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 6005/10000 [=================>............] - ETA: 59:13 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0489
 6006/10000 [=================>............] - ETA: 59:12 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 6007/10000 [=================>............] - ETA: 59:11 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 6008/10000 [=================>............] - ETA: 59:10 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 6009/10000 [=================>............] - ETA: 59:09 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 6010/10000 [=================>............] - ETA: 59:08 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 6011/10000 [=================>............] - ETA: 59:07 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 6012/10000 [=================>............] - ETA: 59:06 - loss: 0.4069 - regression_loss: 0.3580 - classification_loss: 0.0490
 6013/10000 [=================>............] - ETA: 59:06 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 6014/10000 [=================>............] - ETA: 59:05 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 6015/10000 [=================>............] - ETA: 59:04 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 6016/10000 [=================>............] - ETA: 59:03 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 6017/10000 [=================>............] - ETA: 59:02 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 6018/10000 [=================>............] - ETA: 59:01 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 6019/10000 [=================>............] - ETA: 59:00 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 6020/10000 [=================>............] - ETA: 58:59 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 6021/10000 [=================>............] - ETA: 58:58 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 6022/10000 [=================>............] - ETA: 58:57 - loss: 0.4070 - regression_loss: 0.3581 - classification_loss: 0.0490
 6023/10000 [=================>............] - ETA: 58:57 - loss: 0.4070 - regression_loss: 0.3581 - classification_loss: 0.0490
 6024/10000 [=================>............] - ETA: 58:56 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0489
 6025/10000 [=================>............] - ETA: 58:55 - loss: 0.4070 - regression_loss: 0.3581 - classification_loss: 0.0490
 6026/10000 [=================>............] - ETA: 58:54 - loss: 0.4070 - regression_loss: 0.3581 - classification_loss: 0.0489
 6027/10000 [=================>............] - ETA: 58:53 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0489
 6028/10000 [=================>............] - ETA: 58:52 - loss: 0.4069 - regression_loss: 0.3580 - classification_loss: 0.0489
 6029/10000 [=================>............] - ETA: 58:51 - loss: 0.4069 - regression_loss: 0.3580 - classification_loss: 0.0489
 6030/10000 [=================>............] - ETA: 58:50 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0489
 6031/10000 [=================>............] - ETA: 58:49 - loss: 0.4073 - regression_loss: 0.3583 - classification_loss: 0.0490
 6032/10000 [=================>............] - ETA: 58:49 - loss: 0.4079 - regression_loss: 0.3583 - classification_loss: 0.0496
 6033/10000 [=================>............] - ETA: 58:48 - loss: 0.4078 - regression_loss: 0.3583 - classification_loss: 0.0496
 6034/10000 [=================>............] - ETA: 58:47 - loss: 0.4078 - regression_loss: 0.3583 - classification_loss: 0.0495
 6035/10000 [=================>............] - ETA: 58:46 - loss: 0.4079 - regression_loss: 0.3583 - classification_loss: 0.0495
 6036/10000 [=================>............] - ETA: 58:45 - loss: 0.4078 - regression_loss: 0.3583 - classification_loss: 0.0495
 6037/10000 [=================>............] - ETA: 58:44 - loss: 0.4078 - regression_loss: 0.3583 - classification_loss: 0.0495
 6038/10000 [=================>............] - ETA: 58:43 - loss: 0.4078 - regression_loss: 0.3582 - classification_loss: 0.0495
 6039/10000 [=================>............] - ETA: 58:42 - loss: 0.4078 - regression_loss: 0.3582 - classification_loss: 0.0495
 6040/10000 [=================>............] - ETA: 58:41 - loss: 0.4078 - regression_loss: 0.3582 - classification_loss: 0.0495
 6041/10000 [=================>............] - ETA: 58:41 - loss: 0.4078 - regression_loss: 0.3583 - classification_loss: 0.0495
 6042/10000 [=================>............] - ETA: 58:40 - loss: 0.4078 - regression_loss: 0.3583 - classification_loss: 0.0495
 6043/10000 [=================>............] - ETA: 58:39 - loss: 0.4078 - regression_loss: 0.3583 - classification_loss: 0.0495
 6044/10000 [=================>............] - ETA: 58:38 - loss: 0.4078 - regression_loss: 0.3583 - classification_loss: 0.0495
 6045/10000 [=================>............] - ETA: 58:37 - loss: 0.4078 - regression_loss: 0.3583 - classification_loss: 0.0495
 6046/10000 [=================>............] - ETA: 58:36 - loss: 0.4078 - regression_loss: 0.3582 - classification_loss: 0.0495
 6047/10000 [=================>............] - ETA: 58:35 - loss: 0.4077 - regression_loss: 0.3582 - classification_loss: 0.0495
 6048/10000 [=================>............] - ETA: 58:34 - loss: 0.4077 - regression_loss: 0.3582 - classification_loss: 0.0495
 6049/10000 [=================>............] - ETA: 58:33 - loss: 0.4077 - regression_loss: 0.3582 - classification_loss: 0.0495
 6050/10000 [=================>............] - ETA: 58:33 - loss: 0.4078 - regression_loss: 0.3583 - classification_loss: 0.0495
 6051/10000 [=================>............] - ETA: 58:32 - loss: 0.4078 - regression_loss: 0.3583 - classification_loss: 0.0495
 6052/10000 [=================>............] - ETA: 58:31 - loss: 0.4078 - regression_loss: 0.3583 - classification_loss: 0.0495
 6053/10000 [=================>............] - ETA: 58:30 - loss: 0.4078 - regression_loss: 0.3582 - classification_loss: 0.0495
 6054/10000 [=================>............] - ETA: 58:29 - loss: 0.4078 - regression_loss: 0.3582 - classification_loss: 0.0495
 6055/10000 [=================>............] - ETA: 58:28 - loss: 0.4077 - regression_loss: 0.3582 - classification_loss: 0.0495
 6056/10000 [=================>............] - ETA: 58:27 - loss: 0.4077 - regression_loss: 0.3582 - classification_loss: 0.0495
 6057/10000 [=================>............] - ETA: 58:26 - loss: 0.4077 - regression_loss: 0.3582 - classification_loss: 0.0495
 6058/10000 [=================>............] - ETA: 58:25 - loss: 0.4076 - regression_loss: 0.3581 - classification_loss: 0.0495
 6059/10000 [=================>............] - ETA: 58:25 - loss: 0.4076 - regression_loss: 0.3581 - classification_loss: 0.0495
 6060/10000 [=================>............] - ETA: 58:24 - loss: 0.4076 - regression_loss: 0.3581 - classification_loss: 0.0495
 6061/10000 [=================>............] - ETA: 58:23 - loss: 0.4076 - regression_loss: 0.3581 - classification_loss: 0.0495
 6062/10000 [=================>............] - ETA: 58:22 - loss: 0.4076 - regression_loss: 0.3581 - classification_loss: 0.0495
 6063/10000 [=================>............] - ETA: 58:21 - loss: 0.4077 - regression_loss: 0.3582 - classification_loss: 0.0495
 6064/10000 [=================>............] - ETA: 58:20 - loss: 0.4076 - regression_loss: 0.3581 - classification_loss: 0.0495
 6065/10000 [=================>............] - ETA: 58:19 - loss: 0.4076 - regression_loss: 0.3581 - classification_loss: 0.0495
 6066/10000 [=================>............] - ETA: 58:18 - loss: 0.4077 - regression_loss: 0.3582 - classification_loss: 0.0495
 6067/10000 [=================>............] - ETA: 58:17 - loss: 0.4077 - regression_loss: 0.3582 - classification_loss: 0.0495
 6068/10000 [=================>............] - ETA: 58:17 - loss: 0.4077 - regression_loss: 0.3582 - classification_loss: 0.0495
 6069/10000 [=================>............] - ETA: 58:16 - loss: 0.4077 - regression_loss: 0.3582 - classification_loss: 0.0495
 6070/10000 [=================>............] - ETA: 58:15 - loss: 0.4078 - regression_loss: 0.3583 - classification_loss: 0.0495
 6071/10000 [=================>............] - ETA: 58:14 - loss: 0.4078 - regression_loss: 0.3583 - classification_loss: 0.0495
 6072/10000 [=================>............] - ETA: 58:13 - loss: 0.4077 - regression_loss: 0.3582 - classification_loss: 0.0495
 6073/10000 [=================>............] - ETA: 58:12 - loss: 0.4078 - regression_loss: 0.3582 - classification_loss: 0.0495
 6074/10000 [=================>............] - ETA: 58:11 - loss: 0.4077 - regression_loss: 0.3582 - classification_loss: 0.0495
 6075/10000 [=================>............] - ETA: 58:10 - loss: 0.4078 - regression_loss: 0.3582 - classification_loss: 0.0495
 6076/10000 [=================>............] - ETA: 58:09 - loss: 0.4078 - regression_loss: 0.3583 - classification_loss: 0.0495
 6077/10000 [=================>............] - ETA: 58:09 - loss: 0.4078 - regression_loss: 0.3583 - classification_loss: 0.0495
 6078/10000 [=================>............] - ETA: 58:08 - loss: 0.4078 - regression_loss: 0.3583 - classification_loss: 0.0495
 6079/10000 [=================>............] - ETA: 58:07 - loss: 0.4078 - regression_loss: 0.3583 - classification_loss: 0.0495
 6080/10000 [=================>............] - ETA: 58:06 - loss: 0.4078 - regression_loss: 0.3583 - classification_loss: 0.0495
 6081/10000 [=================>............] - ETA: 58:05 - loss: 0.4077 - regression_loss: 0.3582 - classification_loss: 0.0495
 6082/10000 [=================>............] - ETA: 58:04 - loss: 0.4077 - regression_loss: 0.3582 - classification_loss: 0.0495
 6083/10000 [=================>............] - ETA: 58:03 - loss: 0.4077 - regression_loss: 0.3582 - classification_loss: 0.0495
 6084/10000 [=================>............] - ETA: 58:02 - loss: 0.4076 - regression_loss: 0.3581 - classification_loss: 0.0495
 6085/10000 [=================>............] - ETA: 58:01 - loss: 0.4077 - regression_loss: 0.3582 - classification_loss: 0.0495
 6086/10000 [=================>............] - ETA: 58:01 - loss: 0.4077 - regression_loss: 0.3582 - classification_loss: 0.0495
 6087/10000 [=================>............] - ETA: 58:00 - loss: 0.4076 - regression_loss: 0.3581 - classification_loss: 0.0495
 6088/10000 [=================>............] - ETA: 57:59 - loss: 0.4076 - regression_loss: 0.3581 - classification_loss: 0.0495
 6089/10000 [=================>............] - ETA: 57:58 - loss: 0.4076 - regression_loss: 0.3581 - classification_loss: 0.0495
 6090/10000 [=================>............] - ETA: 57:57 - loss: 0.4075 - regression_loss: 0.3580 - classification_loss: 0.0495
 6091/10000 [=================>............] - ETA: 57:56 - loss: 0.4075 - regression_loss: 0.3580 - classification_loss: 0.0495
 6092/10000 [=================>............] - ETA: 57:55 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0495
 6093/10000 [=================>............] - ETA: 57:54 - loss: 0.4075 - regression_loss: 0.3580 - classification_loss: 0.0495
 6094/10000 [=================>............] - ETA: 57:53 - loss: 0.4074 - regression_loss: 0.3580 - classification_loss: 0.0495
 6095/10000 [=================>............] - ETA: 57:52 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0495
 6096/10000 [=================>............] - ETA: 57:52 - loss: 0.4073 - regression_loss: 0.3579 - classification_loss: 0.0495
 6097/10000 [=================>............] - ETA: 57:51 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0495
 6098/10000 [=================>............] - ETA: 57:50 - loss: 0.4074 - regression_loss: 0.3580 - classification_loss: 0.0495
 6099/10000 [=================>............] - ETA: 57:49 - loss: 0.4075 - regression_loss: 0.3580 - classification_loss: 0.0495
 6100/10000 [=================>............] - ETA: 57:48 - loss: 0.4074 - regression_loss: 0.3580 - classification_loss: 0.0495
 6101/10000 [=================>............] - ETA: 57:47 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0495
 6102/10000 [=================>............] - ETA: 57:46 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0495
 6103/10000 [=================>............] - ETA: 57:45 - loss: 0.4073 - regression_loss: 0.3579 - classification_loss: 0.0495
 6104/10000 [=================>............] - ETA: 57:44 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0495
 6105/10000 [=================>............] - ETA: 57:44 - loss: 0.4073 - regression_loss: 0.3579 - classification_loss: 0.0494
 6106/10000 [=================>............] - ETA: 57:43 - loss: 0.4073 - regression_loss: 0.3579 - classification_loss: 0.0494
 6107/10000 [=================>............] - ETA: 57:42 - loss: 0.4073 - regression_loss: 0.3578 - classification_loss: 0.0494
 6108/10000 [=================>............] - ETA: 57:41 - loss: 0.4073 - regression_loss: 0.3578 - classification_loss: 0.0494
 6109/10000 [=================>............] - ETA: 57:40 - loss: 0.4073 - regression_loss: 0.3579 - classification_loss: 0.0494
 6110/10000 [=================>............] - ETA: 57:39 - loss: 0.4073 - regression_loss: 0.3578 - classification_loss: 0.0494
 6111/10000 [=================>............] - ETA: 57:38 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0495
 6112/10000 [=================>............] - ETA: 57:37 - loss: 0.4073 - regression_loss: 0.3579 - classification_loss: 0.0494
 6113/10000 [=================>............] - ETA: 57:36 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0494
 6114/10000 [=================>............] - ETA: 57:36 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0495
 6115/10000 [=================>............] - ETA: 57:35 - loss: 0.4075 - regression_loss: 0.3580 - classification_loss: 0.0495
 6116/10000 [=================>............] - ETA: 57:34 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0495
 6117/10000 [=================>............] - ETA: 57:33 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0495
 6118/10000 [=================>............] - ETA: 57:32 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0495
 6119/10000 [=================>............] - ETA: 57:31 - loss: 0.4073 - regression_loss: 0.3579 - classification_loss: 0.0494
 6120/10000 [=================>............] - ETA: 57:30 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0495
 6121/10000 [=================>............] - ETA: 57:29 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0495
 6122/10000 [=================>............] - ETA: 57:28 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0495
 6123/10000 [=================>............] - ETA: 57:28 - loss: 0.4073 - regression_loss: 0.3579 - classification_loss: 0.0495
 6124/10000 [=================>............] - ETA: 57:27 - loss: 0.4075 - regression_loss: 0.3580 - classification_loss: 0.0495
 6125/10000 [=================>............] - ETA: 57:26 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0495
 6126/10000 [=================>............] - ETA: 57:25 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0495
 6127/10000 [=================>............] - ETA: 57:24 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0495
 6128/10000 [=================>............] - ETA: 57:23 - loss: 0.4073 - regression_loss: 0.3579 - classification_loss: 0.0495
 6129/10000 [=================>............] - ETA: 57:22 - loss: 0.4073 - regression_loss: 0.3579 - classification_loss: 0.0495
 6130/10000 [=================>............] - ETA: 57:21 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0495
 6131/10000 [=================>............] - ETA: 57:20 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0495
 6132/10000 [=================>............] - ETA: 57:20 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0495
 6133/10000 [=================>............] - ETA: 57:19 - loss: 0.4075 - regression_loss: 0.3580 - classification_loss: 0.0495
 6134/10000 [=================>............] - ETA: 57:18 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0495
 6135/10000 [=================>............] - ETA: 57:17 - loss: 0.4074 - regression_loss: 0.3580 - classification_loss: 0.0495
 6136/10000 [=================>............] - ETA: 57:16 - loss: 0.4074 - regression_loss: 0.3580 - classification_loss: 0.0495
 6137/10000 [=================>............] - ETA: 57:15 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0495
 6138/10000 [=================>............] - ETA: 57:14 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0495
 6139/10000 [=================>............] - ETA: 57:13 - loss: 0.4073 - regression_loss: 0.3579 - classification_loss: 0.0495
 6140/10000 [=================>............] - ETA: 57:12 - loss: 0.4073 - regression_loss: 0.3579 - classification_loss: 0.0494
 6141/10000 [=================>............] - ETA: 57:12 - loss: 0.4073 - regression_loss: 0.3578 - classification_loss: 0.0494
 6142/10000 [=================>............] - ETA: 57:11 - loss: 0.4073 - regression_loss: 0.3578 - classification_loss: 0.0494
 6143/10000 [=================>............] - ETA: 57:10 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0495
 6144/10000 [=================>............] - ETA: 57:09 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0495
 6145/10000 [=================>............] - ETA: 57:08 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0495
 6146/10000 [=================>............] - ETA: 57:07 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0495
 6147/10000 [=================>............] - ETA: 57:06 - loss: 0.4074 - regression_loss: 0.3579 - classification_loss: 0.0495
 6148/10000 [=================>............] - ETA: 57:05 - loss: 0.4073 - regression_loss: 0.3579 - classification_loss: 0.0495
 6149/10000 [=================>............] - ETA: 57:04 - loss: 0.4073 - regression_loss: 0.3579 - classification_loss: 0.0495
 6150/10000 [=================>............] - ETA: 57:04 - loss: 0.4073 - regression_loss: 0.3579 - classification_loss: 0.0494
 6151/10000 [=================>............] - ETA: 57:03 - loss: 0.4073 - regression_loss: 0.3579 - classification_loss: 0.0494
 6152/10000 [=================>............] - ETA: 57:02 - loss: 0.4073 - regression_loss: 0.3578 - classification_loss: 0.0494
 6153/10000 [=================>............] - ETA: 57:01 - loss: 0.4073 - regression_loss: 0.3578 - classification_loss: 0.0494
 6154/10000 [=================>............] - ETA: 57:00 - loss: 0.4073 - regression_loss: 0.3578 - classification_loss: 0.0494
 6155/10000 [=================>............] - ETA: 56:59 - loss: 0.4073 - regression_loss: 0.3578 - classification_loss: 0.0494
 6156/10000 [=================>............] - ETA: 56:58 - loss: 0.4073 - regression_loss: 0.3578 - classification_loss: 0.0494
 6157/10000 [=================>............] - ETA: 56:57 - loss: 0.4072 - regression_loss: 0.3578 - classification_loss: 0.0494
 6158/10000 [=================>............] - ETA: 56:56 - loss: 0.4072 - regression_loss: 0.3578 - classification_loss: 0.0494
 6159/10000 [=================>............] - ETA: 56:56 - loss: 0.4073 - regression_loss: 0.3578 - classification_loss: 0.0495
 6160/10000 [=================>............] - ETA: 56:55 - loss: 0.4073 - regression_loss: 0.3578 - classification_loss: 0.0495
 6161/10000 [=================>............] - ETA: 56:54 - loss: 0.4072 - regression_loss: 0.3578 - classification_loss: 0.0495
 6162/10000 [=================>............] - ETA: 56:53 - loss: 0.4073 - regression_loss: 0.3578 - classification_loss: 0.0495
 6163/10000 [=================>............] - ETA: 56:52 - loss: 0.4072 - regression_loss: 0.3578 - classification_loss: 0.0495
 6164/10000 [=================>............] - ETA: 56:51 - loss: 0.4072 - regression_loss: 0.3578 - classification_loss: 0.0494
 6165/10000 [=================>............] - ETA: 56:50 - loss: 0.4072 - regression_loss: 0.3578 - classification_loss: 0.0494
 6166/10000 [=================>............] - ETA: 56:49 - loss: 0.4073 - regression_loss: 0.3578 - classification_loss: 0.0495
 6167/10000 [=================>............] - ETA: 56:48 - loss: 0.4073 - regression_loss: 0.3578 - classification_loss: 0.0495
 6168/10000 [=================>............] - ETA: 56:47 - loss: 0.4073 - regression_loss: 0.3578 - classification_loss: 0.0495
 6169/10000 [=================>............] - ETA: 56:47 - loss: 0.4072 - regression_loss: 0.3578 - classification_loss: 0.0494
 6170/10000 [=================>............] - ETA: 56:46 - loss: 0.4072 - regression_loss: 0.3578 - classification_loss: 0.0494
 6171/10000 [=================>............] - ETA: 56:45 - loss: 0.4072 - regression_loss: 0.3578 - classification_loss: 0.0494
 6172/10000 [=================>............] - ETA: 56:44 - loss: 0.4072 - regression_loss: 0.3578 - classification_loss: 0.0494
 6173/10000 [=================>............] - ETA: 56:43 - loss: 0.4072 - regression_loss: 0.3578 - classification_loss: 0.0494
 6174/10000 [=================>............] - ETA: 56:42 - loss: 0.4072 - regression_loss: 0.3578 - classification_loss: 0.0494
 6175/10000 [=================>............] - ETA: 56:41 - loss: 0.4072 - regression_loss: 0.3578 - classification_loss: 0.0494
 6176/10000 [=================>............] - ETA: 56:40 - loss: 0.4071 - regression_loss: 0.3577 - classification_loss: 0.0494
 6177/10000 [=================>............] - ETA: 56:39 - loss: 0.4071 - regression_loss: 0.3577 - classification_loss: 0.0494
 6178/10000 [=================>............] - ETA: 56:39 - loss: 0.4072 - regression_loss: 0.3578 - classification_loss: 0.0494
 6179/10000 [=================>............] - ETA: 56:38 - loss: 0.4071 - regression_loss: 0.3577 - classification_loss: 0.0494
 6180/10000 [=================>............] - ETA: 56:37 - loss: 0.4071 - regression_loss: 0.3577 - classification_loss: 0.0494
 6181/10000 [=================>............] - ETA: 56:36 - loss: 0.4071 - regression_loss: 0.3577 - classification_loss: 0.0494
 6182/10000 [=================>............] - ETA: 56:35 - loss: 0.4070 - regression_loss: 0.3576 - classification_loss: 0.0494
 6183/10000 [=================>............] - ETA: 56:34 - loss: 0.4070 - regression_loss: 0.3576 - classification_loss: 0.0494
 6184/10000 [=================>............] - ETA: 56:33 - loss: 0.4069 - regression_loss: 0.3575 - classification_loss: 0.0494
 6185/10000 [=================>............] - ETA: 56:32 - loss: 0.4069 - regression_loss: 0.3575 - classification_loss: 0.0494
 6186/10000 [=================>............] - ETA: 56:31 - loss: 0.4069 - regression_loss: 0.3575 - classification_loss: 0.0494
 6187/10000 [=================>............] - ETA: 56:31 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0494
 6188/10000 [=================>............] - ETA: 56:30 - loss: 0.4069 - regression_loss: 0.3575 - classification_loss: 0.0494
 6189/10000 [=================>............] - ETA: 56:29 - loss: 0.4069 - regression_loss: 0.3575 - classification_loss: 0.0494
 6190/10000 [=================>............] - ETA: 56:28 - loss: 0.4069 - regression_loss: 0.3575 - classification_loss: 0.0494
 6191/10000 [=================>............] - ETA: 56:27 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0494
 6192/10000 [=================>............] - ETA: 56:26 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0493
 6193/10000 [=================>............] - ETA: 56:25 - loss: 0.4068 - regression_loss: 0.3574 - classification_loss: 0.0493
 6194/10000 [=================>............] - ETA: 56:24 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0493
 6195/10000 [=================>............] - ETA: 56:23 - loss: 0.4069 - regression_loss: 0.3575 - classification_loss: 0.0493
 6196/10000 [=================>............] - ETA: 56:23 - loss: 0.4069 - regression_loss: 0.3575 - classification_loss: 0.0493
 6197/10000 [=================>............] - ETA: 56:22 - loss: 0.4068 - regression_loss: 0.3574 - classification_loss: 0.0493
 6198/10000 [=================>............] - ETA: 56:21 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0493
 6199/10000 [=================>............] - ETA: 56:20 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0493
 6200/10000 [=================>............] - ETA: 56:19 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0493
 6201/10000 [=================>............] - ETA: 56:18 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0493
 6202/10000 [=================>............] - ETA: 56:17 - loss: 0.4067 - regression_loss: 0.3574 - classification_loss: 0.0493
 6203/10000 [=================>............] - ETA: 56:16 - loss: 0.4067 - regression_loss: 0.3574 - classification_loss: 0.0493
 6204/10000 [=================>............] - ETA: 56:15 - loss: 0.4067 - regression_loss: 0.3574 - classification_loss: 0.0493
 6205/10000 [=================>............] - ETA: 56:15 - loss: 0.4068 - regression_loss: 0.3574 - classification_loss: 0.0493
 6206/10000 [=================>............] - ETA: 56:14 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0493
 6207/10000 [=================>............] - ETA: 56:13 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0493
 6208/10000 [=================>............] - ETA: 56:12 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 6209/10000 [=================>............] - ETA: 56:11 - loss: 0.4069 - regression_loss: 0.3575 - classification_loss: 0.0493
 6210/10000 [=================>............] - ETA: 56:10 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 6211/10000 [=================>............] - ETA: 56:09 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 6212/10000 [=================>............] - ETA: 56:08 - loss: 0.4070 - regression_loss: 0.3577 - classification_loss: 0.0493
 6213/10000 [=================>............] - ETA: 56:07 - loss: 0.4070 - regression_loss: 0.3577 - classification_loss: 0.0493
 6214/10000 [=================>............] - ETA: 56:07 - loss: 0.4071 - regression_loss: 0.3577 - classification_loss: 0.0493
 6215/10000 [=================>............] - ETA: 56:06 - loss: 0.4070 - regression_loss: 0.3577 - classification_loss: 0.0493
 6216/10000 [=================>............] - ETA: 56:05 - loss: 0.4071 - regression_loss: 0.3578 - classification_loss: 0.0493
 6217/10000 [=================>............] - ETA: 56:04 - loss: 0.4071 - regression_loss: 0.3578 - classification_loss: 0.0493
 6218/10000 [=================>............] - ETA: 56:03 - loss: 0.4070 - regression_loss: 0.3577 - classification_loss: 0.0493
 6219/10000 [=================>............] - ETA: 56:02 - loss: 0.4071 - regression_loss: 0.3578 - classification_loss: 0.0493
 6220/10000 [=================>............] - ETA: 56:01 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0493
 6221/10000 [=================>............] - ETA: 56:00 - loss: 0.4072 - regression_loss: 0.3579 - classification_loss: 0.0493
 6222/10000 [=================>............] - ETA: 55:59 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0493
 6223/10000 [=================>............] - ETA: 55:59 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0493
 6224/10000 [=================>............] - ETA: 55:58 - loss: 0.4072 - regression_loss: 0.3579 - classification_loss: 0.0493
 6225/10000 [=================>............] - ETA: 55:57 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0493
 6226/10000 [=================>............] - ETA: 55:56 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0493
 6227/10000 [=================>............] - ETA: 55:55 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0493
 6228/10000 [=================>............] - ETA: 55:54 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0493
 6229/10000 [=================>............] - ETA: 55:53 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0493
 6230/10000 [=================>............] - ETA: 55:52 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0493
 6231/10000 [=================>............] - ETA: 55:51 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0493
 6232/10000 [=================>............] - ETA: 55:51 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0493
 6233/10000 [=================>............] - ETA: 55:50 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0493
 6234/10000 [=================>............] - ETA: 55:49 - loss: 0.4074 - regression_loss: 0.3581 - classification_loss: 0.0493
 6235/10000 [=================>............] - ETA: 55:48 - loss: 0.4074 - regression_loss: 0.3581 - classification_loss: 0.0493
 6236/10000 [=================>............] - ETA: 55:47 - loss: 0.4074 - regression_loss: 0.3581 - classification_loss: 0.0493
 6237/10000 [=================>............] - ETA: 55:46 - loss: 0.4074 - regression_loss: 0.3581 - classification_loss: 0.0493
 6238/10000 [=================>............] - ETA: 55:45 - loss: 0.4074 - regression_loss: 0.3581 - classification_loss: 0.0493
 6239/10000 [=================>............] - ETA: 55:44 - loss: 0.4075 - regression_loss: 0.3582 - classification_loss: 0.0493
 6240/10000 [=================>............] - ETA: 55:43 - loss: 0.4076 - regression_loss: 0.3583 - classification_loss: 0.0493
 6241/10000 [=================>............] - ETA: 55:43 - loss: 0.4075 - regression_loss: 0.3583 - classification_loss: 0.0493
 6242/10000 [=================>............] - ETA: 55:42 - loss: 0.4075 - regression_loss: 0.3582 - classification_loss: 0.0493
 6243/10000 [=================>............] - ETA: 55:41 - loss: 0.4074 - regression_loss: 0.3582 - classification_loss: 0.0493
 6244/10000 [=================>............] - ETA: 55:40 - loss: 0.4074 - regression_loss: 0.3582 - classification_loss: 0.0493
 6245/10000 [=================>............] - ETA: 55:39 - loss: 0.4074 - regression_loss: 0.3581 - classification_loss: 0.0493
 6246/10000 [=================>............] - ETA: 55:38 - loss: 0.4074 - regression_loss: 0.3581 - classification_loss: 0.0493
 6247/10000 [=================>............] - ETA: 55:37 - loss: 0.4074 - regression_loss: 0.3581 - classification_loss: 0.0492
 6248/10000 [=================>............] - ETA: 55:36 - loss: 0.4074 - regression_loss: 0.3581 - classification_loss: 0.0492
 6249/10000 [=================>............] - ETA: 55:35 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6250/10000 [=================>............] - ETA: 55:34 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0492
 6251/10000 [=================>............] - ETA: 55:34 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0492
 6252/10000 [=================>............] - ETA: 55:33 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0492
 6253/10000 [=================>............] - ETA: 55:32 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6254/10000 [=================>............] - ETA: 55:31 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0492
 6255/10000 [=================>............] - ETA: 55:30 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6256/10000 [=================>............] - ETA: 55:29 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6257/10000 [=================>............] - ETA: 55:28 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6258/10000 [=================>............] - ETA: 55:27 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6259/10000 [=================>............] - ETA: 55:26 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6260/10000 [=================>............] - ETA: 55:26 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6261/10000 [=================>............] - ETA: 55:25 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6262/10000 [=================>............] - ETA: 55:24 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6263/10000 [=================>............] - ETA: 55:23 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6264/10000 [=================>............] - ETA: 55:22 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6265/10000 [=================>............] - ETA: 55:21 - loss: 0.4071 - regression_loss: 0.3580 - classification_loss: 0.0492
 6266/10000 [=================>............] - ETA: 55:20 - loss: 0.4071 - regression_loss: 0.3580 - classification_loss: 0.0492
 6267/10000 [=================>............] - ETA: 55:19 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0491
 6268/10000 [=================>............] - ETA: 55:18 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0491
 6269/10000 [=================>............] - ETA: 55:18 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0492
 6270/10000 [=================>............] - ETA: 55:17 - loss: 0.4071 - regression_loss: 0.3580 - classification_loss: 0.0492
 6271/10000 [=================>............] - ETA: 55:16 - loss: 0.4072 - regression_loss: 0.3579 - classification_loss: 0.0493
 6272/10000 [=================>............] - ETA: 55:15 - loss: 0.4072 - regression_loss: 0.3579 - classification_loss: 0.0492
 6273/10000 [=================>............] - ETA: 55:14 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 6274/10000 [=================>............] - ETA: 55:13 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 6275/10000 [=================>............] - ETA: 55:12 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6276/10000 [=================>............] - ETA: 55:11 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6277/10000 [=================>............] - ETA: 55:10 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6278/10000 [=================>............] - ETA: 55:10 - loss: 0.4074 - regression_loss: 0.3581 - classification_loss: 0.0493
 6279/10000 [=================>............] - ETA: 55:09 - loss: 0.4074 - regression_loss: 0.3581 - classification_loss: 0.0493
 6280/10000 [=================>............] - ETA: 55:08 - loss: 0.4074 - regression_loss: 0.3581 - classification_loss: 0.0492
 6281/10000 [=================>............] - ETA: 55:07 - loss: 0.4074 - regression_loss: 0.3581 - classification_loss: 0.0493
 6282/10000 [=================>............] - ETA: 55:06 - loss: 0.4074 - regression_loss: 0.3581 - classification_loss: 0.0493
 6283/10000 [=================>............] - ETA: 55:05 - loss: 0.4074 - regression_loss: 0.3581 - classification_loss: 0.0492
 6284/10000 [=================>............] - ETA: 55:04 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6285/10000 [=================>............] - ETA: 55:03 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0492
 6286/10000 [=================>............] - ETA: 55:02 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6287/10000 [=================>............] - ETA: 55:02 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6288/10000 [=================>............] - ETA: 55:01 - loss: 0.4072 - regression_loss: 0.3579 - classification_loss: 0.0492
 6289/10000 [=================>............] - ETA: 55:00 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6290/10000 [=================>............] - ETA: 54:59 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6291/10000 [=================>............] - ETA: 54:58 - loss: 0.4072 - regression_loss: 0.3579 - classification_loss: 0.0492
 6292/10000 [=================>............] - ETA: 54:57 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 6293/10000 [=================>............] - ETA: 54:56 - loss: 0.4072 - regression_loss: 0.3579 - classification_loss: 0.0492
 6294/10000 [=================>............] - ETA: 54:55 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6295/10000 [=================>............] - ETA: 54:54 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6296/10000 [=================>............] - ETA: 54:54 - loss: 0.4072 - regression_loss: 0.3579 - classification_loss: 0.0492
 6297/10000 [=================>............] - ETA: 54:53 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6298/10000 [=================>............] - ETA: 54:52 - loss: 0.4072 - regression_loss: 0.3579 - classification_loss: 0.0492
 6299/10000 [=================>............] - ETA: 54:51 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 6300/10000 [=================>............] - ETA: 54:50 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 6301/10000 [=================>............] - ETA: 54:49 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 6302/10000 [=================>............] - ETA: 54:48 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6303/10000 [=================>............] - ETA: 54:47 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6304/10000 [=================>............] - ETA: 54:46 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6305/10000 [=================>............] - ETA: 54:46 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6306/10000 [=================>............] - ETA: 54:45 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6307/10000 [=================>............] - ETA: 54:44 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6308/10000 [=================>............] - ETA: 54:43 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6309/10000 [=================>............] - ETA: 54:42 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0492
 6310/10000 [=================>............] - ETA: 54:41 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0492
 6311/10000 [=================>............] - ETA: 54:40 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6312/10000 [=================>............] - ETA: 54:39 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6313/10000 [=================>............] - ETA: 54:38 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6314/10000 [=================>............] - ETA: 54:37 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6315/10000 [=================>............] - ETA: 54:37 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0492
 6316/10000 [=================>............] - ETA: 54:36 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 6317/10000 [=================>............] - ETA: 54:35 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 6318/10000 [=================>............] - ETA: 54:34 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6319/10000 [=================>............] - ETA: 54:33 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0491
 6320/10000 [=================>............] - ETA: 54:32 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0491
 6321/10000 [=================>............] - ETA: 54:31 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 6322/10000 [=================>............] - ETA: 54:30 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 6323/10000 [=================>............] - ETA: 54:29 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 6324/10000 [=================>............] - ETA: 54:29 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0491
 6325/10000 [=================>............] - ETA: 54:28 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 6326/10000 [=================>............] - ETA: 54:27 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 6327/10000 [=================>............] - ETA: 54:26 - loss: 0.4071 - regression_loss: 0.3580 - classification_loss: 0.0491
 6328/10000 [=================>............] - ETA: 54:25 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0491
 6329/10000 [=================>............] - ETA: 54:24 - loss: 0.4071 - regression_loss: 0.3580 - classification_loss: 0.0491
 6330/10000 [=================>............] - ETA: 54:23 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0491
 6331/10000 [=================>............] - ETA: 54:22 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 6332/10000 [=================>............] - ETA: 54:21 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0492
 6333/10000 [=================>............] - ETA: 54:21 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 6334/10000 [==================>...........] - ETA: 54:20 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 6335/10000 [==================>...........] - ETA: 54:19 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 6336/10000 [==================>...........] - ETA: 54:18 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 6337/10000 [==================>...........] - ETA: 54:17 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 6338/10000 [==================>...........] - ETA: 54:16 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 6339/10000 [==================>...........] - ETA: 54:15 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 6340/10000 [==================>...........] - ETA: 54:14 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 6341/10000 [==================>...........] - ETA: 54:13 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 6342/10000 [==================>...........] - ETA: 54:13 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0491
 6343/10000 [==================>...........] - ETA: 54:12 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6344/10000 [==================>...........] - ETA: 54:11 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6345/10000 [==================>...........] - ETA: 54:10 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 6346/10000 [==================>...........] - ETA: 54:09 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 6347/10000 [==================>...........] - ETA: 54:08 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 6348/10000 [==================>...........] - ETA: 54:07 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0492
 6349/10000 [==================>...........] - ETA: 54:06 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 6350/10000 [==================>...........] - ETA: 54:05 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 6351/10000 [==================>...........] - ETA: 54:05 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 6352/10000 [==================>...........] - ETA: 54:04 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 6353/10000 [==================>...........] - ETA: 54:03 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 6354/10000 [==================>...........] - ETA: 54:02 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 6355/10000 [==================>...........] - ETA: 54:01 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 6356/10000 [==================>...........] - ETA: 54:00 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 6357/10000 [==================>...........] - ETA: 53:59 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 6358/10000 [==================>...........] - ETA: 53:58 - loss: 0.4070 - regression_loss: 0.3577 - classification_loss: 0.0492
 6359/10000 [==================>...........] - ETA: 53:57 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 6360/10000 [==================>...........] - ETA: 53:57 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 6361/10000 [==================>...........] - ETA: 53:56 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 6362/10000 [==================>...........] - ETA: 53:55 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 6363/10000 [==================>...........] - ETA: 53:54 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6364/10000 [==================>...........] - ETA: 53:53 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 6365/10000 [==================>...........] - ETA: 53:52 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6366/10000 [==================>...........] - ETA: 53:51 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6367/10000 [==================>...........] - ETA: 53:50 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6368/10000 [==================>...........] - ETA: 53:49 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6369/10000 [==================>...........] - ETA: 53:49 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6370/10000 [==================>...........] - ETA: 53:48 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6371/10000 [==================>...........] - ETA: 53:47 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6372/10000 [==================>...........] - ETA: 53:46 - loss: 0.4070 - regression_loss: 0.3577 - classification_loss: 0.0492
 6373/10000 [==================>...........] - ETA: 53:45 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 6374/10000 [==================>...........] - ETA: 53:44 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 6375/10000 [==================>...........] - ETA: 53:43 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 6376/10000 [==================>...........] - ETA: 53:42 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 6377/10000 [==================>...........] - ETA: 53:41 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 6378/10000 [==================>...........] - ETA: 53:41 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 6379/10000 [==================>...........] - ETA: 53:40 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 6380/10000 [==================>...........] - ETA: 53:39 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 6381/10000 [==================>...........] - ETA: 53:38 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 6382/10000 [==================>...........] - ETA: 53:37 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 6383/10000 [==================>...........] - ETA: 53:36 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 6384/10000 [==================>...........] - ETA: 53:35 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 6385/10000 [==================>...........] - ETA: 53:34 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 6386/10000 [==================>...........] - ETA: 53:33 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 6387/10000 [==================>...........] - ETA: 53:33 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 6388/10000 [==================>...........] - ETA: 53:32 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 6389/10000 [==================>...........] - ETA: 53:31 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 6390/10000 [==================>...........] - ETA: 53:30 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 6391/10000 [==================>...........] - ETA: 53:29 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 6392/10000 [==================>...........] - ETA: 53:28 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 6393/10000 [==================>...........] - ETA: 53:27 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0493
 6394/10000 [==================>...........] - ETA: 53:26 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0493
 6395/10000 [==================>...........] - ETA: 53:25 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0493
 6396/10000 [==================>...........] - ETA: 53:24 - loss: 0.4072 - regression_loss: 0.3579 - classification_loss: 0.0493
 6397/10000 [==================>...........] - ETA: 53:24 - loss: 0.4072 - regression_loss: 0.3579 - classification_loss: 0.0493
 6398/10000 [==================>...........] - ETA: 53:23 - loss: 0.4072 - regression_loss: 0.3579 - classification_loss: 0.0493
 6399/10000 [==================>...........] - ETA: 53:22 - loss: 0.4072 - regression_loss: 0.3579 - classification_loss: 0.0493
 6400/10000 [==================>...........] - ETA: 53:21 - loss: 0.4072 - regression_loss: 0.3579 - classification_loss: 0.0493
 6401/10000 [==================>...........] - ETA: 53:20 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0493
 6402/10000 [==================>...........] - ETA: 53:19 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0493
 6403/10000 [==================>...........] - ETA: 53:18 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0493
 6404/10000 [==================>...........] - ETA: 53:17 - loss: 0.4074 - regression_loss: 0.3581 - classification_loss: 0.0493
 6405/10000 [==================>...........] - ETA: 53:16 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0493
 6406/10000 [==================>...........] - ETA: 53:16 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0493
 6407/10000 [==================>...........] - ETA: 53:15 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0493
 6408/10000 [==================>...........] - ETA: 53:14 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0493
 6409/10000 [==================>...........] - ETA: 53:13 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0492
 6410/10000 [==================>...........] - ETA: 53:12 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0493
 6411/10000 [==================>...........] - ETA: 53:11 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0492
 6412/10000 [==================>...........] - ETA: 53:10 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0492
 6413/10000 [==================>...........] - ETA: 53:09 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0492
 6414/10000 [==================>...........] - ETA: 53:08 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6415/10000 [==================>...........] - ETA: 53:08 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6416/10000 [==================>...........] - ETA: 53:07 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6417/10000 [==================>...........] - ETA: 53:06 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6418/10000 [==================>...........] - ETA: 53:05 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6419/10000 [==================>...........] - ETA: 53:04 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6420/10000 [==================>...........] - ETA: 53:03 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6421/10000 [==================>...........] - ETA: 53:02 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6422/10000 [==================>...........] - ETA: 53:01 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6423/10000 [==================>...........] - ETA: 53:00 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6424/10000 [==================>...........] - ETA: 53:00 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6425/10000 [==================>...........] - ETA: 52:59 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6426/10000 [==================>...........] - ETA: 52:58 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0492
 6427/10000 [==================>...........] - ETA: 52:57 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0492
 6428/10000 [==================>...........] - ETA: 52:56 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0492
 6429/10000 [==================>...........] - ETA: 52:55 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0492
 6430/10000 [==================>...........] - ETA: 52:54 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6431/10000 [==================>...........] - ETA: 52:53 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0492
 6432/10000 [==================>...........] - ETA: 52:52 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6433/10000 [==================>...........] - ETA: 52:52 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6434/10000 [==================>...........] - ETA: 52:51 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6435/10000 [==================>...........] - ETA: 52:50 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6436/10000 [==================>...........] - ETA: 52:49 - loss: 0.4074 - regression_loss: 0.3582 - classification_loss: 0.0492
 6437/10000 [==================>...........] - ETA: 52:48 - loss: 0.4074 - regression_loss: 0.3581 - classification_loss: 0.0492
 6438/10000 [==================>...........] - ETA: 52:47 - loss: 0.4074 - regression_loss: 0.3582 - classification_loss: 0.0492
 6439/10000 [==================>...........] - ETA: 52:46 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6440/10000 [==================>...........] - ETA: 52:45 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6441/10000 [==================>...........] - ETA: 52:44 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6442/10000 [==================>...........] - ETA: 52:44 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6443/10000 [==================>...........] - ETA: 52:43 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6444/10000 [==================>...........] - ETA: 52:42 - loss: 0.4074 - regression_loss: 0.3581 - classification_loss: 0.0492
 6445/10000 [==================>...........] - ETA: 52:41 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6446/10000 [==================>...........] - ETA: 52:40 - loss: 0.4074 - regression_loss: 0.3582 - classification_loss: 0.0492
 6447/10000 [==================>...........] - ETA: 52:39 - loss: 0.4074 - regression_loss: 0.3582 - classification_loss: 0.0492
 6448/10000 [==================>...........] - ETA: 52:38 - loss: 0.4074 - regression_loss: 0.3582 - classification_loss: 0.0492
 6449/10000 [==================>...........] - ETA: 52:37 - loss: 0.4074 - regression_loss: 0.3582 - classification_loss: 0.0492
 6450/10000 [==================>...........] - ETA: 52:36 - loss: 0.4074 - regression_loss: 0.3582 - classification_loss: 0.0492
 6451/10000 [==================>...........] - ETA: 52:36 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6452/10000 [==================>...........] - ETA: 52:35 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6453/10000 [==================>...........] - ETA: 52:34 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6454/10000 [==================>...........] - ETA: 52:33 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6455/10000 [==================>...........] - ETA: 52:32 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6456/10000 [==================>...........] - ETA: 52:31 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6457/10000 [==================>...........] - ETA: 52:30 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6458/10000 [==================>...........] - ETA: 52:29 - loss: 0.4074 - regression_loss: 0.3582 - classification_loss: 0.0492
 6459/10000 [==================>...........] - ETA: 52:28 - loss: 0.4074 - regression_loss: 0.3582 - classification_loss: 0.0492
 6460/10000 [==================>...........] - ETA: 52:28 - loss: 0.4074 - regression_loss: 0.3582 - classification_loss: 0.0492
 6461/10000 [==================>...........] - ETA: 52:27 - loss: 0.4074 - regression_loss: 0.3582 - classification_loss: 0.0492
 6462/10000 [==================>...........] - ETA: 52:26 - loss: 0.4073 - regression_loss: 0.3582 - classification_loss: 0.0492
 6463/10000 [==================>...........] - ETA: 52:25 - loss: 0.4073 - regression_loss: 0.3582 - classification_loss: 0.0492
 6464/10000 [==================>...........] - ETA: 52:24 - loss: 0.4073 - regression_loss: 0.3582 - classification_loss: 0.0492
 6465/10000 [==================>...........] - ETA: 52:23 - loss: 0.4073 - regression_loss: 0.3582 - classification_loss: 0.0492
 6466/10000 [==================>...........] - ETA: 52:22 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6467/10000 [==================>...........] - ETA: 52:21 - loss: 0.4074 - regression_loss: 0.3582 - classification_loss: 0.0492
 6468/10000 [==================>...........] - ETA: 52:20 - loss: 0.4074 - regression_loss: 0.3582 - classification_loss: 0.0492
 6469/10000 [==================>...........] - ETA: 52:20 - loss: 0.4074 - regression_loss: 0.3582 - classification_loss: 0.0492
 6470/10000 [==================>...........] - ETA: 52:19 - loss: 0.4074 - regression_loss: 0.3582 - classification_loss: 0.0492
 6471/10000 [==================>...........] - ETA: 52:18 - loss: 0.4074 - regression_loss: 0.3581 - classification_loss: 0.0492
 6472/10000 [==================>...........] - ETA: 52:17 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6473/10000 [==================>...........] - ETA: 52:16 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6474/10000 [==================>...........] - ETA: 52:15 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6475/10000 [==================>...........] - ETA: 52:14 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6476/10000 [==================>...........] - ETA: 52:13 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6477/10000 [==================>...........] - ETA: 52:12 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 6478/10000 [==================>...........] - ETA: 52:11 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0492
 6479/10000 [==================>...........] - ETA: 52:11 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6480/10000 [==================>...........] - ETA: 52:10 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6481/10000 [==================>...........] - ETA: 52:09 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6482/10000 [==================>...........] - ETA: 52:08 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6483/10000 [==================>...........] - ETA: 52:07 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6484/10000 [==================>...........] - ETA: 52:06 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6485/10000 [==================>...........] - ETA: 52:05 - loss: 0.4071 - regression_loss: 0.3580 - classification_loss: 0.0492
 6486/10000 [==================>...........] - ETA: 52:04 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 6487/10000 [==================>...........] - ETA: 52:03 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 6488/10000 [==================>...........] - ETA: 52:03 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6489/10000 [==================>...........] - ETA: 52:02 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6490/10000 [==================>...........] - ETA: 52:01 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0492
 6491/10000 [==================>...........] - ETA: 52:00 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6492/10000 [==================>...........] - ETA: 51:59 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6493/10000 [==================>...........] - ETA: 51:58 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6494/10000 [==================>...........] - ETA: 51:57 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6495/10000 [==================>...........] - ETA: 51:56 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0491
 6496/10000 [==================>...........] - ETA: 51:55 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 6497/10000 [==================>...........] - ETA: 51:55 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0492
 6498/10000 [==================>...........] - ETA: 51:54 - loss: 0.4071 - regression_loss: 0.3580 - classification_loss: 0.0492
 6499/10000 [==================>...........] - ETA: 51:53 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 6500/10000 [==================>...........] - ETA: 51:52 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 6501/10000 [==================>...........] - ETA: 51:51 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 6502/10000 [==================>...........] - ETA: 51:50 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0492
 6503/10000 [==================>...........] - ETA: 51:49 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 6504/10000 [==================>...........] - ETA: 51:48 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 6505/10000 [==================>...........] - ETA: 51:47 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 6506/10000 [==================>...........] - ETA: 51:47 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6507/10000 [==================>...........] - ETA: 51:46 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6508/10000 [==================>...........] - ETA: 51:45 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6509/10000 [==================>...........] - ETA: 51:44 - loss: 0.4071 - regression_loss: 0.3580 - classification_loss: 0.0492
 6510/10000 [==================>...........] - ETA: 51:43 - loss: 0.4072 - regression_loss: 0.3581 - classification_loss: 0.0492
 6511/10000 [==================>...........] - ETA: 51:42 - loss: 0.4072 - regression_loss: 0.3581 - classification_loss: 0.0492
 6512/10000 [==================>...........] - ETA: 51:41 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 6513/10000 [==================>...........] - ETA: 51:40 - loss: 0.4072 - regression_loss: 0.3581 - classification_loss: 0.0492
 6514/10000 [==================>...........] - ETA: 51:39 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0491
 6515/10000 [==================>...........] - ETA: 51:39 - loss: 0.4071 - regression_loss: 0.3580 - classification_loss: 0.0491
 6516/10000 [==================>...........] - ETA: 51:38 - loss: 0.4071 - regression_loss: 0.3580 - classification_loss: 0.0491
 6517/10000 [==================>...........] - ETA: 51:37 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 6518/10000 [==================>...........] - ETA: 51:36 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 6519/10000 [==================>...........] - ETA: 51:35 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 6520/10000 [==================>...........] - ETA: 51:34 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 6521/10000 [==================>...........] - ETA: 51:33 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 6522/10000 [==================>...........] - ETA: 51:32 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 6523/10000 [==================>...........] - ETA: 51:31 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 6524/10000 [==================>...........] - ETA: 51:31 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 6525/10000 [==================>...........] - ETA: 51:30 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 6526/10000 [==================>...........] - ETA: 51:29 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 6527/10000 [==================>...........] - ETA: 51:28 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 6528/10000 [==================>...........] - ETA: 51:27 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 6529/10000 [==================>...........] - ETA: 51:26 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 6530/10000 [==================>...........] - ETA: 51:25 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 6531/10000 [==================>...........] - ETA: 51:24 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 6532/10000 [==================>...........] - ETA: 51:23 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 6533/10000 [==================>...........] - ETA: 51:23 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 6534/10000 [==================>...........] - ETA: 51:22 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0491
 6535/10000 [==================>...........] - ETA: 51:21 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0491
 6536/10000 [==================>...........] - ETA: 51:20 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0490
 6537/10000 [==================>...........] - ETA: 51:19 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 6538/10000 [==================>...........] - ETA: 51:18 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0491
 6539/10000 [==================>...........] - ETA: 51:17 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 6540/10000 [==================>...........] - ETA: 51:16 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 6541/10000 [==================>...........] - ETA: 51:15 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 6542/10000 [==================>...........] - ETA: 51:15 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 6543/10000 [==================>...........] - ETA: 51:14 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 6544/10000 [==================>...........] - ETA: 51:13 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 6545/10000 [==================>...........] - ETA: 51:12 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 6546/10000 [==================>...........] - ETA: 51:11 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 6547/10000 [==================>...........] - ETA: 51:10 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 6548/10000 [==================>...........] - ETA: 51:09 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0491
 6549/10000 [==================>...........] - ETA: 51:08 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0491
 6550/10000 [==================>...........] - ETA: 51:07 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0491
 6551/10000 [==================>...........] - ETA: 51:07 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0491
 6552/10000 [==================>...........] - ETA: 51:06 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0491
 6553/10000 [==================>...........] - ETA: 51:05 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0491
 6554/10000 [==================>...........] - ETA: 51:04 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0491
 6555/10000 [==================>...........] - ETA: 51:03 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0491
 6556/10000 [==================>...........] - ETA: 51:02 - loss: 0.4066 - regression_loss: 0.3575 - classification_loss: 0.0490
 6557/10000 [==================>...........] - ETA: 51:01 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0490
 6558/10000 [==================>...........] - ETA: 51:00 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0491
 6559/10000 [==================>...........] - ETA: 50:59 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0490
 6560/10000 [==================>...........] - ETA: 50:59 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0490
 6561/10000 [==================>...........] - ETA: 50:58 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6562/10000 [==================>...........] - ETA: 50:57 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0490
 6563/10000 [==================>...........] - ETA: 50:56 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0490
 6564/10000 [==================>...........] - ETA: 50:55 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0490
 6565/10000 [==================>...........] - ETA: 50:54 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0490
 6566/10000 [==================>...........] - ETA: 50:53 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0490
 6567/10000 [==================>...........] - ETA: 50:52 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0490
 6568/10000 [==================>...........] - ETA: 50:51 - loss: 0.4066 - regression_loss: 0.3575 - classification_loss: 0.0491
 6569/10000 [==================>...........] - ETA: 50:51 - loss: 0.4066 - regression_loss: 0.3575 - classification_loss: 0.0491
 6570/10000 [==================>...........] - ETA: 50:50 - loss: 0.4066 - regression_loss: 0.3575 - classification_loss: 0.0491
 6571/10000 [==================>...........] - ETA: 50:49 - loss: 0.4066 - regression_loss: 0.3575 - classification_loss: 0.0491
 6572/10000 [==================>...........] - ETA: 50:48 - loss: 0.4066 - regression_loss: 0.3575 - classification_loss: 0.0491
 6573/10000 [==================>...........] - ETA: 50:47 - loss: 0.4065 - regression_loss: 0.3575 - classification_loss: 0.0491
 6574/10000 [==================>...........] - ETA: 50:46 - loss: 0.4065 - regression_loss: 0.3575 - classification_loss: 0.0491
 6575/10000 [==================>...........] - ETA: 50:45 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0491
 6576/10000 [==================>...........] - ETA: 50:44 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0491
 6577/10000 [==================>...........] - ETA: 50:43 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0491
 6578/10000 [==================>...........] - ETA: 50:42 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 6579/10000 [==================>...........] - ETA: 50:42 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 6580/10000 [==================>...........] - ETA: 50:41 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 6581/10000 [==================>...........] - ETA: 50:40 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0491
 6582/10000 [==================>...........] - ETA: 50:39 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 6583/10000 [==================>...........] - ETA: 50:38 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 6584/10000 [==================>...........] - ETA: 50:37 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0491
 6585/10000 [==================>...........] - ETA: 50:36 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0491
 6586/10000 [==================>...........] - ETA: 50:35 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0491
 6587/10000 [==================>...........] - ETA: 50:34 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 6588/10000 [==================>...........] - ETA: 50:34 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0491
 6589/10000 [==================>...........] - ETA: 50:33 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 6590/10000 [==================>...........] - ETA: 50:32 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 6591/10000 [==================>...........] - ETA: 50:31 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 6592/10000 [==================>...........] - ETA: 50:30 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 6593/10000 [==================>...........] - ETA: 50:29 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6594/10000 [==================>...........] - ETA: 50:28 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6595/10000 [==================>...........] - ETA: 50:27 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6596/10000 [==================>...........] - ETA: 50:26 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0490
 6597/10000 [==================>...........] - ETA: 50:26 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0490
 6598/10000 [==================>...........] - ETA: 50:25 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6599/10000 [==================>...........] - ETA: 50:24 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6600/10000 [==================>...........] - ETA: 50:23 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6601/10000 [==================>...........] - ETA: 50:22 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6602/10000 [==================>...........] - ETA: 50:21 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0490
 6603/10000 [==================>...........] - ETA: 50:20 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6604/10000 [==================>...........] - ETA: 50:20 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6605/10000 [==================>...........] - ETA: 50:19 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6606/10000 [==================>...........] - ETA: 50:18 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6607/10000 [==================>...........] - ETA: 50:17 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6608/10000 [==================>...........] - ETA: 50:16 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6609/10000 [==================>...........] - ETA: 50:15 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0490
 6610/10000 [==================>...........] - ETA: 50:15 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6611/10000 [==================>...........] - ETA: 50:14 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6612/10000 [==================>...........] - ETA: 50:13 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0490
 6613/10000 [==================>...........] - ETA: 50:12 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0490
 6614/10000 [==================>...........] - ETA: 50:11 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0490
 6615/10000 [==================>...........] - ETA: 50:10 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0490
 6616/10000 [==================>...........] - ETA: 50:09 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0490
 6617/10000 [==================>...........] - ETA: 50:08 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0490
 6618/10000 [==================>...........] - ETA: 50:07 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0490
 6619/10000 [==================>...........] - ETA: 50:07 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0490
 6620/10000 [==================>...........] - ETA: 50:06 - loss: 0.4065 - regression_loss: 0.3575 - classification_loss: 0.0490
 6621/10000 [==================>...........] - ETA: 50:05 - loss: 0.4065 - regression_loss: 0.3575 - classification_loss: 0.0490
 6622/10000 [==================>...........] - ETA: 50:04 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6623/10000 [==================>...........] - ETA: 50:03 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6624/10000 [==================>...........] - ETA: 50:02 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6625/10000 [==================>...........] - ETA: 50:01 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6626/10000 [==================>...........] - ETA: 50:00 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6627/10000 [==================>...........] - ETA: 49:59 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6628/10000 [==================>...........] - ETA: 49:59 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6629/10000 [==================>...........] - ETA: 49:58 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6630/10000 [==================>...........] - ETA: 49:57 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6631/10000 [==================>...........] - ETA: 49:56 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6632/10000 [==================>...........] - ETA: 49:55 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6633/10000 [==================>...........] - ETA: 49:54 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6634/10000 [==================>...........] - ETA: 49:53 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6635/10000 [==================>...........] - ETA: 49:52 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6636/10000 [==================>...........] - ETA: 49:51 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6637/10000 [==================>...........] - ETA: 49:51 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6638/10000 [==================>...........] - ETA: 49:50 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6639/10000 [==================>...........] - ETA: 49:49 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6640/10000 [==================>...........] - ETA: 49:48 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6641/10000 [==================>...........] - ETA: 49:47 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0490
 6642/10000 [==================>...........] - ETA: 49:46 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6643/10000 [==================>...........] - ETA: 49:45 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0490
 6644/10000 [==================>...........] - ETA: 49:44 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6645/10000 [==================>...........] - ETA: 49:43 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6646/10000 [==================>...........] - ETA: 49:42 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6647/10000 [==================>...........] - ETA: 49:42 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6648/10000 [==================>...........] - ETA: 49:41 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6649/10000 [==================>...........] - ETA: 49:40 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6650/10000 [==================>...........] - ETA: 49:39 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6651/10000 [==================>...........] - ETA: 49:38 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6652/10000 [==================>...........] - ETA: 49:37 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0490
 6653/10000 [==================>...........] - ETA: 49:36 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6654/10000 [==================>...........] - ETA: 49:35 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6655/10000 [==================>...........] - ETA: 49:34 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6656/10000 [==================>...........] - ETA: 49:34 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6657/10000 [==================>...........] - ETA: 49:33 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6658/10000 [==================>...........] - ETA: 49:32 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6659/10000 [==================>...........] - ETA: 49:31 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6660/10000 [==================>...........] - ETA: 49:30 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6661/10000 [==================>...........] - ETA: 49:29 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6662/10000 [==================>...........] - ETA: 49:28 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6663/10000 [==================>...........] - ETA: 49:27 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6664/10000 [==================>...........] - ETA: 49:26 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6665/10000 [==================>...........] - ETA: 49:26 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0490
 6666/10000 [==================>...........] - ETA: 49:25 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0490
 6667/10000 [===================>..........] - ETA: 49:24 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0490
 6668/10000 [===================>..........] - ETA: 49:23 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0490
 6669/10000 [===================>..........] - ETA: 49:22 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0490
 6670/10000 [===================>..........] - ETA: 49:21 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0490
 6671/10000 [===================>..........] - ETA: 49:20 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0490
 6672/10000 [===================>..........] - ETA: 49:19 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0490
 6673/10000 [===================>..........] - ETA: 49:18 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0490
 6674/10000 [===================>..........] - ETA: 49:18 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0490
 6675/10000 [===================>..........] - ETA: 49:17 - loss: 0.4065 - regression_loss: 0.3575 - classification_loss: 0.0489
 6676/10000 [===================>..........] - ETA: 49:16 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0489
 6677/10000 [===================>..........] - ETA: 49:15 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0489
 6678/10000 [===================>..........] - ETA: 49:14 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0489
 6679/10000 [===================>..........] - ETA: 49:13 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0489
 6680/10000 [===================>..........] - ETA: 49:12 - loss: 0.4065 - regression_loss: 0.3575 - classification_loss: 0.0489
 6681/10000 [===================>..........] - ETA: 49:11 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0490
 6682/10000 [===================>..........] - ETA: 49:10 - loss: 0.4065 - regression_loss: 0.3575 - classification_loss: 0.0489
 6683/10000 [===================>..........] - ETA: 49:10 - loss: 0.4065 - regression_loss: 0.3575 - classification_loss: 0.0489
 6684/10000 [===================>..........] - ETA: 49:09 - loss: 0.4065 - regression_loss: 0.3575 - classification_loss: 0.0489
 6685/10000 [===================>..........] - ETA: 49:08 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0489
 6686/10000 [===================>..........] - ETA: 49:07 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0490
 6687/10000 [===================>..........] - ETA: 49:06 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6688/10000 [===================>..........] - ETA: 49:05 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0490
 6689/10000 [===================>..........] - ETA: 49:04 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6690/10000 [===================>..........] - ETA: 49:03 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6691/10000 [===================>..........] - ETA: 49:02 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6692/10000 [===================>..........] - ETA: 49:02 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0490
 6693/10000 [===================>..........] - ETA: 49:01 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 6694/10000 [===================>..........] - ETA: 49:00 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 6695/10000 [===================>..........] - ETA: 48:59 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0490
 6696/10000 [===================>..........] - ETA: 48:58 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0490
 6697/10000 [===================>..........] - ETA: 48:57 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 6698/10000 [===================>..........] - ETA: 48:56 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0490
 6699/10000 [===================>..........] - ETA: 48:55 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 6700/10000 [===================>..........] - ETA: 48:54 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0490
 6701/10000 [===================>..........] - ETA: 48:54 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 6702/10000 [===================>..........] - ETA: 48:53 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 6703/10000 [===================>..........] - ETA: 48:52 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 6704/10000 [===================>..........] - ETA: 48:51 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 6705/10000 [===================>..........] - ETA: 48:50 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0490
 6706/10000 [===================>..........] - ETA: 48:49 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0490
 6707/10000 [===================>..........] - ETA: 48:48 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6708/10000 [===================>..........] - ETA: 48:47 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6709/10000 [===================>..........] - ETA: 48:46 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6710/10000 [===================>..........] - ETA: 48:46 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6711/10000 [===================>..........] - ETA: 48:45 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6712/10000 [===================>..........] - ETA: 48:44 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6713/10000 [===================>..........] - ETA: 48:43 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6714/10000 [===================>..........] - ETA: 48:42 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6715/10000 [===================>..........] - ETA: 48:41 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6716/10000 [===================>..........] - ETA: 48:40 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6717/10000 [===================>..........] - ETA: 48:39 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6718/10000 [===================>..........] - ETA: 48:38 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0490
 6719/10000 [===================>..........] - ETA: 48:38 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0490
 6720/10000 [===================>..........] - ETA: 48:37 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0490
 6721/10000 [===================>..........] - ETA: 48:36 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0490
 6722/10000 [===================>..........] - ETA: 48:35 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6723/10000 [===================>..........] - ETA: 48:34 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 6724/10000 [===================>..........] - ETA: 48:33 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 6725/10000 [===================>..........] - ETA: 48:32 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 6726/10000 [===================>..........] - ETA: 48:31 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 6727/10000 [===================>..........] - ETA: 48:30 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0489
 6728/10000 [===================>..........] - ETA: 48:30 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0489
 6729/10000 [===================>..........] - ETA: 48:29 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 6730/10000 [===================>..........] - ETA: 48:28 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 6731/10000 [===================>..........] - ETA: 48:27 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0489
 6732/10000 [===================>..........] - ETA: 48:26 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 6733/10000 [===================>..........] - ETA: 48:25 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 6734/10000 [===================>..........] - ETA: 48:24 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 6735/10000 [===================>..........] - ETA: 48:23 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0489
 6736/10000 [===================>..........] - ETA: 48:22 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 6737/10000 [===================>..........] - ETA: 48:22 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 6738/10000 [===================>..........] - ETA: 48:21 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 6739/10000 [===================>..........] - ETA: 48:20 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 6740/10000 [===================>..........] - ETA: 48:19 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 6741/10000 [===================>..........] - ETA: 48:18 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 6742/10000 [===================>..........] - ETA: 48:17 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 6743/10000 [===================>..........] - ETA: 48:16 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 6744/10000 [===================>..........] - ETA: 48:15 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 6745/10000 [===================>..........] - ETA: 48:14 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0489
 6746/10000 [===================>..........] - ETA: 48:14 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0489
 6747/10000 [===================>..........] - ETA: 48:13 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0489
 6748/10000 [===================>..........] - ETA: 48:12 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0489
 6749/10000 [===================>..........] - ETA: 48:11 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0489
 6750/10000 [===================>..........] - ETA: 48:10 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0489
 6751/10000 [===================>..........] - ETA: 48:09 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 6752/10000 [===================>..........] - ETA: 48:08 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0488
 6753/10000 [===================>..........] - ETA: 48:07 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0488
 6754/10000 [===================>..........] - ETA: 48:06 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0488
 6755/10000 [===================>..........] - ETA: 48:05 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 6756/10000 [===================>..........] - ETA: 48:05 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 6757/10000 [===================>..........] - ETA: 48:04 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 6758/10000 [===================>..........] - ETA: 48:03 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0488
 6759/10000 [===================>..........] - ETA: 48:02 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 6760/10000 [===================>..........] - ETA: 48:01 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 6761/10000 [===================>..........] - ETA: 48:00 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 6762/10000 [===================>..........] - ETA: 47:59 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 6763/10000 [===================>..........] - ETA: 47:58 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0489
 6764/10000 [===================>..........] - ETA: 47:57 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0489
 6765/10000 [===================>..........] - ETA: 47:57 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0489
 6766/10000 [===================>..........] - ETA: 47:56 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0489
 6767/10000 [===================>..........] - ETA: 47:55 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0489
 6768/10000 [===================>..........] - ETA: 47:54 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0489
 6769/10000 [===================>..........] - ETA: 47:53 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0489
 6770/10000 [===================>..........] - ETA: 47:52 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0489
 6771/10000 [===================>..........] - ETA: 47:51 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0489
 6772/10000 [===================>..........] - ETA: 47:50 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0489
 6773/10000 [===================>..........] - ETA: 47:49 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0489
 6774/10000 [===================>..........] - ETA: 47:49 - loss: 0.4065 - regression_loss: 0.3575 - classification_loss: 0.0490
 6775/10000 [===================>..........] - ETA: 47:48 - loss: 0.4065 - regression_loss: 0.3575 - classification_loss: 0.0490
 6776/10000 [===================>..........] - ETA: 47:47 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0490
 6777/10000 [===================>..........] - ETA: 47:46 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0490
 6778/10000 [===================>..........] - ETA: 47:45 - loss: 0.4064 - regression_loss: 0.3574 - classification_loss: 0.0490
 6779/10000 [===================>..........] - ETA: 47:44 - loss: 0.4064 - regression_loss: 0.3574 - classification_loss: 0.0490
 6780/10000 [===================>..........] - ETA: 47:43 - loss: 0.4064 - regression_loss: 0.3574 - classification_loss: 0.0490
 6781/10000 [===================>..........] - ETA: 47:42 - loss: 0.4064 - regression_loss: 0.3574 - classification_loss: 0.0490
 6782/10000 [===================>..........] - ETA: 47:41 - loss: 0.4064 - regression_loss: 0.3574 - classification_loss: 0.0490
 6783/10000 [===================>..........] - ETA: 47:41 - loss: 0.4064 - regression_loss: 0.3574 - classification_loss: 0.0490
 6784/10000 [===================>..........] - ETA: 47:40 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0490
 6785/10000 [===================>..........] - ETA: 47:39 - loss: 0.4063 - regression_loss: 0.3573 - classification_loss: 0.0490
 6786/10000 [===================>..........] - ETA: 47:38 - loss: 0.4063 - regression_loss: 0.3573 - classification_loss: 0.0490
 6787/10000 [===================>..........] - ETA: 47:37 - loss: 0.4063 - regression_loss: 0.3573 - classification_loss: 0.0490
 6788/10000 [===================>..........] - ETA: 47:36 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0490
 6789/10000 [===================>..........] - ETA: 47:35 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0490
 6790/10000 [===================>..........] - ETA: 47:34 - loss: 0.4062 - regression_loss: 0.3572 - classification_loss: 0.0490
 6791/10000 [===================>..........] - ETA: 47:33 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0490
 6792/10000 [===================>..........] - ETA: 47:33 - loss: 0.4063 - regression_loss: 0.3573 - classification_loss: 0.0490
 6793/10000 [===================>..........] - ETA: 47:32 - loss: 0.4063 - regression_loss: 0.3573 - classification_loss: 0.0490
 6794/10000 [===================>..........] - ETA: 47:31 - loss: 0.4063 - regression_loss: 0.3573 - classification_loss: 0.0490
 6795/10000 [===================>..........] - ETA: 47:30 - loss: 0.4063 - regression_loss: 0.3573 - classification_loss: 0.0490
 6796/10000 [===================>..........] - ETA: 47:29 - loss: 0.4063 - regression_loss: 0.3573 - classification_loss: 0.0490
 6797/10000 [===================>..........] - ETA: 47:28 - loss: 0.4063 - regression_loss: 0.3573 - classification_loss: 0.0490
 6798/10000 [===================>..........] - ETA: 47:27 - loss: 0.4063 - regression_loss: 0.3573 - classification_loss: 0.0490
 6799/10000 [===================>..........] - ETA: 47:26 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0490
 6800/10000 [===================>..........] - ETA: 47:25 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0490
 6801/10000 [===================>..........] - ETA: 47:25 - loss: 0.4064 - regression_loss: 0.3574 - classification_loss: 0.0490
 6802/10000 [===================>..........] - ETA: 47:24 - loss: 0.4064 - regression_loss: 0.3574 - classification_loss: 0.0490
 6803/10000 [===================>..........] - ETA: 47:23 - loss: 0.4064 - regression_loss: 0.3574 - classification_loss: 0.0490
 6804/10000 [===================>..........] - ETA: 47:22 - loss: 0.4064 - regression_loss: 0.3574 - classification_loss: 0.0490
 6805/10000 [===================>..........] - ETA: 47:21 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0490
 6806/10000 [===================>..........] - ETA: 47:20 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0489
 6807/10000 [===================>..........] - ETA: 47:19 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0490
 6808/10000 [===================>..........] - ETA: 47:18 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0490
 6809/10000 [===================>..........] - ETA: 47:17 - loss: 0.4063 - regression_loss: 0.3573 - classification_loss: 0.0489
 6810/10000 [===================>..........] - ETA: 47:17 - loss: 0.4063 - regression_loss: 0.3573 - classification_loss: 0.0489
 6811/10000 [===================>..........] - ETA: 47:16 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0489
 6812/10000 [===================>..........] - ETA: 47:15 - loss: 0.4064 - regression_loss: 0.3574 - classification_loss: 0.0489
 6813/10000 [===================>..........] - ETA: 47:14 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0489
 6814/10000 [===================>..........] - ETA: 47:13 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0489
 6815/10000 [===================>..........] - ETA: 47:12 - loss: 0.4064 - regression_loss: 0.3574 - classification_loss: 0.0489
 6816/10000 [===================>..........] - ETA: 47:11 - loss: 0.4064 - regression_loss: 0.3574 - classification_loss: 0.0489
 6817/10000 [===================>..........] - ETA: 47:10 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0489
 6818/10000 [===================>..........] - ETA: 47:09 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0489
 6819/10000 [===================>..........] - ETA: 47:09 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0489
 6820/10000 [===================>..........] - ETA: 47:08 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0489
 6821/10000 [===================>..........] - ETA: 47:07 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0489
 6822/10000 [===================>..........] - ETA: 47:06 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0489
 6823/10000 [===================>..........] - ETA: 47:05 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0489
 6824/10000 [===================>..........] - ETA: 47:04 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0489
 6825/10000 [===================>..........] - ETA: 47:03 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0489
 6826/10000 [===================>..........] - ETA: 47:02 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0489
 6827/10000 [===================>..........] - ETA: 47:01 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0489
 6828/10000 [===================>..........] - ETA: 47:01 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0489
 6829/10000 [===================>..........] - ETA: 47:00 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0489
 6830/10000 [===================>..........] - ETA: 46:59 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0489
 6831/10000 [===================>..........] - ETA: 46:58 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0489
 6832/10000 [===================>..........] - ETA: 46:57 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0488
 6833/10000 [===================>..........] - ETA: 46:56 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0488
 6834/10000 [===================>..........] - ETA: 46:55 - loss: 0.4061 - regression_loss: 0.3573 - classification_loss: 0.0488
 6835/10000 [===================>..........] - ETA: 46:54 - loss: 0.4061 - regression_loss: 0.3573 - classification_loss: 0.0488
 6836/10000 [===================>..........] - ETA: 46:53 - loss: 0.4061 - regression_loss: 0.3573 - classification_loss: 0.0488
 6837/10000 [===================>..........] - ETA: 46:53 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0488
 6838/10000 [===================>..........] - ETA: 46:52 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0488
 6839/10000 [===================>..........] - ETA: 46:51 - loss: 0.4061 - regression_loss: 0.3573 - classification_loss: 0.0488
 6840/10000 [===================>..........] - ETA: 46:50 - loss: 0.4061 - regression_loss: 0.3573 - classification_loss: 0.0488
 6841/10000 [===================>..........] - ETA: 46:49 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0488
 6842/10000 [===================>..........] - ETA: 46:48 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0488
 6843/10000 [===================>..........] - ETA: 46:47 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 6844/10000 [===================>..........] - ETA: 46:46 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0488
 6845/10000 [===================>..........] - ETA: 46:45 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0488
 6846/10000 [===================>..........] - ETA: 46:45 - loss: 0.4061 - regression_loss: 0.3573 - classification_loss: 0.0488
 6847/10000 [===================>..........] - ETA: 46:44 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0488
 6848/10000 [===================>..........] - ETA: 46:43 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 6849/10000 [===================>..........] - ETA: 46:42 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0488
 6850/10000 [===================>..........] - ETA: 46:41 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 6851/10000 [===================>..........] - ETA: 46:40 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 6852/10000 [===================>..........] - ETA: 46:39 - loss: 0.4061 - regression_loss: 0.3573 - classification_loss: 0.0488
 6853/10000 [===================>..........] - ETA: 46:38 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 6854/10000 [===================>..........] - ETA: 46:37 - loss: 0.4061 - regression_loss: 0.3573 - classification_loss: 0.0488
 6855/10000 [===================>..........] - ETA: 46:36 - loss: 0.4061 - regression_loss: 0.3573 - classification_loss: 0.0488
 6856/10000 [===================>..........] - ETA: 46:36 - loss: 0.4060 - regression_loss: 0.3573 - classification_loss: 0.0488
 6857/10000 [===================>..........] - ETA: 46:35 - loss: 0.4060 - regression_loss: 0.3572 - classification_loss: 0.0488
 6858/10000 [===================>..........] - ETA: 46:34 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0489
 6859/10000 [===================>..........] - ETA: 46:33 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0489
 6860/10000 [===================>..........] - ETA: 46:32 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0489
 6861/10000 [===================>..........] - ETA: 46:31 - loss: 0.4063 - regression_loss: 0.3573 - classification_loss: 0.0489
 6862/10000 [===================>..........] - ETA: 46:30 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 6863/10000 [===================>..........] - ETA: 46:29 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0489
 6864/10000 [===================>..........] - ETA: 46:28 - loss: 0.4063 - regression_loss: 0.3573 - classification_loss: 0.0489
 6865/10000 [===================>..........] - ETA: 46:28 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 6866/10000 [===================>..........] - ETA: 46:27 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 6867/10000 [===================>..........] - ETA: 46:26 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 6868/10000 [===================>..........] - ETA: 46:25 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 6869/10000 [===================>..........] - ETA: 46:24 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 6870/10000 [===================>..........] - ETA: 46:23 - loss: 0.4060 - regression_loss: 0.3572 - classification_loss: 0.0489
 6871/10000 [===================>..........] - ETA: 46:22 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 6872/10000 [===================>..........] - ETA: 46:21 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 6873/10000 [===================>..........] - ETA: 46:20 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 6874/10000 [===================>..........] - ETA: 46:20 - loss: 0.4060 - regression_loss: 0.3571 - classification_loss: 0.0489
 6875/10000 [===================>..........] - ETA: 46:19 - loss: 0.4060 - regression_loss: 0.3571 - classification_loss: 0.0489
 6876/10000 [===================>..........] - ETA: 46:18 - loss: 0.4060 - regression_loss: 0.3571 - classification_loss: 0.0489
 6877/10000 [===================>..........] - ETA: 46:17 - loss: 0.4060 - regression_loss: 0.3572 - classification_loss: 0.0489
 6878/10000 [===================>..........] - ETA: 46:16 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 6879/10000 [===================>..........] - ETA: 46:15 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 6880/10000 [===================>..........] - ETA: 46:14 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 6881/10000 [===================>..........] - ETA: 46:13 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 6882/10000 [===================>..........] - ETA: 46:12 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 6883/10000 [===================>..........] - ETA: 46:12 - loss: 0.4061 - regression_loss: 0.3573 - classification_loss: 0.0489
 6884/10000 [===================>..........] - ETA: 46:11 - loss: 0.4061 - regression_loss: 0.3573 - classification_loss: 0.0489
 6885/10000 [===================>..........] - ETA: 46:10 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0488
 6886/10000 [===================>..........] - ETA: 46:09 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0488
 6887/10000 [===================>..........] - ETA: 46:08 - loss: 0.4061 - regression_loss: 0.3573 - classification_loss: 0.0488
 6888/10000 [===================>..........] - ETA: 46:07 - loss: 0.4061 - regression_loss: 0.3573 - classification_loss: 0.0488
 6889/10000 [===================>..........] - ETA: 46:06 - loss: 0.4061 - regression_loss: 0.3573 - classification_loss: 0.0488
 6890/10000 [===================>..........] - ETA: 46:05 - loss: 0.4061 - regression_loss: 0.3573 - classification_loss: 0.0488
 6891/10000 [===================>..........] - ETA: 46:04 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0488
 6892/10000 [===================>..........] - ETA: 46:04 - loss: 0.4061 - regression_loss: 0.3573 - classification_loss: 0.0488
 6893/10000 [===================>..........] - ETA: 46:03 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0488
 6894/10000 [===================>..........] - ETA: 46:02 - loss: 0.4061 - regression_loss: 0.3573 - classification_loss: 0.0488
 6895/10000 [===================>..........] - ETA: 46:01 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0488
 6896/10000 [===================>..........] - ETA: 46:00 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0488
 6897/10000 [===================>..........] - ETA: 45:59 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0488
 6898/10000 [===================>..........] - ETA: 45:58 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 6899/10000 [===================>..........] - ETA: 45:57 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0488
 6900/10000 [===================>..........] - ETA: 45:56 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0488
 6901/10000 [===================>..........] - ETA: 45:56 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0488
 6902/10000 [===================>..........] - ETA: 45:55 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 6903/10000 [===================>..........] - ETA: 45:54 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 6904/10000 [===================>..........] - ETA: 45:53 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 6905/10000 [===================>..........] - ETA: 45:52 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 6906/10000 [===================>..........] - ETA: 45:51 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0488
 6907/10000 [===================>..........] - ETA: 45:50 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 6908/10000 [===================>..........] - ETA: 45:49 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 6909/10000 [===================>..........] - ETA: 45:48 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0488
 6910/10000 [===================>..........] - ETA: 45:48 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 6911/10000 [===================>..........] - ETA: 45:47 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 6912/10000 [===================>..........] - ETA: 45:46 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 6913/10000 [===================>..........] - ETA: 45:45 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 6914/10000 [===================>..........] - ETA: 45:44 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 6915/10000 [===================>..........] - ETA: 45:43 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0488
 6916/10000 [===================>..........] - ETA: 45:42 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0488
 6917/10000 [===================>..........] - ETA: 45:41 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 6918/10000 [===================>..........] - ETA: 45:40 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 6919/10000 [===================>..........] - ETA: 45:40 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 6920/10000 [===================>..........] - ETA: 45:39 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 6921/10000 [===================>..........] - ETA: 45:38 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 6922/10000 [===================>..........] - ETA: 45:37 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 6923/10000 [===================>..........] - ETA: 45:36 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 6924/10000 [===================>..........] - ETA: 45:35 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 6925/10000 [===================>..........] - ETA: 45:34 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 6926/10000 [===================>..........] - ETA: 45:33 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 6927/10000 [===================>..........] - ETA: 45:32 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 6928/10000 [===================>..........] - ETA: 45:32 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 6929/10000 [===================>..........] - ETA: 45:31 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 6930/10000 [===================>..........] - ETA: 45:30 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 6931/10000 [===================>..........] - ETA: 45:29 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 6932/10000 [===================>..........] - ETA: 45:28 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 6933/10000 [===================>..........] - ETA: 45:27 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 6934/10000 [===================>..........] - ETA: 45:26 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 6935/10000 [===================>..........] - ETA: 45:25 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 6936/10000 [===================>..........] - ETA: 45:24 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 6937/10000 [===================>..........] - ETA: 45:24 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 6938/10000 [===================>..........] - ETA: 45:23 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 6939/10000 [===================>..........] - ETA: 45:22 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0488
 6940/10000 [===================>..........] - ETA: 45:21 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 6941/10000 [===================>..........] - ETA: 45:20 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 6942/10000 [===================>..........] - ETA: 45:19 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 6943/10000 [===================>..........] - ETA: 45:18 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0488
 6944/10000 [===================>..........] - ETA: 45:17 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0488
 6945/10000 [===================>..........] - ETA: 45:16 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0488
 6946/10000 [===================>..........] - ETA: 45:16 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0488
 6947/10000 [===================>..........] - ETA: 45:15 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 6948/10000 [===================>..........] - ETA: 45:14 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 6949/10000 [===================>..........] - ETA: 45:13 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 6950/10000 [===================>..........] - ETA: 45:12 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 6951/10000 [===================>..........] - ETA: 45:11 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 6952/10000 [===================>..........] - ETA: 45:10 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 6953/10000 [===================>..........] - ETA: 45:09 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 6954/10000 [===================>..........] - ETA: 45:08 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 6955/10000 [===================>..........] - ETA: 45:07 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 6956/10000 [===================>..........] - ETA: 45:07 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 6957/10000 [===================>..........] - ETA: 45:06 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 6958/10000 [===================>..........] - ETA: 45:05 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 6959/10000 [===================>..........] - ETA: 45:04 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0488
 6960/10000 [===================>..........] - ETA: 45:03 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 6961/10000 [===================>..........] - ETA: 45:02 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 6962/10000 [===================>..........] - ETA: 45:01 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0488
 6963/10000 [===================>..........] - ETA: 45:00 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 6964/10000 [===================>..........] - ETA: 44:59 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 6965/10000 [===================>..........] - ETA: 44:59 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 6966/10000 [===================>..........] - ETA: 44:58 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0488
 6967/10000 [===================>..........] - ETA: 44:57 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 6968/10000 [===================>..........] - ETA: 44:56 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 6969/10000 [===================>..........] - ETA: 44:55 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 6970/10000 [===================>..........] - ETA: 44:54 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0488
 6971/10000 [===================>..........] - ETA: 44:53 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0488
 6972/10000 [===================>..........] - ETA: 44:52 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0490
 6973/10000 [===================>..........] - ETA: 44:51 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0490
 6974/10000 [===================>..........] - ETA: 44:51 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0490
 6975/10000 [===================>..........] - ETA: 44:50 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0489
 6976/10000 [===================>..........] - ETA: 44:49 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 6977/10000 [===================>..........] - ETA: 44:48 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 6978/10000 [===================>..........] - ETA: 44:47 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 6979/10000 [===================>..........] - ETA: 44:46 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 6980/10000 [===================>..........] - ETA: 44:45 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 6981/10000 [===================>..........] - ETA: 44:44 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 6982/10000 [===================>..........] - ETA: 44:43 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 6983/10000 [===================>..........] - ETA: 44:43 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 6984/10000 [===================>..........] - ETA: 44:42 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6985/10000 [===================>..........] - ETA: 44:41 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 6986/10000 [===================>..........] - ETA: 44:40 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 6987/10000 [===================>..........] - ETA: 44:39 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 6988/10000 [===================>..........] - ETA: 44:38 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0490
 6989/10000 [===================>..........] - ETA: 44:37 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 6990/10000 [===================>..........] - ETA: 44:36 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0490
 6991/10000 [===================>..........] - ETA: 44:35 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6992/10000 [===================>..........] - ETA: 44:35 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6993/10000 [===================>..........] - ETA: 44:34 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6994/10000 [===================>..........] - ETA: 44:33 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6995/10000 [===================>..........] - ETA: 44:32 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6996/10000 [===================>..........] - ETA: 44:31 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6997/10000 [===================>..........] - ETA: 44:30 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0490
 6998/10000 [===================>..........] - ETA: 44:29 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 6999/10000 [===================>..........] - ETA: 44:28 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 7000/10000 [====================>.........] - ETA: 44:27 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 7001/10000 [====================>.........] - ETA: 44:27 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 7002/10000 [====================>.........] - ETA: 44:26 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7003/10000 [====================>.........] - ETA: 44:25 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0489
 7004/10000 [====================>.........] - ETA: 44:24 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0489
 7005/10000 [====================>.........] - ETA: 44:23 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7006/10000 [====================>.........] - ETA: 44:22 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7007/10000 [====================>.........] - ETA: 44:21 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7008/10000 [====================>.........] - ETA: 44:20 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7009/10000 [====================>.........] - ETA: 44:19 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7010/10000 [====================>.........] - ETA: 44:19 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7011/10000 [====================>.........] - ETA: 44:18 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7012/10000 [====================>.........] - ETA: 44:17 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7013/10000 [====================>.........] - ETA: 44:16 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7014/10000 [====================>.........] - ETA: 44:15 - loss: 0.4069 - regression_loss: 0.3580 - classification_loss: 0.0489
 7015/10000 [====================>.........] - ETA: 44:14 - loss: 0.4069 - regression_loss: 0.3580 - classification_loss: 0.0489
 7016/10000 [====================>.........] - ETA: 44:13 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0489
 7017/10000 [====================>.........] - ETA: 44:12 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7018/10000 [====================>.........] - ETA: 44:11 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0489
 7019/10000 [====================>.........] - ETA: 44:11 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0489
 7020/10000 [====================>.........] - ETA: 44:10 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7021/10000 [====================>.........] - ETA: 44:09 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7022/10000 [====================>.........] - ETA: 44:08 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7023/10000 [====================>.........] - ETA: 44:07 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7024/10000 [====================>.........] - ETA: 44:06 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0489
 7025/10000 [====================>.........] - ETA: 44:05 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7026/10000 [====================>.........] - ETA: 44:04 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7027/10000 [====================>.........] - ETA: 44:03 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7028/10000 [====================>.........] - ETA: 44:03 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7029/10000 [====================>.........] - ETA: 44:02 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7030/10000 [====================>.........] - ETA: 44:01 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7031/10000 [====================>.........] - ETA: 44:00 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7032/10000 [====================>.........] - ETA: 43:59 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7033/10000 [====================>.........] - ETA: 43:58 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7034/10000 [====================>.........] - ETA: 43:57 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0489
 7035/10000 [====================>.........] - ETA: 43:56 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0489
 7036/10000 [====================>.........] - ETA: 43:55 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7037/10000 [====================>.........] - ETA: 43:55 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7038/10000 [====================>.........] - ETA: 43:54 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7039/10000 [====================>.........] - ETA: 43:53 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7040/10000 [====================>.........] - ETA: 43:52 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7041/10000 [====================>.........] - ETA: 43:51 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7042/10000 [====================>.........] - ETA: 43:50 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7043/10000 [====================>.........] - ETA: 43:49 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7044/10000 [====================>.........] - ETA: 43:48 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7045/10000 [====================>.........] - ETA: 43:47 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7046/10000 [====================>.........] - ETA: 43:47 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7047/10000 [====================>.........] - ETA: 43:46 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7048/10000 [====================>.........] - ETA: 43:45 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7049/10000 [====================>.........] - ETA: 43:44 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7050/10000 [====================>.........] - ETA: 43:43 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7051/10000 [====================>.........] - ETA: 43:42 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7052/10000 [====================>.........] - ETA: 43:41 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0489
 7053/10000 [====================>.........] - ETA: 43:40 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7054/10000 [====================>.........] - ETA: 43:39 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0489
 7055/10000 [====================>.........] - ETA: 43:39 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7056/10000 [====================>.........] - ETA: 43:38 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0489
 7057/10000 [====================>.........] - ETA: 43:37 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7058/10000 [====================>.........] - ETA: 43:36 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7059/10000 [====================>.........] - ETA: 43:35 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7060/10000 [====================>.........] - ETA: 43:34 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7061/10000 [====================>.........] - ETA: 43:33 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7062/10000 [====================>.........] - ETA: 43:32 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7063/10000 [====================>.........] - ETA: 43:31 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7064/10000 [====================>.........] - ETA: 43:30 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0489
 7065/10000 [====================>.........] - ETA: 43:30 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0489
 7066/10000 [====================>.........] - ETA: 43:29 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0489
 7067/10000 [====================>.........] - ETA: 43:28 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7068/10000 [====================>.........] - ETA: 43:27 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0489
 7069/10000 [====================>.........] - ETA: 43:26 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0489
 7070/10000 [====================>.........] - ETA: 43:25 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0489
 7071/10000 [====================>.........] - ETA: 43:24 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0488
 7072/10000 [====================>.........] - ETA: 43:23 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0488
 7073/10000 [====================>.........] - ETA: 43:22 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0488
 7074/10000 [====================>.........] - ETA: 43:22 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0488
 7075/10000 [====================>.........] - ETA: 43:21 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0488
 7076/10000 [====================>.........] - ETA: 43:20 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0488
 7077/10000 [====================>.........] - ETA: 43:19 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7078/10000 [====================>.........] - ETA: 43:18 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0488
 7079/10000 [====================>.........] - ETA: 43:17 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7080/10000 [====================>.........] - ETA: 43:16 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7081/10000 [====================>.........] - ETA: 43:15 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 7082/10000 [====================>.........] - ETA: 43:14 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 7083/10000 [====================>.........] - ETA: 43:14 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 7084/10000 [====================>.........] - ETA: 43:13 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 7085/10000 [====================>.........] - ETA: 43:12 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 7086/10000 [====================>.........] - ETA: 43:11 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 7087/10000 [====================>.........] - ETA: 43:10 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 7088/10000 [====================>.........] - ETA: 43:09 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 7089/10000 [====================>.........] - ETA: 43:08 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 7090/10000 [====================>.........] - ETA: 43:07 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 7091/10000 [====================>.........] - ETA: 43:06 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 7092/10000 [====================>.........] - ETA: 43:06 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 7093/10000 [====================>.........] - ETA: 43:05 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 7094/10000 [====================>.........] - ETA: 43:04 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 7095/10000 [====================>.........] - ETA: 43:03 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0489
 7096/10000 [====================>.........] - ETA: 43:02 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0489
 7097/10000 [====================>.........] - ETA: 43:01 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0489
 7098/10000 [====================>.........] - ETA: 43:00 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0489
 7099/10000 [====================>.........] - ETA: 42:59 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0489
 7100/10000 [====================>.........] - ETA: 42:58 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0489
 7101/10000 [====================>.........] - ETA: 42:58 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0489
 7102/10000 [====================>.........] - ETA: 42:57 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0488
 7103/10000 [====================>.........] - ETA: 42:56 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0488
 7104/10000 [====================>.........] - ETA: 42:55 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0488
 7105/10000 [====================>.........] - ETA: 42:54 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7106/10000 [====================>.........] - ETA: 42:53 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0489
 7107/10000 [====================>.........] - ETA: 42:52 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7108/10000 [====================>.........] - ETA: 42:51 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7109/10000 [====================>.........] - ETA: 42:50 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7110/10000 [====================>.........] - ETA: 42:50 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7111/10000 [====================>.........] - ETA: 42:49 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0489
 7112/10000 [====================>.........] - ETA: 42:48 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0489
 7113/10000 [====================>.........] - ETA: 42:47 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7114/10000 [====================>.........] - ETA: 42:46 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0489
 7115/10000 [====================>.........] - ETA: 42:45 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7116/10000 [====================>.........] - ETA: 42:44 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7117/10000 [====================>.........] - ETA: 42:43 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7118/10000 [====================>.........] - ETA: 42:42 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7119/10000 [====================>.........] - ETA: 42:42 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7120/10000 [====================>.........] - ETA: 42:41 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7121/10000 [====================>.........] - ETA: 42:40 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7122/10000 [====================>.........] - ETA: 42:39 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0489
 7123/10000 [====================>.........] - ETA: 42:38 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7124/10000 [====================>.........] - ETA: 42:37 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7125/10000 [====================>.........] - ETA: 42:36 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7126/10000 [====================>.........] - ETA: 42:35 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0489
 7127/10000 [====================>.........] - ETA: 42:34 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0489
 7128/10000 [====================>.........] - ETA: 42:34 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7129/10000 [====================>.........] - ETA: 42:33 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0489
 7130/10000 [====================>.........] - ETA: 42:32 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7131/10000 [====================>.........] - ETA: 42:31 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7132/10000 [====================>.........] - ETA: 42:30 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7133/10000 [====================>.........] - ETA: 42:29 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7134/10000 [====================>.........] - ETA: 42:28 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7135/10000 [====================>.........] - ETA: 42:27 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7136/10000 [====================>.........] - ETA: 42:26 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7137/10000 [====================>.........] - ETA: 42:26 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0489
 7138/10000 [====================>.........] - ETA: 42:25 - loss: 0.4069 - regression_loss: 0.3580 - classification_loss: 0.0489
 7139/10000 [====================>.........] - ETA: 42:24 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0489
 7140/10000 [====================>.........] - ETA: 42:23 - loss: 0.4069 - regression_loss: 0.3580 - classification_loss: 0.0489
 7141/10000 [====================>.........] - ETA: 42:22 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0489
 7142/10000 [====================>.........] - ETA: 42:21 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0489
 7143/10000 [====================>.........] - ETA: 42:20 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7144/10000 [====================>.........] - ETA: 42:19 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7145/10000 [====================>.........] - ETA: 42:18 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7146/10000 [====================>.........] - ETA: 42:18 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7147/10000 [====================>.........] - ETA: 42:17 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7148/10000 [====================>.........] - ETA: 42:16 - loss: 0.4069 - regression_loss: 0.3580 - classification_loss: 0.0489
 7149/10000 [====================>.........] - ETA: 42:15 - loss: 0.4069 - regression_loss: 0.3580 - classification_loss: 0.0489
 7150/10000 [====================>.........] - ETA: 42:14 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7151/10000 [====================>.........] - ETA: 42:13 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7152/10000 [====================>.........] - ETA: 42:12 - loss: 0.4069 - regression_loss: 0.3580 - classification_loss: 0.0489
 7153/10000 [====================>.........] - ETA: 42:11 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7154/10000 [====================>.........] - ETA: 42:10 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0489
 7155/10000 [====================>.........] - ETA: 42:10 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0489
 7156/10000 [====================>.........] - ETA: 42:09 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7157/10000 [====================>.........] - ETA: 42:08 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7158/10000 [====================>.........] - ETA: 42:07 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7159/10000 [====================>.........] - ETA: 42:06 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7160/10000 [====================>.........] - ETA: 42:05 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7161/10000 [====================>.........] - ETA: 42:04 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7162/10000 [====================>.........] - ETA: 42:03 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7163/10000 [====================>.........] - ETA: 42:02 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7164/10000 [====================>.........] - ETA: 42:02 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7165/10000 [====================>.........] - ETA: 42:01 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0489
 7166/10000 [====================>.........] - ETA: 42:00 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7167/10000 [====================>.........] - ETA: 41:59 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7168/10000 [====================>.........] - ETA: 41:58 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0489
 7169/10000 [====================>.........] - ETA: 41:57 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7170/10000 [====================>.........] - ETA: 41:56 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7171/10000 [====================>.........] - ETA: 41:55 - loss: 0.4069 - regression_loss: 0.3580 - classification_loss: 0.0489
 7172/10000 [====================>.........] - ETA: 41:54 - loss: 0.4069 - regression_loss: 0.3580 - classification_loss: 0.0489
 7173/10000 [====================>.........] - ETA: 41:54 - loss: 0.4069 - regression_loss: 0.3580 - classification_loss: 0.0489
 7174/10000 [====================>.........] - ETA: 41:53 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0489
 7175/10000 [====================>.........] - ETA: 41:52 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7176/10000 [====================>.........] - ETA: 41:51 - loss: 0.4069 - regression_loss: 0.3580 - classification_loss: 0.0489
 7177/10000 [====================>.........] - ETA: 41:50 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0489
 7178/10000 [====================>.........] - ETA: 41:49 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0489
 7179/10000 [====================>.........] - ETA: 41:48 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0489
 7180/10000 [====================>.........] - ETA: 41:47 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7181/10000 [====================>.........] - ETA: 41:46 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7182/10000 [====================>.........] - ETA: 41:45 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7183/10000 [====================>.........] - ETA: 41:45 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7184/10000 [====================>.........] - ETA: 41:44 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7185/10000 [====================>.........] - ETA: 41:43 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7186/10000 [====================>.........] - ETA: 41:42 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7187/10000 [====================>.........] - ETA: 41:41 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0489
 7188/10000 [====================>.........] - ETA: 41:40 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7189/10000 [====================>.........] - ETA: 41:39 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7190/10000 [====================>.........] - ETA: 41:38 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0489
 7191/10000 [====================>.........] - ETA: 41:37 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0489
 7192/10000 [====================>.........] - ETA: 41:37 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7193/10000 [====================>.........] - ETA: 41:36 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7194/10000 [====================>.........] - ETA: 41:35 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7195/10000 [====================>.........] - ETA: 41:34 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7196/10000 [====================>.........] - ETA: 41:33 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7197/10000 [====================>.........] - ETA: 41:32 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7198/10000 [====================>.........] - ETA: 41:31 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7199/10000 [====================>.........] - ETA: 41:30 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 7200/10000 [====================>.........] - ETA: 41:29 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0489
 7201/10000 [====================>.........] - ETA: 41:29 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7202/10000 [====================>.........] - ETA: 41:28 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7203/10000 [====================>.........] - ETA: 41:27 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7204/10000 [====================>.........] - ETA: 41:26 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7205/10000 [====================>.........] - ETA: 41:25 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7206/10000 [====================>.........] - ETA: 41:24 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7207/10000 [====================>.........] - ETA: 41:23 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7208/10000 [====================>.........] - ETA: 41:22 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7209/10000 [====================>.........] - ETA: 41:21 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7210/10000 [====================>.........] - ETA: 41:21 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7211/10000 [====================>.........] - ETA: 41:20 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7212/10000 [====================>.........] - ETA: 41:19 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7213/10000 [====================>.........] - ETA: 41:18 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7214/10000 [====================>.........] - ETA: 41:17 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7215/10000 [====================>.........] - ETA: 41:16 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7216/10000 [====================>.........] - ETA: 41:15 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0489
 7217/10000 [====================>.........] - ETA: 41:14 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7218/10000 [====================>.........] - ETA: 41:13 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7219/10000 [====================>.........] - ETA: 41:13 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7220/10000 [====================>.........] - ETA: 41:12 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7221/10000 [====================>.........] - ETA: 41:11 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7222/10000 [====================>.........] - ETA: 41:10 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0489
 7223/10000 [====================>.........] - ETA: 41:09 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7224/10000 [====================>.........] - ETA: 41:08 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0489
 7225/10000 [====================>.........] - ETA: 41:07 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0489
 7226/10000 [====================>.........] - ETA: 41:06 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7227/10000 [====================>.........] - ETA: 41:05 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7228/10000 [====================>.........] - ETA: 41:05 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7229/10000 [====================>.........] - ETA: 41:04 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7230/10000 [====================>.........] - ETA: 41:03 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7231/10000 [====================>.........] - ETA: 41:02 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0489
 7232/10000 [====================>.........] - ETA: 41:01 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7233/10000 [====================>.........] - ETA: 41:00 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0489
 7234/10000 [====================>.........] - ETA: 40:59 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7235/10000 [====================>.........] - ETA: 40:58 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7236/10000 [====================>.........] - ETA: 40:57 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7237/10000 [====================>.........] - ETA: 40:57 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7238/10000 [====================>.........] - ETA: 40:56 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7239/10000 [====================>.........] - ETA: 40:55 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7240/10000 [====================>.........] - ETA: 40:54 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7241/10000 [====================>.........] - ETA: 40:53 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0489
 7242/10000 [====================>.........] - ETA: 40:52 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7243/10000 [====================>.........] - ETA: 40:51 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7244/10000 [====================>.........] - ETA: 40:50 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0489
 7245/10000 [====================>.........] - ETA: 40:49 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7246/10000 [====================>.........] - ETA: 40:49 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0489
 7247/10000 [====================>.........] - ETA: 40:48 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0489
 7248/10000 [====================>.........] - ETA: 40:47 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0489
 7249/10000 [====================>.........] - ETA: 40:46 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7250/10000 [====================>.........] - ETA: 40:45 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7251/10000 [====================>.........] - ETA: 40:44 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 7252/10000 [====================>.........] - ETA: 40:43 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0488
 7253/10000 [====================>.........] - ETA: 40:42 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0488
 7254/10000 [====================>.........] - ETA: 40:41 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7255/10000 [====================>.........] - ETA: 40:41 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7256/10000 [====================>.........] - ETA: 40:40 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7257/10000 [====================>.........] - ETA: 40:39 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 7258/10000 [====================>.........] - ETA: 40:38 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0489
 7259/10000 [====================>.........] - ETA: 40:37 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7260/10000 [====================>.........] - ETA: 40:36 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7261/10000 [====================>.........] - ETA: 40:35 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0489
 7262/10000 [====================>.........] - ETA: 40:34 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7263/10000 [====================>.........] - ETA: 40:33 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0489
 7264/10000 [====================>.........] - ETA: 40:33 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7265/10000 [====================>.........] - ETA: 40:32 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7266/10000 [====================>.........] - ETA: 40:31 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0489
 7267/10000 [====================>.........] - ETA: 40:30 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0489
 7268/10000 [====================>.........] - ETA: 40:29 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7269/10000 [====================>.........] - ETA: 40:28 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7270/10000 [====================>.........] - ETA: 40:27 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7271/10000 [====================>.........] - ETA: 40:26 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0489
 7272/10000 [====================>.........] - ETA: 40:25 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0489
 7273/10000 [====================>.........] - ETA: 40:25 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0489
 7274/10000 [====================>.........] - ETA: 40:24 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0490
 7275/10000 [====================>.........] - ETA: 40:23 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0490
 7276/10000 [====================>.........] - ETA: 40:22 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0490
 7277/10000 [====================>.........] - ETA: 40:21 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0489
 7278/10000 [====================>.........] - ETA: 40:20 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0490
 7279/10000 [====================>.........] - ETA: 40:19 - loss: 0.4066 - regression_loss: 0.3576 - classification_loss: 0.0489
 7280/10000 [====================>.........] - ETA: 40:18 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0489
 7281/10000 [====================>.........] - ETA: 40:17 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0489
 7282/10000 [====================>.........] - ETA: 40:17 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0489
 7283/10000 [====================>.........] - ETA: 40:16 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0489
 7284/10000 [====================>.........] - ETA: 40:15 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0489
 7285/10000 [====================>.........] - ETA: 40:14 - loss: 0.4065 - regression_loss: 0.3575 - classification_loss: 0.0489
 7286/10000 [====================>.........] - ETA: 40:13 - loss: 0.4065 - regression_loss: 0.3575 - classification_loss: 0.0489
 7287/10000 [====================>.........] - ETA: 40:12 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0489
 7288/10000 [====================>.........] - ETA: 40:11 - loss: 0.4065 - regression_loss: 0.3575 - classification_loss: 0.0489
 7289/10000 [====================>.........] - ETA: 40:10 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0489
 7290/10000 [====================>.........] - ETA: 40:09 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0489
 7291/10000 [====================>.........] - ETA: 40:08 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0489
 7292/10000 [====================>.........] - ETA: 40:08 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0489
 7293/10000 [====================>.........] - ETA: 40:07 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0489
 7294/10000 [====================>.........] - ETA: 40:06 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0489
 7295/10000 [====================>.........] - ETA: 40:05 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0489
 7296/10000 [====================>.........] - ETA: 40:04 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0489
 7297/10000 [====================>.........] - ETA: 40:03 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0489
 7298/10000 [====================>.........] - ETA: 40:02 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0489
 7299/10000 [====================>.........] - ETA: 40:01 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0489
 7300/10000 [====================>.........] - ETA: 40:00 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0489
 7301/10000 [====================>.........] - ETA: 40:00 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0489
 7302/10000 [====================>.........] - ETA: 39:59 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0489
 7303/10000 [====================>.........] - ETA: 39:58 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0489
 7304/10000 [====================>.........] - ETA: 39:57 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0489
 7305/10000 [====================>.........] - ETA: 39:56 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0489
 7306/10000 [====================>.........] - ETA: 39:55 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0489
 7307/10000 [====================>.........] - ETA: 39:54 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0489
 7308/10000 [====================>.........] - ETA: 39:53 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7309/10000 [====================>.........] - ETA: 39:52 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7310/10000 [====================>.........] - ETA: 39:52 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7311/10000 [====================>.........] - ETA: 39:51 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7312/10000 [====================>.........] - ETA: 39:50 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7313/10000 [====================>.........] - ETA: 39:49 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7314/10000 [====================>.........] - ETA: 39:48 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 7315/10000 [====================>.........] - ETA: 39:47 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7316/10000 [====================>.........] - ETA: 39:46 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7317/10000 [====================>.........] - ETA: 39:45 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7318/10000 [====================>.........] - ETA: 39:44 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7319/10000 [====================>.........] - ETA: 39:44 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0489
 7320/10000 [====================>.........] - ETA: 39:43 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0489
 7321/10000 [====================>.........] - ETA: 39:42 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0489
 7322/10000 [====================>.........] - ETA: 39:41 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0489
 7323/10000 [====================>.........] - ETA: 39:40 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7324/10000 [====================>.........] - ETA: 39:39 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 7325/10000 [====================>.........] - ETA: 39:38 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0489
 7326/10000 [====================>.........] - ETA: 39:37 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0489
 7327/10000 [====================>.........] - ETA: 39:36 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0489
 7328/10000 [====================>.........] - ETA: 39:36 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0489
 7329/10000 [====================>.........] - ETA: 39:35 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0489
 7330/10000 [====================>.........] - ETA: 39:34 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0489
 7331/10000 [====================>.........] - ETA: 39:33 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0489
 7332/10000 [====================>.........] - ETA: 39:32 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7333/10000 [====================>.........] - ETA: 39:31 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0488
 7334/10000 [=====================>........] - ETA: 39:30 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7335/10000 [=====================>........] - ETA: 39:29 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7336/10000 [=====================>........] - ETA: 39:28 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7337/10000 [=====================>........] - ETA: 39:28 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7338/10000 [=====================>........] - ETA: 39:27 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0488
 7339/10000 [=====================>........] - ETA: 39:26 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 7340/10000 [=====================>........] - ETA: 39:25 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 7341/10000 [=====================>........] - ETA: 39:24 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0488
 7342/10000 [=====================>........] - ETA: 39:23 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0488
 7343/10000 [=====================>........] - ETA: 39:22 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7344/10000 [=====================>........] - ETA: 39:21 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7345/10000 [=====================>........] - ETA: 39:20 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7346/10000 [=====================>........] - ETA: 39:20 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7347/10000 [=====================>........] - ETA: 39:19 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7348/10000 [=====================>........] - ETA: 39:18 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7349/10000 [=====================>........] - ETA: 39:17 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0488
 7350/10000 [=====================>........] - ETA: 39:16 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7351/10000 [=====================>........] - ETA: 39:15 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7352/10000 [=====================>........] - ETA: 39:14 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0488
 7353/10000 [=====================>........] - ETA: 39:13 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7354/10000 [=====================>........] - ETA: 39:12 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7355/10000 [=====================>........] - ETA: 39:12 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7356/10000 [=====================>........] - ETA: 39:11 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7357/10000 [=====================>........] - ETA: 39:10 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0488
 7358/10000 [=====================>........] - ETA: 39:09 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7359/10000 [=====================>........] - ETA: 39:08 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7360/10000 [=====================>........] - ETA: 39:07 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7361/10000 [=====================>........] - ETA: 39:06 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7362/10000 [=====================>........] - ETA: 39:05 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 7363/10000 [=====================>........] - ETA: 39:04 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0488
 7364/10000 [=====================>........] - ETA: 39:04 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 7365/10000 [=====================>........] - ETA: 39:03 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 7366/10000 [=====================>........] - ETA: 39:02 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 7367/10000 [=====================>........] - ETA: 39:01 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 7368/10000 [=====================>........] - ETA: 39:00 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0488
 7369/10000 [=====================>........] - ETA: 38:59 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0488
 7370/10000 [=====================>........] - ETA: 38:58 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 7371/10000 [=====================>........] - ETA: 38:57 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 7372/10000 [=====================>........] - ETA: 38:56 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 7373/10000 [=====================>........] - ETA: 38:56 - loss: 0.4067 - regression_loss: 0.3580 - classification_loss: 0.0488
 7374/10000 [=====================>........] - ETA: 38:55 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 7375/10000 [=====================>........] - ETA: 38:54 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 7376/10000 [=====================>........] - ETA: 38:53 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 7377/10000 [=====================>........] - ETA: 38:52 - loss: 0.4067 - regression_loss: 0.3580 - classification_loss: 0.0488
 7378/10000 [=====================>........] - ETA: 38:51 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 7379/10000 [=====================>........] - ETA: 38:50 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 7380/10000 [=====================>........] - ETA: 38:49 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 7381/10000 [=====================>........] - ETA: 38:48 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 7382/10000 [=====================>........] - ETA: 38:48 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 7383/10000 [=====================>........] - ETA: 38:47 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 7384/10000 [=====================>........] - ETA: 38:46 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 7385/10000 [=====================>........] - ETA: 38:45 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 7386/10000 [=====================>........] - ETA: 38:44 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0488
 7387/10000 [=====================>........] - ETA: 38:43 - loss: 0.4069 - regression_loss: 0.3581 - classification_loss: 0.0488
 7388/10000 [=====================>........] - ETA: 38:42 - loss: 0.4070 - regression_loss: 0.3582 - classification_loss: 0.0489
 7389/10000 [=====================>........] - ETA: 38:41 - loss: 0.4070 - regression_loss: 0.3582 - classification_loss: 0.0489
 7390/10000 [=====================>........] - ETA: 38:40 - loss: 0.4070 - regression_loss: 0.3582 - classification_loss: 0.0488
 7391/10000 [=====================>........] - ETA: 38:40 - loss: 0.4070 - regression_loss: 0.3581 - classification_loss: 0.0488
 7392/10000 [=====================>........] - ETA: 38:39 - loss: 0.4070 - regression_loss: 0.3581 - classification_loss: 0.0488
 7393/10000 [=====================>........] - ETA: 38:38 - loss: 0.4069 - regression_loss: 0.3581 - classification_loss: 0.0488
 7394/10000 [=====================>........] - ETA: 38:37 - loss: 0.4070 - regression_loss: 0.3582 - classification_loss: 0.0488
 7395/10000 [=====================>........] - ETA: 38:36 - loss: 0.4070 - regression_loss: 0.3582 - classification_loss: 0.0488
 7396/10000 [=====================>........] - ETA: 38:35 - loss: 0.4070 - regression_loss: 0.3582 - classification_loss: 0.0488
 7397/10000 [=====================>........] - ETA: 38:34 - loss: 0.4070 - regression_loss: 0.3582 - classification_loss: 0.0488
 7398/10000 [=====================>........] - ETA: 38:33 - loss: 0.4070 - regression_loss: 0.3582 - classification_loss: 0.0488
 7399/10000 [=====================>........] - ETA: 38:32 - loss: 0.4071 - regression_loss: 0.3582 - classification_loss: 0.0488
 7400/10000 [=====================>........] - ETA: 38:32 - loss: 0.4071 - regression_loss: 0.3583 - classification_loss: 0.0488
 7401/10000 [=====================>........] - ETA: 38:31 - loss: 0.4071 - regression_loss: 0.3583 - classification_loss: 0.0488
 7402/10000 [=====================>........] - ETA: 38:30 - loss: 0.4071 - regression_loss: 0.3583 - classification_loss: 0.0488
 7403/10000 [=====================>........] - ETA: 38:29 - loss: 0.4071 - regression_loss: 0.3582 - classification_loss: 0.0488
 7404/10000 [=====================>........] - ETA: 38:28 - loss: 0.4071 - regression_loss: 0.3582 - classification_loss: 0.0489
 7405/10000 [=====================>........] - ETA: 38:27 - loss: 0.4071 - regression_loss: 0.3582 - classification_loss: 0.0488
 7406/10000 [=====================>........] - ETA: 38:26 - loss: 0.4071 - regression_loss: 0.3582 - classification_loss: 0.0488
 7407/10000 [=====================>........] - ETA: 38:25 - loss: 0.4070 - regression_loss: 0.3582 - classification_loss: 0.0488
 7408/10000 [=====================>........] - ETA: 38:24 - loss: 0.4070 - regression_loss: 0.3582 - classification_loss: 0.0488
 7409/10000 [=====================>........] - ETA: 38:23 - loss: 0.4070 - regression_loss: 0.3582 - classification_loss: 0.0488
 7410/10000 [=====================>........] - ETA: 38:23 - loss: 0.4069 - regression_loss: 0.3581 - classification_loss: 0.0488
 7411/10000 [=====================>........] - ETA: 38:22 - loss: 0.4069 - regression_loss: 0.3581 - classification_loss: 0.0488
 7412/10000 [=====================>........] - ETA: 38:21 - loss: 0.4069 - regression_loss: 0.3581 - classification_loss: 0.0488
 7413/10000 [=====================>........] - ETA: 38:20 - loss: 0.4069 - regression_loss: 0.3581 - classification_loss: 0.0488
 7414/10000 [=====================>........] - ETA: 38:19 - loss: 0.4069 - regression_loss: 0.3581 - classification_loss: 0.0488
 7415/10000 [=====================>........] - ETA: 38:18 - loss: 0.4070 - regression_loss: 0.3581 - classification_loss: 0.0488
 7416/10000 [=====================>........] - ETA: 38:17 - loss: 0.4069 - regression_loss: 0.3581 - classification_loss: 0.0488
 7417/10000 [=====================>........] - ETA: 38:16 - loss: 0.4069 - regression_loss: 0.3581 - classification_loss: 0.0488
 7418/10000 [=====================>........] - ETA: 38:15 - loss: 0.4070 - regression_loss: 0.3581 - classification_loss: 0.0488
 7419/10000 [=====================>........] - ETA: 38:15 - loss: 0.4069 - regression_loss: 0.3581 - classification_loss: 0.0488
 7420/10000 [=====================>........] - ETA: 38:14 - loss: 0.4070 - regression_loss: 0.3582 - classification_loss: 0.0488
 7421/10000 [=====================>........] - ETA: 38:13 - loss: 0.4069 - regression_loss: 0.3581 - classification_loss: 0.0488
 7422/10000 [=====================>........] - ETA: 38:12 - loss: 0.4069 - regression_loss: 0.3581 - classification_loss: 0.0488
 7423/10000 [=====================>........] - ETA: 38:11 - loss: 0.4070 - regression_loss: 0.3582 - classification_loss: 0.0488
 7424/10000 [=====================>........] - ETA: 38:10 - loss: 0.4069 - regression_loss: 0.3581 - classification_loss: 0.0488
 7425/10000 [=====================>........] - ETA: 38:09 - loss: 0.4069 - regression_loss: 0.3581 - classification_loss: 0.0488
 7426/10000 [=====================>........] - ETA: 38:08 - loss: 0.4069 - regression_loss: 0.3581 - classification_loss: 0.0488
 7427/10000 [=====================>........] - ETA: 38:07 - loss: 0.4069 - regression_loss: 0.3581 - classification_loss: 0.0488
 7428/10000 [=====================>........] - ETA: 38:07 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 7429/10000 [=====================>........] - ETA: 38:06 - loss: 0.4069 - regression_loss: 0.3581 - classification_loss: 0.0488
 7430/10000 [=====================>........] - ETA: 38:05 - loss: 0.4069 - regression_loss: 0.3581 - classification_loss: 0.0488
 7431/10000 [=====================>........] - ETA: 38:04 - loss: 0.4069 - regression_loss: 0.3581 - classification_loss: 0.0488
 7432/10000 [=====================>........] - ETA: 38:03 - loss: 0.4069 - regression_loss: 0.3581 - classification_loss: 0.0488
 7433/10000 [=====================>........] - ETA: 38:02 - loss: 0.4069 - regression_loss: 0.3581 - classification_loss: 0.0488
 7434/10000 [=====================>........] - ETA: 38:01 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 7435/10000 [=====================>........] - ETA: 38:00 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 7436/10000 [=====================>........] - ETA: 37:59 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 7437/10000 [=====================>........] - ETA: 37:59 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 7438/10000 [=====================>........] - ETA: 37:58 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 7439/10000 [=====================>........] - ETA: 37:57 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 7440/10000 [=====================>........] - ETA: 37:56 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 7441/10000 [=====================>........] - ETA: 37:55 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 7442/10000 [=====================>........] - ETA: 37:54 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 7443/10000 [=====================>........] - ETA: 37:53 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 7444/10000 [=====================>........] - ETA: 37:52 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 7445/10000 [=====================>........] - ETA: 37:51 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 7446/10000 [=====================>........] - ETA: 37:51 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 7447/10000 [=====================>........] - ETA: 37:50 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0488
 7448/10000 [=====================>........] - ETA: 37:49 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0488
 7449/10000 [=====================>........] - ETA: 37:48 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 7450/10000 [=====================>........] - ETA: 37:47 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 7451/10000 [=====================>........] - ETA: 37:46 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 7452/10000 [=====================>........] - ETA: 37:45 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0488
 7453/10000 [=====================>........] - ETA: 37:44 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 7454/10000 [=====================>........] - ETA: 37:43 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 7455/10000 [=====================>........] - ETA: 37:43 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 7456/10000 [=====================>........] - ETA: 37:42 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 7457/10000 [=====================>........] - ETA: 37:41 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0488
 7458/10000 [=====================>........] - ETA: 37:40 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0488
 7459/10000 [=====================>........] - ETA: 37:39 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0487
 7460/10000 [=====================>........] - ETA: 37:38 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0487
 7461/10000 [=====================>........] - ETA: 37:37 - loss: 0.4067 - regression_loss: 0.3580 - classification_loss: 0.0487
 7462/10000 [=====================>........] - ETA: 37:36 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0487
 7463/10000 [=====================>........] - ETA: 37:35 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0487
 7464/10000 [=====================>........] - ETA: 37:35 - loss: 0.4067 - regression_loss: 0.3580 - classification_loss: 0.0487
 7465/10000 [=====================>........] - ETA: 37:34 - loss: 0.4067 - regression_loss: 0.3580 - classification_loss: 0.0487
 7466/10000 [=====================>........] - ETA: 37:33 - loss: 0.4067 - regression_loss: 0.3580 - classification_loss: 0.0487
 7467/10000 [=====================>........] - ETA: 37:32 - loss: 0.4068 - regression_loss: 0.3581 - classification_loss: 0.0488
 7468/10000 [=====================>........] - ETA: 37:31 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0487
 7469/10000 [=====================>........] - ETA: 37:30 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0487
 7470/10000 [=====================>........] - ETA: 37:29 - loss: 0.4067 - regression_loss: 0.3580 - classification_loss: 0.0487
 7471/10000 [=====================>........] - ETA: 37:28 - loss: 0.4067 - regression_loss: 0.3580 - classification_loss: 0.0487
 7472/10000 [=====================>........] - ETA: 37:27 - loss: 0.4067 - regression_loss: 0.3580 - classification_loss: 0.0487
 7473/10000 [=====================>........] - ETA: 37:27 - loss: 0.4067 - regression_loss: 0.3580 - classification_loss: 0.0487
 7474/10000 [=====================>........] - ETA: 37:26 - loss: 0.4067 - regression_loss: 0.3580 - classification_loss: 0.0487
 7475/10000 [=====================>........] - ETA: 37:25 - loss: 0.4067 - regression_loss: 0.3580 - classification_loss: 0.0488
 7476/10000 [=====================>........] - ETA: 37:24 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 7477/10000 [=====================>........] - ETA: 37:23 - loss: 0.4067 - regression_loss: 0.3580 - classification_loss: 0.0487
 7478/10000 [=====================>........] - ETA: 37:22 - loss: 0.4067 - regression_loss: 0.3580 - classification_loss: 0.0487
 7479/10000 [=====================>........] - ETA: 37:21 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0487
 7480/10000 [=====================>........] - ETA: 37:20 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0487
 7481/10000 [=====================>........] - ETA: 37:19 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0487
 7482/10000 [=====================>........] - ETA: 37:19 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0487
 7483/10000 [=====================>........] - ETA: 37:18 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 7484/10000 [=====================>........] - ETA: 37:17 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 7485/10000 [=====================>........] - ETA: 37:16 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 7486/10000 [=====================>........] - ETA: 37:15 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 7487/10000 [=====================>........] - ETA: 37:14 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 7488/10000 [=====================>........] - ETA: 37:13 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 7489/10000 [=====================>........] - ETA: 37:12 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 7490/10000 [=====================>........] - ETA: 37:11 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 7491/10000 [=====================>........] - ETA: 37:11 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 7492/10000 [=====================>........] - ETA: 37:10 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 7493/10000 [=====================>........] - ETA: 37:09 - loss: 0.4064 - regression_loss: 0.3578 - classification_loss: 0.0487
 7494/10000 [=====================>........] - ETA: 37:08 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 7495/10000 [=====================>........] - ETA: 37:07 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 7496/10000 [=====================>........] - ETA: 37:06 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 7497/10000 [=====================>........] - ETA: 37:05 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 7498/10000 [=====================>........] - ETA: 37:04 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 7499/10000 [=====================>........] - ETA: 37:03 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 7500/10000 [=====================>........] - ETA: 37:03 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 7501/10000 [=====================>........] - ETA: 37:02 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 7502/10000 [=====================>........] - ETA: 37:01 - loss: 0.4063 - regression_loss: 0.3577 - classification_loss: 0.0487
 7503/10000 [=====================>........] - ETA: 37:00 - loss: 0.4063 - regression_loss: 0.3577 - classification_loss: 0.0487
 7504/10000 [=====================>........] - ETA: 36:59 - loss: 0.4063 - regression_loss: 0.3577 - classification_loss: 0.0487
 7505/10000 [=====================>........] - ETA: 36:58 - loss: 0.4063 - regression_loss: 0.3577 - classification_loss: 0.0487
 7506/10000 [=====================>........] - ETA: 36:57 - loss: 0.4063 - regression_loss: 0.3577 - classification_loss: 0.0487
 7507/10000 [=====================>........] - ETA: 36:56 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 7508/10000 [=====================>........] - ETA: 36:55 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 7509/10000 [=====================>........] - ETA: 36:55 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 7510/10000 [=====================>........] - ETA: 36:54 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 7511/10000 [=====================>........] - ETA: 36:53 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 7512/10000 [=====================>........] - ETA: 36:52 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 7513/10000 [=====================>........] - ETA: 36:51 - loss: 0.4062 - regression_loss: 0.3576 - classification_loss: 0.0487
 7514/10000 [=====================>........] - ETA: 36:50 - loss: 0.4062 - regression_loss: 0.3575 - classification_loss: 0.0487
 7515/10000 [=====================>........] - ETA: 36:49 - loss: 0.4062 - regression_loss: 0.3575 - classification_loss: 0.0487
 7516/10000 [=====================>........] - ETA: 36:48 - loss: 0.4062 - regression_loss: 0.3575 - classification_loss: 0.0487
 7517/10000 [=====================>........] - ETA: 36:47 - loss: 0.4061 - regression_loss: 0.3575 - classification_loss: 0.0487
 7518/10000 [=====================>........] - ETA: 36:47 - loss: 0.4061 - regression_loss: 0.3575 - classification_loss: 0.0487
 7519/10000 [=====================>........] - ETA: 36:46 - loss: 0.4061 - regression_loss: 0.3575 - classification_loss: 0.0487
 7520/10000 [=====================>........] - ETA: 36:45 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0487
 7521/10000 [=====================>........] - ETA: 36:44 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0487
 7522/10000 [=====================>........] - ETA: 36:43 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0487
 7523/10000 [=====================>........] - ETA: 36:42 - loss: 0.4061 - regression_loss: 0.3574 - classification_loss: 0.0487
 7524/10000 [=====================>........] - ETA: 36:41 - loss: 0.4061 - regression_loss: 0.3574 - classification_loss: 0.0487
 7525/10000 [=====================>........] - ETA: 36:40 - loss: 0.4061 - regression_loss: 0.3574 - classification_loss: 0.0487
 7526/10000 [=====================>........] - ETA: 36:39 - loss: 0.4061 - regression_loss: 0.3574 - classification_loss: 0.0487
 7527/10000 [=====================>........] - ETA: 36:38 - loss: 0.4060 - regression_loss: 0.3573 - classification_loss: 0.0487
 7528/10000 [=====================>........] - ETA: 36:38 - loss: 0.4060 - regression_loss: 0.3573 - classification_loss: 0.0487
 7529/10000 [=====================>........] - ETA: 36:37 - loss: 0.4060 - regression_loss: 0.3573 - classification_loss: 0.0487
 7530/10000 [=====================>........] - ETA: 36:36 - loss: 0.4060 - regression_loss: 0.3573 - classification_loss: 0.0487
 7531/10000 [=====================>........] - ETA: 36:35 - loss: 0.4060 - regression_loss: 0.3573 - classification_loss: 0.0487
 7532/10000 [=====================>........] - ETA: 36:34 - loss: 0.4060 - regression_loss: 0.3573 - classification_loss: 0.0487
 7533/10000 [=====================>........] - ETA: 36:33 - loss: 0.4060 - regression_loss: 0.3573 - classification_loss: 0.0487
 7534/10000 [=====================>........] - ETA: 36:32 - loss: 0.4060 - regression_loss: 0.3573 - classification_loss: 0.0487
 7535/10000 [=====================>........] - ETA: 36:31 - loss: 0.4059 - regression_loss: 0.3572 - classification_loss: 0.0487
 7536/10000 [=====================>........] - ETA: 36:30 - loss: 0.4059 - regression_loss: 0.3572 - classification_loss: 0.0487
 7537/10000 [=====================>........] - ETA: 36:30 - loss: 0.4059 - regression_loss: 0.3572 - classification_loss: 0.0487
 7538/10000 [=====================>........] - ETA: 36:29 - loss: 0.4060 - regression_loss: 0.3573 - classification_loss: 0.0487
 7539/10000 [=====================>........] - ETA: 36:28 - loss: 0.4060 - regression_loss: 0.3573 - classification_loss: 0.0487
 7540/10000 [=====================>........] - ETA: 36:27 - loss: 0.4060 - regression_loss: 0.3573 - classification_loss: 0.0487
 7541/10000 [=====================>........] - ETA: 36:26 - loss: 0.4060 - regression_loss: 0.3573 - classification_loss: 0.0487
 7542/10000 [=====================>........] - ETA: 36:25 - loss: 0.4059 - regression_loss: 0.3572 - classification_loss: 0.0487
 7543/10000 [=====================>........] - ETA: 36:24 - loss: 0.4059 - regression_loss: 0.3572 - classification_loss: 0.0487
 7544/10000 [=====================>........] - ETA: 36:23 - loss: 0.4059 - regression_loss: 0.3572 - classification_loss: 0.0487
 7545/10000 [=====================>........] - ETA: 36:22 - loss: 0.4059 - regression_loss: 0.3572 - classification_loss: 0.0487
 7546/10000 [=====================>........] - ETA: 36:22 - loss: 0.4059 - regression_loss: 0.3572 - classification_loss: 0.0487
 7547/10000 [=====================>........] - ETA: 36:21 - loss: 0.4059 - regression_loss: 0.3572 - classification_loss: 0.0487
 7548/10000 [=====================>........] - ETA: 36:20 - loss: 0.4058 - regression_loss: 0.3572 - classification_loss: 0.0487
 7549/10000 [=====================>........] - ETA: 36:19 - loss: 0.4058 - regression_loss: 0.3572 - classification_loss: 0.0487
 7550/10000 [=====================>........] - ETA: 36:18 - loss: 0.4058 - regression_loss: 0.3571 - classification_loss: 0.0487
 7551/10000 [=====================>........] - ETA: 36:17 - loss: 0.4058 - regression_loss: 0.3571 - classification_loss: 0.0487
 7552/10000 [=====================>........] - ETA: 36:16 - loss: 0.4059 - regression_loss: 0.3572 - classification_loss: 0.0487
 7553/10000 [=====================>........] - ETA: 36:15 - loss: 0.4059 - regression_loss: 0.3572 - classification_loss: 0.0487
 7554/10000 [=====================>........] - ETA: 36:14 - loss: 0.4059 - regression_loss: 0.3572 - classification_loss: 0.0487
 7555/10000 [=====================>........] - ETA: 36:14 - loss: 0.4058 - regression_loss: 0.3572 - classification_loss: 0.0487
 7556/10000 [=====================>........] - ETA: 36:13 - loss: 0.4058 - regression_loss: 0.3572 - classification_loss: 0.0487
 7557/10000 [=====================>........] - ETA: 36:12 - loss: 0.4058 - regression_loss: 0.3571 - classification_loss: 0.0487
 7558/10000 [=====================>........] - ETA: 36:11 - loss: 0.4058 - regression_loss: 0.3572 - classification_loss: 0.0487
 7559/10000 [=====================>........] - ETA: 36:10 - loss: 0.4058 - regression_loss: 0.3572 - classification_loss: 0.0487
 7560/10000 [=====================>........] - ETA: 36:09 - loss: 0.4058 - regression_loss: 0.3571 - classification_loss: 0.0487
 7561/10000 [=====================>........] - ETA: 36:08 - loss: 0.4058 - regression_loss: 0.3572 - classification_loss: 0.0487
 7562/10000 [=====================>........] - ETA: 36:07 - loss: 0.4059 - regression_loss: 0.3572 - classification_loss: 0.0487
 7563/10000 [=====================>........] - ETA: 36:06 - loss: 0.4058 - regression_loss: 0.3572 - classification_loss: 0.0486
 7564/10000 [=====================>........] - ETA: 36:06 - loss: 0.4058 - regression_loss: 0.3571 - classification_loss: 0.0486
 7565/10000 [=====================>........] - ETA: 36:05 - loss: 0.4058 - regression_loss: 0.3571 - classification_loss: 0.0486
 7566/10000 [=====================>........] - ETA: 36:04 - loss: 0.4058 - regression_loss: 0.3571 - classification_loss: 0.0486
 7567/10000 [=====================>........] - ETA: 36:03 - loss: 0.4057 - regression_loss: 0.3571 - classification_loss: 0.0486
 7568/10000 [=====================>........] - ETA: 36:02 - loss: 0.4057 - regression_loss: 0.3571 - classification_loss: 0.0486
 7569/10000 [=====================>........] - ETA: 36:01 - loss: 0.4057 - regression_loss: 0.3571 - classification_loss: 0.0486
 7570/10000 [=====================>........] - ETA: 36:00 - loss: 0.4057 - regression_loss: 0.3571 - classification_loss: 0.0486
 7571/10000 [=====================>........] - ETA: 35:59 - loss: 0.4057 - regression_loss: 0.3570 - classification_loss: 0.0486
 7572/10000 [=====================>........] - ETA: 35:58 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 7573/10000 [=====================>........] - ETA: 35:58 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 7574/10000 [=====================>........] - ETA: 35:57 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 7575/10000 [=====================>........] - ETA: 35:56 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 7576/10000 [=====================>........] - ETA: 35:55 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7577/10000 [=====================>........] - ETA: 35:54 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7578/10000 [=====================>........] - ETA: 35:53 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7579/10000 [=====================>........] - ETA: 35:52 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7580/10000 [=====================>........] - ETA: 35:51 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7581/10000 [=====================>........] - ETA: 35:50 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7582/10000 [=====================>........] - ETA: 35:50 - loss: 0.4054 - regression_loss: 0.3569 - classification_loss: 0.0486
 7583/10000 [=====================>........] - ETA: 35:49 - loss: 0.4054 - regression_loss: 0.3569 - classification_loss: 0.0486
 7584/10000 [=====================>........] - ETA: 35:48 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 7585/10000 [=====================>........] - ETA: 35:47 - loss: 0.4055 - regression_loss: 0.3570 - classification_loss: 0.0486
 7586/10000 [=====================>........] - ETA: 35:46 - loss: 0.4055 - regression_loss: 0.3570 - classification_loss: 0.0486
 7587/10000 [=====================>........] - ETA: 35:45 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 7588/10000 [=====================>........] - ETA: 35:44 - loss: 0.4056 - regression_loss: 0.3569 - classification_loss: 0.0487
 7589/10000 [=====================>........] - ETA: 35:43 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0487
 7590/10000 [=====================>........] - ETA: 35:42 - loss: 0.4056 - regression_loss: 0.3569 - classification_loss: 0.0487
 7591/10000 [=====================>........] - ETA: 35:42 - loss: 0.4056 - regression_loss: 0.3569 - classification_loss: 0.0486
 7592/10000 [=====================>........] - ETA: 35:41 - loss: 0.4056 - regression_loss: 0.3569 - classification_loss: 0.0486
 7593/10000 [=====================>........] - ETA: 35:40 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7594/10000 [=====================>........] - ETA: 35:39 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7595/10000 [=====================>........] - ETA: 35:38 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7596/10000 [=====================>........] - ETA: 35:37 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7597/10000 [=====================>........] - ETA: 35:36 - loss: 0.4057 - regression_loss: 0.3570 - classification_loss: 0.0487
 7598/10000 [=====================>........] - ETA: 35:35 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0487
 7599/10000 [=====================>........] - ETA: 35:34 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0487
 7600/10000 [=====================>........] - ETA: 35:34 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 7601/10000 [=====================>........] - ETA: 35:33 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 7602/10000 [=====================>........] - ETA: 35:32 - loss: 0.4056 - regression_loss: 0.3569 - classification_loss: 0.0486
 7603/10000 [=====================>........] - ETA: 35:31 - loss: 0.4056 - regression_loss: 0.3569 - classification_loss: 0.0486
 7604/10000 [=====================>........] - ETA: 35:30 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7605/10000 [=====================>........] - ETA: 35:29 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 7606/10000 [=====================>........] - ETA: 35:28 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 7607/10000 [=====================>........] - ETA: 35:27 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 7608/10000 [=====================>........] - ETA: 35:26 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 7609/10000 [=====================>........] - ETA: 35:26 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 7610/10000 [=====================>........] - ETA: 35:25 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 7611/10000 [=====================>........] - ETA: 35:24 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 7612/10000 [=====================>........] - ETA: 35:23 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 7613/10000 [=====================>........] - ETA: 35:22 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0487
 7614/10000 [=====================>........] - ETA: 35:21 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 7615/10000 [=====================>........] - ETA: 35:20 - loss: 0.4056 - regression_loss: 0.3569 - classification_loss: 0.0486
 7616/10000 [=====================>........] - ETA: 35:19 - loss: 0.4056 - regression_loss: 0.3569 - classification_loss: 0.0486
 7617/10000 [=====================>........] - ETA: 35:18 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 7618/10000 [=====================>........] - ETA: 35:18 - loss: 0.4056 - regression_loss: 0.3569 - classification_loss: 0.0486
 7619/10000 [=====================>........] - ETA: 35:17 - loss: 0.4056 - regression_loss: 0.3569 - classification_loss: 0.0486
 7620/10000 [=====================>........] - ETA: 35:16 - loss: 0.4056 - regression_loss: 0.3569 - classification_loss: 0.0486
 7621/10000 [=====================>........] - ETA: 35:15 - loss: 0.4057 - regression_loss: 0.3571 - classification_loss: 0.0487
 7622/10000 [=====================>........] - ETA: 35:14 - loss: 0.4057 - regression_loss: 0.3570 - classification_loss: 0.0487
 7623/10000 [=====================>........] - ETA: 35:13 - loss: 0.4057 - regression_loss: 0.3571 - classification_loss: 0.0487
 7624/10000 [=====================>........] - ETA: 35:12 - loss: 0.4057 - regression_loss: 0.3571 - classification_loss: 0.0487
 7625/10000 [=====================>........] - ETA: 35:11 - loss: 0.4057 - regression_loss: 0.3570 - classification_loss: 0.0487
 7626/10000 [=====================>........] - ETA: 35:10 - loss: 0.4057 - regression_loss: 0.3570 - classification_loss: 0.0487
 7627/10000 [=====================>........] - ETA: 35:10 - loss: 0.4057 - regression_loss: 0.3570 - classification_loss: 0.0487
 7628/10000 [=====================>........] - ETA: 35:09 - loss: 0.4057 - regression_loss: 0.3570 - classification_loss: 0.0487
 7629/10000 [=====================>........] - ETA: 35:08 - loss: 0.4057 - regression_loss: 0.3570 - classification_loss: 0.0487
 7630/10000 [=====================>........] - ETA: 35:07 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 7631/10000 [=====================>........] - ETA: 35:06 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 7632/10000 [=====================>........] - ETA: 35:05 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 7633/10000 [=====================>........] - ETA: 35:04 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 7634/10000 [=====================>........] - ETA: 35:03 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 7635/10000 [=====================>........] - ETA: 35:02 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 7636/10000 [=====================>........] - ETA: 35:02 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 7637/10000 [=====================>........] - ETA: 35:01 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 7638/10000 [=====================>........] - ETA: 35:00 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 7639/10000 [=====================>........] - ETA: 34:59 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 7640/10000 [=====================>........] - ETA: 34:58 - loss: 0.4056 - regression_loss: 0.3569 - classification_loss: 0.0486
 7641/10000 [=====================>........] - ETA: 34:57 - loss: 0.4056 - regression_loss: 0.3569 - classification_loss: 0.0486
 7642/10000 [=====================>........] - ETA: 34:56 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0486
 7643/10000 [=====================>........] - ETA: 34:55 - loss: 0.4056 - regression_loss: 0.3569 - classification_loss: 0.0486
 7644/10000 [=====================>........] - ETA: 34:54 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7645/10000 [=====================>........] - ETA: 34:54 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7646/10000 [=====================>........] - ETA: 34:53 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7647/10000 [=====================>........] - ETA: 34:52 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7648/10000 [=====================>........] - ETA: 34:51 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 7649/10000 [=====================>........] - ETA: 34:50 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7650/10000 [=====================>........] - ETA: 34:49 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 7651/10000 [=====================>........] - ETA: 34:48 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 7652/10000 [=====================>........] - ETA: 34:47 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 7653/10000 [=====================>........] - ETA: 34:46 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7654/10000 [=====================>........] - ETA: 34:46 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7655/10000 [=====================>........] - ETA: 34:45 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7656/10000 [=====================>........] - ETA: 34:44 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7657/10000 [=====================>........] - ETA: 34:43 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7658/10000 [=====================>........] - ETA: 34:42 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 7659/10000 [=====================>........] - ETA: 34:41 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 7660/10000 [=====================>........] - ETA: 34:40 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 7661/10000 [=====================>........] - ETA: 34:39 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 7662/10000 [=====================>........] - ETA: 34:38 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 7663/10000 [=====================>........] - ETA: 34:38 - loss: 0.4053 - regression_loss: 0.3567 - classification_loss: 0.0486
 7664/10000 [=====================>........] - ETA: 34:37 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 7665/10000 [=====================>........] - ETA: 34:36 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 7666/10000 [=====================>........] - ETA: 34:35 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0487
 7667/10000 [======================>.......] - ETA: 34:34 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0487
 7668/10000 [======================>.......] - ETA: 34:33 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0487
 7669/10000 [======================>.......] - ETA: 34:32 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0487
 7670/10000 [======================>.......] - ETA: 34:31 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7671/10000 [======================>.......] - ETA: 34:30 - loss: 0.4055 - regression_loss: 0.3568 - classification_loss: 0.0486
 7672/10000 [======================>.......] - ETA: 34:30 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 7673/10000 [======================>.......] - ETA: 34:29 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 7674/10000 [======================>.......] - ETA: 34:28 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 7675/10000 [======================>.......] - ETA: 34:27 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 7676/10000 [======================>.......] - ETA: 34:26 - loss: 0.4054 - regression_loss: 0.3567 - classification_loss: 0.0486
 7677/10000 [======================>.......] - ETA: 34:25 - loss: 0.4054 - regression_loss: 0.3567 - classification_loss: 0.0486
 7678/10000 [======================>.......] - ETA: 34:24 - loss: 0.4053 - regression_loss: 0.3567 - classification_loss: 0.0486
 7679/10000 [======================>.......] - ETA: 34:23 - loss: 0.4053 - regression_loss: 0.3567 - classification_loss: 0.0486
 7680/10000 [======================>.......] - ETA: 34:22 - loss: 0.4053 - regression_loss: 0.3566 - classification_loss: 0.0486
 7681/10000 [======================>.......] - ETA: 34:22 - loss: 0.4053 - regression_loss: 0.3567 - classification_loss: 0.0486
 7682/10000 [======================>.......] - ETA: 34:21 - loss: 0.4053 - regression_loss: 0.3566 - classification_loss: 0.0486
 7683/10000 [======================>.......] - ETA: 34:20 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 7684/10000 [======================>.......] - ETA: 34:19 - loss: 0.4054 - regression_loss: 0.3567 - classification_loss: 0.0486
 7685/10000 [======================>.......] - ETA: 34:18 - loss: 0.4053 - regression_loss: 0.3567 - classification_loss: 0.0486
 7686/10000 [======================>.......] - ETA: 34:17 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 7687/10000 [======================>.......] - ETA: 34:16 - loss: 0.4055 - regression_loss: 0.3568 - classification_loss: 0.0487
 7688/10000 [======================>.......] - ETA: 34:15 - loss: 0.4055 - regression_loss: 0.3568 - classification_loss: 0.0487
 7689/10000 [======================>.......] - ETA: 34:14 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 7690/10000 [======================>.......] - ETA: 34:14 - loss: 0.4055 - regression_loss: 0.3568 - classification_loss: 0.0487
 7691/10000 [======================>.......] - ETA: 34:13 - loss: 0.4055 - regression_loss: 0.3568 - classification_loss: 0.0486
 7692/10000 [======================>.......] - ETA: 34:12 - loss: 0.4055 - regression_loss: 0.3568 - classification_loss: 0.0486
 7693/10000 [======================>.......] - ETA: 34:11 - loss: 0.4055 - regression_loss: 0.3568 - classification_loss: 0.0486
 7694/10000 [======================>.......] - ETA: 34:10 - loss: 0.4055 - regression_loss: 0.3568 - classification_loss: 0.0486
 7695/10000 [======================>.......] - ETA: 34:09 - loss: 0.4055 - regression_loss: 0.3568 - classification_loss: 0.0486
 7696/10000 [======================>.......] - ETA: 34:08 - loss: 0.4055 - regression_loss: 0.3568 - classification_loss: 0.0486
 7697/10000 [======================>.......] - ETA: 34:07 - loss: 0.4055 - regression_loss: 0.3568 - classification_loss: 0.0486
 7698/10000 [======================>.......] - ETA: 34:06 - loss: 0.4055 - regression_loss: 0.3568 - classification_loss: 0.0486
 7699/10000 [======================>.......] - ETA: 34:05 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 7700/10000 [======================>.......] - ETA: 34:05 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7701/10000 [======================>.......] - ETA: 34:04 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7702/10000 [======================>.......] - ETA: 34:03 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7703/10000 [======================>.......] - ETA: 34:02 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7704/10000 [======================>.......] - ETA: 34:01 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7705/10000 [======================>.......] - ETA: 34:00 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7706/10000 [======================>.......] - ETA: 33:59 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7707/10000 [======================>.......] - ETA: 33:58 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 7708/10000 [======================>.......] - ETA: 33:57 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 7709/10000 [======================>.......] - ETA: 33:57 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 7710/10000 [======================>.......] - ETA: 33:56 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7711/10000 [======================>.......] - ETA: 33:55 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7712/10000 [======================>.......] - ETA: 33:54 - loss: 0.4055 - regression_loss: 0.3568 - classification_loss: 0.0486
 7713/10000 [======================>.......] - ETA: 33:53 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 7714/10000 [======================>.......] - ETA: 33:52 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 7715/10000 [======================>.......] - ETA: 33:51 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 7716/10000 [======================>.......] - ETA: 33:50 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0486
 7717/10000 [======================>.......] - ETA: 33:49 - loss: 0.4053 - regression_loss: 0.3567 - classification_loss: 0.0486
 7718/10000 [======================>.......] - ETA: 33:49 - loss: 0.4053 - regression_loss: 0.3567 - classification_loss: 0.0486
 7719/10000 [======================>.......] - ETA: 33:48 - loss: 0.4053 - regression_loss: 0.3567 - classification_loss: 0.0486
 7720/10000 [======================>.......] - ETA: 33:47 - loss: 0.4053 - regression_loss: 0.3567 - classification_loss: 0.0486
 7721/10000 [======================>.......] - ETA: 33:46 - loss: 0.4053 - regression_loss: 0.3567 - classification_loss: 0.0486
 7722/10000 [======================>.......] - ETA: 33:45 - loss: 0.4053 - regression_loss: 0.3567 - classification_loss: 0.0486
 7723/10000 [======================>.......] - ETA: 33:44 - loss: 0.4053 - regression_loss: 0.3567 - classification_loss: 0.0486
 7724/10000 [======================>.......] - ETA: 33:43 - loss: 0.4053 - regression_loss: 0.3567 - classification_loss: 0.0486
 7725/10000 [======================>.......] - ETA: 33:42 - loss: 0.4053 - regression_loss: 0.3567 - classification_loss: 0.0486
 7726/10000 [======================>.......] - ETA: 33:41 - loss: 0.4052 - regression_loss: 0.3567 - classification_loss: 0.0486
 7727/10000 [======================>.......] - ETA: 33:41 - loss: 0.4052 - regression_loss: 0.3567 - classification_loss: 0.0486
 7728/10000 [======================>.......] - ETA: 33:40 - loss: 0.4052 - regression_loss: 0.3567 - classification_loss: 0.0486
 7729/10000 [======================>.......] - ETA: 33:39 - loss: 0.4052 - regression_loss: 0.3567 - classification_loss: 0.0486
 7730/10000 [======================>.......] - ETA: 33:38 - loss: 0.4052 - regression_loss: 0.3566 - classification_loss: 0.0486
 7731/10000 [======================>.......] - ETA: 33:37 - loss: 0.4052 - regression_loss: 0.3567 - classification_loss: 0.0486
 7732/10000 [======================>.......] - ETA: 33:36 - loss: 0.4053 - regression_loss: 0.3567 - classification_loss: 0.0487
 7733/10000 [======================>.......] - ETA: 33:35 - loss: 0.4053 - regression_loss: 0.3567 - classification_loss: 0.0487
 7734/10000 [======================>.......] - ETA: 33:34 - loss: 0.4053 - regression_loss: 0.3567 - classification_loss: 0.0487
 7735/10000 [======================>.......] - ETA: 33:33 - loss: 0.4053 - regression_loss: 0.3567 - classification_loss: 0.0487
 7736/10000 [======================>.......] - ETA: 33:33 - loss: 0.4053 - regression_loss: 0.3567 - classification_loss: 0.0487
 7737/10000 [======================>.......] - ETA: 33:32 - loss: 0.4053 - regression_loss: 0.3566 - classification_loss: 0.0487
 7738/10000 [======================>.......] - ETA: 33:31 - loss: 0.4053 - regression_loss: 0.3566 - classification_loss: 0.0487
 7739/10000 [======================>.......] - ETA: 33:30 - loss: 0.4053 - regression_loss: 0.3566 - classification_loss: 0.0487
 7740/10000 [======================>.......] - ETA: 33:29 - loss: 0.4052 - regression_loss: 0.3566 - classification_loss: 0.0487
 7741/10000 [======================>.......] - ETA: 33:28 - loss: 0.4053 - regression_loss: 0.3566 - classification_loss: 0.0487
 7742/10000 [======================>.......] - ETA: 33:27 - loss: 0.4053 - regression_loss: 0.3566 - classification_loss: 0.0487
 7743/10000 [======================>.......] - ETA: 33:26 - loss: 0.4052 - regression_loss: 0.3566 - classification_loss: 0.0486
 7744/10000 [======================>.......] - ETA: 33:25 - loss: 0.4052 - regression_loss: 0.3566 - classification_loss: 0.0486
 7745/10000 [======================>.......] - ETA: 33:25 - loss: 0.4053 - regression_loss: 0.3566 - classification_loss: 0.0486
 7746/10000 [======================>.......] - ETA: 33:24 - loss: 0.4053 - regression_loss: 0.3566 - classification_loss: 0.0486
 7747/10000 [======================>.......] - ETA: 33:23 - loss: 0.4053 - regression_loss: 0.3566 - classification_loss: 0.0486
 7748/10000 [======================>.......] - ETA: 33:22 - loss: 0.4052 - regression_loss: 0.3566 - classification_loss: 0.0486
 7749/10000 [======================>.......] - ETA: 33:21 - loss: 0.4052 - regression_loss: 0.3566 - classification_loss: 0.0486
 7750/10000 [======================>.......] - ETA: 33:20 - loss: 0.4052 - regression_loss: 0.3566 - classification_loss: 0.0486
 7751/10000 [======================>.......] - ETA: 33:19 - loss: 0.4052 - regression_loss: 0.3566 - classification_loss: 0.0486
 7752/10000 [======================>.......] - ETA: 33:18 - loss: 0.4053 - regression_loss: 0.3567 - classification_loss: 0.0486
 7753/10000 [======================>.......] - ETA: 33:17 - loss: 0.4053 - regression_loss: 0.3566 - classification_loss: 0.0486
 7754/10000 [======================>.......] - ETA: 33:17 - loss: 0.4052 - regression_loss: 0.3566 - classification_loss: 0.0486
 7755/10000 [======================>.......] - ETA: 33:16 - loss: 0.4052 - regression_loss: 0.3566 - classification_loss: 0.0486
 7756/10000 [======================>.......] - ETA: 33:15 - loss: 0.4052 - regression_loss: 0.3566 - classification_loss: 0.0486
 7757/10000 [======================>.......] - ETA: 33:14 - loss: 0.4052 - regression_loss: 0.3566 - classification_loss: 0.0486
 7758/10000 [======================>.......] - ETA: 33:13 - loss: 0.4052 - regression_loss: 0.3566 - classification_loss: 0.0486
 7759/10000 [======================>.......] - ETA: 33:12 - loss: 0.4052 - regression_loss: 0.3566 - classification_loss: 0.0487
 7760/10000 [======================>.......] - ETA: 33:11 - loss: 0.4052 - regression_loss: 0.3565 - classification_loss: 0.0486
 7761/10000 [======================>.......] - ETA: 33:10 - loss: 0.4051 - regression_loss: 0.3565 - classification_loss: 0.0486
 7762/10000 [======================>.......] - ETA: 33:09 - loss: 0.4051 - regression_loss: 0.3565 - classification_loss: 0.0486
 7763/10000 [======================>.......] - ETA: 33:09 - loss: 0.4052 - regression_loss: 0.3565 - classification_loss: 0.0486
 7764/10000 [======================>.......] - ETA: 33:08 - loss: 0.4052 - regression_loss: 0.3565 - classification_loss: 0.0486
 7765/10000 [======================>.......] - ETA: 33:07 - loss: 0.4051 - regression_loss: 0.3565 - classification_loss: 0.0486
 7766/10000 [======================>.......] - ETA: 33:06 - loss: 0.4051 - regression_loss: 0.3565 - classification_loss: 0.0486
 7767/10000 [======================>.......] - ETA: 33:05 - loss: 0.4051 - regression_loss: 0.3565 - classification_loss: 0.0486
 7768/10000 [======================>.......] - ETA: 33:04 - loss: 0.4051 - regression_loss: 0.3565 - classification_loss: 0.0486
 7769/10000 [======================>.......] - ETA: 33:03 - loss: 0.4051 - regression_loss: 0.3565 - classification_loss: 0.0486
 7770/10000 [======================>.......] - ETA: 33:02 - loss: 0.4051 - regression_loss: 0.3565 - classification_loss: 0.0486
 7771/10000 [======================>.......] - ETA: 33:01 - loss: 0.4051 - regression_loss: 0.3565 - classification_loss: 0.0486
 7772/10000 [======================>.......] - ETA: 33:01 - loss: 0.4051 - regression_loss: 0.3565 - classification_loss: 0.0486
 7773/10000 [======================>.......] - ETA: 33:00 - loss: 0.4051 - regression_loss: 0.3564 - classification_loss: 0.0486
 7774/10000 [======================>.......] - ETA: 32:59 - loss: 0.4050 - regression_loss: 0.3564 - classification_loss: 0.0486
 7775/10000 [======================>.......] - ETA: 32:58 - loss: 0.4050 - regression_loss: 0.3564 - classification_loss: 0.0486
 7776/10000 [======================>.......] - ETA: 32:57 - loss: 0.4050 - regression_loss: 0.3564 - classification_loss: 0.0486
 7777/10000 [======================>.......] - ETA: 32:56 - loss: 0.4049 - regression_loss: 0.3563 - classification_loss: 0.0486
 7778/10000 [======================>.......] - ETA: 32:55 - loss: 0.4050 - regression_loss: 0.3564 - classification_loss: 0.0486
 7779/10000 [======================>.......] - ETA: 32:54 - loss: 0.4050 - regression_loss: 0.3564 - classification_loss: 0.0486
 7780/10000 [======================>.......] - ETA: 32:53 - loss: 0.4050 - regression_loss: 0.3564 - classification_loss: 0.0486
 7781/10000 [======================>.......] - ETA: 32:53 - loss: 0.4050 - regression_loss: 0.3564 - classification_loss: 0.0486
 7782/10000 [======================>.......] - ETA: 32:52 - loss: 0.4050 - regression_loss: 0.3564 - classification_loss: 0.0486
 7783/10000 [======================>.......] - ETA: 32:51 - loss: 0.4050 - regression_loss: 0.3564 - classification_loss: 0.0486
 7784/10000 [======================>.......] - ETA: 32:50 - loss: 0.4050 - regression_loss: 0.3564 - classification_loss: 0.0486
 7785/10000 [======================>.......] - ETA: 32:49 - loss: 0.4050 - regression_loss: 0.3564 - classification_loss: 0.0486
 7786/10000 [======================>.......] - ETA: 32:48 - loss: 0.4051 - regression_loss: 0.3565 - classification_loss: 0.0486
 7787/10000 [======================>.......] - ETA: 32:47 - loss: 0.4050 - regression_loss: 0.3565 - classification_loss: 0.0486
 7788/10000 [======================>.......] - ETA: 32:46 - loss: 0.4051 - regression_loss: 0.3565 - classification_loss: 0.0486
 7789/10000 [======================>.......] - ETA: 32:45 - loss: 0.4051 - regression_loss: 0.3565 - classification_loss: 0.0486
 7790/10000 [======================>.......] - ETA: 32:45 - loss: 0.4051 - regression_loss: 0.3565 - classification_loss: 0.0486
 7791/10000 [======================>.......] - ETA: 32:44 - loss: 0.4050 - regression_loss: 0.3565 - classification_loss: 0.0486
 7792/10000 [======================>.......] - ETA: 32:43 - loss: 0.4050 - regression_loss: 0.3565 - classification_loss: 0.0486
 7793/10000 [======================>.......] - ETA: 32:42 - loss: 0.4051 - regression_loss: 0.3565 - classification_loss: 0.0486
 7794/10000 [======================>.......] - ETA: 32:41 - loss: 0.4050 - regression_loss: 0.3565 - classification_loss: 0.0486
 7795/10000 [======================>.......] - ETA: 32:40 - loss: 0.4050 - regression_loss: 0.3565 - classification_loss: 0.0486
 7796/10000 [======================>.......] - ETA: 32:39 - loss: 0.4050 - regression_loss: 0.3565 - classification_loss: 0.0486
 7797/10000 [======================>.......] - ETA: 32:38 - loss: 0.4051 - regression_loss: 0.3565 - classification_loss: 0.0486
 7798/10000 [======================>.......] - ETA: 32:37 - loss: 0.4051 - regression_loss: 0.3565 - classification_loss: 0.0486
 7799/10000 [======================>.......] - ETA: 32:37 - loss: 0.4051 - regression_loss: 0.3566 - classification_loss: 0.0485
 7800/10000 [======================>.......] - ETA: 32:36 - loss: 0.4051 - regression_loss: 0.3566 - classification_loss: 0.0485
 7801/10000 [======================>.......] - ETA: 32:35 - loss: 0.4051 - regression_loss: 0.3566 - classification_loss: 0.0485
 7802/10000 [======================>.......] - ETA: 32:34 - loss: 0.4051 - regression_loss: 0.3566 - classification_loss: 0.0485
 7803/10000 [======================>.......] - ETA: 32:33 - loss: 0.4051 - regression_loss: 0.3566 - classification_loss: 0.0485
 7804/10000 [======================>.......] - ETA: 32:32 - loss: 0.4051 - regression_loss: 0.3566 - classification_loss: 0.0485
 7805/10000 [======================>.......] - ETA: 32:31 - loss: 0.4051 - regression_loss: 0.3566 - classification_loss: 0.0485
 7806/10000 [======================>.......] - ETA: 32:30 - loss: 0.4051 - regression_loss: 0.3565 - classification_loss: 0.0485
 7807/10000 [======================>.......] - ETA: 32:29 - loss: 0.4050 - regression_loss: 0.3565 - classification_loss: 0.0485
 7808/10000 [======================>.......] - ETA: 32:29 - loss: 0.4050 - regression_loss: 0.3565 - classification_loss: 0.0485
 7809/10000 [======================>.......] - ETA: 32:28 - loss: 0.4050 - regression_loss: 0.3565 - classification_loss: 0.0485
 7810/10000 [======================>.......] - ETA: 32:27 - loss: 0.4050 - regression_loss: 0.3565 - classification_loss: 0.0485
 7811/10000 [======================>.......] - ETA: 32:26 - loss: 0.4050 - regression_loss: 0.3565 - classification_loss: 0.0485
 7812/10000 [======================>.......] - ETA: 32:25 - loss: 0.4051 - regression_loss: 0.3566 - classification_loss: 0.0485
 7813/10000 [======================>.......] - ETA: 32:24 - loss: 0.4051 - regression_loss: 0.3566 - classification_loss: 0.0485
 7814/10000 [======================>.......] - ETA: 32:23 - loss: 0.4050 - regression_loss: 0.3565 - classification_loss: 0.0485
 7815/10000 [======================>.......] - ETA: 32:22 - loss: 0.4050 - regression_loss: 0.3565 - classification_loss: 0.0485
 7816/10000 [======================>.......] - ETA: 32:21 - loss: 0.4050 - regression_loss: 0.3565 - classification_loss: 0.0485
 7817/10000 [======================>.......] - ETA: 32:21 - loss: 0.4050 - regression_loss: 0.3565 - classification_loss: 0.0485
 7818/10000 [======================>.......] - ETA: 32:20 - loss: 0.4051 - regression_loss: 0.3566 - classification_loss: 0.0485
 7819/10000 [======================>.......] - ETA: 32:19 - loss: 0.4051 - regression_loss: 0.3566 - classification_loss: 0.0485
 7820/10000 [======================>.......] - ETA: 32:18 - loss: 0.4052 - regression_loss: 0.3567 - classification_loss: 0.0485
 7821/10000 [======================>.......] - ETA: 32:17 - loss: 0.4051 - regression_loss: 0.3566 - classification_loss: 0.0485
 7822/10000 [======================>.......] - ETA: 32:16 - loss: 0.4052 - regression_loss: 0.3567 - classification_loss: 0.0485
 7823/10000 [======================>.......] - ETA: 32:15 - loss: 0.4052 - regression_loss: 0.3567 - classification_loss: 0.0485
 7824/10000 [======================>.......] - ETA: 32:14 - loss: 0.4052 - regression_loss: 0.3567 - classification_loss: 0.0485
 7825/10000 [======================>.......] - ETA: 32:13 - loss: 0.4052 - regression_loss: 0.3567 - classification_loss: 0.0485
 7826/10000 [======================>.......] - ETA: 32:13 - loss: 0.4052 - regression_loss: 0.3567 - classification_loss: 0.0485
 7827/10000 [======================>.......] - ETA: 32:12 - loss: 0.4054 - regression_loss: 0.3567 - classification_loss: 0.0487
 7828/10000 [======================>.......] - ETA: 32:11 - loss: 0.4054 - regression_loss: 0.3568 - classification_loss: 0.0487
 7829/10000 [======================>.......] - ETA: 32:10 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0487
 7830/10000 [======================>.......] - ETA: 32:09 - loss: 0.4055 - regression_loss: 0.3568 - classification_loss: 0.0487
 7831/10000 [======================>.......] - ETA: 32:08 - loss: 0.4055 - regression_loss: 0.3568 - classification_loss: 0.0487
 7832/10000 [======================>.......] - ETA: 32:07 - loss: 0.4055 - regression_loss: 0.3568 - classification_loss: 0.0487
 7833/10000 [======================>.......] - ETA: 32:06 - loss: 0.4055 - regression_loss: 0.3568 - classification_loss: 0.0487
 7834/10000 [======================>.......] - ETA: 32:05 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0487
 7835/10000 [======================>.......] - ETA: 32:05 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0487
 7836/10000 [======================>.......] - ETA: 32:04 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0487
 7837/10000 [======================>.......] - ETA: 32:03 - loss: 0.4056 - regression_loss: 0.3569 - classification_loss: 0.0487
 7838/10000 [======================>.......] - ETA: 32:02 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0487
 7839/10000 [======================>.......] - ETA: 32:01 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0487
 7840/10000 [======================>.......] - ETA: 32:00 - loss: 0.4056 - regression_loss: 0.3569 - classification_loss: 0.0487
 7841/10000 [======================>.......] - ETA: 31:59 - loss: 0.4056 - regression_loss: 0.3569 - classification_loss: 0.0487
 7842/10000 [======================>.......] - ETA: 31:58 - loss: 0.4056 - regression_loss: 0.3569 - classification_loss: 0.0487
 7843/10000 [======================>.......] - ETA: 31:57 - loss: 0.4056 - regression_loss: 0.3569 - classification_loss: 0.0487
 7844/10000 [======================>.......] - ETA: 31:57 - loss: 0.4056 - regression_loss: 0.3569 - classification_loss: 0.0487
 7845/10000 [======================>.......] - ETA: 31:56 - loss: 0.4056 - regression_loss: 0.3569 - classification_loss: 0.0487
 7846/10000 [======================>.......] - ETA: 31:55 - loss: 0.4056 - regression_loss: 0.3569 - classification_loss: 0.0487
 7847/10000 [======================>.......] - ETA: 31:54 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7848/10000 [======================>.......] - ETA: 31:53 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7849/10000 [======================>.......] - ETA: 31:52 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7850/10000 [======================>.......] - ETA: 31:51 - loss: 0.4055 - regression_loss: 0.3569 - classification_loss: 0.0486
 7851/10000 [======================>.......] - ETA: 31:50 - loss: 0.4057 - regression_loss: 0.3570 - classification_loss: 0.0487
 7852/10000 [======================>.......] - ETA: 31:49 - loss: 0.4057 - regression_loss: 0.3570 - classification_loss: 0.0487
 7853/10000 [======================>.......] - ETA: 31:49 - loss: 0.4057 - regression_loss: 0.3570 - classification_loss: 0.0487
 7854/10000 [======================>.......] - ETA: 31:48 - loss: 0.4057 - regression_loss: 0.3570 - classification_loss: 0.0487
 7855/10000 [======================>.......] - ETA: 31:47 - loss: 0.4056 - regression_loss: 0.3570 - classification_loss: 0.0487
 7856/10000 [======================>.......] - ETA: 31:46 - loss: 0.4057 - regression_loss: 0.3570 - classification_loss: 0.0488
 7857/10000 [======================>.......] - ETA: 31:45 - loss: 0.4057 - regression_loss: 0.3570 - classification_loss: 0.0488
 7858/10000 [======================>.......] - ETA: 31:44 - loss: 0.4057 - regression_loss: 0.3570 - classification_loss: 0.0488
 7859/10000 [======================>.......] - ETA: 31:43 - loss: 0.4058 - regression_loss: 0.3570 - classification_loss: 0.0488
 7860/10000 [======================>.......] - ETA: 31:42 - loss: 0.4058 - regression_loss: 0.3570 - classification_loss: 0.0488
 7861/10000 [======================>.......] - ETA: 31:41 - loss: 0.4058 - regression_loss: 0.3571 - classification_loss: 0.0488
 7862/10000 [======================>.......] - ETA: 31:41 - loss: 0.4058 - regression_loss: 0.3571 - classification_loss: 0.0488
 7863/10000 [======================>.......] - ETA: 31:40 - loss: 0.4058 - regression_loss: 0.3571 - classification_loss: 0.0488
 7864/10000 [======================>.......] - ETA: 31:39 - loss: 0.4059 - regression_loss: 0.3571 - classification_loss: 0.0488
 7865/10000 [======================>.......] - ETA: 31:38 - loss: 0.4058 - regression_loss: 0.3571 - classification_loss: 0.0488
 7866/10000 [======================>.......] - ETA: 31:37 - loss: 0.4058 - regression_loss: 0.3570 - classification_loss: 0.0488
 7867/10000 [======================>.......] - ETA: 31:36 - loss: 0.4058 - regression_loss: 0.3570 - classification_loss: 0.0488
 7868/10000 [======================>.......] - ETA: 31:35 - loss: 0.4058 - regression_loss: 0.3570 - classification_loss: 0.0488
 7869/10000 [======================>.......] - ETA: 31:34 - loss: 0.4057 - regression_loss: 0.3570 - classification_loss: 0.0488
 7870/10000 [======================>.......] - ETA: 31:33 - loss: 0.4057 - regression_loss: 0.3570 - classification_loss: 0.0487
 7871/10000 [======================>.......] - ETA: 31:33 - loss: 0.4057 - regression_loss: 0.3570 - classification_loss: 0.0487
 7872/10000 [======================>.......] - ETA: 31:32 - loss: 0.4058 - regression_loss: 0.3570 - classification_loss: 0.0487
 7873/10000 [======================>.......] - ETA: 31:31 - loss: 0.4057 - regression_loss: 0.3570 - classification_loss: 0.0487
 7874/10000 [======================>.......] - ETA: 31:30 - loss: 0.4058 - regression_loss: 0.3571 - classification_loss: 0.0488
 7875/10000 [======================>.......] - ETA: 31:29 - loss: 0.4058 - regression_loss: 0.3571 - classification_loss: 0.0488
 7876/10000 [======================>.......] - ETA: 31:28 - loss: 0.4058 - regression_loss: 0.3571 - classification_loss: 0.0488
 7877/10000 [======================>.......] - ETA: 31:27 - loss: 0.4058 - regression_loss: 0.3570 - classification_loss: 0.0487
 7878/10000 [======================>.......] - ETA: 31:26 - loss: 0.4058 - regression_loss: 0.3570 - classification_loss: 0.0487
 7879/10000 [======================>.......] - ETA: 31:25 - loss: 0.4058 - regression_loss: 0.3570 - classification_loss: 0.0487
 7880/10000 [======================>.......] - ETA: 31:25 - loss: 0.4058 - regression_loss: 0.3570 - classification_loss: 0.0487
 7881/10000 [======================>.......] - ETA: 31:24 - loss: 0.4058 - regression_loss: 0.3570 - classification_loss: 0.0488
 7882/10000 [======================>.......] - ETA: 31:23 - loss: 0.4058 - regression_loss: 0.3570 - classification_loss: 0.0488
 7883/10000 [======================>.......] - ETA: 31:22 - loss: 0.4058 - regression_loss: 0.3570 - classification_loss: 0.0488
 7884/10000 [======================>.......] - ETA: 31:21 - loss: 0.4058 - regression_loss: 0.3570 - classification_loss: 0.0488
 7885/10000 [======================>.......] - ETA: 31:20 - loss: 0.4058 - regression_loss: 0.3570 - classification_loss: 0.0488
 7886/10000 [======================>.......] - ETA: 31:19 - loss: 0.4058 - regression_loss: 0.3571 - classification_loss: 0.0488
 7887/10000 [======================>.......] - ETA: 31:18 - loss: 0.4058 - regression_loss: 0.3570 - classification_loss: 0.0488
 7888/10000 [======================>.......] - ETA: 31:17 - loss: 0.4058 - regression_loss: 0.3570 - classification_loss: 0.0488
 7889/10000 [======================>.......] - ETA: 31:16 - loss: 0.4058 - regression_loss: 0.3570 - classification_loss: 0.0488
 7890/10000 [======================>.......] - ETA: 31:16 - loss: 0.4058 - regression_loss: 0.3570 - classification_loss: 0.0488
 7891/10000 [======================>.......] - ETA: 31:15 - loss: 0.4058 - regression_loss: 0.3570 - classification_loss: 0.0488
 7892/10000 [======================>.......] - ETA: 31:14 - loss: 0.4058 - regression_loss: 0.3570 - classification_loss: 0.0488
 7893/10000 [======================>.......] - ETA: 31:13 - loss: 0.4058 - regression_loss: 0.3570 - classification_loss: 0.0488
 7894/10000 [======================>.......] - ETA: 31:12 - loss: 0.4058 - regression_loss: 0.3570 - classification_loss: 0.0488
 7895/10000 [======================>.......] - ETA: 31:11 - loss: 0.4058 - regression_loss: 0.3570 - classification_loss: 0.0488
 7896/10000 [======================>.......] - ETA: 31:10 - loss: 0.4060 - regression_loss: 0.3572 - classification_loss: 0.0488
 7897/10000 [======================>.......] - ETA: 31:09 - loss: 0.4060 - regression_loss: 0.3572 - classification_loss: 0.0488
 7898/10000 [======================>.......] - ETA: 31:08 - loss: 0.4060 - regression_loss: 0.3572 - classification_loss: 0.0488
 7899/10000 [======================>.......] - ETA: 31:08 - loss: 0.4060 - regression_loss: 0.3572 - classification_loss: 0.0488
 7900/10000 [======================>.......] - ETA: 31:07 - loss: 0.4060 - regression_loss: 0.3572 - classification_loss: 0.0488
 7901/10000 [======================>.......] - ETA: 31:06 - loss: 0.4060 - regression_loss: 0.3572 - classification_loss: 0.0488
 7902/10000 [======================>.......] - ETA: 31:05 - loss: 0.4060 - regression_loss: 0.3572 - classification_loss: 0.0488
 7903/10000 [======================>.......] - ETA: 31:04 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 7904/10000 [======================>.......] - ETA: 31:03 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 7905/10000 [======================>.......] - ETA: 31:02 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 7906/10000 [======================>.......] - ETA: 31:01 - loss: 0.4061 - regression_loss: 0.3573 - classification_loss: 0.0489
 7907/10000 [======================>.......] - ETA: 31:00 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 7908/10000 [======================>.......] - ETA: 31:00 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 7909/10000 [======================>.......] - ETA: 30:59 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 7910/10000 [======================>.......] - ETA: 30:58 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 7911/10000 [======================>.......] - ETA: 30:57 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 7912/10000 [======================>.......] - ETA: 30:56 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 7913/10000 [======================>.......] - ETA: 30:55 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 7914/10000 [======================>.......] - ETA: 30:54 - loss: 0.4060 - regression_loss: 0.3572 - classification_loss: 0.0489
 7915/10000 [======================>.......] - ETA: 30:53 - loss: 0.4060 - regression_loss: 0.3571 - classification_loss: 0.0489
 7916/10000 [======================>.......] - ETA: 30:52 - loss: 0.4060 - regression_loss: 0.3572 - classification_loss: 0.0489
 7917/10000 [======================>.......] - ETA: 30:52 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 7918/10000 [======================>.......] - ETA: 30:51 - loss: 0.4060 - regression_loss: 0.3572 - classification_loss: 0.0489
 7919/10000 [======================>.......] - ETA: 30:50 - loss: 0.4060 - regression_loss: 0.3572 - classification_loss: 0.0489
 7920/10000 [======================>.......] - ETA: 30:49 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 7921/10000 [======================>.......] - ETA: 30:48 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 7922/10000 [======================>.......] - ETA: 30:47 - loss: 0.4060 - regression_loss: 0.3572 - classification_loss: 0.0489
 7923/10000 [======================>.......] - ETA: 30:46 - loss: 0.4060 - regression_loss: 0.3572 - classification_loss: 0.0489
 7924/10000 [======================>.......] - ETA: 30:45 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 7925/10000 [======================>.......] - ETA: 30:44 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 7926/10000 [======================>.......] - ETA: 30:44 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 7927/10000 [======================>.......] - ETA: 30:43 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 7928/10000 [======================>.......] - ETA: 30:42 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 7929/10000 [======================>.......] - ETA: 30:41 - loss: 0.4061 - regression_loss: 0.3573 - classification_loss: 0.0489
 7930/10000 [======================>.......] - ETA: 30:40 - loss: 0.4061 - regression_loss: 0.3573 - classification_loss: 0.0489
 7931/10000 [======================>.......] - ETA: 30:39 - loss: 0.4061 - regression_loss: 0.3573 - classification_loss: 0.0489
 7932/10000 [======================>.......] - ETA: 30:38 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 7933/10000 [======================>.......] - ETA: 30:37 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 7934/10000 [======================>.......] - ETA: 30:36 - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 7935/10000 [======================>.......] - ETA: 30:36 - loss: 0.4061 - regression_loss: 0.3573 - classification_loss: 0.0489
 7936/10000 [======================>.......] - ETA: 30:35 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0489
 7937/10000 [======================>.......] - ETA: 30:34 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0489
 7938/10000 [======================>.......] - ETA: 30:33 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0489
 7939/10000 [======================>.......] - ETA: 30:32 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0489
 7940/10000 [======================>.......] - ETA: 30:31 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 7941/10000 [======================>.......] - ETA: 30:30 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0489
 7942/10000 [======================>.......] - ETA: 30:29 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 7943/10000 [======================>.......] - ETA: 30:28 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 7944/10000 [======================>.......] - ETA: 30:28 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 7945/10000 [======================>.......] - ETA: 30:27 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 7946/10000 [======================>.......] - ETA: 30:26 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0489
 7947/10000 [======================>.......] - ETA: 30:25 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0489
 7948/10000 [======================>.......] - ETA: 30:24 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0489
 7949/10000 [======================>.......] - ETA: 30:23 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 7950/10000 [======================>.......] - ETA: 30:22 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 7951/10000 [======================>.......] - ETA: 30:21 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0488
 7952/10000 [======================>.......] - ETA: 30:20 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0488
 7953/10000 [======================>.......] - ETA: 30:20 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 7954/10000 [======================>.......] - ETA: 30:19 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0488
 7955/10000 [======================>.......] - ETA: 30:18 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0488
 7956/10000 [======================>.......] - ETA: 30:17 - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0488
 7957/10000 [======================>.......] - ETA: 30:16 - loss: 0.4061 - regression_loss: 0.3573 - classification_loss: 0.0488
 7958/10000 [======================>.......] - ETA: 30:15 - loss: 0.4061 - regression_loss: 0.3573 - classification_loss: 0.0488
 7959/10000 [======================>.......] - ETA: 30:14 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 7960/10000 [======================>.......] - ETA: 30:13 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 7961/10000 [======================>.......] - ETA: 30:12 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 7962/10000 [======================>.......] - ETA: 30:12 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 7963/10000 [======================>.......] - ETA: 30:11 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0488
 7964/10000 [======================>.......] - ETA: 30:10 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0488
 7965/10000 [======================>.......] - ETA: 30:09 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0488
 7966/10000 [======================>.......] - ETA: 30:08 - loss: 0.4063 - regression_loss: 0.3574 - classification_loss: 0.0488
 7967/10000 [======================>.......] - ETA: 30:07 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 7968/10000 [======================>.......] - ETA: 30:06 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 7969/10000 [======================>.......] - ETA: 30:05 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 7970/10000 [======================>.......] - ETA: 30:04 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 7971/10000 [======================>.......] - ETA: 30:04 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 7972/10000 [======================>.......] - ETA: 30:03 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 7973/10000 [======================>.......] - ETA: 30:02 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 7974/10000 [======================>.......] - ETA: 30:01 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 7975/10000 [======================>.......] - ETA: 30:00 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 7976/10000 [======================>.......] - ETA: 29:59 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 7977/10000 [======================>.......] - ETA: 29:58 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 7978/10000 [======================>.......] - ETA: 29:57 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 7979/10000 [======================>.......] - ETA: 29:56 - loss: 0.4061 - regression_loss: 0.3573 - classification_loss: 0.0488
 7980/10000 [======================>.......] - ETA: 29:56 - loss: 0.4061 - regression_loss: 0.3573 - classification_loss: 0.0488
 7981/10000 [======================>.......] - ETA: 29:55 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 7982/10000 [======================>.......] - ETA: 29:54 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 7983/10000 [======================>.......] - ETA: 29:53 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0488
 7984/10000 [======================>.......] - ETA: 29:52 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 7985/10000 [======================>.......] - ETA: 29:51 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 7986/10000 [======================>.......] - ETA: 29:50 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 7987/10000 [======================>.......] - ETA: 29:49 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 7988/10000 [======================>.......] - ETA: 29:48 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 7989/10000 [======================>.......] - ETA: 29:48 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 7990/10000 [======================>.......] - ETA: 29:47 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 7991/10000 [======================>.......] - ETA: 29:46 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 7992/10000 [======================>.......] - ETA: 29:45 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 7993/10000 [======================>.......] - ETA: 29:44 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 7994/10000 [======================>.......] - ETA: 29:43 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 7995/10000 [======================>.......] - ETA: 29:42 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 7996/10000 [======================>.......] - ETA: 29:41 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 7997/10000 [======================>.......] - ETA: 29:40 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 7998/10000 [======================>.......] - ETA: 29:40 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 7999/10000 [======================>.......] - ETA: 29:39 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8000/10000 [=======================>......] - ETA: 29:38 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8001/10000 [=======================>......] - ETA: 29:37 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8002/10000 [=======================>......] - ETA: 29:36 - loss: 0.4061 - regression_loss: 0.3574 - classification_loss: 0.0488
 8003/10000 [=======================>......] - ETA: 29:35 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8004/10000 [=======================>......] - ETA: 29:34 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8005/10000 [=======================>......] - ETA: 29:33 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8006/10000 [=======================>......] - ETA: 29:32 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8007/10000 [=======================>......] - ETA: 29:32 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8008/10000 [=======================>......] - ETA: 29:31 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8009/10000 [=======================>......] - ETA: 29:30 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8010/10000 [=======================>......] - ETA: 29:29 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8011/10000 [=======================>......] - ETA: 29:28 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8012/10000 [=======================>......] - ETA: 29:27 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8013/10000 [=======================>......] - ETA: 29:26 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8014/10000 [=======================>......] - ETA: 29:25 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8015/10000 [=======================>......] - ETA: 29:24 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8016/10000 [=======================>......] - ETA: 29:24 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8017/10000 [=======================>......] - ETA: 29:23 - loss: 0.4061 - regression_loss: 0.3574 - classification_loss: 0.0488
 8018/10000 [=======================>......] - ETA: 29:22 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8019/10000 [=======================>......] - ETA: 29:21 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8020/10000 [=======================>......] - ETA: 29:20 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8021/10000 [=======================>......] - ETA: 29:19 - loss: 0.4061 - regression_loss: 0.3574 - classification_loss: 0.0488
 8022/10000 [=======================>......] - ETA: 29:18 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8023/10000 [=======================>......] - ETA: 29:17 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8024/10000 [=======================>......] - ETA: 29:16 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8025/10000 [=======================>......] - ETA: 29:16 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8026/10000 [=======================>......] - ETA: 29:15 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8027/10000 [=======================>......] - ETA: 29:14 - loss: 0.4061 - regression_loss: 0.3574 - classification_loss: 0.0488
 8028/10000 [=======================>......] - ETA: 29:13 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8029/10000 [=======================>......] - ETA: 29:12 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8030/10000 [=======================>......] - ETA: 29:11 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8031/10000 [=======================>......] - ETA: 29:10 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8032/10000 [=======================>......] - ETA: 29:09 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8033/10000 [=======================>......] - ETA: 29:08 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8034/10000 [=======================>......] - ETA: 29:08 - loss: 0.4062 - regression_loss: 0.3575 - classification_loss: 0.0488
 8035/10000 [=======================>......] - ETA: 29:07 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 8036/10000 [=======================>......] - ETA: 29:06 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 8037/10000 [=======================>......] - ETA: 29:05 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 8038/10000 [=======================>......] - ETA: 29:04 - loss: 0.4062 - regression_loss: 0.3575 - classification_loss: 0.0488
 8039/10000 [=======================>......] - ETA: 29:03 - loss: 0.4062 - regression_loss: 0.3575 - classification_loss: 0.0488
 8040/10000 [=======================>......] - ETA: 29:02 - loss: 0.4062 - regression_loss: 0.3575 - classification_loss: 0.0488
 8041/10000 [=======================>......] - ETA: 29:01 - loss: 0.4062 - regression_loss: 0.3574 - classification_loss: 0.0488
 8042/10000 [=======================>......] - ETA: 29:00 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8043/10000 [=======================>......] - ETA: 29:00 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8044/10000 [=======================>......] - ETA: 28:59 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8045/10000 [=======================>......] - ETA: 28:58 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8046/10000 [=======================>......] - ETA: 28:57 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0488
 8047/10000 [=======================>......] - ETA: 28:56 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8048/10000 [=======================>......] - ETA: 28:55 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8049/10000 [=======================>......] - ETA: 28:54 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8050/10000 [=======================>......] - ETA: 28:53 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0489
 8051/10000 [=======================>......] - ETA: 28:52 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0489
 8052/10000 [=======================>......] - ETA: 28:52 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 8053/10000 [=======================>......] - ETA: 28:51 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 8054/10000 [=======================>......] - ETA: 28:50 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 8055/10000 [=======================>......] - ETA: 28:49 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 8056/10000 [=======================>......] - ETA: 28:48 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 8057/10000 [=======================>......] - ETA: 28:47 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 8058/10000 [=======================>......] - ETA: 28:46 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0489
 8059/10000 [=======================>......] - ETA: 28:45 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0489
 8060/10000 [=======================>......] - ETA: 28:44 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0488
 8061/10000 [=======================>......] - ETA: 28:43 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0488
 8062/10000 [=======================>......] - ETA: 28:43 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8063/10000 [=======================>......] - ETA: 28:42 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8064/10000 [=======================>......] - ETA: 28:41 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0488
 8065/10000 [=======================>......] - ETA: 28:40 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0488
 8066/10000 [=======================>......] - ETA: 28:39 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8067/10000 [=======================>......] - ETA: 28:38 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0488
 8068/10000 [=======================>......] - ETA: 28:37 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0488
 8069/10000 [=======================>......] - ETA: 28:36 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8070/10000 [=======================>......] - ETA: 28:35 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8071/10000 [=======================>......] - ETA: 28:35 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8072/10000 [=======================>......] - ETA: 28:34 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0488
 8073/10000 [=======================>......] - ETA: 28:33 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0488
 8074/10000 [=======================>......] - ETA: 28:32 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8075/10000 [=======================>......] - ETA: 28:31 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8076/10000 [=======================>......] - ETA: 28:30 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8077/10000 [=======================>......] - ETA: 28:29 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8078/10000 [=======================>......] - ETA: 28:28 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8079/10000 [=======================>......] - ETA: 28:27 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8080/10000 [=======================>......] - ETA: 28:27 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8081/10000 [=======================>......] - ETA: 28:26 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8082/10000 [=======================>......] - ETA: 28:25 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8083/10000 [=======================>......] - ETA: 28:24 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8084/10000 [=======================>......] - ETA: 28:23 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8085/10000 [=======================>......] - ETA: 28:22 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0488
 8086/10000 [=======================>......] - ETA: 28:21 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8087/10000 [=======================>......] - ETA: 28:20 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0488
 8088/10000 [=======================>......] - ETA: 28:19 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8089/10000 [=======================>......] - ETA: 28:19 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8090/10000 [=======================>......] - ETA: 28:18 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8091/10000 [=======================>......] - ETA: 28:17 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0488
 8092/10000 [=======================>......] - ETA: 28:16 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8093/10000 [=======================>......] - ETA: 28:15 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8094/10000 [=======================>......] - ETA: 28:14 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8095/10000 [=======================>......] - ETA: 28:13 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8096/10000 [=======================>......] - ETA: 28:12 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8097/10000 [=======================>......] - ETA: 28:11 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8098/10000 [=======================>......] - ETA: 28:11 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8099/10000 [=======================>......] - ETA: 28:10 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8100/10000 [=======================>......] - ETA: 28:09 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8101/10000 [=======================>......] - ETA: 28:08 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8102/10000 [=======================>......] - ETA: 28:07 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0488
 8103/10000 [=======================>......] - ETA: 28:06 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8104/10000 [=======================>......] - ETA: 28:05 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8105/10000 [=======================>......] - ETA: 28:04 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0488
 8106/10000 [=======================>......] - ETA: 28:03 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 8107/10000 [=======================>......] - ETA: 28:03 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 8108/10000 [=======================>......] - ETA: 28:02 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 8109/10000 [=======================>......] - ETA: 28:01 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0489
 8110/10000 [=======================>......] - ETA: 28:00 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 8111/10000 [=======================>......] - ETA: 27:59 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 8112/10000 [=======================>......] - ETA: 27:58 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 8113/10000 [=======================>......] - ETA: 27:57 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 8114/10000 [=======================>......] - ETA: 27:56 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 8115/10000 [=======================>......] - ETA: 27:55 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 8116/10000 [=======================>......] - ETA: 27:55 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 8117/10000 [=======================>......] - ETA: 27:54 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 8118/10000 [=======================>......] - ETA: 27:53 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 8119/10000 [=======================>......] - ETA: 27:52 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 8120/10000 [=======================>......] - ETA: 27:51 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 8121/10000 [=======================>......] - ETA: 27:50 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0489
 8122/10000 [=======================>......] - ETA: 27:49 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 8123/10000 [=======================>......] - ETA: 27:48 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0489
 8124/10000 [=======================>......] - ETA: 27:47 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0489
 8125/10000 [=======================>......] - ETA: 27:47 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 8126/10000 [=======================>......] - ETA: 27:46 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0489
 8127/10000 [=======================>......] - ETA: 27:45 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0489
 8128/10000 [=======================>......] - ETA: 27:44 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0489
 8129/10000 [=======================>......] - ETA: 27:43 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0489
 8130/10000 [=======================>......] - ETA: 27:42 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0488
 8131/10000 [=======================>......] - ETA: 27:41 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8132/10000 [=======================>......] - ETA: 27:40 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0488
 8133/10000 [=======================>......] - ETA: 27:39 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0488
 8134/10000 [=======================>......] - ETA: 27:39 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8135/10000 [=======================>......] - ETA: 27:38 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8136/10000 [=======================>......] - ETA: 27:37 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0488
 8137/10000 [=======================>......] - ETA: 27:36 - loss: 0.4065 - regression_loss: 0.3576 - classification_loss: 0.0488
 8138/10000 [=======================>......] - ETA: 27:35 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8139/10000 [=======================>......] - ETA: 27:34 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8140/10000 [=======================>......] - ETA: 27:33 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8141/10000 [=======================>......] - ETA: 27:32 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8142/10000 [=======================>......] - ETA: 27:31 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8143/10000 [=======================>......] - ETA: 27:31 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 8144/10000 [=======================>......] - ETA: 27:30 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 8145/10000 [=======================>......] - ETA: 27:29 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 8146/10000 [=======================>......] - ETA: 27:28 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 8147/10000 [=======================>......] - ETA: 27:27 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 8148/10000 [=======================>......] - ETA: 27:26 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 8149/10000 [=======================>......] - ETA: 27:25 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 8150/10000 [=======================>......] - ETA: 27:24 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 8151/10000 [=======================>......] - ETA: 27:23 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 8152/10000 [=======================>......] - ETA: 27:23 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 8153/10000 [=======================>......] - ETA: 27:22 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0488
 8154/10000 [=======================>......] - ETA: 27:21 - loss: 0.4064 - regression_loss: 0.3575 - classification_loss: 0.0488
 8155/10000 [=======================>......] - ETA: 27:20 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8156/10000 [=======================>......] - ETA: 27:19 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8157/10000 [=======================>......] - ETA: 27:18 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8158/10000 [=======================>......] - ETA: 27:17 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8159/10000 [=======================>......] - ETA: 27:16 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8160/10000 [=======================>......] - ETA: 27:15 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8161/10000 [=======================>......] - ETA: 27:15 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8162/10000 [=======================>......] - ETA: 27:14 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8163/10000 [=======================>......] - ETA: 27:13 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8164/10000 [=======================>......] - ETA: 27:12 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8165/10000 [=======================>......] - ETA: 27:11 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8166/10000 [=======================>......] - ETA: 27:10 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8167/10000 [=======================>......] - ETA: 27:09 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8168/10000 [=======================>......] - ETA: 27:08 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8169/10000 [=======================>......] - ETA: 27:07 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8170/10000 [=======================>......] - ETA: 27:07 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8171/10000 [=======================>......] - ETA: 27:06 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8172/10000 [=======================>......] - ETA: 27:05 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8173/10000 [=======================>......] - ETA: 27:04 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0488
 8174/10000 [=======================>......] - ETA: 27:03 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8175/10000 [=======================>......] - ETA: 27:02 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8176/10000 [=======================>......] - ETA: 27:01 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8177/10000 [=======================>......] - ETA: 27:00 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8178/10000 [=======================>......] - ETA: 26:59 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8179/10000 [=======================>......] - ETA: 26:59 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8180/10000 [=======================>......] - ETA: 26:58 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8181/10000 [=======================>......] - ETA: 26:57 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8182/10000 [=======================>......] - ETA: 26:56 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8183/10000 [=======================>......] - ETA: 26:55 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8184/10000 [=======================>......] - ETA: 26:54 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8185/10000 [=======================>......] - ETA: 26:53 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8186/10000 [=======================>......] - ETA: 26:52 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8187/10000 [=======================>......] - ETA: 26:51 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8188/10000 [=======================>......] - ETA: 26:51 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8189/10000 [=======================>......] - ETA: 26:50 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8190/10000 [=======================>......] - ETA: 26:49 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8191/10000 [=======================>......] - ETA: 26:48 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8192/10000 [=======================>......] - ETA: 26:47 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8193/10000 [=======================>......] - ETA: 26:46 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8194/10000 [=======================>......] - ETA: 26:45 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8195/10000 [=======================>......] - ETA: 26:44 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8196/10000 [=======================>......] - ETA: 26:43 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8197/10000 [=======================>......] - ETA: 26:43 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0488
 8198/10000 [=======================>......] - ETA: 26:42 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8199/10000 [=======================>......] - ETA: 26:41 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8200/10000 [=======================>......] - ETA: 26:40 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8201/10000 [=======================>......] - ETA: 26:39 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8202/10000 [=======================>......] - ETA: 26:38 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0488
 8203/10000 [=======================>......] - ETA: 26:37 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0488
 8204/10000 [=======================>......] - ETA: 26:36 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8205/10000 [=======================>......] - ETA: 26:35 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8206/10000 [=======================>......] - ETA: 26:35 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8207/10000 [=======================>......] - ETA: 26:34 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8208/10000 [=======================>......] - ETA: 26:33 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0488
 8209/10000 [=======================>......] - ETA: 26:32 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0488
 8210/10000 [=======================>......] - ETA: 26:31 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8211/10000 [=======================>......] - ETA: 26:30 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8212/10000 [=======================>......] - ETA: 26:29 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8213/10000 [=======================>......] - ETA: 26:28 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0488
 8214/10000 [=======================>......] - ETA: 26:27 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0488
 8215/10000 [=======================>......] - ETA: 26:27 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8216/10000 [=======================>......] - ETA: 26:26 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8217/10000 [=======================>......] - ETA: 26:25 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8218/10000 [=======================>......] - ETA: 26:24 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0488
 8219/10000 [=======================>......] - ETA: 26:23 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0487
 8220/10000 [=======================>......] - ETA: 26:22 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0487
 8221/10000 [=======================>......] - ETA: 26:21 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0487
 8222/10000 [=======================>......] - ETA: 26:20 - loss: 0.4067 - regression_loss: 0.3580 - classification_loss: 0.0487
 8223/10000 [=======================>......] - ETA: 26:20 - loss: 0.4067 - regression_loss: 0.3580 - classification_loss: 0.0487
 8224/10000 [=======================>......] - ETA: 26:19 - loss: 0.4067 - regression_loss: 0.3580 - classification_loss: 0.0487
 8225/10000 [=======================>......] - ETA: 26:18 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0487
 8226/10000 [=======================>......] - ETA: 26:17 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0487
 8227/10000 [=======================>......] - ETA: 26:16 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0487
 8228/10000 [=======================>......] - ETA: 26:15 - loss: 0.4067 - regression_loss: 0.3580 - classification_loss: 0.0487
 8229/10000 [=======================>......] - ETA: 26:14 - loss: 0.4067 - regression_loss: 0.3580 - classification_loss: 0.0487
 8230/10000 [=======================>......] - ETA: 26:13 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0487
 8231/10000 [=======================>......] - ETA: 26:12 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0487
 8232/10000 [=======================>......] - ETA: 26:12 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0487
 8233/10000 [=======================>......] - ETA: 26:11 - loss: 0.4067 - regression_loss: 0.3580 - classification_loss: 0.0487
 8234/10000 [=======================>......] - ETA: 26:10 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0489
 8235/10000 [=======================>......] - ETA: 26:09 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 8236/10000 [=======================>......] - ETA: 26:08 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 8237/10000 [=======================>......] - ETA: 26:07 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8238/10000 [=======================>......] - ETA: 26:06 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0488
 8239/10000 [=======================>......] - ETA: 26:05 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0488
 8240/10000 [=======================>......] - ETA: 26:04 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8241/10000 [=======================>......] - ETA: 26:04 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0488
 8242/10000 [=======================>......] - ETA: 26:03 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0488
 8243/10000 [=======================>......] - ETA: 26:02 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8244/10000 [=======================>......] - ETA: 26:01 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8245/10000 [=======================>......] - ETA: 26:00 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8246/10000 [=======================>......] - ETA: 25:59 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8247/10000 [=======================>......] - ETA: 25:58 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8248/10000 [=======================>......] - ETA: 25:57 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8249/10000 [=======================>......] - ETA: 25:56 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8250/10000 [=======================>......] - ETA: 25:56 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8251/10000 [=======================>......] - ETA: 25:55 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8252/10000 [=======================>......] - ETA: 25:54 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0488
 8253/10000 [=======================>......] - ETA: 25:53 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8254/10000 [=======================>......] - ETA: 25:52 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8255/10000 [=======================>......] - ETA: 25:51 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8256/10000 [=======================>......] - ETA: 25:50 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8257/10000 [=======================>......] - ETA: 25:49 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8258/10000 [=======================>......] - ETA: 25:48 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8259/10000 [=======================>......] - ETA: 25:48 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8260/10000 [=======================>......] - ETA: 25:47 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8261/10000 [=======================>......] - ETA: 25:46 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8262/10000 [=======================>......] - ETA: 25:45 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8263/10000 [=======================>......] - ETA: 25:44 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8264/10000 [=======================>......] - ETA: 25:43 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8265/10000 [=======================>......] - ETA: 25:42 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8266/10000 [=======================>......] - ETA: 25:41 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8267/10000 [=======================>......] - ETA: 25:40 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8268/10000 [=======================>......] - ETA: 25:40 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8269/10000 [=======================>......] - ETA: 25:39 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8270/10000 [=======================>......] - ETA: 25:38 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8271/10000 [=======================>......] - ETA: 25:37 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8272/10000 [=======================>......] - ETA: 25:36 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8273/10000 [=======================>......] - ETA: 25:35 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0488
 8274/10000 [=======================>......] - ETA: 25:34 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8275/10000 [=======================>......] - ETA: 25:33 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8276/10000 [=======================>......] - ETA: 25:32 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8277/10000 [=======================>......] - ETA: 25:32 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8278/10000 [=======================>......] - ETA: 25:31 - loss: 0.4066 - regression_loss: 0.3577 - classification_loss: 0.0488
 8279/10000 [=======================>......] - ETA: 25:30 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8280/10000 [=======================>......] - ETA: 25:29 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8281/10000 [=======================>......] - ETA: 25:28 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8282/10000 [=======================>......] - ETA: 25:27 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8283/10000 [=======================>......] - ETA: 25:26 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8284/10000 [=======================>......] - ETA: 25:25 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8285/10000 [=======================>......] - ETA: 25:24 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8286/10000 [=======================>......] - ETA: 25:24 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8287/10000 [=======================>......] - ETA: 25:23 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8288/10000 [=======================>......] - ETA: 25:22 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8289/10000 [=======================>......] - ETA: 25:21 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8290/10000 [=======================>......] - ETA: 25:20 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8291/10000 [=======================>......] - ETA: 25:19 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 8292/10000 [=======================>......] - ETA: 25:18 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 8293/10000 [=======================>......] - ETA: 25:17 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 8294/10000 [=======================>......] - ETA: 25:16 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 8295/10000 [=======================>......] - ETA: 25:16 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 8296/10000 [=======================>......] - ETA: 25:15 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 8297/10000 [=======================>......] - ETA: 25:14 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 8298/10000 [=======================>......] - ETA: 25:13 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 8299/10000 [=======================>......] - ETA: 25:12 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 8300/10000 [=======================>......] - ETA: 25:11 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 8301/10000 [=======================>......] - ETA: 25:10 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 8302/10000 [=======================>......] - ETA: 25:09 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8303/10000 [=======================>......] - ETA: 25:08 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 8304/10000 [=======================>......] - ETA: 25:08 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 8305/10000 [=======================>......] - ETA: 25:07 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8306/10000 [=======================>......] - ETA: 25:06 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8307/10000 [=======================>......] - ETA: 25:05 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8308/10000 [=======================>......] - ETA: 25:04 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8309/10000 [=======================>......] - ETA: 25:03 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8310/10000 [=======================>......] - ETA: 25:02 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8311/10000 [=======================>......] - ETA: 25:01 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8312/10000 [=======================>......] - ETA: 25:00 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8313/10000 [=======================>......] - ETA: 25:00 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8314/10000 [=======================>......] - ETA: 24:59 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8315/10000 [=======================>......] - ETA: 24:58 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8316/10000 [=======================>......] - ETA: 24:57 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8317/10000 [=======================>......] - ETA: 24:56 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8318/10000 [=======================>......] - ETA: 24:55 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8319/10000 [=======================>......] - ETA: 24:54 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8320/10000 [=======================>......] - ETA: 24:53 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8321/10000 [=======================>......] - ETA: 24:52 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8322/10000 [=======================>......] - ETA: 24:52 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8323/10000 [=======================>......] - ETA: 24:51 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8324/10000 [=======================>......] - ETA: 24:50 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8325/10000 [=======================>......] - ETA: 24:49 - loss: 0.4067 - regression_loss: 0.3580 - classification_loss: 0.0488
 8326/10000 [=======================>......] - ETA: 24:48 - loss: 0.4067 - regression_loss: 0.3580 - classification_loss: 0.0488
 8327/10000 [=======================>......] - ETA: 24:47 - loss: 0.4067 - regression_loss: 0.3580 - classification_loss: 0.0488
 8328/10000 [=======================>......] - ETA: 24:46 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 8329/10000 [=======================>......] - ETA: 24:45 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 8330/10000 [=======================>......] - ETA: 24:44 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 8331/10000 [=======================>......] - ETA: 24:43 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 8332/10000 [=======================>......] - ETA: 24:43 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 8333/10000 [=======================>......] - ETA: 24:42 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 8334/10000 [========================>.....] - ETA: 24:41 - loss: 0.4069 - regression_loss: 0.3581 - classification_loss: 0.0488
 8335/10000 [========================>.....] - ETA: 24:40 - loss: 0.4069 - regression_loss: 0.3581 - classification_loss: 0.0488
 8336/10000 [========================>.....] - ETA: 24:39 - loss: 0.4069 - regression_loss: 0.3581 - classification_loss: 0.0488
 8337/10000 [========================>.....] - ETA: 24:38 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 8338/10000 [========================>.....] - ETA: 24:37 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 8339/10000 [========================>.....] - ETA: 24:36 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 8340/10000 [========================>.....] - ETA: 24:35 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 8341/10000 [========================>.....] - ETA: 24:35 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 8342/10000 [========================>.....] - ETA: 24:34 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 8343/10000 [========================>.....] - ETA: 24:33 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 8344/10000 [========================>.....] - ETA: 24:32 - loss: 0.4067 - regression_loss: 0.3580 - classification_loss: 0.0488
 8345/10000 [========================>.....] - ETA: 24:31 - loss: 0.4068 - regression_loss: 0.3580 - classification_loss: 0.0488
 8346/10000 [========================>.....] - ETA: 24:30 - loss: 0.4067 - regression_loss: 0.3580 - classification_loss: 0.0488
 8347/10000 [========================>.....] - ETA: 24:29 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8348/10000 [========================>.....] - ETA: 24:28 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8349/10000 [========================>.....] - ETA: 24:27 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8350/10000 [========================>.....] - ETA: 24:27 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0488
 8351/10000 [========================>.....] - ETA: 24:26 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8352/10000 [========================>.....] - ETA: 24:25 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8353/10000 [========================>.....] - ETA: 24:24 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8354/10000 [========================>.....] - ETA: 24:23 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0488
 8355/10000 [========================>.....] - ETA: 24:22 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8356/10000 [========================>.....] - ETA: 24:21 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8357/10000 [========================>.....] - ETA: 24:20 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8358/10000 [========================>.....] - ETA: 24:19 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0488
 8359/10000 [========================>.....] - ETA: 24:19 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0488
 8360/10000 [========================>.....] - ETA: 24:18 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8361/10000 [========================>.....] - ETA: 24:17 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8362/10000 [========================>.....] - ETA: 24:16 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8363/10000 [========================>.....] - ETA: 24:15 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8364/10000 [========================>.....] - ETA: 24:14 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8365/10000 [========================>.....] - ETA: 24:13 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0488
 8366/10000 [========================>.....] - ETA: 24:12 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8367/10000 [========================>.....] - ETA: 24:11 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8368/10000 [========================>.....] - ETA: 24:11 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8369/10000 [========================>.....] - ETA: 24:10 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8370/10000 [========================>.....] - ETA: 24:09 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8371/10000 [========================>.....] - ETA: 24:08 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8372/10000 [========================>.....] - ETA: 24:07 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8373/10000 [========================>.....] - ETA: 24:06 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8374/10000 [========================>.....] - ETA: 24:05 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8375/10000 [========================>.....] - ETA: 24:04 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0488
 8376/10000 [========================>.....] - ETA: 24:03 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8377/10000 [========================>.....] - ETA: 24:03 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8378/10000 [========================>.....] - ETA: 24:02 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8379/10000 [========================>.....] - ETA: 24:01 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8380/10000 [========================>.....] - ETA: 24:00 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8381/10000 [========================>.....] - ETA: 23:59 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8382/10000 [========================>.....] - ETA: 23:58 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8383/10000 [========================>.....] - ETA: 23:57 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8384/10000 [========================>.....] - ETA: 23:56 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8385/10000 [========================>.....] - ETA: 23:55 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8386/10000 [========================>.....] - ETA: 23:55 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8387/10000 [========================>.....] - ETA: 23:54 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8388/10000 [========================>.....] - ETA: 23:53 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8389/10000 [========================>.....] - ETA: 23:52 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8390/10000 [========================>.....] - ETA: 23:51 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8391/10000 [========================>.....] - ETA: 23:50 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8392/10000 [========================>.....] - ETA: 23:49 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8393/10000 [========================>.....] - ETA: 23:48 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8394/10000 [========================>.....] - ETA: 23:47 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8395/10000 [========================>.....] - ETA: 23:47 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8396/10000 [========================>.....] - ETA: 23:46 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8397/10000 [========================>.....] - ETA: 23:45 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8398/10000 [========================>.....] - ETA: 23:44 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8399/10000 [========================>.....] - ETA: 23:43 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8400/10000 [========================>.....] - ETA: 23:42 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8401/10000 [========================>.....] - ETA: 23:41 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8402/10000 [========================>.....] - ETA: 23:40 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8403/10000 [========================>.....] - ETA: 23:39 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8404/10000 [========================>.....] - ETA: 23:39 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8405/10000 [========================>.....] - ETA: 23:38 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8406/10000 [========================>.....] - ETA: 23:37 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8407/10000 [========================>.....] - ETA: 23:36 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8408/10000 [========================>.....] - ETA: 23:35 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8409/10000 [========================>.....] - ETA: 23:34 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8410/10000 [========================>.....] - ETA: 23:33 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8411/10000 [========================>.....] - ETA: 23:32 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8412/10000 [========================>.....] - ETA: 23:31 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8413/10000 [========================>.....] - ETA: 23:31 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8414/10000 [========================>.....] - ETA: 23:30 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8415/10000 [========================>.....] - ETA: 23:29 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8416/10000 [========================>.....] - ETA: 23:28 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8417/10000 [========================>.....] - ETA: 23:27 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0488
 8418/10000 [========================>.....] - ETA: 23:26 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0488
 8419/10000 [========================>.....] - ETA: 23:25 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8420/10000 [========================>.....] - ETA: 23:24 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8421/10000 [========================>.....] - ETA: 23:23 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8422/10000 [========================>.....] - ETA: 23:23 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8423/10000 [========================>.....] - ETA: 23:22 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8424/10000 [========================>.....] - ETA: 23:21 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8425/10000 [========================>.....] - ETA: 23:20 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8426/10000 [========================>.....] - ETA: 23:19 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8427/10000 [========================>.....] - ETA: 23:18 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8428/10000 [========================>.....] - ETA: 23:17 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8429/10000 [========================>.....] - ETA: 23:16 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8430/10000 [========================>.....] - ETA: 23:15 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8431/10000 [========================>.....] - ETA: 23:15 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8432/10000 [========================>.....] - ETA: 23:14 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0488
 8433/10000 [========================>.....] - ETA: 23:13 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8434/10000 [========================>.....] - ETA: 23:12 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8435/10000 [========================>.....] - ETA: 23:11 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8436/10000 [========================>.....] - ETA: 23:10 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8437/10000 [========================>.....] - ETA: 23:09 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8438/10000 [========================>.....] - ETA: 23:08 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8439/10000 [========================>.....] - ETA: 23:07 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0488
 8440/10000 [========================>.....] - ETA: 23:07 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0488
 8441/10000 [========================>.....] - ETA: 23:06 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8442/10000 [========================>.....] - ETA: 23:05 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8443/10000 [========================>.....] - ETA: 23:04 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8444/10000 [========================>.....] - ETA: 23:03 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8445/10000 [========================>.....] - ETA: 23:02 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8446/10000 [========================>.....] - ETA: 23:01 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8447/10000 [========================>.....] - ETA: 23:00 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8448/10000 [========================>.....] - ETA: 22:59 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8449/10000 [========================>.....] - ETA: 22:59 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8450/10000 [========================>.....] - ETA: 22:58 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8451/10000 [========================>.....] - ETA: 22:57 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8452/10000 [========================>.....] - ETA: 22:56 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8453/10000 [========================>.....] - ETA: 22:55 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8454/10000 [========================>.....] - ETA: 22:54 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8455/10000 [========================>.....] - ETA: 22:53 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8456/10000 [========================>.....] - ETA: 22:52 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8457/10000 [========================>.....] - ETA: 22:51 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8458/10000 [========================>.....] - ETA: 22:51 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8459/10000 [========================>.....] - ETA: 22:50 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8460/10000 [========================>.....] - ETA: 22:49 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8461/10000 [========================>.....] - ETA: 22:48 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8462/10000 [========================>.....] - ETA: 22:47 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8463/10000 [========================>.....] - ETA: 22:46 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8464/10000 [========================>.....] - ETA: 22:45 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8465/10000 [========================>.....] - ETA: 22:44 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8466/10000 [========================>.....] - ETA: 22:43 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8467/10000 [========================>.....] - ETA: 22:43 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8468/10000 [========================>.....] - ETA: 22:42 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0488
 8469/10000 [========================>.....] - ETA: 22:41 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8470/10000 [========================>.....] - ETA: 22:40 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8471/10000 [========================>.....] - ETA: 22:39 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0488
 8472/10000 [========================>.....] - ETA: 22:38 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8473/10000 [========================>.....] - ETA: 22:37 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8474/10000 [========================>.....] - ETA: 22:36 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8475/10000 [========================>.....] - ETA: 22:35 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8476/10000 [========================>.....] - ETA: 22:35 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0488
 8477/10000 [========================>.....] - ETA: 22:34 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0487
 8478/10000 [========================>.....] - ETA: 22:33 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0487
 8479/10000 [========================>.....] - ETA: 22:32 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8480/10000 [========================>.....] - ETA: 22:31 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8481/10000 [========================>.....] - ETA: 22:30 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0488
 8482/10000 [========================>.....] - ETA: 22:29 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8483/10000 [========================>.....] - ETA: 22:28 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0487
 8484/10000 [========================>.....] - ETA: 22:27 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0487
 8485/10000 [========================>.....] - ETA: 22:27 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8486/10000 [========================>.....] - ETA: 22:26 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8487/10000 [========================>.....] - ETA: 22:25 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8488/10000 [========================>.....] - ETA: 22:24 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8489/10000 [========================>.....] - ETA: 22:23 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8490/10000 [========================>.....] - ETA: 22:22 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8491/10000 [========================>.....] - ETA: 22:21 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8492/10000 [========================>.....] - ETA: 22:20 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8493/10000 [========================>.....] - ETA: 22:19 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8494/10000 [========================>.....] - ETA: 22:19 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8495/10000 [========================>.....] - ETA: 22:18 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8496/10000 [========================>.....] - ETA: 22:17 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8497/10000 [========================>.....] - ETA: 22:16 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8498/10000 [========================>.....] - ETA: 22:15 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8499/10000 [========================>.....] - ETA: 22:14 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8500/10000 [========================>.....] - ETA: 22:13 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0488
 8501/10000 [========================>.....] - ETA: 22:12 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8502/10000 [========================>.....] - ETA: 22:11 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8503/10000 [========================>.....] - ETA: 22:11 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8504/10000 [========================>.....] - ETA: 22:10 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8505/10000 [========================>.....] - ETA: 22:09 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8506/10000 [========================>.....] - ETA: 22:08 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8507/10000 [========================>.....] - ETA: 22:07 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8508/10000 [========================>.....] - ETA: 22:06 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8509/10000 [========================>.....] - ETA: 22:05 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8510/10000 [========================>.....] - ETA: 22:04 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0488
 8511/10000 [========================>.....] - ETA: 22:03 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8512/10000 [========================>.....] - ETA: 22:03 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0488
 8513/10000 [========================>.....] - ETA: 22:02 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0488
 8514/10000 [========================>.....] - ETA: 22:01 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8515/10000 [========================>.....] - ETA: 22:00 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0488
 8516/10000 [========================>.....] - ETA: 21:59 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0488
 8517/10000 [========================>.....] - ETA: 21:58 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0487
 8518/10000 [========================>.....] - ETA: 21:57 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8519/10000 [========================>.....] - ETA: 21:56 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8520/10000 [========================>.....] - ETA: 21:55 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0488
 8521/10000 [========================>.....] - ETA: 21:55 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0488
 8522/10000 [========================>.....] - ETA: 21:54 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8523/10000 [========================>.....] - ETA: 21:53 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0487
 8524/10000 [========================>.....] - ETA: 21:52 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8525/10000 [========================>.....] - ETA: 21:51 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0487
 8526/10000 [========================>.....] - ETA: 21:50 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8527/10000 [========================>.....] - ETA: 21:49 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8528/10000 [========================>.....] - ETA: 21:48 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8529/10000 [========================>.....] - ETA: 21:47 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8530/10000 [========================>.....] - ETA: 21:47 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8531/10000 [========================>.....] - ETA: 21:46 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0487
 8532/10000 [========================>.....] - ETA: 21:45 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0487
 8533/10000 [========================>.....] - ETA: 21:44 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8534/10000 [========================>.....] - ETA: 21:43 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8535/10000 [========================>.....] - ETA: 21:42 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0487
 8536/10000 [========================>.....] - ETA: 21:41 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8537/10000 [========================>.....] - ETA: 21:40 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8538/10000 [========================>.....] - ETA: 21:39 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8539/10000 [========================>.....] - ETA: 21:38 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 8540/10000 [========================>.....] - ETA: 21:38 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 8541/10000 [========================>.....] - ETA: 21:37 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0487
 8542/10000 [========================>.....] - ETA: 21:36 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 8543/10000 [========================>.....] - ETA: 21:35 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 8544/10000 [========================>.....] - ETA: 21:34 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 8545/10000 [========================>.....] - ETA: 21:33 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 8546/10000 [========================>.....] - ETA: 21:32 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 8547/10000 [========================>.....] - ETA: 21:31 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 8548/10000 [========================>.....] - ETA: 21:30 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 8549/10000 [========================>.....] - ETA: 21:30 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8550/10000 [========================>.....] - ETA: 21:29 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8551/10000 [========================>.....] - ETA: 21:28 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0488
 8552/10000 [========================>.....] - ETA: 21:27 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8553/10000 [========================>.....] - ETA: 21:26 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8554/10000 [========================>.....] - ETA: 21:25 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0488
 8555/10000 [========================>.....] - ETA: 21:24 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0488
 8556/10000 [========================>.....] - ETA: 21:23 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0488
 8557/10000 [========================>.....] - ETA: 21:22 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0488
 8558/10000 [========================>.....] - ETA: 21:22 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8559/10000 [========================>.....] - ETA: 21:21 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0488
 8560/10000 [========================>.....] - ETA: 21:20 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8561/10000 [========================>.....] - ETA: 21:19 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8562/10000 [========================>.....] - ETA: 21:18 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8563/10000 [========================>.....] - ETA: 21:17 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8564/10000 [========================>.....] - ETA: 21:16 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8565/10000 [========================>.....] - ETA: 21:15 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0488
 8566/10000 [========================>.....] - ETA: 21:14 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0488
 8567/10000 [========================>.....] - ETA: 21:14 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0488
 8568/10000 [========================>.....] - ETA: 21:13 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8569/10000 [========================>.....] - ETA: 21:12 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8570/10000 [========================>.....] - ETA: 21:11 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0488
 8571/10000 [========================>.....] - ETA: 21:10 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8572/10000 [========================>.....] - ETA: 21:09 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0488
 8573/10000 [========================>.....] - ETA: 21:08 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0488
 8574/10000 [========================>.....] - ETA: 21:07 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8575/10000 [========================>.....] - ETA: 21:06 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8576/10000 [========================>.....] - ETA: 21:06 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0487
 8577/10000 [========================>.....] - ETA: 21:05 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 8578/10000 [========================>.....] - ETA: 21:04 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0487
 8579/10000 [========================>.....] - ETA: 21:03 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 8580/10000 [========================>.....] - ETA: 21:02 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 8581/10000 [========================>.....] - ETA: 21:01 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 8582/10000 [========================>.....] - ETA: 21:00 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8583/10000 [========================>.....] - ETA: 20:59 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0487
 8584/10000 [========================>.....] - ETA: 20:58 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8585/10000 [========================>.....] - ETA: 20:58 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8586/10000 [========================>.....] - ETA: 20:57 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8587/10000 [========================>.....] - ETA: 20:56 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0487
 8588/10000 [========================>.....] - ETA: 20:55 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8589/10000 [========================>.....] - ETA: 20:54 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8590/10000 [========================>.....] - ETA: 20:53 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8591/10000 [========================>.....] - ETA: 20:52 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8592/10000 [========================>.....] - ETA: 20:51 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0487
 8593/10000 [========================>.....] - ETA: 20:50 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0487
 8594/10000 [========================>.....] - ETA: 20:50 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8595/10000 [========================>.....] - ETA: 20:49 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0487
 8596/10000 [========================>.....] - ETA: 20:48 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8597/10000 [========================>.....] - ETA: 20:47 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0487
 8598/10000 [========================>.....] - ETA: 20:46 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8599/10000 [========================>.....] - ETA: 20:45 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8600/10000 [========================>.....] - ETA: 20:44 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8601/10000 [========================>.....] - ETA: 20:43 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0487
 8602/10000 [========================>.....] - ETA: 20:42 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8603/10000 [========================>.....] - ETA: 20:42 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8604/10000 [========================>.....] - ETA: 20:41 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8605/10000 [========================>.....] - ETA: 20:40 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8606/10000 [========================>.....] - ETA: 20:39 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8607/10000 [========================>.....] - ETA: 20:38 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8608/10000 [========================>.....] - ETA: 20:37 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8609/10000 [========================>.....] - ETA: 20:36 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8610/10000 [========================>.....] - ETA: 20:35 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8611/10000 [========================>.....] - ETA: 20:34 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8612/10000 [========================>.....] - ETA: 20:34 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8613/10000 [========================>.....] - ETA: 20:33 - loss: 0.4064 - regression_loss: 0.3578 - classification_loss: 0.0487
 8614/10000 [========================>.....] - ETA: 20:32 - loss: 0.4064 - regression_loss: 0.3578 - classification_loss: 0.0487
 8615/10000 [========================>.....] - ETA: 20:31 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8616/10000 [========================>.....] - ETA: 20:30 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8617/10000 [========================>.....] - ETA: 20:29 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8618/10000 [========================>.....] - ETA: 20:28 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8619/10000 [========================>.....] - ETA: 20:27 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8620/10000 [========================>.....] - ETA: 20:26 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8621/10000 [========================>.....] - ETA: 20:26 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8622/10000 [========================>.....] - ETA: 20:25 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8623/10000 [========================>.....] - ETA: 20:24 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0487
 8624/10000 [========================>.....] - ETA: 20:23 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8625/10000 [========================>.....] - ETA: 20:22 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8626/10000 [========================>.....] - ETA: 20:21 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0487
 8627/10000 [========================>.....] - ETA: 20:20 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0487
 8628/10000 [========================>.....] - ETA: 20:19 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0487
 8629/10000 [========================>.....] - ETA: 20:18 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0487
 8630/10000 [========================>.....] - ETA: 20:18 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0487
 8631/10000 [========================>.....] - ETA: 20:17 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8632/10000 [========================>.....] - ETA: 20:16 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8633/10000 [========================>.....] - ETA: 20:15 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0487
 8634/10000 [========================>.....] - ETA: 20:14 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0487
 8635/10000 [========================>.....] - ETA: 20:13 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0487
 8636/10000 [========================>.....] - ETA: 20:12 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0487
 8637/10000 [========================>.....] - ETA: 20:11 - loss: 0.4065 - regression_loss: 0.3579 - classification_loss: 0.0487
 8638/10000 [========================>.....] - ETA: 20:10 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0487
 8639/10000 [========================>.....] - ETA: 20:10 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0487
 8640/10000 [========================>.....] - ETA: 20:09 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0487
 8641/10000 [========================>.....] - ETA: 20:08 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0487
 8642/10000 [========================>.....] - ETA: 20:07 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0487
 8643/10000 [========================>.....] - ETA: 20:06 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0487
 8644/10000 [========================>.....] - ETA: 20:05 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0487
 8645/10000 [========================>.....] - ETA: 20:04 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8646/10000 [========================>.....] - ETA: 20:03 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8647/10000 [========================>.....] - ETA: 20:02 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0487
 8648/10000 [========================>.....] - ETA: 20:02 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8649/10000 [========================>.....] - ETA: 20:01 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8650/10000 [========================>.....] - ETA: 20:00 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8651/10000 [========================>.....] - ETA: 19:59 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8652/10000 [========================>.....] - ETA: 19:58 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8653/10000 [========================>.....] - ETA: 19:57 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8654/10000 [========================>.....] - ETA: 19:56 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 8655/10000 [========================>.....] - ETA: 19:55 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8656/10000 [========================>.....] - ETA: 19:54 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8657/10000 [========================>.....] - ETA: 19:54 - loss: 0.4063 - regression_loss: 0.3577 - classification_loss: 0.0487
 8658/10000 [========================>.....] - ETA: 19:53 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 8659/10000 [========================>.....] - ETA: 19:52 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 8660/10000 [========================>.....] - ETA: 19:51 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 8661/10000 [========================>.....] - ETA: 19:50 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 8662/10000 [========================>.....] - ETA: 19:49 - loss: 0.4062 - regression_loss: 0.3576 - classification_loss: 0.0487
 8663/10000 [========================>.....] - ETA: 19:48 - loss: 0.4062 - regression_loss: 0.3575 - classification_loss: 0.0487
 8664/10000 [========================>.....] - ETA: 19:47 - loss: 0.4062 - regression_loss: 0.3576 - classification_loss: 0.0487
 8665/10000 [========================>.....] - ETA: 19:46 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 8666/10000 [========================>.....] - ETA: 19:46 - loss: 0.4062 - regression_loss: 0.3576 - classification_loss: 0.0487
 8667/10000 [=========================>....] - ETA: 19:45 - loss: 0.4062 - regression_loss: 0.3575 - classification_loss: 0.0487
 8668/10000 [=========================>....] - ETA: 19:44 - loss: 0.4062 - regression_loss: 0.3575 - classification_loss: 0.0487
 8669/10000 [=========================>....] - ETA: 19:43 - loss: 0.4062 - regression_loss: 0.3575 - classification_loss: 0.0487
 8670/10000 [=========================>....] - ETA: 19:42 - loss: 0.4061 - regression_loss: 0.3575 - classification_loss: 0.0487
 8671/10000 [=========================>....] - ETA: 19:41 - loss: 0.4062 - regression_loss: 0.3575 - classification_loss: 0.0487
 8672/10000 [=========================>....] - ETA: 19:40 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 8673/10000 [=========================>....] - ETA: 19:39 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 8674/10000 [=========================>....] - ETA: 19:38 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 8675/10000 [=========================>....] - ETA: 19:38 - loss: 0.4062 - regression_loss: 0.3575 - classification_loss: 0.0487
 8676/10000 [=========================>....] - ETA: 19:37 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8677/10000 [=========================>....] - ETA: 19:36 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8678/10000 [=========================>....] - ETA: 19:35 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8679/10000 [=========================>....] - ETA: 19:34 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8680/10000 [=========================>....] - ETA: 19:33 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8681/10000 [=========================>....] - ETA: 19:32 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8682/10000 [=========================>....] - ETA: 19:31 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0488
 8683/10000 [=========================>....] - ETA: 19:30 - loss: 0.4063 - regression_loss: 0.3575 - classification_loss: 0.0488
 8684/10000 [=========================>....] - ETA: 19:30 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8685/10000 [=========================>....] - ETA: 19:29 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8686/10000 [=========================>....] - ETA: 19:28 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8687/10000 [=========================>....] - ETA: 19:27 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8688/10000 [=========================>....] - ETA: 19:26 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8689/10000 [=========================>....] - ETA: 19:25 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8690/10000 [=========================>....] - ETA: 19:24 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8691/10000 [=========================>....] - ETA: 19:23 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8692/10000 [=========================>....] - ETA: 19:22 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8693/10000 [=========================>....] - ETA: 19:22 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8694/10000 [=========================>....] - ETA: 19:21 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8695/10000 [=========================>....] - ETA: 19:20 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8696/10000 [=========================>....] - ETA: 19:19 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8697/10000 [=========================>....] - ETA: 19:18 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0488
 8698/10000 [=========================>....] - ETA: 19:17 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8699/10000 [=========================>....] - ETA: 19:16 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8700/10000 [=========================>....] - ETA: 19:15 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0488
 8701/10000 [=========================>....] - ETA: 19:14 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8702/10000 [=========================>....] - ETA: 19:14 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8703/10000 [=========================>....] - ETA: 19:13 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8704/10000 [=========================>....] - ETA: 19:12 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8705/10000 [=========================>....] - ETA: 19:11 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8706/10000 [=========================>....] - ETA: 19:10 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8707/10000 [=========================>....] - ETA: 19:09 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8708/10000 [=========================>....] - ETA: 19:08 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8709/10000 [=========================>....] - ETA: 19:07 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8710/10000 [=========================>....] - ETA: 19:06 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8711/10000 [=========================>....] - ETA: 19:06 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8712/10000 [=========================>....] - ETA: 19:05 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8713/10000 [=========================>....] - ETA: 19:04 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8714/10000 [=========================>....] - ETA: 19:03 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8715/10000 [=========================>....] - ETA: 19:02 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8716/10000 [=========================>....] - ETA: 19:01 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8717/10000 [=========================>....] - ETA: 19:00 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8718/10000 [=========================>....] - ETA: 18:59 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8719/10000 [=========================>....] - ETA: 18:58 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8720/10000 [=========================>....] - ETA: 18:58 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8721/10000 [=========================>....] - ETA: 18:57 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8722/10000 [=========================>....] - ETA: 18:56 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8723/10000 [=========================>....] - ETA: 18:55 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8724/10000 [=========================>....] - ETA: 18:54 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8725/10000 [=========================>....] - ETA: 18:53 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8726/10000 [=========================>....] - ETA: 18:52 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8727/10000 [=========================>....] - ETA: 18:51 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8728/10000 [=========================>....] - ETA: 18:50 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8729/10000 [=========================>....] - ETA: 18:50 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8730/10000 [=========================>....] - ETA: 18:49 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8731/10000 [=========================>....] - ETA: 18:48 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8732/10000 [=========================>....] - ETA: 18:47 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8733/10000 [=========================>....] - ETA: 18:46 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8734/10000 [=========================>....] - ETA: 18:45 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8735/10000 [=========================>....] - ETA: 18:44 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8736/10000 [=========================>....] - ETA: 18:43 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8737/10000 [=========================>....] - ETA: 18:42 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8738/10000 [=========================>....] - ETA: 18:42 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8739/10000 [=========================>....] - ETA: 18:41 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8740/10000 [=========================>....] - ETA: 18:40 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8741/10000 [=========================>....] - ETA: 18:39 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8742/10000 [=========================>....] - ETA: 18:38 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0488
 8743/10000 [=========================>....] - ETA: 18:37 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0488
 8744/10000 [=========================>....] - ETA: 18:36 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0488
 8745/10000 [=========================>....] - ETA: 18:35 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8746/10000 [=========================>....] - ETA: 18:34 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8747/10000 [=========================>....] - ETA: 18:34 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0488
 8748/10000 [=========================>....] - ETA: 18:33 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8749/10000 [=========================>....] - ETA: 18:32 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8750/10000 [=========================>....] - ETA: 18:31 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0488
 8751/10000 [=========================>....] - ETA: 18:30 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0488
 8752/10000 [=========================>....] - ETA: 18:29 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8753/10000 [=========================>....] - ETA: 18:28 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0488
 8754/10000 [=========================>....] - ETA: 18:27 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0488
 8755/10000 [=========================>....] - ETA: 18:26 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8756/10000 [=========================>....] - ETA: 18:26 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8757/10000 [=========================>....] - ETA: 18:25 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8758/10000 [=========================>....] - ETA: 18:24 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8759/10000 [=========================>....] - ETA: 18:23 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8760/10000 [=========================>....] - ETA: 18:22 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8761/10000 [=========================>....] - ETA: 18:21 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8762/10000 [=========================>....] - ETA: 18:20 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8763/10000 [=========================>....] - ETA: 18:19 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0487
 8764/10000 [=========================>....] - ETA: 18:18 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8765/10000 [=========================>....] - ETA: 18:18 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8766/10000 [=========================>....] - ETA: 18:17 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0487
 8767/10000 [=========================>....] - ETA: 18:16 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8768/10000 [=========================>....] - ETA: 18:15 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8769/10000 [=========================>....] - ETA: 18:14 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8770/10000 [=========================>....] - ETA: 18:13 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8771/10000 [=========================>....] - ETA: 18:12 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8772/10000 [=========================>....] - ETA: 18:11 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8773/10000 [=========================>....] - ETA: 18:10 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8774/10000 [=========================>....] - ETA: 18:10 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8775/10000 [=========================>....] - ETA: 18:09 - loss: 0.4064 - regression_loss: 0.3576 - classification_loss: 0.0487
 8776/10000 [=========================>....] - ETA: 18:08 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 8777/10000 [=========================>....] - ETA: 18:07 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 8778/10000 [=========================>....] - ETA: 18:06 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 8779/10000 [=========================>....] - ETA: 18:05 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 8780/10000 [=========================>....] - ETA: 18:04 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8781/10000 [=========================>....] - ETA: 18:03 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8782/10000 [=========================>....] - ETA: 18:02 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8783/10000 [=========================>....] - ETA: 18:02 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8784/10000 [=========================>....] - ETA: 18:01 - loss: 0.4063 - regression_loss: 0.3576 - classification_loss: 0.0487
 8785/10000 [=========================>....] - ETA: 18:00 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8786/10000 [=========================>....] - ETA: 17:59 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8787/10000 [=========================>....] - ETA: 17:58 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8788/10000 [=========================>....] - ETA: 17:57 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8789/10000 [=========================>....] - ETA: 17:56 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8790/10000 [=========================>....] - ETA: 17:55 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8791/10000 [=========================>....] - ETA: 17:54 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8792/10000 [=========================>....] - ETA: 17:54 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8793/10000 [=========================>....] - ETA: 17:53 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0488
 8794/10000 [=========================>....] - ETA: 17:52 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8795/10000 [=========================>....] - ETA: 17:51 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8796/10000 [=========================>....] - ETA: 17:50 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8797/10000 [=========================>....] - ETA: 17:49 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8798/10000 [=========================>....] - ETA: 17:48 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8799/10000 [=========================>....] - ETA: 17:47 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0488
 8800/10000 [=========================>....] - ETA: 17:46 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8801/10000 [=========================>....] - ETA: 17:46 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8802/10000 [=========================>....] - ETA: 17:45 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0488
 8803/10000 [=========================>....] - ETA: 17:44 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8804/10000 [=========================>....] - ETA: 17:43 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0488
 8805/10000 [=========================>....] - ETA: 17:42 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0488
 8806/10000 [=========================>....] - ETA: 17:41 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0488
 8807/10000 [=========================>....] - ETA: 17:40 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0488
 8808/10000 [=========================>....] - ETA: 17:39 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0488
 8809/10000 [=========================>....] - ETA: 17:38 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8810/10000 [=========================>....] - ETA: 17:38 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8811/10000 [=========================>....] - ETA: 17:37 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8812/10000 [=========================>....] - ETA: 17:36 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8813/10000 [=========================>....] - ETA: 17:35 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8814/10000 [=========================>....] - ETA: 17:34 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0488
 8815/10000 [=========================>....] - ETA: 17:33 - loss: 0.4066 - regression_loss: 0.3579 - classification_loss: 0.0487
 8816/10000 [=========================>....] - ETA: 17:32 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0487
 8817/10000 [=========================>....] - ETA: 17:31 - loss: 0.4066 - regression_loss: 0.3578 - classification_loss: 0.0487
 8818/10000 [=========================>....] - ETA: 17:30 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8819/10000 [=========================>....] - ETA: 17:30 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8820/10000 [=========================>....] - ETA: 17:29 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8821/10000 [=========================>....] - ETA: 17:28 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8822/10000 [=========================>....] - ETA: 17:27 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8823/10000 [=========================>....] - ETA: 17:26 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0487
 8824/10000 [=========================>....] - ETA: 17:25 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8825/10000 [=========================>....] - ETA: 17:24 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0487
 8826/10000 [=========================>....] - ETA: 17:23 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8827/10000 [=========================>....] - ETA: 17:22 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8828/10000 [=========================>....] - ETA: 17:22 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8829/10000 [=========================>....] - ETA: 17:21 - loss: 0.4064 - regression_loss: 0.3577 - classification_loss: 0.0487
 8830/10000 [=========================>....] - ETA: 17:20 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0487
 8831/10000 [=========================>....] - ETA: 17:19 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8832/10000 [=========================>....] - ETA: 17:18 - loss: 0.4065 - regression_loss: 0.3577 - classification_loss: 0.0487
 8833/10000 [=========================>....] - ETA: 17:17 - loss: 0.4065 - regression_loss: 0.3578 - classification_loss: 0.0487
 8834/10000 [=========================>....] - ETA: 17:16 - loss: 0.4068 - regression_loss: 0.3579 - classification_loss: 0.0489
 8835/10000 [=========================>....] - ETA: 17:15 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0489
 8836/10000 [=========================>....] - ETA: 17:14 - loss: 0.4067 - regression_loss: 0.3579 - classification_loss: 0.0489
 8837/10000 [=========================>....] - ETA: 17:13 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 8838/10000 [=========================>....] - ETA: 17:13 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 8839/10000 [=========================>....] - ETA: 17:12 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 8840/10000 [=========================>....] - ETA: 17:11 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 8841/10000 [=========================>....] - ETA: 17:10 - loss: 0.4067 - regression_loss: 0.3578 - classification_loss: 0.0489
 8842/10000 [=========================>....] - ETA: 17:09 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 8843/10000 [=========================>....] - ETA: 17:08 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 8844/10000 [=========================>....] - ETA: 17:07 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 8845/10000 [=========================>....] - ETA: 17:06 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 8846/10000 [=========================>....] - ETA: 17:05 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 8847/10000 [=========================>....] - ETA: 17:05 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 8848/10000 [=========================>....] - ETA: 17:04 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8849/10000 [=========================>....] - ETA: 17:03 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8850/10000 [=========================>....] - ETA: 17:02 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8851/10000 [=========================>....] - ETA: 17:01 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8852/10000 [=========================>....] - ETA: 17:00 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8853/10000 [=========================>....] - ETA: 16:59 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8854/10000 [=========================>....] - ETA: 16:58 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8855/10000 [=========================>....] - ETA: 16:57 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8856/10000 [=========================>....] - ETA: 16:57 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8857/10000 [=========================>....] - ETA: 16:56 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8858/10000 [=========================>....] - ETA: 16:55 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8859/10000 [=========================>....] - ETA: 16:54 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8860/10000 [=========================>....] - ETA: 16:53 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8861/10000 [=========================>....] - ETA: 16:52 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8862/10000 [=========================>....] - ETA: 16:51 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8863/10000 [=========================>....] - ETA: 16:50 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8864/10000 [=========================>....] - ETA: 16:49 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8865/10000 [=========================>....] - ETA: 16:49 - loss: 0.4069 - regression_loss: 0.3580 - classification_loss: 0.0490
 8866/10000 [=========================>....] - ETA: 16:48 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8867/10000 [=========================>....] - ETA: 16:47 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8868/10000 [=========================>....] - ETA: 16:46 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8869/10000 [=========================>....] - ETA: 16:45 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 8870/10000 [=========================>....] - ETA: 16:44 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0490
 8871/10000 [=========================>....] - ETA: 16:43 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0490
 8872/10000 [=========================>....] - ETA: 16:42 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0490
 8873/10000 [=========================>....] - ETA: 16:41 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0490
 8874/10000 [=========================>....] - ETA: 16:41 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0490
 8875/10000 [=========================>....] - ETA: 16:40 - loss: 0.4072 - regression_loss: 0.3582 - classification_loss: 0.0490
 8876/10000 [=========================>....] - ETA: 16:39 - loss: 0.4072 - regression_loss: 0.3581 - classification_loss: 0.0490
 8877/10000 [=========================>....] - ETA: 16:38 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 8878/10000 [=========================>....] - ETA: 16:37 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 8879/10000 [=========================>....] - ETA: 16:36 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 8880/10000 [=========================>....] - ETA: 16:35 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 8881/10000 [=========================>....] - ETA: 16:34 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 8882/10000 [=========================>....] - ETA: 16:33 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 8883/10000 [=========================>....] - ETA: 16:33 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 8884/10000 [=========================>....] - ETA: 16:32 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 8885/10000 [=========================>....] - ETA: 16:31 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 8886/10000 [=========================>....] - ETA: 16:30 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 8887/10000 [=========================>....] - ETA: 16:29 - loss: 0.4071 - regression_loss: 0.3581 - classification_loss: 0.0490
 8888/10000 [=========================>....] - ETA: 16:28 - loss: 0.4071 - regression_loss: 0.3580 - classification_loss: 0.0490
 8889/10000 [=========================>....] - ETA: 16:27 - loss: 0.4071 - regression_loss: 0.3580 - classification_loss: 0.0490
 8890/10000 [=========================>....] - ETA: 16:26 - loss: 0.4071 - regression_loss: 0.3580 - classification_loss: 0.0490
 8891/10000 [=========================>....] - ETA: 16:25 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 8892/10000 [=========================>....] - ETA: 16:25 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 8893/10000 [=========================>....] - ETA: 16:24 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 8894/10000 [=========================>....] - ETA: 16:23 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 8895/10000 [=========================>....] - ETA: 16:22 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 8896/10000 [=========================>....] - ETA: 16:21 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 8897/10000 [=========================>....] - ETA: 16:20 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 8898/10000 [=========================>....] - ETA: 16:19 - loss: 0.4070 - regression_loss: 0.3580 - classification_loss: 0.0490
 8899/10000 [=========================>....] - ETA: 16:18 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8900/10000 [=========================>....] - ETA: 16:17 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8901/10000 [=========================>....] - ETA: 16:17 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8902/10000 [=========================>....] - ETA: 16:16 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8903/10000 [=========================>....] - ETA: 16:15 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8904/10000 [=========================>....] - ETA: 16:14 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8905/10000 [=========================>....] - ETA: 16:13 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8906/10000 [=========================>....] - ETA: 16:12 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0490
 8907/10000 [=========================>....] - ETA: 16:11 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8908/10000 [=========================>....] - ETA: 16:10 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8909/10000 [=========================>....] - ETA: 16:09 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8910/10000 [=========================>....] - ETA: 16:09 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8911/10000 [=========================>....] - ETA: 16:08 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8912/10000 [=========================>....] - ETA: 16:07 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8913/10000 [=========================>....] - ETA: 16:06 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 8914/10000 [=========================>....] - ETA: 16:05 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 8915/10000 [=========================>....] - ETA: 16:04 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0490
 8916/10000 [=========================>....] - ETA: 16:03 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0490
 8917/10000 [=========================>....] - ETA: 16:02 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 8918/10000 [=========================>....] - ETA: 16:01 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 8919/10000 [=========================>....] - ETA: 16:01 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 8920/10000 [=========================>....] - ETA: 16:00 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 8921/10000 [=========================>....] - ETA: 15:59 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0490
 8922/10000 [=========================>....] - ETA: 15:58 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8923/10000 [=========================>....] - ETA: 15:57 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0491
 8924/10000 [=========================>....] - ETA: 15:56 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0491
 8925/10000 [=========================>....] - ETA: 15:55 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0490
 8926/10000 [=========================>....] - ETA: 15:54 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0490
 8927/10000 [=========================>....] - ETA: 15:53 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0490
 8928/10000 [=========================>....] - ETA: 15:53 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0490
 8929/10000 [=========================>....] - ETA: 15:52 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0490
 8930/10000 [=========================>....] - ETA: 15:51 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0490
 8931/10000 [=========================>....] - ETA: 15:50 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 8932/10000 [=========================>....] - ETA: 15:49 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 8933/10000 [=========================>....] - ETA: 15:48 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 8934/10000 [=========================>....] - ETA: 15:47 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8935/10000 [=========================>....] - ETA: 15:46 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8936/10000 [=========================>....] - ETA: 15:45 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8937/10000 [=========================>....] - ETA: 15:45 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8938/10000 [=========================>....] - ETA: 15:44 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8939/10000 [=========================>....] - ETA: 15:43 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0490
 8940/10000 [=========================>....] - ETA: 15:42 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8941/10000 [=========================>....] - ETA: 15:41 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8942/10000 [=========================>....] - ETA: 15:40 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8943/10000 [=========================>....] - ETA: 15:39 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8944/10000 [=========================>....] - ETA: 15:38 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8945/10000 [=========================>....] - ETA: 15:37 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8946/10000 [=========================>....] - ETA: 15:37 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0490
 8947/10000 [=========================>....] - ETA: 15:36 - loss: 0.4069 - regression_loss: 0.3579 - classification_loss: 0.0490
 8948/10000 [=========================>....] - ETA: 15:35 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 8949/10000 [=========================>....] - ETA: 15:34 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 8950/10000 [=========================>....] - ETA: 15:33 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 8951/10000 [=========================>....] - ETA: 15:32 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 8952/10000 [=========================>....] - ETA: 15:31 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 8953/10000 [=========================>....] - ETA: 15:30 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 8954/10000 [=========================>....] - ETA: 15:29 - loss: 0.4068 - regression_loss: 0.3578 - classification_loss: 0.0490
 8955/10000 [=========================>....] - ETA: 15:29 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 8956/10000 [=========================>....] - ETA: 15:28 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 8957/10000 [=========================>....] - ETA: 15:27 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 8958/10000 [=========================>....] - ETA: 15:26 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 8959/10000 [=========================>....] - ETA: 15:25 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 8960/10000 [=========================>....] - ETA: 15:24 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 8961/10000 [=========================>....] - ETA: 15:23 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 8962/10000 [=========================>....] - ETA: 15:22 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 8963/10000 [=========================>....] - ETA: 15:21 - loss: 0.4072 - regression_loss: 0.3581 - classification_loss: 0.0492
 8964/10000 [=========================>....] - ETA: 15:21 - loss: 0.4072 - regression_loss: 0.3581 - classification_loss: 0.0492
 8965/10000 [=========================>....] - ETA: 15:20 - loss: 0.4072 - regression_loss: 0.3581 - classification_loss: 0.0492
 8966/10000 [=========================>....] - ETA: 15:19 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 8967/10000 [=========================>....] - ETA: 15:18 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 8968/10000 [=========================>....] - ETA: 15:17 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 8969/10000 [=========================>....] - ETA: 15:16 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 8970/10000 [=========================>....] - ETA: 15:15 - loss: 0.4071 - regression_loss: 0.3580 - classification_loss: 0.0492
 8971/10000 [=========================>....] - ETA: 15:14 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 8972/10000 [=========================>....] - ETA: 15:13 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 8973/10000 [=========================>....] - ETA: 15:13 - loss: 0.4071 - regression_loss: 0.3580 - classification_loss: 0.0492
 8974/10000 [=========================>....] - ETA: 15:12 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 8975/10000 [=========================>....] - ETA: 15:11 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0491
 8976/10000 [=========================>....] - ETA: 15:10 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 8977/10000 [=========================>....] - ETA: 15:09 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 8978/10000 [=========================>....] - ETA: 15:08 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 8979/10000 [=========================>....] - ETA: 15:07 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 8980/10000 [=========================>....] - ETA: 15:06 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0491
 8981/10000 [=========================>....] - ETA: 15:05 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0491
 8982/10000 [=========================>....] - ETA: 15:05 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 8983/10000 [=========================>....] - ETA: 15:04 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 8984/10000 [=========================>....] - ETA: 15:03 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 8985/10000 [=========================>....] - ETA: 15:02 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0491
 8986/10000 [=========================>....] - ETA: 15:01 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0491
 8987/10000 [=========================>....] - ETA: 15:00 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0491
 8988/10000 [=========================>....] - ETA: 14:59 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 8989/10000 [=========================>....] - ETA: 14:58 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 8990/10000 [=========================>....] - ETA: 14:57 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0491
 8991/10000 [=========================>....] - ETA: 14:57 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0491
 8992/10000 [=========================>....] - ETA: 14:56 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 8993/10000 [=========================>....] - ETA: 14:55 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 8994/10000 [=========================>....] - ETA: 14:54 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 8995/10000 [=========================>....] - ETA: 14:53 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 8996/10000 [=========================>....] - ETA: 14:52 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 8997/10000 [=========================>....] - ETA: 14:51 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 8998/10000 [=========================>....] - ETA: 14:50 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 8999/10000 [=========================>....] - ETA: 14:49 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 9000/10000 [==========================>...] - ETA: 14:49 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 9001/10000 [==========================>...] - ETA: 14:48 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0491
 9002/10000 [==========================>...] - ETA: 14:47 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 9003/10000 [==========================>...] - ETA: 14:46 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 9004/10000 [==========================>...] - ETA: 14:45 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 9005/10000 [==========================>...] - ETA: 14:44 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 9006/10000 [==========================>...] - ETA: 14:43 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 9007/10000 [==========================>...] - ETA: 14:42 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 9008/10000 [==========================>...] - ETA: 14:41 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0491
 9009/10000 [==========================>...] - ETA: 14:41 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0491
 9010/10000 [==========================>...] - ETA: 14:40 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 9011/10000 [==========================>...] - ETA: 14:39 - loss: 0.4070 - regression_loss: 0.3579 - classification_loss: 0.0491
 9012/10000 [==========================>...] - ETA: 14:38 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0491
 9013/10000 [==========================>...] - ETA: 14:37 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 9014/10000 [==========================>...] - ETA: 14:36 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 9015/10000 [==========================>...] - ETA: 14:35 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 9016/10000 [==========================>...] - ETA: 14:34 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 9017/10000 [==========================>...] - ETA: 14:33 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 9018/10000 [==========================>...] - ETA: 14:33 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 9019/10000 [==========================>...] - ETA: 14:32 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 9020/10000 [==========================>...] - ETA: 14:31 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 9021/10000 [==========================>...] - ETA: 14:30 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 9022/10000 [==========================>...] - ETA: 14:29 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 9023/10000 [==========================>...] - ETA: 14:28 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 9024/10000 [==========================>...] - ETA: 14:27 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 9025/10000 [==========================>...] - ETA: 14:26 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 9026/10000 [==========================>...] - ETA: 14:25 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0491
 9027/10000 [==========================>...] - ETA: 14:25 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 9028/10000 [==========================>...] - ETA: 14:24 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 9029/10000 [==========================>...] - ETA: 14:23 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 9030/10000 [==========================>...] - ETA: 14:22 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 9031/10000 [==========================>...] - ETA: 14:21 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 9032/10000 [==========================>...] - ETA: 14:20 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 9033/10000 [==========================>...] - ETA: 14:19 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0491
 9034/10000 [==========================>...] - ETA: 14:18 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 9035/10000 [==========================>...] - ETA: 14:17 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0491
 9036/10000 [==========================>...] - ETA: 14:17 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 9037/10000 [==========================>...] - ETA: 14:16 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 9038/10000 [==========================>...] - ETA: 14:15 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 9039/10000 [==========================>...] - ETA: 14:14 - loss: 0.4067 - regression_loss: 0.3577 - classification_loss: 0.0491
 9040/10000 [==========================>...] - ETA: 14:13 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0491
 9041/10000 [==========================>...] - ETA: 14:12 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9042/10000 [==========================>...] - ETA: 14:11 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9043/10000 [==========================>...] - ETA: 14:10 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9044/10000 [==========================>...] - ETA: 14:09 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9045/10000 [==========================>...] - ETA: 14:09 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9046/10000 [==========================>...] - ETA: 14:08 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9047/10000 [==========================>...] - ETA: 14:07 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9048/10000 [==========================>...] - ETA: 14:06 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9049/10000 [==========================>...] - ETA: 14:05 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9050/10000 [==========================>...] - ETA: 14:04 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9051/10000 [==========================>...] - ETA: 14:03 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9052/10000 [==========================>...] - ETA: 14:02 - loss: 0.4070 - regression_loss: 0.3577 - classification_loss: 0.0492
 9053/10000 [==========================>...] - ETA: 14:01 - loss: 0.4070 - regression_loss: 0.3577 - classification_loss: 0.0492
 9054/10000 [==========================>...] - ETA: 14:01 - loss: 0.4070 - regression_loss: 0.3577 - classification_loss: 0.0492
 9055/10000 [==========================>...] - ETA: 14:00 - loss: 0.4070 - regression_loss: 0.3577 - classification_loss: 0.0492
 9056/10000 [==========================>...] - ETA: 13:59 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 9057/10000 [==========================>...] - ETA: 13:58 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 9058/10000 [==========================>...] - ETA: 13:57 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 9059/10000 [==========================>...] - ETA: 13:56 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9060/10000 [==========================>...] - ETA: 13:55 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9061/10000 [==========================>...] - ETA: 13:54 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9062/10000 [==========================>...] - ETA: 13:53 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9063/10000 [==========================>...] - ETA: 13:53 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9064/10000 [==========================>...] - ETA: 13:52 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9065/10000 [==========================>...] - ETA: 13:51 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9066/10000 [==========================>...] - ETA: 13:50 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9067/10000 [==========================>...] - ETA: 13:49 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9068/10000 [==========================>...] - ETA: 13:48 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9069/10000 [==========================>...] - ETA: 13:47 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9070/10000 [==========================>...] - ETA: 13:46 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9071/10000 [==========================>...] - ETA: 13:45 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9072/10000 [==========================>...] - ETA: 13:45 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9073/10000 [==========================>...] - ETA: 13:44 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9074/10000 [==========================>...] - ETA: 13:43 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9075/10000 [==========================>...] - ETA: 13:42 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9076/10000 [==========================>...] - ETA: 13:41 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9077/10000 [==========================>...] - ETA: 13:40 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9078/10000 [==========================>...] - ETA: 13:39 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9079/10000 [==========================>...] - ETA: 13:38 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9080/10000 [==========================>...] - ETA: 13:37 - loss: 0.4066 - regression_loss: 0.3575 - classification_loss: 0.0492
 9081/10000 [==========================>...] - ETA: 13:37 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9082/10000 [==========================>...] - ETA: 13:36 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9083/10000 [==========================>...] - ETA: 13:35 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0491
 9084/10000 [==========================>...] - ETA: 13:34 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0491
 9085/10000 [==========================>...] - ETA: 13:33 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9086/10000 [==========================>...] - ETA: 13:32 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0491
 9087/10000 [==========================>...] - ETA: 13:31 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9088/10000 [==========================>...] - ETA: 13:30 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9089/10000 [==========================>...] - ETA: 13:29 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9090/10000 [==========================>...] - ETA: 13:29 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0491
 9091/10000 [==========================>...] - ETA: 13:28 - loss: 0.4066 - regression_loss: 0.3575 - classification_loss: 0.0492
 9092/10000 [==========================>...] - ETA: 13:27 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0491
 9093/10000 [==========================>...] - ETA: 13:26 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9094/10000 [==========================>...] - ETA: 13:25 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9095/10000 [==========================>...] - ETA: 13:24 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9096/10000 [==========================>...] - ETA: 13:23 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0491
 9097/10000 [==========================>...] - ETA: 13:22 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9098/10000 [==========================>...] - ETA: 13:21 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0491
 9099/10000 [==========================>...] - ETA: 13:21 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9100/10000 [==========================>...] - ETA: 13:20 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9101/10000 [==========================>...] - ETA: 13:19 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9102/10000 [==========================>...] - ETA: 13:18 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9103/10000 [==========================>...] - ETA: 13:17 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9104/10000 [==========================>...] - ETA: 13:16 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9105/10000 [==========================>...] - ETA: 13:15 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0492
 9106/10000 [==========================>...] - ETA: 13:14 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0492
 9107/10000 [==========================>...] - ETA: 13:13 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0492
 9108/10000 [==========================>...] - ETA: 13:13 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0492
 9109/10000 [==========================>...] - ETA: 13:12 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0492
 9110/10000 [==========================>...] - ETA: 13:11 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9111/10000 [==========================>...] - ETA: 13:10 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9112/10000 [==========================>...] - ETA: 13:09 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9113/10000 [==========================>...] - ETA: 13:08 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0492
 9114/10000 [==========================>...] - ETA: 13:07 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9115/10000 [==========================>...] - ETA: 13:06 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9116/10000 [==========================>...] - ETA: 13:05 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9117/10000 [==========================>...] - ETA: 13:05 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0492
 9118/10000 [==========================>...] - ETA: 13:04 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0492
 9119/10000 [==========================>...] - ETA: 13:03 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0492
 9120/10000 [==========================>...] - ETA: 13:02 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9121/10000 [==========================>...] - ETA: 13:01 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9122/10000 [==========================>...] - ETA: 13:00 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9123/10000 [==========================>...] - ETA: 12:59 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9124/10000 [==========================>...] - ETA: 12:58 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9125/10000 [==========================>...] - ETA: 12:57 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9126/10000 [==========================>...] - ETA: 12:57 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9127/10000 [==========================>...] - ETA: 12:56 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9128/10000 [==========================>...] - ETA: 12:55 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0492
 9129/10000 [==========================>...] - ETA: 12:54 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 9130/10000 [==========================>...] - ETA: 12:53 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 9131/10000 [==========================>...] - ETA: 12:52 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0492
 9132/10000 [==========================>...] - ETA: 12:51 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9133/10000 [==========================>...] - ETA: 12:50 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9134/10000 [==========================>...] - ETA: 12:49 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9135/10000 [==========================>...] - ETA: 12:49 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9136/10000 [==========================>...] - ETA: 12:48 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9137/10000 [==========================>...] - ETA: 12:47 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9138/10000 [==========================>...] - ETA: 12:46 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9139/10000 [==========================>...] - ETA: 12:45 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9140/10000 [==========================>...] - ETA: 12:44 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9141/10000 [==========================>...] - ETA: 12:43 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9142/10000 [==========================>...] - ETA: 12:42 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9143/10000 [==========================>...] - ETA: 12:41 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9144/10000 [==========================>...] - ETA: 12:41 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9145/10000 [==========================>...] - ETA: 12:40 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9146/10000 [==========================>...] - ETA: 12:39 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9147/10000 [==========================>...] - ETA: 12:38 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9148/10000 [==========================>...] - ETA: 12:37 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9149/10000 [==========================>...] - ETA: 12:36 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9150/10000 [==========================>...] - ETA: 12:35 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9151/10000 [==========================>...] - ETA: 12:34 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9152/10000 [==========================>...] - ETA: 12:33 - loss: 0.4066 - regression_loss: 0.3575 - classification_loss: 0.0492
 9153/10000 [==========================>...] - ETA: 12:33 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9154/10000 [==========================>...] - ETA: 12:32 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9155/10000 [==========================>...] - ETA: 12:31 - loss: 0.4066 - regression_loss: 0.3575 - classification_loss: 0.0492
 9156/10000 [==========================>...] - ETA: 12:30 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9157/10000 [==========================>...] - ETA: 12:29 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9158/10000 [==========================>...] - ETA: 12:28 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0492
 9159/10000 [==========================>...] - ETA: 12:27 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9160/10000 [==========================>...] - ETA: 12:26 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9161/10000 [==========================>...] - ETA: 12:25 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9162/10000 [==========================>...] - ETA: 12:25 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9163/10000 [==========================>...] - ETA: 12:24 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9164/10000 [==========================>...] - ETA: 12:23 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9165/10000 [==========================>...] - ETA: 12:22 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9166/10000 [==========================>...] - ETA: 12:21 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9167/10000 [==========================>...] - ETA: 12:20 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0492
 9168/10000 [==========================>...] - ETA: 12:19 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9169/10000 [==========================>...] - ETA: 12:18 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0492
 9170/10000 [==========================>...] - ETA: 12:17 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0492
 9171/10000 [==========================>...] - ETA: 12:16 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0492
 9172/10000 [==========================>...] - ETA: 12:16 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9173/10000 [==========================>...] - ETA: 12:15 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9174/10000 [==========================>...] - ETA: 12:14 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9175/10000 [==========================>...] - ETA: 12:13 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0492
 9176/10000 [==========================>...] - ETA: 12:12 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9177/10000 [==========================>...] - ETA: 12:11 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9178/10000 [==========================>...] - ETA: 12:10 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9179/10000 [==========================>...] - ETA: 12:09 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9180/10000 [==========================>...] - ETA: 12:08 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9181/10000 [==========================>...] - ETA: 12:08 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9182/10000 [==========================>...] - ETA: 12:07 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9183/10000 [==========================>...] - ETA: 12:06 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9184/10000 [==========================>...] - ETA: 12:05 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0492
 9185/10000 [==========================>...] - ETA: 12:04 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9186/10000 [==========================>...] - ETA: 12:03 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9187/10000 [==========================>...] - ETA: 12:02 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9188/10000 [==========================>...] - ETA: 12:01 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0492
 9189/10000 [==========================>...] - ETA: 12:00 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0492
 9190/10000 [==========================>...] - ETA: 12:00 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0492
 9191/10000 [==========================>...] - ETA: 11:59 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0492
 9192/10000 [==========================>...] - ETA: 11:58 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9193/10000 [==========================>...] - ETA: 11:57 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9194/10000 [==========================>...] - ETA: 11:56 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0492
 9195/10000 [==========================>...] - ETA: 11:55 - loss: 0.4070 - regression_loss: 0.3577 - classification_loss: 0.0493
 9196/10000 [==========================>...] - ETA: 11:54 - loss: 0.4070 - regression_loss: 0.3577 - classification_loss: 0.0493
 9197/10000 [==========================>...] - ETA: 11:53 - loss: 0.4070 - regression_loss: 0.3577 - classification_loss: 0.0493
 9198/10000 [==========================>...] - ETA: 11:52 - loss: 0.4070 - regression_loss: 0.3577 - classification_loss: 0.0493
 9199/10000 [==========================>...] - ETA: 11:52 - loss: 0.4071 - regression_loss: 0.3578 - classification_loss: 0.0493
 9200/10000 [==========================>...] - ETA: 11:51 - loss: 0.4071 - regression_loss: 0.3578 - classification_loss: 0.0493
 9201/10000 [==========================>...] - ETA: 11:50 - loss: 0.4071 - regression_loss: 0.3578 - classification_loss: 0.0493
 9202/10000 [==========================>...] - ETA: 11:49 - loss: 0.4071 - regression_loss: 0.3578 - classification_loss: 0.0493
 9203/10000 [==========================>...] - ETA: 11:48 - loss: 0.4071 - regression_loss: 0.3578 - classification_loss: 0.0493
 9204/10000 [==========================>...] - ETA: 11:47 - loss: 0.4071 - regression_loss: 0.3578 - classification_loss: 0.0493
 9205/10000 [==========================>...] - ETA: 11:46 - loss: 0.4071 - regression_loss: 0.3578 - classification_loss: 0.0493
 9206/10000 [==========================>...] - ETA: 11:45 - loss: 0.4071 - regression_loss: 0.3578 - classification_loss: 0.0493
 9207/10000 [==========================>...] - ETA: 11:44 - loss: 0.4071 - regression_loss: 0.3578 - classification_loss: 0.0493
 9208/10000 [==========================>...] - ETA: 11:44 - loss: 0.4071 - regression_loss: 0.3578 - classification_loss: 0.0493
 9209/10000 [==========================>...] - ETA: 11:43 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0493
 9210/10000 [==========================>...] - ETA: 11:42 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0493
 9211/10000 [==========================>...] - ETA: 11:41 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0493
 9212/10000 [==========================>...] - ETA: 11:40 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0493
 9213/10000 [==========================>...] - ETA: 11:39 - loss: 0.4070 - regression_loss: 0.3577 - classification_loss: 0.0493
 9214/10000 [==========================>...] - ETA: 11:38 - loss: 0.4070 - regression_loss: 0.3577 - classification_loss: 0.0493
 9215/10000 [==========================>...] - ETA: 11:37 - loss: 0.4070 - regression_loss: 0.3577 - classification_loss: 0.0493
 9216/10000 [==========================>...] - ETA: 11:36 - loss: 0.4070 - regression_loss: 0.3577 - classification_loss: 0.0493
 9217/10000 [==========================>...] - ETA: 11:36 - loss: 0.4070 - regression_loss: 0.3577 - classification_loss: 0.0493
 9218/10000 [==========================>...] - ETA: 11:35 - loss: 0.4071 - regression_loss: 0.3578 - classification_loss: 0.0493
 9219/10000 [==========================>...] - ETA: 11:34 - loss: 0.4072 - regression_loss: 0.3579 - classification_loss: 0.0493
 9220/10000 [==========================>...] - ETA: 11:33 - loss: 0.4072 - regression_loss: 0.3579 - classification_loss: 0.0493
 9221/10000 [==========================>...] - ETA: 11:32 - loss: 0.4072 - regression_loss: 0.3579 - classification_loss: 0.0493
 9222/10000 [==========================>...] - ETA: 11:31 - loss: 0.4072 - regression_loss: 0.3579 - classification_loss: 0.0493
 9223/10000 [==========================>...] - ETA: 11:30 - loss: 0.4072 - regression_loss: 0.3579 - classification_loss: 0.0493
 9224/10000 [==========================>...] - ETA: 11:29 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0493
 9225/10000 [==========================>...] - ETA: 11:28 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0493
 9226/10000 [==========================>...] - ETA: 11:28 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0493
 9227/10000 [==========================>...] - ETA: 11:27 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0493
 9228/10000 [==========================>...] - ETA: 11:26 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0493
 9229/10000 [==========================>...] - ETA: 11:25 - loss: 0.4072 - regression_loss: 0.3579 - classification_loss: 0.0493
 9230/10000 [==========================>...] - ETA: 11:24 - loss: 0.4072 - regression_loss: 0.3579 - classification_loss: 0.0493
 9231/10000 [==========================>...] - ETA: 11:23 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0493
 9232/10000 [==========================>...] - ETA: 11:22 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 9233/10000 [==========================>...] - ETA: 11:21 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 9234/10000 [==========================>...] - ETA: 11:20 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 9235/10000 [==========================>...] - ETA: 11:20 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 9236/10000 [==========================>...] - ETA: 11:19 - loss: 0.4072 - regression_loss: 0.3579 - classification_loss: 0.0492
 9237/10000 [==========================>...] - ETA: 11:18 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9238/10000 [==========================>...] - ETA: 11:17 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9239/10000 [==========================>...] - ETA: 11:16 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9240/10000 [==========================>...] - ETA: 11:15 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9241/10000 [==========================>...] - ETA: 11:14 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9242/10000 [==========================>...] - ETA: 11:13 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9243/10000 [==========================>...] - ETA: 11:12 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9244/10000 [==========================>...] - ETA: 11:12 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9245/10000 [==========================>...] - ETA: 11:11 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9246/10000 [==========================>...] - ETA: 11:10 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9247/10000 [==========================>...] - ETA: 11:09 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9248/10000 [==========================>...] - ETA: 11:08 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9249/10000 [==========================>...] - ETA: 11:07 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9250/10000 [==========================>...] - ETA: 11:06 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9251/10000 [==========================>...] - ETA: 11:05 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9252/10000 [==========================>...] - ETA: 11:04 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9253/10000 [==========================>...] - ETA: 11:04 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9254/10000 [==========================>...] - ETA: 11:03 - loss: 0.4072 - regression_loss: 0.3579 - classification_loss: 0.0492
 9255/10000 [==========================>...] - ETA: 11:02 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 9256/10000 [==========================>...] - ETA: 11:01 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0492
 9257/10000 [==========================>...] - ETA: 11:00 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 9258/10000 [==========================>...] - ETA: 10:59 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 9259/10000 [==========================>...] - ETA: 10:58 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 9260/10000 [==========================>...] - ETA: 10:57 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 9261/10000 [==========================>...] - ETA: 10:56 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 9262/10000 [==========================>...] - ETA: 10:56 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0493
 9263/10000 [==========================>...] - ETA: 10:55 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 9264/10000 [==========================>...] - ETA: 10:54 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 9265/10000 [==========================>...] - ETA: 10:53 - loss: 0.4073 - regression_loss: 0.3581 - classification_loss: 0.0492
 9266/10000 [==========================>...] - ETA: 10:52 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0492
 9267/10000 [==========================>...] - ETA: 10:51 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0492
 9268/10000 [==========================>...] - ETA: 10:50 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 9269/10000 [==========================>...] - ETA: 10:49 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 9270/10000 [==========================>...] - ETA: 10:48 - loss: 0.4073 - regression_loss: 0.3580 - classification_loss: 0.0492
 9271/10000 [==========================>...] - ETA: 10:48 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 9272/10000 [==========================>...] - ETA: 10:47 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 9273/10000 [==========================>...] - ETA: 10:46 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 9274/10000 [==========================>...] - ETA: 10:45 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 9275/10000 [==========================>...] - ETA: 10:44 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9276/10000 [==========================>...] - ETA: 10:43 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 9277/10000 [==========================>...] - ETA: 10:42 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 9278/10000 [==========================>...] - ETA: 10:41 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 9279/10000 [==========================>...] - ETA: 10:40 - loss: 0.4072 - regression_loss: 0.3579 - classification_loss: 0.0492
 9280/10000 [==========================>...] - ETA: 10:40 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9281/10000 [==========================>...] - ETA: 10:39 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9282/10000 [==========================>...] - ETA: 10:38 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9283/10000 [==========================>...] - ETA: 10:37 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9284/10000 [==========================>...] - ETA: 10:36 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9285/10000 [==========================>...] - ETA: 10:35 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9286/10000 [==========================>...] - ETA: 10:34 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9287/10000 [==========================>...] - ETA: 10:33 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9288/10000 [==========================>...] - ETA: 10:32 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9289/10000 [==========================>...] - ETA: 10:32 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 9290/10000 [==========================>...] - ETA: 10:31 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 9291/10000 [==========================>...] - ETA: 10:30 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 9292/10000 [==========================>...] - ETA: 10:29 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 9293/10000 [==========================>...] - ETA: 10:28 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 9294/10000 [==========================>...] - ETA: 10:27 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 9295/10000 [==========================>...] - ETA: 10:26 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 9296/10000 [==========================>...] - ETA: 10:25 - loss: 0.4072 - regression_loss: 0.3580 - classification_loss: 0.0492
 9297/10000 [==========================>...] - ETA: 10:24 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9298/10000 [==========================>...] - ETA: 10:24 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9299/10000 [==========================>...] - ETA: 10:23 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9300/10000 [==========================>...] - ETA: 10:22 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9301/10000 [==========================>...] - ETA: 10:21 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9302/10000 [==========================>...] - ETA: 10:20 - loss: 0.4071 - regression_loss: 0.3579 - classification_loss: 0.0492
 9303/10000 [==========================>...] - ETA: 10:19 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9304/10000 [==========================>...] - ETA: 10:18 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9305/10000 [==========================>...] - ETA: 10:17 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9306/10000 [==========================>...] - ETA: 10:16 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9307/10000 [==========================>...] - ETA: 10:16 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9308/10000 [==========================>...] - ETA: 10:15 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9309/10000 [==========================>...] - ETA: 10:14 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0492
 9310/10000 [==========================>...] - ETA: 10:13 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0492
 9311/10000 [==========================>...] - ETA: 10:12 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9312/10000 [==========================>...] - ETA: 10:11 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9313/10000 [==========================>...] - ETA: 10:10 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9314/10000 [==========================>...] - ETA: 10:09 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9315/10000 [==========================>...] - ETA: 10:08 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9316/10000 [==========================>...] - ETA: 10:08 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9317/10000 [==========================>...] - ETA: 10:07 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9318/10000 [==========================>...] - ETA: 10:06 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9319/10000 [==========================>...] - ETA: 10:05 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0492
 9320/10000 [==========================>...] - ETA: 10:04 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9321/10000 [==========================>...] - ETA: 10:03 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0492
 9322/10000 [==========================>...] - ETA: 10:02 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9323/10000 [==========================>...] - ETA: 10:01 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9324/10000 [==========================>...] - ETA: 10:00 - loss: 0.4070 - regression_loss: 0.3578 - classification_loss: 0.0492
 9325/10000 [==========================>...] - ETA: 10:00 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0492
 9326/10000 [==========================>...] - ETA: 9:59 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0492 
 9327/10000 [==========================>...] - ETA: 9:58 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 9328/10000 [==========================>...] - ETA: 9:57 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0492
 9329/10000 [==========================>...] - ETA: 9:56 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0492
 9330/10000 [==========================>...] - ETA: 9:55 - loss: 0.4069 - regression_loss: 0.3578 - classification_loss: 0.0492
 9331/10000 [==========================>...] - ETA: 9:54 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 9332/10000 [==========================>...] - ETA: 9:53 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 9333/10000 [==========================>...] - ETA: 9:52 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 9334/10000 [===========================>..] - ETA: 9:52 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0492
 9335/10000 [===========================>..] - ETA: 9:51 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 9336/10000 [===========================>..] - ETA: 9:50 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 9337/10000 [===========================>..] - ETA: 9:49 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 9338/10000 [===========================>..] - ETA: 9:48 - loss: 0.4068 - regression_loss: 0.3577 - classification_loss: 0.0491
 9339/10000 [===========================>..] - ETA: 9:47 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0491
 9340/10000 [===========================>..] - ETA: 9:46 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0491
 9341/10000 [===========================>..] - ETA: 9:45 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0491
 9342/10000 [===========================>..] - ETA: 9:44 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0491
 9343/10000 [===========================>..] - ETA: 9:44 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0491
 9344/10000 [===========================>..] - ETA: 9:43 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0491
 9345/10000 [===========================>..] - ETA: 9:42 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0491
 9346/10000 [===========================>..] - ETA: 9:41 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0491
 9347/10000 [===========================>..] - ETA: 9:40 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0491
 9348/10000 [===========================>..] - ETA: 9:39 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0491
 9349/10000 [===========================>..] - ETA: 9:38 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0491
 9350/10000 [===========================>..] - ETA: 9:37 - loss: 0.4067 - regression_loss: 0.3576 - classification_loss: 0.0491
 9351/10000 [===========================>..] - ETA: 9:36 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0491
 9352/10000 [===========================>..] - ETA: 9:36 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0491
 9353/10000 [===========================>..] - ETA: 9:35 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0491
 9354/10000 [===========================>..] - ETA: 9:34 - loss: 0.4070 - regression_loss: 0.3577 - classification_loss: 0.0494
 9355/10000 [===========================>..] - ETA: 9:33 - loss: 0.4070 - regression_loss: 0.3576 - classification_loss: 0.0494
 9356/10000 [===========================>..] - ETA: 9:32 - loss: 0.4070 - regression_loss: 0.3576 - classification_loss: 0.0494
 9357/10000 [===========================>..] - ETA: 9:31 - loss: 0.4070 - regression_loss: 0.3576 - classification_loss: 0.0494
 9358/10000 [===========================>..] - ETA: 9:30 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0494
 9359/10000 [===========================>..] - ETA: 9:29 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0494
 9360/10000 [===========================>..] - ETA: 9:28 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9361/10000 [===========================>..] - ETA: 9:28 - loss: 0.4070 - regression_loss: 0.3576 - classification_loss: 0.0494
 9362/10000 [===========================>..] - ETA: 9:27 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0494
 9363/10000 [===========================>..] - ETA: 9:26 - loss: 0.4070 - regression_loss: 0.3576 - classification_loss: 0.0494
 9364/10000 [===========================>..] - ETA: 9:25 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9365/10000 [===========================>..] - ETA: 9:24 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9366/10000 [===========================>..] - ETA: 9:23 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9367/10000 [===========================>..] - ETA: 9:22 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0494
 9368/10000 [===========================>..] - ETA: 9:21 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9369/10000 [===========================>..] - ETA: 9:20 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9370/10000 [===========================>..] - ETA: 9:20 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9371/10000 [===========================>..] - ETA: 9:19 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9372/10000 [===========================>..] - ETA: 9:18 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9373/10000 [===========================>..] - ETA: 9:17 - loss: 0.4070 - regression_loss: 0.3576 - classification_loss: 0.0493
 9374/10000 [===========================>..] - ETA: 9:16 - loss: 0.4070 - regression_loss: 0.3576 - classification_loss: 0.0493
 9375/10000 [===========================>..] - ETA: 9:15 - loss: 0.4070 - regression_loss: 0.3576 - classification_loss: 0.0493
 9376/10000 [===========================>..] - ETA: 9:14 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9377/10000 [===========================>..] - ETA: 9:13 - loss: 0.4070 - regression_loss: 0.3576 - classification_loss: 0.0493
 9378/10000 [===========================>..] - ETA: 9:12 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9379/10000 [===========================>..] - ETA: 9:12 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9380/10000 [===========================>..] - ETA: 9:11 - loss: 0.4069 - regression_loss: 0.3575 - classification_loss: 0.0493
 9381/10000 [===========================>..] - ETA: 9:10 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0493
 9382/10000 [===========================>..] - ETA: 9:09 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0493
 9383/10000 [===========================>..] - ETA: 9:08 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0493
 9384/10000 [===========================>..] - ETA: 9:07 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0493
 9385/10000 [===========================>..] - ETA: 9:06 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0493
 9386/10000 [===========================>..] - ETA: 9:05 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0493
 9387/10000 [===========================>..] - ETA: 9:04 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9388/10000 [===========================>..] - ETA: 9:04 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9389/10000 [===========================>..] - ETA: 9:03 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9390/10000 [===========================>..] - ETA: 9:02 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9391/10000 [===========================>..] - ETA: 9:01 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9392/10000 [===========================>..] - ETA: 9:00 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9393/10000 [===========================>..] - ETA: 8:59 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9394/10000 [===========================>..] - ETA: 8:58 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9395/10000 [===========================>..] - ETA: 8:57 - loss: 0.4069 - regression_loss: 0.3575 - classification_loss: 0.0493
 9396/10000 [===========================>..] - ETA: 8:56 - loss: 0.4069 - regression_loss: 0.3575 - classification_loss: 0.0493
 9397/10000 [===========================>..] - ETA: 8:56 - loss: 0.4069 - regression_loss: 0.3575 - classification_loss: 0.0493
 9398/10000 [===========================>..] - ETA: 8:55 - loss: 0.4069 - regression_loss: 0.3575 - classification_loss: 0.0493
 9399/10000 [===========================>..] - ETA: 8:54 - loss: 0.4070 - regression_loss: 0.3576 - classification_loss: 0.0494
 9400/10000 [===========================>..] - ETA: 8:53 - loss: 0.4070 - regression_loss: 0.3576 - classification_loss: 0.0494
 9401/10000 [===========================>..] - ETA: 8:52 - loss: 0.4070 - regression_loss: 0.3576 - classification_loss: 0.0494
 9402/10000 [===========================>..] - ETA: 8:51 - loss: 0.4070 - regression_loss: 0.3576 - classification_loss: 0.0493
 9403/10000 [===========================>..] - ETA: 8:50 - loss: 0.4070 - regression_loss: 0.3576 - classification_loss: 0.0494
 9404/10000 [===========================>..] - ETA: 8:49 - loss: 0.4070 - regression_loss: 0.3576 - classification_loss: 0.0493
 9405/10000 [===========================>..] - ETA: 8:48 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9406/10000 [===========================>..] - ETA: 8:48 - loss: 0.4070 - regression_loss: 0.3576 - classification_loss: 0.0493
 9407/10000 [===========================>..] - ETA: 8:47 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9408/10000 [===========================>..] - ETA: 8:46 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9409/10000 [===========================>..] - ETA: 8:45 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9410/10000 [===========================>..] - ETA: 8:44 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9411/10000 [===========================>..] - ETA: 8:43 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9412/10000 [===========================>..] - ETA: 8:42 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9413/10000 [===========================>..] - ETA: 8:41 - loss: 0.4069 - regression_loss: 0.3575 - classification_loss: 0.0493
 9414/10000 [===========================>..] - ETA: 8:40 - loss: 0.4069 - regression_loss: 0.3575 - classification_loss: 0.0493
 9415/10000 [===========================>..] - ETA: 8:40 - loss: 0.4069 - regression_loss: 0.3575 - classification_loss: 0.0493
 9416/10000 [===========================>..] - ETA: 8:39 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0493
 9417/10000 [===========================>..] - ETA: 8:38 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0493
 9418/10000 [===========================>..] - ETA: 8:37 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0493
 9419/10000 [===========================>..] - ETA: 8:36 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0493
 9420/10000 [===========================>..] - ETA: 8:35 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0493
 9421/10000 [===========================>..] - ETA: 8:34 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0493
 9422/10000 [===========================>..] - ETA: 8:33 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0493
 9423/10000 [===========================>..] - ETA: 8:32 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0493
 9424/10000 [===========================>..] - ETA: 8:32 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0493
 9425/10000 [===========================>..] - ETA: 8:31 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0493
 9426/10000 [===========================>..] - ETA: 8:30 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0493
 9427/10000 [===========================>..] - ETA: 8:29 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0493
 9428/10000 [===========================>..] - ETA: 8:28 - loss: 0.4068 - regression_loss: 0.3574 - classification_loss: 0.0493
 9429/10000 [===========================>..] - ETA: 8:27 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0493
 9430/10000 [===========================>..] - ETA: 8:26 - loss: 0.4068 - regression_loss: 0.3574 - classification_loss: 0.0493
 9431/10000 [===========================>..] - ETA: 8:25 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0493
 9432/10000 [===========================>..] - ETA: 8:24 - loss: 0.4067 - regression_loss: 0.3574 - classification_loss: 0.0493
 9433/10000 [===========================>..] - ETA: 8:24 - loss: 0.4067 - regression_loss: 0.3574 - classification_loss: 0.0493
 9434/10000 [===========================>..] - ETA: 8:23 - loss: 0.4067 - regression_loss: 0.3574 - classification_loss: 0.0493
 9435/10000 [===========================>..] - ETA: 8:22 - loss: 0.4067 - regression_loss: 0.3574 - classification_loss: 0.0493
 9436/10000 [===========================>..] - ETA: 8:21 - loss: 0.4067 - regression_loss: 0.3574 - classification_loss: 0.0493
 9437/10000 [===========================>..] - ETA: 8:20 - loss: 0.4066 - regression_loss: 0.3573 - classification_loss: 0.0493
 9438/10000 [===========================>..] - ETA: 8:19 - loss: 0.4066 - regression_loss: 0.3573 - classification_loss: 0.0493
 9439/10000 [===========================>..] - ETA: 8:18 - loss: 0.4066 - regression_loss: 0.3573 - classification_loss: 0.0493
 9440/10000 [===========================>..] - ETA: 8:17 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0493
 9441/10000 [===========================>..] - ETA: 8:16 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0493
 9442/10000 [===========================>..] - ETA: 8:16 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0493
 9443/10000 [===========================>..] - ETA: 8:15 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0493
 9444/10000 [===========================>..] - ETA: 8:14 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0493
 9445/10000 [===========================>..] - ETA: 8:13 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0493
 9446/10000 [===========================>..] - ETA: 8:12 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0493
 9447/10000 [===========================>..] - ETA: 8:11 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0493
 9448/10000 [===========================>..] - ETA: 8:10 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0493
 9449/10000 [===========================>..] - ETA: 8:09 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0493
 9450/10000 [===========================>..] - ETA: 8:08 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0493
 9451/10000 [===========================>..] - ETA: 8:08 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0493
 9452/10000 [===========================>..] - ETA: 8:07 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0493
 9453/10000 [===========================>..] - ETA: 8:06 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0493
 9454/10000 [===========================>..] - ETA: 8:05 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9455/10000 [===========================>..] - ETA: 8:04 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0492
 9456/10000 [===========================>..] - ETA: 8:03 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0492
 9457/10000 [===========================>..] - ETA: 8:02 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0492
 9458/10000 [===========================>..] - ETA: 8:01 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0492
 9459/10000 [===========================>..] - ETA: 8:00 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9460/10000 [===========================>..] - ETA: 8:00 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9461/10000 [===========================>..] - ETA: 7:59 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9462/10000 [===========================>..] - ETA: 7:58 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9463/10000 [===========================>..] - ETA: 7:57 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9464/10000 [===========================>..] - ETA: 7:56 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9465/10000 [===========================>..] - ETA: 7:55 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9466/10000 [===========================>..] - ETA: 7:54 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9467/10000 [===========================>..] - ETA: 7:53 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9468/10000 [===========================>..] - ETA: 7:52 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9469/10000 [===========================>..] - ETA: 7:52 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9470/10000 [===========================>..] - ETA: 7:51 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9471/10000 [===========================>..] - ETA: 7:50 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9472/10000 [===========================>..] - ETA: 7:49 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9473/10000 [===========================>..] - ETA: 7:48 - loss: 0.4064 - regression_loss: 0.3571 - classification_loss: 0.0492
 9474/10000 [===========================>..] - ETA: 7:47 - loss: 0.4064 - regression_loss: 0.3571 - classification_loss: 0.0492
 9475/10000 [===========================>..] - ETA: 7:46 - loss: 0.4063 - regression_loss: 0.3571 - classification_loss: 0.0492
 9476/10000 [===========================>..] - ETA: 7:45 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9477/10000 [===========================>..] - ETA: 7:44 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9478/10000 [===========================>..] - ETA: 7:44 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9479/10000 [===========================>..] - ETA: 7:43 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9480/10000 [===========================>..] - ETA: 7:42 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0492
 9481/10000 [===========================>..] - ETA: 7:41 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0492
 9482/10000 [===========================>..] - ETA: 7:40 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0492
 9483/10000 [===========================>..] - ETA: 7:39 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9484/10000 [===========================>..] - ETA: 7:38 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0492
 9485/10000 [===========================>..] - ETA: 7:37 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9486/10000 [===========================>..] - ETA: 7:36 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0492
 9487/10000 [===========================>..] - ETA: 7:36 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9488/10000 [===========================>..] - ETA: 7:35 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9489/10000 [===========================>..] - ETA: 7:34 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9490/10000 [===========================>..] - ETA: 7:33 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0492
 9491/10000 [===========================>..] - ETA: 7:32 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9492/10000 [===========================>..] - ETA: 7:31 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9493/10000 [===========================>..] - ETA: 7:30 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9494/10000 [===========================>..] - ETA: 7:29 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9495/10000 [===========================>..] - ETA: 7:28 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9496/10000 [===========================>..] - ETA: 7:28 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9497/10000 [===========================>..] - ETA: 7:27 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9498/10000 [===========================>..] - ETA: 7:26 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9499/10000 [===========================>..] - ETA: 7:25 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9500/10000 [===========================>..] - ETA: 7:24 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9501/10000 [===========================>..] - ETA: 7:23 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9502/10000 [===========================>..] - ETA: 7:22 - loss: 0.4066 - regression_loss: 0.3573 - classification_loss: 0.0492
 9503/10000 [===========================>..] - ETA: 7:21 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9504/10000 [===========================>..] - ETA: 7:20 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9505/10000 [===========================>..] - ETA: 7:20 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9506/10000 [===========================>..] - ETA: 7:19 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9507/10000 [===========================>..] - ETA: 7:18 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9508/10000 [===========================>..] - ETA: 7:17 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9509/10000 [===========================>..] - ETA: 7:16 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9510/10000 [===========================>..] - ETA: 7:15 - loss: 0.4066 - regression_loss: 0.3573 - classification_loss: 0.0492
 9511/10000 [===========================>..] - ETA: 7:14 - loss: 0.4066 - regression_loss: 0.3573 - classification_loss: 0.0492
 9512/10000 [===========================>..] - ETA: 7:13 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9513/10000 [===========================>..] - ETA: 7:12 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9514/10000 [===========================>..] - ETA: 7:12 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9515/10000 [===========================>..] - ETA: 7:11 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9516/10000 [===========================>..] - ETA: 7:10 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9517/10000 [===========================>..] - ETA: 7:09 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0492
 9518/10000 [===========================>..] - ETA: 7:08 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0492
 9519/10000 [===========================>..] - ETA: 7:07 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9520/10000 [===========================>..] - ETA: 7:06 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0493
 9521/10000 [===========================>..] - ETA: 7:05 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0492
 9522/10000 [===========================>..] - ETA: 7:04 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0492
 9523/10000 [===========================>..] - ETA: 7:04 - loss: 0.4069 - regression_loss: 0.3577 - classification_loss: 0.0492
 9524/10000 [===========================>..] - ETA: 7:03 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0492
 9525/10000 [===========================>..] - ETA: 7:02 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0492
 9526/10000 [===========================>..] - ETA: 7:01 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0492
 9527/10000 [===========================>..] - ETA: 7:00 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0492
 9528/10000 [===========================>..] - ETA: 6:59 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0492
 9529/10000 [===========================>..] - ETA: 6:58 - loss: 0.4069 - regression_loss: 0.3576 - classification_loss: 0.0492
 9530/10000 [===========================>..] - ETA: 6:57 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9531/10000 [===========================>..] - ETA: 6:56 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9532/10000 [===========================>..] - ETA: 6:56 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9533/10000 [===========================>..] - ETA: 6:55 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9534/10000 [===========================>..] - ETA: 6:54 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9535/10000 [===========================>..] - ETA: 6:53 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9536/10000 [===========================>..] - ETA: 6:52 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9537/10000 [===========================>..] - ETA: 6:51 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9538/10000 [===========================>..] - ETA: 6:50 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9539/10000 [===========================>..] - ETA: 6:49 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9540/10000 [===========================>..] - ETA: 6:48 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9541/10000 [===========================>..] - ETA: 6:48 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9542/10000 [===========================>..] - ETA: 6:47 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9543/10000 [===========================>..] - ETA: 6:46 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9544/10000 [===========================>..] - ETA: 6:45 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9545/10000 [===========================>..] - ETA: 6:44 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9546/10000 [===========================>..] - ETA: 6:43 - loss: 0.4068 - regression_loss: 0.3576 - classification_loss: 0.0492
 9547/10000 [===========================>..] - ETA: 6:42 - loss: 0.4068 - regression_loss: 0.3575 - classification_loss: 0.0492
 9548/10000 [===========================>..] - ETA: 6:41 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9549/10000 [===========================>..] - ETA: 6:40 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9550/10000 [===========================>..] - ETA: 6:40 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9551/10000 [===========================>..] - ETA: 6:39 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9552/10000 [===========================>..] - ETA: 6:38 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9553/10000 [===========================>..] - ETA: 6:37 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9554/10000 [===========================>..] - ETA: 6:36 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9555/10000 [===========================>..] - ETA: 6:35 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9556/10000 [===========================>..] - ETA: 6:34 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9557/10000 [===========================>..] - ETA: 6:33 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9558/10000 [===========================>..] - ETA: 6:32 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9559/10000 [===========================>..] - ETA: 6:32 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9560/10000 [===========================>..] - ETA: 6:31 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9561/10000 [===========================>..] - ETA: 6:30 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9562/10000 [===========================>..] - ETA: 6:29 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9563/10000 [===========================>..] - ETA: 6:28 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9564/10000 [===========================>..] - ETA: 6:27 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9565/10000 [===========================>..] - ETA: 6:26 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9566/10000 [===========================>..] - ETA: 6:25 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9567/10000 [===========================>..] - ETA: 6:24 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9568/10000 [===========================>..] - ETA: 6:24 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9569/10000 [===========================>..] - ETA: 6:23 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9570/10000 [===========================>..] - ETA: 6:22 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9571/10000 [===========================>..] - ETA: 6:21 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9572/10000 [===========================>..] - ETA: 6:20 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9573/10000 [===========================>..] - ETA: 6:19 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9574/10000 [===========================>..] - ETA: 6:18 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9575/10000 [===========================>..] - ETA: 6:17 - loss: 0.4066 - regression_loss: 0.3573 - classification_loss: 0.0493
 9576/10000 [===========================>..] - ETA: 6:16 - loss: 0.4066 - regression_loss: 0.3573 - classification_loss: 0.0493
 9577/10000 [===========================>..] - ETA: 6:16 - loss: 0.4066 - regression_loss: 0.3573 - classification_loss: 0.0493
 9578/10000 [===========================>..] - ETA: 6:15 - loss: 0.4066 - regression_loss: 0.3573 - classification_loss: 0.0493
 9579/10000 [===========================>..] - ETA: 6:14 - loss: 0.4066 - regression_loss: 0.3573 - classification_loss: 0.0493
 9580/10000 [===========================>..] - ETA: 6:13 - loss: 0.4066 - regression_loss: 0.3573 - classification_loss: 0.0493
 9581/10000 [===========================>..] - ETA: 6:12 - loss: 0.4066 - regression_loss: 0.3573 - classification_loss: 0.0493
 9582/10000 [===========================>..] - ETA: 6:11 - loss: 0.4066 - regression_loss: 0.3573 - classification_loss: 0.0493
 9583/10000 [===========================>..] - ETA: 6:10 - loss: 0.4066 - regression_loss: 0.3573 - classification_loss: 0.0493
 9584/10000 [===========================>..] - ETA: 6:09 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0493
 9585/10000 [===========================>..] - ETA: 6:08 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0493
 9586/10000 [===========================>..] - ETA: 6:08 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0493
 9587/10000 [===========================>..] - ETA: 6:07 - loss: 0.4066 - regression_loss: 0.3573 - classification_loss: 0.0493
 9588/10000 [===========================>..] - ETA: 6:06 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0493
 9589/10000 [===========================>..] - ETA: 6:05 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0493
 9590/10000 [===========================>..] - ETA: 6:04 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0493
 9591/10000 [===========================>..] - ETA: 6:03 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0493
 9592/10000 [===========================>..] - ETA: 6:02 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0493
 9593/10000 [===========================>..] - ETA: 6:01 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0493
 9594/10000 [===========================>..] - ETA: 6:00 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0493
 9595/10000 [===========================>..] - ETA: 6:00 - loss: 0.4066 - regression_loss: 0.3573 - classification_loss: 0.0493
 9596/10000 [===========================>..] - ETA: 5:59 - loss: 0.4066 - regression_loss: 0.3573 - classification_loss: 0.0493
 9597/10000 [===========================>..] - ETA: 5:58 - loss: 0.4066 - regression_loss: 0.3573 - classification_loss: 0.0493
 9598/10000 [===========================>..] - ETA: 5:57 - loss: 0.4066 - regression_loss: 0.3573 - classification_loss: 0.0493
 9599/10000 [===========================>..] - ETA: 5:56 - loss: 0.4066 - regression_loss: 0.3573 - classification_loss: 0.0493
 9600/10000 [===========================>..] - ETA: 5:55 - loss: 0.4066 - regression_loss: 0.3573 - classification_loss: 0.0493
 9601/10000 [===========================>..] - ETA: 5:54 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0493
 9602/10000 [===========================>..] - ETA: 5:53 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0493
 9603/10000 [===========================>..] - ETA: 5:52 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0493
 9604/10000 [===========================>..] - ETA: 5:52 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0492
 9605/10000 [===========================>..] - ETA: 5:51 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9606/10000 [===========================>..] - ETA: 5:50 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9607/10000 [===========================>..] - ETA: 5:49 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0492
 9608/10000 [===========================>..] - ETA: 5:48 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9609/10000 [===========================>..] - ETA: 5:47 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9610/10000 [===========================>..] - ETA: 5:46 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9611/10000 [===========================>..] - ETA: 5:45 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0492
 9612/10000 [===========================>..] - ETA: 5:44 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9613/10000 [===========================>..] - ETA: 5:44 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9614/10000 [===========================>..] - ETA: 5:43 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9615/10000 [===========================>..] - ETA: 5:42 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0492
 9616/10000 [===========================>..] - ETA: 5:41 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9617/10000 [===========================>..] - ETA: 5:40 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9618/10000 [===========================>..] - ETA: 5:39 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9619/10000 [===========================>..] - ETA: 5:38 - loss: 0.4066 - regression_loss: 0.3573 - classification_loss: 0.0492
 9620/10000 [===========================>..] - ETA: 5:37 - loss: 0.4066 - regression_loss: 0.3573 - classification_loss: 0.0492
 9621/10000 [===========================>..] - ETA: 5:36 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9622/10000 [===========================>..] - ETA: 5:36 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9623/10000 [===========================>..] - ETA: 5:35 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9624/10000 [===========================>..] - ETA: 5:34 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9625/10000 [===========================>..] - ETA: 5:33 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9626/10000 [===========================>..] - ETA: 5:32 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9627/10000 [===========================>..] - ETA: 5:31 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9628/10000 [===========================>..] - ETA: 5:30 - loss: 0.4065 - regression_loss: 0.3572 - classification_loss: 0.0492
 9629/10000 [===========================>..] - ETA: 5:29 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9630/10000 [===========================>..] - ETA: 5:28 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9631/10000 [===========================>..] - ETA: 5:28 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9632/10000 [===========================>..] - ETA: 5:27 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9633/10000 [===========================>..] - ETA: 5:26 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9634/10000 [===========================>..] - ETA: 5:25 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9635/10000 [===========================>..] - ETA: 5:24 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9636/10000 [===========================>..] - ETA: 5:23 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9637/10000 [===========================>..] - ETA: 5:22 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9638/10000 [===========================>..] - ETA: 5:21 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9639/10000 [===========================>..] - ETA: 5:20 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9640/10000 [===========================>..] - ETA: 5:20 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9641/10000 [===========================>..] - ETA: 5:19 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9642/10000 [===========================>..] - ETA: 5:18 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9643/10000 [===========================>..] - ETA: 5:17 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9644/10000 [===========================>..] - ETA: 5:16 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0492
 9645/10000 [===========================>..] - ETA: 5:15 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0492
 9646/10000 [===========================>..] - ETA: 5:14 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0492
 9647/10000 [===========================>..] - ETA: 5:13 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0492
 9648/10000 [===========================>..] - ETA: 5:12 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9649/10000 [===========================>..] - ETA: 5:12 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9650/10000 [===========================>..] - ETA: 5:11 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9651/10000 [===========================>..] - ETA: 5:10 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9652/10000 [===========================>..] - ETA: 5:09 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9653/10000 [===========================>..] - ETA: 5:08 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9654/10000 [===========================>..] - ETA: 5:07 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9655/10000 [===========================>..] - ETA: 5:06 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9656/10000 [===========================>..] - ETA: 5:05 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9657/10000 [===========================>..] - ETA: 5:04 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9658/10000 [===========================>..] - ETA: 5:04 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9659/10000 [===========================>..] - ETA: 5:03 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0492
 9660/10000 [===========================>..] - ETA: 5:02 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0492
 9661/10000 [===========================>..] - ETA: 5:01 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0492
 9662/10000 [===========================>..] - ETA: 5:00 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9663/10000 [===========================>..] - ETA: 4:59 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9664/10000 [===========================>..] - ETA: 4:58 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0492
 9665/10000 [===========================>..] - ETA: 4:57 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9666/10000 [===========================>..] - ETA: 4:56 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9667/10000 [============================>.] - ETA: 4:56 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0491
 9668/10000 [============================>.] - ETA: 4:55 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0491
 9669/10000 [============================>.] - ETA: 4:54 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0491
 9670/10000 [============================>.] - ETA: 4:53 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9671/10000 [============================>.] - ETA: 4:52 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0492
 9672/10000 [============================>.] - ETA: 4:51 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0492
 9673/10000 [============================>.] - ETA: 4:50 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0492
 9674/10000 [============================>.] - ETA: 4:49 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0492
 9675/10000 [============================>.] - ETA: 4:48 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9676/10000 [============================>.] - ETA: 4:48 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9677/10000 [============================>.] - ETA: 4:47 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9678/10000 [============================>.] - ETA: 4:46 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9679/10000 [============================>.] - ETA: 4:45 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9680/10000 [============================>.] - ETA: 4:44 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9681/10000 [============================>.] - ETA: 4:43 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9682/10000 [============================>.] - ETA: 4:42 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9683/10000 [============================>.] - ETA: 4:41 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9684/10000 [============================>.] - ETA: 4:40 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9685/10000 [============================>.] - ETA: 4:40 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9686/10000 [============================>.] - ETA: 4:39 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9687/10000 [============================>.] - ETA: 4:38 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9688/10000 [============================>.] - ETA: 4:37 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9689/10000 [============================>.] - ETA: 4:36 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9690/10000 [============================>.] - ETA: 4:35 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9691/10000 [============================>.] - ETA: 4:34 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9692/10000 [============================>.] - ETA: 4:33 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9693/10000 [============================>.] - ETA: 4:32 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9694/10000 [============================>.] - ETA: 4:32 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9695/10000 [============================>.] - ETA: 4:31 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9696/10000 [============================>.] - ETA: 4:30 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0491
 9697/10000 [============================>.] - ETA: 4:29 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0491
 9698/10000 [============================>.] - ETA: 4:28 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0491
 9699/10000 [============================>.] - ETA: 4:27 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9700/10000 [============================>.] - ETA: 4:26 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9701/10000 [============================>.] - ETA: 4:25 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9702/10000 [============================>.] - ETA: 4:24 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9703/10000 [============================>.] - ETA: 4:24 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0491
 9704/10000 [============================>.] - ETA: 4:23 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9705/10000 [============================>.] - ETA: 4:22 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0491
 9706/10000 [============================>.] - ETA: 4:21 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0491
 9707/10000 [============================>.] - ETA: 4:20 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9708/10000 [============================>.] - ETA: 4:19 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9709/10000 [============================>.] - ETA: 4:18 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9710/10000 [============================>.] - ETA: 4:17 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9711/10000 [============================>.] - ETA: 4:16 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9712/10000 [============================>.] - ETA: 4:16 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9713/10000 [============================>.] - ETA: 4:15 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9714/10000 [============================>.] - ETA: 4:14 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9715/10000 [============================>.] - ETA: 4:13 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0491
 9716/10000 [============================>.] - ETA: 4:12 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9717/10000 [============================>.] - ETA: 4:11 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9718/10000 [============================>.] - ETA: 4:10 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9719/10000 [============================>.] - ETA: 4:09 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9720/10000 [============================>.] - ETA: 4:08 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9721/10000 [============================>.] - ETA: 4:08 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9722/10000 [============================>.] - ETA: 4:07 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
 9723/10000 [============================>.] - ETA: 4:06 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
 9724/10000 [============================>.] - ETA: 4:05 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
 9725/10000 [============================>.] - ETA: 4:04 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
 9726/10000 [============================>.] - ETA: 4:03 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
 9727/10000 [============================>.] - ETA: 4:02 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
 9728/10000 [============================>.] - ETA: 4:01 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
 9729/10000 [============================>.] - ETA: 4:00 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
 9730/10000 [============================>.] - ETA: 4:00 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
 9731/10000 [============================>.] - ETA: 3:59 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
 9732/10000 [============================>.] - ETA: 3:58 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9733/10000 [============================>.] - ETA: 3:57 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
 9734/10000 [============================>.] - ETA: 3:56 - loss: 0.4063 - regression_loss: 0.3573 - classification_loss: 0.0491
 9735/10000 [============================>.] - ETA: 3:55 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
 9736/10000 [============================>.] - ETA: 3:54 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
 9737/10000 [============================>.] - ETA: 3:53 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
 9738/10000 [============================>.] - ETA: 3:52 - loss: 0.4062 - regression_loss: 0.3572 - classification_loss: 0.0491
 9739/10000 [============================>.] - ETA: 3:52 - loss: 0.4062 - regression_loss: 0.3572 - classification_loss: 0.0491
 9740/10000 [============================>.] - ETA: 3:51 - loss: 0.4062 - regression_loss: 0.3572 - classification_loss: 0.0491
 9741/10000 [============================>.] - ETA: 3:50 - loss: 0.4062 - regression_loss: 0.3572 - classification_loss: 0.0491
 9742/10000 [============================>.] - ETA: 3:49 - loss: 0.4062 - regression_loss: 0.3572 - classification_loss: 0.0491
 9743/10000 [============================>.] - ETA: 3:48 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
 9744/10000 [============================>.] - ETA: 3:47 - loss: 0.4063 - regression_loss: 0.3573 - classification_loss: 0.0491
 9745/10000 [============================>.] - ETA: 3:46 - loss: 0.4063 - regression_loss: 0.3573 - classification_loss: 0.0491
 9746/10000 [============================>.] - ETA: 3:45 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9747/10000 [============================>.] - ETA: 3:44 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9748/10000 [============================>.] - ETA: 3:44 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9749/10000 [============================>.] - ETA: 3:43 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9750/10000 [============================>.] - ETA: 3:42 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9751/10000 [============================>.] - ETA: 3:41 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9752/10000 [============================>.] - ETA: 3:40 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9753/10000 [============================>.] - ETA: 3:39 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9754/10000 [============================>.] - ETA: 3:38 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9755/10000 [============================>.] - ETA: 3:37 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9756/10000 [============================>.] - ETA: 3:36 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9757/10000 [============================>.] - ETA: 3:36 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0492
 9758/10000 [============================>.] - ETA: 3:35 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0492
 9759/10000 [============================>.] - ETA: 3:34 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9760/10000 [============================>.] - ETA: 3:33 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0491
 9761/10000 [============================>.] - ETA: 3:32 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0492
 9762/10000 [============================>.] - ETA: 3:31 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0492
 9763/10000 [============================>.] - ETA: 3:30 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9764/10000 [============================>.] - ETA: 3:29 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0492
 9765/10000 [============================>.] - ETA: 3:28 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0492
 9766/10000 [============================>.] - ETA: 3:28 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0492
 9767/10000 [============================>.] - ETA: 3:27 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9768/10000 [============================>.] - ETA: 3:26 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9769/10000 [============================>.] - ETA: 3:25 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9770/10000 [============================>.] - ETA: 3:24 - loss: 0.4066 - regression_loss: 0.3575 - classification_loss: 0.0492
 9771/10000 [============================>.] - ETA: 3:23 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9772/10000 [============================>.] - ETA: 3:22 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9773/10000 [============================>.] - ETA: 3:21 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0492
 9774/10000 [============================>.] - ETA: 3:20 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0491
 9775/10000 [============================>.] - ETA: 3:20 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0491
 9776/10000 [============================>.] - ETA: 3:19 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0491
 9777/10000 [============================>.] - ETA: 3:18 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9778/10000 [============================>.] - ETA: 3:17 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9779/10000 [============================>.] - ETA: 3:16 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9780/10000 [============================>.] - ETA: 3:15 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9781/10000 [============================>.] - ETA: 3:14 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9782/10000 [============================>.] - ETA: 3:13 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9783/10000 [============================>.] - ETA: 3:12 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9784/10000 [============================>.] - ETA: 3:12 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0491
 9785/10000 [============================>.] - ETA: 3:11 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9786/10000 [============================>.] - ETA: 3:10 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9787/10000 [============================>.] - ETA: 3:09 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9788/10000 [============================>.] - ETA: 3:08 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9789/10000 [============================>.] - ETA: 3:07 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9790/10000 [============================>.] - ETA: 3:06 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9791/10000 [============================>.] - ETA: 3:05 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9792/10000 [============================>.] - ETA: 3:04 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9793/10000 [============================>.] - ETA: 3:04 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9794/10000 [============================>.] - ETA: 3:03 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9795/10000 [============================>.] - ETA: 3:02 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9796/10000 [============================>.] - ETA: 3:01 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9797/10000 [============================>.] - ETA: 3:00 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9798/10000 [============================>.] - ETA: 2:59 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9799/10000 [============================>.] - ETA: 2:58 - loss: 0.4066 - regression_loss: 0.3575 - classification_loss: 0.0491
 9800/10000 [============================>.] - ETA: 2:57 - loss: 0.4066 - regression_loss: 0.3575 - classification_loss: 0.0491
 9801/10000 [============================>.] - ETA: 2:56 - loss: 0.4066 - regression_loss: 0.3575 - classification_loss: 0.0491
 9802/10000 [============================>.] - ETA: 2:56 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0491
 9803/10000 [============================>.] - ETA: 2:55 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0491
 9804/10000 [============================>.] - ETA: 2:54 - loss: 0.4066 - regression_loss: 0.3575 - classification_loss: 0.0491
 9805/10000 [============================>.] - ETA: 2:53 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0491
 9806/10000 [============================>.] - ETA: 2:52 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9807/10000 [============================>.] - ETA: 2:51 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9808/10000 [============================>.] - ETA: 2:50 - loss: 0.4067 - regression_loss: 0.3575 - classification_loss: 0.0492
 9809/10000 [============================>.] - ETA: 2:49 - loss: 0.4066 - regression_loss: 0.3575 - classification_loss: 0.0491
 9810/10000 [============================>.] - ETA: 2:48 - loss: 0.4066 - regression_loss: 0.3575 - classification_loss: 0.0491
 9811/10000 [============================>.] - ETA: 2:48 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0491
 9812/10000 [============================>.] - ETA: 2:47 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0491
 9813/10000 [============================>.] - ETA: 2:46 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9814/10000 [============================>.] - ETA: 2:45 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9815/10000 [============================>.] - ETA: 2:44 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9816/10000 [============================>.] - ETA: 2:43 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0491
 9817/10000 [============================>.] - ETA: 2:42 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0491
 9818/10000 [============================>.] - ETA: 2:41 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9819/10000 [============================>.] - ETA: 2:40 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9820/10000 [============================>.] - ETA: 2:40 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9821/10000 [============================>.] - ETA: 2:39 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0491
 9822/10000 [============================>.] - ETA: 2:38 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0491
 9823/10000 [============================>.] - ETA: 2:37 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9824/10000 [============================>.] - ETA: 2:36 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9825/10000 [============================>.] - ETA: 2:35 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0491
 9826/10000 [============================>.] - ETA: 2:34 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0491
 9827/10000 [============================>.] - ETA: 2:33 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0491
 9828/10000 [============================>.] - ETA: 2:32 - loss: 0.4066 - regression_loss: 0.3574 - classification_loss: 0.0491
 9829/10000 [============================>.] - ETA: 2:32 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9830/10000 [============================>.] - ETA: 2:31 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9831/10000 [============================>.] - ETA: 2:30 - loss: 0.4065 - regression_loss: 0.3574 - classification_loss: 0.0491
 9832/10000 [============================>.] - ETA: 2:29 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0491
 9833/10000 [============================>.] - ETA: 2:28 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9834/10000 [============================>.] - ETA: 2:27 - loss: 0.4065 - regression_loss: 0.3573 - classification_loss: 0.0491
 9835/10000 [============================>.] - ETA: 2:26 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9836/10000 [============================>.] - ETA: 2:25 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9837/10000 [============================>.] - ETA: 2:24 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9838/10000 [============================>.] - ETA: 2:24 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0491
 9839/10000 [============================>.] - ETA: 2:23 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
 9840/10000 [============================>.] - ETA: 2:22 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
 9841/10000 [============================>.] - ETA: 2:21 - loss: 0.4064 - regression_loss: 0.3572 - classification_loss: 0.0491
 9842/10000 [============================>.] - ETA: 2:20 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9843/10000 [============================>.] - ETA: 2:19 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
 9844/10000 [============================>.] - ETA: 2:18 - loss: 0.4064 - regression_loss: 0.3573 - classification_loss: 0.0491
 9845/10000 [============================>.] - ETA: 2:17 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
 9846/10000 [============================>.] - ETA: 2:16 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
 9847/10000 [============================>.] - ETA: 2:16 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
 9848/10000 [============================>.] - ETA: 2:15 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
 9849/10000 [============================>.] - ETA: 2:14 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
 9850/10000 [============================>.] - ETA: 2:13 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
 9851/10000 [============================>.] - ETA: 2:12 - loss: 0.4063 - regression_loss: 0.3572 - classification_loss: 0.0491
 9852/10000 [============================>.] - ETA: 2:11 - loss: 0.4062 - regression_loss: 0.3572 - classification_loss: 0.0491
 9853/10000 [============================>.] - ETA: 2:10 - loss: 0.4062 - regression_loss: 0.3571 - classification_loss: 0.0491
 9854/10000 [============================>.] - ETA: 2:09 - loss: 0.4062 - regression_loss: 0.3571 - classification_loss: 0.0491
 9855/10000 [============================>.] - ETA: 2:08 - loss: 0.4062 - regression_loss: 0.3571 - classification_loss: 0.0491
 9856/10000 [============================>.] - ETA: 2:08 - loss: 0.4062 - regression_loss: 0.3571 - classification_loss: 0.0491
 9857/10000 [============================>.] - ETA: 2:07 - loss: 0.4062 - regression_loss: 0.3571 - classification_loss: 0.0491
 9858/10000 [============================>.] - ETA: 2:06 - loss: 0.4062 - regression_loss: 0.3571 - classification_loss: 0.0491
 9859/10000 [============================>.] - ETA: 2:05 - loss: 0.4062 - regression_loss: 0.3571 - classification_loss: 0.0491
 9860/10000 [============================>.] - ETA: 2:04 - loss: 0.4062 - regression_loss: 0.3571 - classification_loss: 0.0491
 9861/10000 [============================>.] - ETA: 2:03 - loss: 0.4061 - regression_loss: 0.3571 - classification_loss: 0.0491
 9862/10000 [============================>.] - ETA: 2:02 - loss: 0.4061 - regression_loss: 0.3571 - classification_loss: 0.0491
 9863/10000 [============================>.] - ETA: 2:01 - loss: 0.4061 - regression_loss: 0.3571 - classification_loss: 0.0491
 9864/10000 [============================>.] - ETA: 2:00 - loss: 0.4061 - regression_loss: 0.3570 - classification_loss: 0.0491
 9865/10000 [============================>.] - ETA: 2:00 - loss: 0.4061 - regression_loss: 0.3570 - classification_loss: 0.0491
 9866/10000 [============================>.] - ETA: 1:59 - loss: 0.4062 - regression_loss: 0.3571 - classification_loss: 0.0491
 9867/10000 [============================>.] - ETA: 1:58 - loss: 0.4062 - regression_loss: 0.3571 - classification_loss: 0.0491
 9868/10000 [============================>.] - ETA: 1:57 - loss: 0.4061 - regression_loss: 0.3571 - classification_loss: 0.0491
 9869/10000 [============================>.] - ETA: 1:56 - loss: 0.4061 - regression_loss: 0.3570 - classification_loss: 0.0490
 9870/10000 [============================>.] - ETA: 1:55 - loss: 0.4061 - regression_loss: 0.3570 - classification_loss: 0.0490
 9871/10000 [============================>.] - ETA: 1:54 - loss: 0.4061 - regression_loss: 0.3571 - classification_loss: 0.0490
 9872/10000 [============================>.] - ETA: 1:53 - loss: 0.4061 - regression_loss: 0.3571 - classification_loss: 0.0490
 9873/10000 [============================>.] - ETA: 1:52 - loss: 0.4061 - regression_loss: 0.3570 - classification_loss: 0.0490
 9874/10000 [============================>.] - ETA: 1:52 - loss: 0.4061 - regression_loss: 0.3570 - classification_loss: 0.0490
 9875/10000 [============================>.] - ETA: 1:51 - loss: 0.4061 - regression_loss: 0.3570 - classification_loss: 0.0490
 9876/10000 [============================>.] - ETA: 1:50 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9877/10000 [============================>.] - ETA: 1:49 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9878/10000 [============================>.] - ETA: 1:48 - loss: 0.4061 - regression_loss: 0.3570 - classification_loss: 0.0490
 9879/10000 [============================>.] - ETA: 1:47 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9880/10000 [============================>.] - ETA: 1:46 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9881/10000 [============================>.] - ETA: 1:45 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9882/10000 [============================>.] - ETA: 1:44 - loss: 0.4061 - regression_loss: 0.3570 - classification_loss: 0.0490
 9883/10000 [============================>.] - ETA: 1:44 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9884/10000 [============================>.] - ETA: 1:43 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9885/10000 [============================>.] - ETA: 1:42 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9886/10000 [============================>.] - ETA: 1:41 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9887/10000 [============================>.] - ETA: 1:40 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9888/10000 [============================>.] - ETA: 1:39 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9889/10000 [============================>.] - ETA: 1:38 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9890/10000 [============================>.] - ETA: 1:37 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9891/10000 [============================>.] - ETA: 1:36 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9892/10000 [============================>.] - ETA: 1:36 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9893/10000 [============================>.] - ETA: 1:35 - loss: 0.4061 - regression_loss: 0.3571 - classification_loss: 0.0490
 9894/10000 [============================>.] - ETA: 1:34 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9895/10000 [============================>.] - ETA: 1:33 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9896/10000 [============================>.] - ETA: 1:32 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9897/10000 [============================>.] - ETA: 1:31 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9898/10000 [============================>.] - ETA: 1:30 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9899/10000 [============================>.] - ETA: 1:29 - loss: 0.4059 - regression_loss: 0.3570 - classification_loss: 0.0490
 9900/10000 [============================>.] - ETA: 1:28 - loss: 0.4059 - regression_loss: 0.3569 - classification_loss: 0.0490
 9901/10000 [============================>.] - ETA: 1:28 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9902/10000 [============================>.] - ETA: 1:27 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9903/10000 [============================>.] - ETA: 1:26 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9904/10000 [============================>.] - ETA: 1:25 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9905/10000 [============================>.] - ETA: 1:24 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9906/10000 [============================>.] - ETA: 1:23 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9907/10000 [============================>.] - ETA: 1:22 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9908/10000 [============================>.] - ETA: 1:21 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9909/10000 [============================>.] - ETA: 1:20 - loss: 0.4061 - regression_loss: 0.3571 - classification_loss: 0.0490
 9910/10000 [============================>.] - ETA: 1:20 - loss: 0.4060 - regression_loss: 0.3571 - classification_loss: 0.0490
 9911/10000 [============================>.] - ETA: 1:19 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9912/10000 [============================>.] - ETA: 1:18 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9913/10000 [============================>.] - ETA: 1:17 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9914/10000 [============================>.] - ETA: 1:16 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9915/10000 [============================>.] - ETA: 1:15 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9916/10000 [============================>.] - ETA: 1:14 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9917/10000 [============================>.] - ETA: 1:13 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9918/10000 [============================>.] - ETA: 1:12 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9919/10000 [============================>.] - ETA: 1:12 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9920/10000 [============================>.] - ETA: 1:11 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9921/10000 [============================>.] - ETA: 1:10 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9922/10000 [============================>.] - ETA: 1:09 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9923/10000 [============================>.] - ETA: 1:08 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9924/10000 [============================>.] - ETA: 1:07 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9925/10000 [============================>.] - ETA: 1:06 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9926/10000 [============================>.] - ETA: 1:05 - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0490
 9927/10000 [============================>.] - ETA: 1:04 - loss: 0.4060 - regression_loss: 0.3571 - classification_loss: 0.0490
 9928/10000 [============================>.] - ETA: 1:04 - loss: 0.4060 - regression_loss: 0.3571 - classification_loss: 0.0490
 9929/10000 [============================>.] - ETA: 1:03 - loss: 0.4060 - regression_loss: 0.3571 - classification_loss: 0.0490
 9930/10000 [============================>.] - ETA: 1:02 - loss: 0.4060 - regression_loss: 0.3571 - classification_loss: 0.0490
 9931/10000 [============================>.] - ETA: 1:01 - loss: 0.4060 - regression_loss: 0.3571 - classification_loss: 0.0490
 9932/10000 [============================>.] - ETA: 1:00 - loss: 0.4060 - regression_loss: 0.3571 - classification_loss: 0.0490
 9933/10000 [============================>.] - ETA: 59s - loss: 0.4060 - regression_loss: 0.3571 - classification_loss: 0.0489 
 9934/10000 [============================>.] - ETA: 58s - loss: 0.4060 - regression_loss: 0.3571 - classification_loss: 0.0489
 9935/10000 [============================>.] - ETA: 57s - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0489
 9936/10000 [============================>.] - ETA: 56s - loss: 0.4060 - regression_loss: 0.3570 - classification_loss: 0.0489
 9937/10000 [============================>.] - ETA: 56s - loss: 0.4060 - regression_loss: 0.3571 - classification_loss: 0.0489
 9938/10000 [============================>.] - ETA: 55s - loss: 0.4060 - regression_loss: 0.3571 - classification_loss: 0.0489
 9939/10000 [============================>.] - ETA: 54s - loss: 0.4061 - regression_loss: 0.3571 - classification_loss: 0.0489
 9940/10000 [============================>.] - ETA: 53s - loss: 0.4061 - regression_loss: 0.3571 - classification_loss: 0.0489
 9941/10000 [============================>.] - ETA: 52s - loss: 0.4061 - regression_loss: 0.3571 - classification_loss: 0.0489
 9942/10000 [============================>.] - ETA: 51s - loss: 0.4061 - regression_loss: 0.3571 - classification_loss: 0.0489
 9943/10000 [============================>.] - ETA: 50s - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 9944/10000 [============================>.] - ETA: 49s - loss: 0.4060 - regression_loss: 0.3571 - classification_loss: 0.0489
 9945/10000 [============================>.] - ETA: 48s - loss: 0.4060 - regression_loss: 0.3571 - classification_loss: 0.0489
 9946/10000 [============================>.] - ETA: 48s - loss: 0.4060 - regression_loss: 0.3571 - classification_loss: 0.0489
 9947/10000 [============================>.] - ETA: 47s - loss: 0.4060 - regression_loss: 0.3571 - classification_loss: 0.0489
 9948/10000 [============================>.] - ETA: 46s - loss: 0.4060 - regression_loss: 0.3571 - classification_loss: 0.0489
 9949/10000 [============================>.] - ETA: 45s - loss: 0.4060 - regression_loss: 0.3571 - classification_loss: 0.0489
 9950/10000 [============================>.] - ETA: 44s - loss: 0.4062 - regression_loss: 0.3572 - classification_loss: 0.0490
 9951/10000 [============================>.] - ETA: 43s - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0490
 9952/10000 [============================>.] - ETA: 42s - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0490
 9953/10000 [============================>.] - ETA: 41s - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0490
 9954/10000 [============================>.] - ETA: 40s - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0490
 9955/10000 [============================>.] - ETA: 40s - loss: 0.4061 - regression_loss: 0.3571 - classification_loss: 0.0490
 9956/10000 [============================>.] - ETA: 39s - loss: 0.4061 - regression_loss: 0.3571 - classification_loss: 0.0490
 9957/10000 [============================>.] - ETA: 38s - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0490
 9958/10000 [============================>.] - ETA: 37s - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0490
 9959/10000 [============================>.] - ETA: 36s - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0490
 9960/10000 [============================>.] - ETA: 35s - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 9961/10000 [============================>.] - ETA: 34s - loss: 0.4061 - regression_loss: 0.3571 - classification_loss: 0.0489
 9962/10000 [============================>.] - ETA: 33s - loss: 0.4061 - regression_loss: 0.3571 - classification_loss: 0.0489
 9963/10000 [============================>.] - ETA: 32s - loss: 0.4061 - regression_loss: 0.3571 - classification_loss: 0.0489
 9964/10000 [============================>.] - ETA: 32s - loss: 0.4061 - regression_loss: 0.3571 - classification_loss: 0.0489
 9965/10000 [============================>.] - ETA: 31s - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 9966/10000 [============================>.] - ETA: 30s - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 9967/10000 [============================>.] - ETA: 29s - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 9968/10000 [============================>.] - ETA: 28s - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 9969/10000 [============================>.] - ETA: 27s - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 9970/10000 [============================>.] - ETA: 26s - loss: 0.4062 - regression_loss: 0.3572 - classification_loss: 0.0489
 9971/10000 [============================>.] - ETA: 25s - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 9972/10000 [============================>.] - ETA: 24s - loss: 0.4062 - regression_loss: 0.3572 - classification_loss: 0.0489
 9973/10000 [============================>.] - ETA: 24s - loss: 0.4062 - regression_loss: 0.3572 - classification_loss: 0.0489
 9974/10000 [============================>.] - ETA: 23s - loss: 0.4062 - regression_loss: 0.3572 - classification_loss: 0.0489
 9975/10000 [============================>.] - ETA: 22s - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 9976/10000 [============================>.] - ETA: 21s - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 9977/10000 [============================>.] - ETA: 20s - loss: 0.4061 - regression_loss: 0.3572 - classification_loss: 0.0489
 9978/10000 [============================>.] - ETA: 19s - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 9979/10000 [============================>.] - ETA: 18s - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 9980/10000 [============================>.] - ETA: 17s - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 9981/10000 [============================>.] - ETA: 16s - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 9982/10000 [============================>.] - ETA: 16s - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 9983/10000 [============================>.] - ETA: 15s - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 9984/10000 [============================>.] - ETA: 14s - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 9985/10000 [============================>.] - ETA: 13s - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 9986/10000 [============================>.] - ETA: 12s - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 9987/10000 [============================>.] - ETA: 11s - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 9988/10000 [============================>.] - ETA: 10s - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 9989/10000 [============================>.] - ETA: 9s - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489 
 9990/10000 [============================>.] - ETA: 8s - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 9991/10000 [============================>.] - ETA: 8s - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 9992/10000 [============================>.] - ETA: 7s - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 9993/10000 [============================>.] - ETA: 6s - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 9994/10000 [============================>.] - ETA: 5s - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 9995/10000 [============================>.] - ETA: 4s - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 9996/10000 [============================>.] - ETA: 3s - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 9997/10000 [============================>.] - ETA: 2s - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 9998/10000 [============================>.] - ETA: 1s - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
 9999/10000 [============================>.] - ETA: 0s - loss: 0.4062 - regression_loss: 0.3573 - classification_loss: 0.0489
10000/10000 [==============================] - 8889s 889ms/step - loss: 0.4061 - regression_loss: 0.3573 - classification_loss: 0.0489

Epoch 00001: saving model to ./snapshots/vgg16_csv_01.h5
Epoch 2/50

    1/10000 [..............................] - ETA: 2:28:15 - loss: 0.4109 - regression_loss: 0.3394 - classification_loss: 0.0715
    2/10000 [..............................] - ETA: 2:27:51 - loss: 0.4409 - regression_loss: 0.4024 - classification_loss: 0.0385
    3/10000 [..............................] - ETA: 2:27:45 - loss: 0.4048 - regression_loss: 0.3679 - classification_loss: 0.0369
    4/10000 [..............................] - ETA: 2:27:52 - loss: 0.4883 - regression_loss: 0.4525 - classification_loss: 0.0358
    5/10000 [..............................] - ETA: 2:27:45 - loss: 0.4287 - regression_loss: 0.3901 - classification_loss: 0.0386
    6/10000 [..............................] - ETA: 2:27:42 - loss: 0.4061 - regression_loss: 0.3707 - classification_loss: 0.0354
    7/10000 [..............................] - ETA: 2:27:42 - loss: 0.4397 - regression_loss: 0.4048 - classification_loss: 0.0349
    8/10000 [..............................] - ETA: 2:27:43 - loss: 0.4032 - regression_loss: 0.3718 - classification_loss: 0.0314
    9/10000 [..............................] - ETA: 2:27:43 - loss: 0.4005 - regression_loss: 0.3527 - classification_loss: 0.0477
   10/10000 [..............................] - ETA: 2:27:43 - loss: 0.3748 - regression_loss: 0.3314 - classification_loss: 0.0434
   11/10000 [..............................] - ETA: 2:27:44 - loss: 0.3869 - regression_loss: 0.3463 - classification_loss: 0.0405
   12/10000 [..............................] - ETA: 2:27:42 - loss: 0.3853 - regression_loss: 0.3459 - classification_loss: 0.0394
   13/10000 [..............................] - ETA: 2:27:40 - loss: 0.3940 - regression_loss: 0.3542 - classification_loss: 0.0398
   14/10000 [..............................] - ETA: 2:27:39 - loss: 0.3849 - regression_loss: 0.3473 - classification_loss: 0.0377
   15/10000 [..............................] - ETA: 2:27:39 - loss: 0.3730 - regression_loss: 0.3373 - classification_loss: 0.0357
   16/10000 [..............................] - ETA: 2:27:38 - loss: 0.3594 - regression_loss: 0.3247 - classification_loss: 0.0347
   17/10000 [..............................] - ETA: 2:27:38 - loss: 0.3554 - regression_loss: 0.3224 - classification_loss: 0.0330
   18/10000 [..............................] - ETA: 2:27:39 - loss: 0.3427 - regression_loss: 0.3100 - classification_loss: 0.0327
   19/10000 [..............................] - ETA: 2:27:38 - loss: 0.3425 - regression_loss: 0.3097 - classification_loss: 0.0328
   20/10000 [..............................] - ETA: 2:27:39 - loss: 0.3486 - regression_loss: 0.3167 - classification_loss: 0.0318
   21/10000 [..............................] - ETA: 2:27:39 - loss: 0.3422 - regression_loss: 0.3113 - classification_loss: 0.0309
   22/10000 [..............................] - ETA: 2:27:37 - loss: 0.3299 - regression_loss: 0.3002 - classification_loss: 0.0297
   23/10000 [..............................] - ETA: 2:27:37 - loss: 0.3287 - regression_loss: 0.2973 - classification_loss: 0.0314
   24/10000 [..............................] - ETA: 2:27:36 - loss: 0.3276 - regression_loss: 0.2964 - classification_loss: 0.0311
   25/10000 [..............................] - ETA: 2:27:34 - loss: 0.3492 - regression_loss: 0.3163 - classification_loss: 0.0329
   26/10000 [..............................] - ETA: 2:27:32 - loss: 0.3578 - regression_loss: 0.3251 - classification_loss: 0.0326
   27/10000 [..............................] - ETA: 2:27:33 - loss: 0.3555 - regression_loss: 0.3231 - classification_loss: 0.0324
   28/10000 [..............................] - ETA: 2:27:32 - loss: 0.3601 - regression_loss: 0.3267 - classification_loss: 0.0333
   29/10000 [..............................] - ETA: 2:27:31 - loss: 0.3627 - regression_loss: 0.3300 - classification_loss: 0.0326
   30/10000 [..............................] - ETA: 2:27:31 - loss: 0.3624 - regression_loss: 0.3299 - classification_loss: 0.0325
   31/10000 [..............................] - ETA: 2:27:30 - loss: 0.3691 - regression_loss: 0.3354 - classification_loss: 0.0338
   32/10000 [..............................] - ETA: 2:27:27 - loss: 0.3633 - regression_loss: 0.3298 - classification_loss: 0.0335
   33/10000 [..............................] - ETA: 2:27:26 - loss: 0.3901 - regression_loss: 0.3514 - classification_loss: 0.0387
   34/10000 [..............................] - ETA: 2:27:25 - loss: 0.3843 - regression_loss: 0.3459 - classification_loss: 0.0384
   35/10000 [..............................] - ETA: 2:27:24 - loss: 0.3784 - regression_loss: 0.3404 - classification_loss: 0.0380
   36/10000 [..............................] - ETA: 2:27:23 - loss: 0.3814 - regression_loss: 0.3429 - classification_loss: 0.0385
   37/10000 [..............................] - ETA: 2:27:21 - loss: 0.3831 - regression_loss: 0.3427 - classification_loss: 0.0403
   38/10000 [..............................] - ETA: 2:27:21 - loss: 0.3837 - regression_loss: 0.3439 - classification_loss: 0.0399
   39/10000 [..............................] - ETA: 2:27:19 - loss: 0.3968 - regression_loss: 0.3545 - classification_loss: 0.0424
   40/10000 [..............................] - ETA: 2:27:19 - loss: 0.3909 - regression_loss: 0.3491 - classification_loss: 0.0418
   41/10000 [..............................] - ETA: 2:27:17 - loss: 0.3874 - regression_loss: 0.3464 - classification_loss: 0.0410
   42/10000 [..............................] - ETA: 2:27:16 - loss: 0.3820 - regression_loss: 0.3418 - classification_loss: 0.0401
   43/10000 [..............................] - ETA: 2:27:14 - loss: 0.3801 - regression_loss: 0.3406 - classification_loss: 0.0395
   44/10000 [..............................] - ETA: 2:27:13 - loss: 0.3844 - regression_loss: 0.3414 - classification_loss: 0.0430
   45/10000 [..............................] - ETA: 2:27:12 - loss: 0.3878 - regression_loss: 0.3448 - classification_loss: 0.0430
   46/10000 [..............................] - ETA: 2:27:11 - loss: 0.3870 - regression_loss: 0.3448 - classification_loss: 0.0422
   47/10000 [..............................] - ETA: 2:27:10 - loss: 0.3891 - regression_loss: 0.3469 - classification_loss: 0.0422
   48/10000 [..............................] - ETA: 2:27:08 - loss: 0.3901 - regression_loss: 0.3487 - classification_loss: 0.0414
   49/10000 [..............................] - ETA: 2:27:06 - loss: 0.3858 - regression_loss: 0.3450 - classification_loss: 0.0408
   50/10000 [..............................] - ETA: 2:27:07 - loss: 0.3869 - regression_loss: 0.3460 - classification_loss: 0.0410
   51/10000 [..............................] - ETA: 2:27:06 - loss: 0.3857 - regression_loss: 0.3450 - classification_loss: 0.0407
   52/10000 [..............................] - ETA: 2:27:06 - loss: 0.3859 - regression_loss: 0.3456 - classification_loss: 0.0403
   53/10000 [..............................] - ETA: 2:27:05 - loss: 0.3827 - regression_loss: 0.3425 - classification_loss: 0.0401
   54/10000 [..............................] - ETA: 2:27:04 - loss: 0.3818 - regression_loss: 0.3421 - classification_loss: 0.0397
   55/10000 [..............................] - ETA: 2:27:02 - loss: 0.3864 - regression_loss: 0.3436 - classification_loss: 0.0428
   56/10000 [..............................] - ETA: 2:27:01 - loss: 0.3823 - regression_loss: 0.3399 - classification_loss: 0.0424
   57/10000 [..............................] - ETA: 2:27:01 - loss: 0.3798 - regression_loss: 0.3378 - classification_loss: 0.0420
   58/10000 [..............................] - ETA: 2:26:59 - loss: 0.3774 - regression_loss: 0.3347 - classification_loss: 0.0427
   59/10000 [..............................] - ETA: 2:26:58 - loss: 0.3830 - regression_loss: 0.3393 - classification_loss: 0.0437
   60/10000 [..............................] - ETA: 2:26:57 - loss: 0.3766 - regression_loss: 0.3337 - classification_loss: 0.0430
   61/10000 [..............................] - ETA: 2:26:56 - loss: 0.3756 - regression_loss: 0.3331 - classification_loss: 0.0425
   62/10000 [..............................] - ETA: 2:26:55 - loss: 0.3723 - regression_loss: 0.3299 - classification_loss: 0.0424
   63/10000 [..............................] - ETA: 2:26:54 - loss: 0.3722 - regression_loss: 0.3298 - classification_loss: 0.0424
   64/10000 [..............................] - ETA: 2:26:54 - loss: 0.3715 - regression_loss: 0.3293 - classification_loss: 0.0423
   65/10000 [..............................] - ETA: 2:26:53 - loss: 0.3738 - regression_loss: 0.3313 - classification_loss: 0.0425
   66/10000 [..............................] - ETA: 2:26:52 - loss: 0.3786 - regression_loss: 0.3294 - classification_loss: 0.0492
   67/10000 [..............................] - ETA: 2:26:51 - loss: 0.3866 - regression_loss: 0.3375 - classification_loss: 0.0491
   68/10000 [..............................] - ETA: 2:26:50 - loss: 0.3891 - regression_loss: 0.3401 - classification_loss: 0.0490
   69/10000 [..............................] - ETA: 2:26:50 - loss: 0.3869 - regression_loss: 0.3384 - classification_loss: 0.0485
   70/10000 [..............................] - ETA: 2:26:50 - loss: 0.3846 - regression_loss: 0.3360 - classification_loss: 0.0486
   71/10000 [..............................] - ETA: 2:26:49 - loss: 0.3852 - regression_loss: 0.3368 - classification_loss: 0.0484
   72/10000 [..............................] - ETA: 2:26:48 - loss: 0.3844 - regression_loss: 0.3364 - classification_loss: 0.0479
   73/10000 [..............................] - ETA: 2:26:48 - loss: 0.3828 - regression_loss: 0.3349 - classification_loss: 0.0479
   74/10000 [..............................] - ETA: 2:26:47 - loss: 0.3812 - regression_loss: 0.3335 - classification_loss: 0.0477
   75/10000 [..............................] - ETA: 2:26:46 - loss: 0.3801 - regression_loss: 0.3325 - classification_loss: 0.0476
   76/10000 [..............................] - ETA: 2:26:45 - loss: 0.3853 - regression_loss: 0.3353 - classification_loss: 0.0500
   77/10000 [..............................] - ETA: 2:26:44 - loss: 0.3894 - regression_loss: 0.3393 - classification_loss: 0.0501
   78/10000 [..............................] - ETA: 2:26:43 - loss: 0.3888 - regression_loss: 0.3391 - classification_loss: 0.0497
   79/10000 [..............................] - ETA: 2:26:42 - loss: 0.3855 - regression_loss: 0.3362 - classification_loss: 0.0493
   80/10000 [..............................] - ETA: 2:26:42 - loss: 0.3831 - regression_loss: 0.3342 - classification_loss: 0.0489
   81/10000 [..............................] - ETA: 2:26:42 - loss: 0.3820 - regression_loss: 0.3335 - classification_loss: 0.0485
   82/10000 [..............................] - ETA: 2:26:41 - loss: 0.3792 - regression_loss: 0.3312 - classification_loss: 0.0480
   83/10000 [..............................] - ETA: 2:26:40 - loss: 0.3772 - regression_loss: 0.3295 - classification_loss: 0.0476
   84/10000 [..............................] - ETA: 2:26:39 - loss: 0.3739 - regression_loss: 0.3267 - classification_loss: 0.0471
   85/10000 [..............................] - ETA: 2:26:39 - loss: 0.3733 - regression_loss: 0.3266 - classification_loss: 0.0467
   86/10000 [..............................] - ETA: 2:26:38 - loss: 0.3742 - regression_loss: 0.3277 - classification_loss: 0.0465
   87/10000 [..............................] - ETA: 2:26:38 - loss: 0.3718 - regression_loss: 0.3258 - classification_loss: 0.0460
   88/10000 [..............................] - ETA: 2:26:37 - loss: 0.3701 - regression_loss: 0.3244 - classification_loss: 0.0457
   89/10000 [..............................] - ETA: 2:26:36 - loss: 0.3729 - regression_loss: 0.3273 - classification_loss: 0.0456
   90/10000 [..............................] - ETA: 2:26:35 - loss: 0.3715 - regression_loss: 0.3260 - classification_loss: 0.0455
   91/10000 [..............................] - ETA: 2:26:34 - loss: 0.3729 - regression_loss: 0.3278 - classification_loss: 0.0451
   92/10000 [..............................] - ETA: 2:26:33 - loss: 0.3722 - regression_loss: 0.3273 - classification_loss: 0.0449
   93/10000 [..............................] - ETA: 2:26:32 - loss: 0.3708 - regression_loss: 0.3261 - classification_loss: 0.0447
   94/10000 [..............................] - ETA: 2:26:31 - loss: 0.3714 - regression_loss: 0.3271 - classification_loss: 0.0443
   95/10000 [..............................] - ETA: 2:26:30 - loss: 0.3701 - regression_loss: 0.3262 - classification_loss: 0.0439
   96/10000 [..............................] - ETA: 2:26:29 - loss: 0.3720 - regression_loss: 0.3279 - classification_loss: 0.0440
   97/10000 [..............................] - ETA: 2:26:28 - loss: 0.3726 - regression_loss: 0.3286 - classification_loss: 0.0440
   98/10000 [..............................] - ETA: 2:26:27 - loss: 0.3704 - regression_loss: 0.3267 - classification_loss: 0.0436
   99/10000 [..............................] - ETA: 2:26:26 - loss: 0.3697 - regression_loss: 0.3258 - classification_loss: 0.0438
  100/10000 [..............................] - ETA: 2:26:25 - loss: 0.3688 - regression_loss: 0.3254 - classification_loss: 0.0435
  101/10000 [..............................] - ETA: 2:26:24 - loss: 0.3699 - regression_loss: 0.3262 - classification_loss: 0.0437
  102/10000 [..............................] - ETA: 2:26:23 - loss: 0.3681 - regression_loss: 0.3248 - classification_loss: 0.0433
  103/10000 [..............................] - ETA: 2:26:22 - loss: 0.3672 - regression_loss: 0.3240 - classification_loss: 0.0432
  104/10000 [..............................] - ETA: 2:26:21 - loss: 0.3663 - regression_loss: 0.3232 - classification_loss: 0.0430
  105/10000 [..............................] - ETA: 2:26:20 - loss: 0.3657 - regression_loss: 0.3225 - classification_loss: 0.0431
  106/10000 [..............................] - ETA: 2:26:19 - loss: 0.3652 - regression_loss: 0.3220 - classification_loss: 0.0432
  107/10000 [..............................] - ETA: 2:26:18 - loss: 0.3651 - regression_loss: 0.3220 - classification_loss: 0.0431
  108/10000 [..............................] - ETA: 2:26:17 - loss: 0.3671 - regression_loss: 0.3236 - classification_loss: 0.0435
  109/10000 [..............................] - ETA: 2:26:17 - loss: 0.3724 - regression_loss: 0.3277 - classification_loss: 0.0447
  110/10000 [..............................] - ETA: 2:26:15 - loss: 0.3733 - regression_loss: 0.3280 - classification_loss: 0.0452
  111/10000 [..............................] - ETA: 2:26:14 - loss: 0.3720 - regression_loss: 0.3269 - classification_loss: 0.0450
  112/10000 [..............................] - ETA: 2:26:14 - loss: 0.3768 - regression_loss: 0.3311 - classification_loss: 0.0456
  113/10000 [..............................] - ETA: 2:26:13 - loss: 0.3775 - regression_loss: 0.3319 - classification_loss: 0.0456
  114/10000 [..............................] - ETA: 2:26:12 - loss: 0.3780 - regression_loss: 0.3325 - classification_loss: 0.0455
  115/10000 [..............................] - ETA: 2:26:11 - loss: 0.3747 - regression_loss: 0.3296 - classification_loss: 0.0451
  116/10000 [..............................] - ETA: 2:26:10 - loss: 0.3746 - regression_loss: 0.3295 - classification_loss: 0.0452
  117/10000 [..............................] - ETA: 2:26:09 - loss: 0.3727 - regression_loss: 0.3278 - classification_loss: 0.0449
  118/10000 [..............................] - ETA: 2:26:09 - loss: 0.3753 - regression_loss: 0.3304 - classification_loss: 0.0448
  119/10000 [..............................] - ETA: 2:26:08 - loss: 0.3744 - regression_loss: 0.3299 - classification_loss: 0.0445
  120/10000 [..............................] - ETA: 2:26:07 - loss: 0.3744 - regression_loss: 0.3300 - classification_loss: 0.0444
  121/10000 [..............................] - ETA: 2:26:06 - loss: 0.3754 - regression_loss: 0.3309 - classification_loss: 0.0445
  122/10000 [..............................] - ETA: 2:26:05 - loss: 0.3763 - regression_loss: 0.3321 - classification_loss: 0.0442
  123/10000 [..............................] - ETA: 2:26:04 - loss: 0.3827 - regression_loss: 0.3378 - classification_loss: 0.0449
  124/10000 [..............................] - ETA: 2:26:03 - loss: 0.3819 - regression_loss: 0.3371 - classification_loss: 0.0448
  125/10000 [..............................] - ETA: 2:26:02 - loss: 0.3866 - regression_loss: 0.3412 - classification_loss: 0.0454
  126/10000 [..............................] - ETA: 2:26:01 - loss: 0.3853 - regression_loss: 0.3401 - classification_loss: 0.0452
  127/10000 [..............................] - ETA: 2:26:00 - loss: 0.3896 - regression_loss: 0.3440 - classification_loss: 0.0456
  128/10000 [..............................] - ETA: 2:25:59 - loss: 0.3895 - regression_loss: 0.3440 - classification_loss: 0.0455
  129/10000 [..............................] - ETA: 2:25:58 - loss: 0.3887 - regression_loss: 0.3434 - classification_loss: 0.0453
  130/10000 [..............................] - ETA: 2:25:58 - loss: 0.3905 - regression_loss: 0.3454 - classification_loss: 0.0451
  131/10000 [..............................] - ETA: 2:25:57 - loss: 0.3892 - regression_loss: 0.3443 - classification_loss: 0.0449
  132/10000 [..............................] - ETA: 2:25:57 - loss: 0.3898 - regression_loss: 0.3449 - classification_loss: 0.0449
  133/10000 [..............................] - ETA: 2:25:56 - loss: 0.3898 - regression_loss: 0.3452 - classification_loss: 0.0446
  134/10000 [..............................] - ETA: 2:25:55 - loss: 0.3903 - regression_loss: 0.3459 - classification_loss: 0.0445
  135/10000 [..............................] - ETA: 2:25:54 - loss: 0.3901 - regression_loss: 0.3460 - classification_loss: 0.0442
  136/10000 [..............................] - ETA: 2:25:53 - loss: 0.3892 - regression_loss: 0.3452 - classification_loss: 0.0441
  137/10000 [..............................] - ETA: 2:25:52 - loss: 0.3886 - regression_loss: 0.3448 - classification_loss: 0.0438
  138/10000 [..............................] - ETA: 2:25:51 - loss: 0.3905 - regression_loss: 0.3464 - classification_loss: 0.0441
  139/10000 [..............................] - ETA: 2:25:50 - loss: 0.3897 - regression_loss: 0.3454 - classification_loss: 0.0443
  140/10000 [..............................] - ETA: 2:25:49 - loss: 0.3907 - regression_loss: 0.3463 - classification_loss: 0.0444
  141/10000 [..............................] - ETA: 2:25:49 - loss: 0.3896 - regression_loss: 0.3454 - classification_loss: 0.0442
  142/10000 [..............................] - ETA: 2:25:48 - loss: 0.3909 - regression_loss: 0.3467 - classification_loss: 0.0442
  143/10000 [..............................] - ETA: 2:25:47 - loss: 0.3911 - regression_loss: 0.3467 - classification_loss: 0.0443
  144/10000 [..............................] - ETA: 2:25:46 - loss: 0.3906 - regression_loss: 0.3463 - classification_loss: 0.0443
  145/10000 [..............................] - ETA: 2:25:46 - loss: 0.3915 - regression_loss: 0.3474 - classification_loss: 0.0442
  146/10000 [..............................] - ETA: 2:25:45 - loss: 0.3888 - regression_loss: 0.3450 - classification_loss: 0.0439
  147/10000 [..............................] - ETA: 2:25:44 - loss: 0.3879 - regression_loss: 0.3442 - classification_loss: 0.0437
  148/10000 [..............................] - ETA: 2:25:43 - loss: 0.3901 - regression_loss: 0.3463 - classification_loss: 0.0437
  149/10000 [..............................] - ETA: 2:25:42 - loss: 0.3874 - regression_loss: 0.3440 - classification_loss: 0.0434
  150/10000 [..............................] - ETA: 2:25:41 - loss: 0.3859 - regression_loss: 0.3426 - classification_loss: 0.0433
  151/10000 [..............................] - ETA: 2:25:40 - loss: 0.3856 - regression_loss: 0.3421 - classification_loss: 0.0435
  152/10000 [..............................] - ETA: 2:25:39 - loss: 0.3857 - regression_loss: 0.3421 - classification_loss: 0.0435
  153/10000 [..............................] - ETA: 2:25:38 - loss: 0.3831 - regression_loss: 0.3399 - classification_loss: 0.0432
  154/10000 [..............................] - ETA: 2:25:37 - loss: 0.3824 - regression_loss: 0.3394 - classification_loss: 0.0430
  155/10000 [..............................] - ETA: 2:25:37 - loss: 0.3824 - regression_loss: 0.3396 - classification_loss: 0.0428
  156/10000 [..............................] - ETA: 2:25:35 - loss: 0.3831 - regression_loss: 0.3405 - classification_loss: 0.0426
  157/10000 [..............................] - ETA: 2:25:35 - loss: 0.3825 - regression_loss: 0.3400 - classification_loss: 0.0425
  158/10000 [..............................] - ETA: 2:25:34 - loss: 0.3828 - regression_loss: 0.3401 - classification_loss: 0.0428
  159/10000 [..............................] - ETA: 2:25:33 - loss: 0.3826 - regression_loss: 0.3400 - classification_loss: 0.0426
  160/10000 [..............................] - ETA: 2:25:32 - loss: 0.3822 - regression_loss: 0.3398 - classification_loss: 0.0424
  161/10000 [..............................] - ETA: 2:25:31 - loss: 0.3822 - regression_loss: 0.3400 - classification_loss: 0.0422
  162/10000 [..............................] - ETA: 2:25:30 - loss: 0.3816 - regression_loss: 0.3395 - classification_loss: 0.0421
  163/10000 [..............................] - ETA: 2:25:29 - loss: 0.3812 - regression_loss: 0.3391 - classification_loss: 0.0420
  164/10000 [..............................] - ETA: 2:25:28 - loss: 0.3798 - regression_loss: 0.3379 - classification_loss: 0.0419
  165/10000 [..............................] - ETA: 2:25:27 - loss: 0.3797 - regression_loss: 0.3363 - classification_loss: 0.0434
  166/10000 [..............................] - ETA: 2:25:27 - loss: 0.3833 - regression_loss: 0.3399 - classification_loss: 0.0434
  167/10000 [..............................] - ETA: 2:25:26 - loss: 0.3825 - regression_loss: 0.3392 - classification_loss: 0.0433
  168/10000 [..............................] - ETA: 2:25:25 - loss: 0.3849 - regression_loss: 0.3418 - classification_loss: 0.0431
  169/10000 [..............................] - ETA: 2:25:24 - loss: 0.3839 - regression_loss: 0.3409 - classification_loss: 0.0430
  170/10000 [..............................] - ETA: 2:25:23 - loss: 0.3850 - regression_loss: 0.3420 - classification_loss: 0.0430
  171/10000 [..............................] - ETA: 2:25:22 - loss: 0.3846 - regression_loss: 0.3418 - classification_loss: 0.0428
  172/10000 [..............................] - ETA: 2:25:21 - loss: 0.3845 - regression_loss: 0.3418 - classification_loss: 0.0426
  173/10000 [..............................] - ETA: 2:25:20 - loss: 0.3837 - regression_loss: 0.3412 - classification_loss: 0.0425
  174/10000 [..............................] - ETA: 2:25:19 - loss: 0.3829 - regression_loss: 0.3406 - classification_loss: 0.0423
  175/10000 [..............................] - ETA: 2:25:19 - loss: 0.3834 - regression_loss: 0.3411 - classification_loss: 0.0423
  176/10000 [..............................] - ETA: 2:25:18 - loss: 0.3826 - regression_loss: 0.3401 - classification_loss: 0.0425
  177/10000 [..............................] - ETA: 2:25:17 - loss: 0.3848 - regression_loss: 0.3422 - classification_loss: 0.0426
  178/10000 [..............................] - ETA: 2:25:16 - loss: 0.3867 - regression_loss: 0.3432 - classification_loss: 0.0435
  179/10000 [..............................] - ETA: 2:25:15 - loss: 0.3860 - regression_loss: 0.3427 - classification_loss: 0.0434
  180/10000 [..............................] - ETA: 2:25:14 - loss: 0.3855 - regression_loss: 0.3423 - classification_loss: 0.0432
  181/10000 [..............................] - ETA: 2:25:13 - loss: 0.3861 - regression_loss: 0.3427 - classification_loss: 0.0434
  182/10000 [..............................] - ETA: 2:25:12 - loss: 0.3857 - regression_loss: 0.3424 - classification_loss: 0.0432
  183/10000 [..............................] - ETA: 2:25:11 - loss: 0.3844 - regression_loss: 0.3413 - classification_loss: 0.0431
  184/10000 [..............................] - ETA: 2:25:10 - loss: 0.3823 - regression_loss: 0.3395 - classification_loss: 0.0429
  185/10000 [..............................] - ETA: 2:25:10 - loss: 0.3819 - regression_loss: 0.3388 - classification_loss: 0.0431
  186/10000 [..............................] - ETA: 2:25:09 - loss: 0.3832 - regression_loss: 0.3400 - classification_loss: 0.0432
  187/10000 [..............................] - ETA: 2:25:08 - loss: 0.3839 - regression_loss: 0.3406 - classification_loss: 0.0433
  188/10000 [..............................] - ETA: 2:25:07 - loss: 0.3907 - regression_loss: 0.3467 - classification_loss: 0.0440
  189/10000 [..............................] - ETA: 2:25:06 - loss: 0.3896 - regression_loss: 0.3458 - classification_loss: 0.0438
  190/10000 [..............................] - ETA: 2:25:05 - loss: 0.3907 - regression_loss: 0.3467 - classification_loss: 0.0440
  191/10000 [..............................] - ETA: 2:25:04 - loss: 0.3896 - regression_loss: 0.3458 - classification_loss: 0.0438
  192/10000 [..............................] - ETA: 2:25:03 - loss: 0.3876 - regression_loss: 0.3440 - classification_loss: 0.0435
  193/10000 [..............................] - ETA: 2:25:02 - loss: 0.3875 - regression_loss: 0.3439 - classification_loss: 0.0436
  194/10000 [..............................] - ETA: 2:25:01 - loss: 0.3879 - regression_loss: 0.3445 - classification_loss: 0.0434
  195/10000 [..............................] - ETA: 2:25:00 - loss: 0.3859 - regression_loss: 0.3427 - classification_loss: 0.0432
  196/10000 [..............................] - ETA: 2:24:59 - loss: 0.3865 - regression_loss: 0.3428 - classification_loss: 0.0437
  197/10000 [..............................] - ETA: 2:24:59 - loss: 0.3854 - regression_loss: 0.3418 - classification_loss: 0.0436
  198/10000 [..............................] - ETA: 2:24:58 - loss: 0.3876 - regression_loss: 0.3442 - classification_loss: 0.0434
  199/10000 [..............................] - ETA: 2:24:57 - loss: 0.3888 - regression_loss: 0.3451 - classification_loss: 0.0436
  200/10000 [..............................] - ETA: 2:24:56 - loss: 0.3889 - regression_loss: 0.3454 - classification_loss: 0.0435
  201/10000 [..............................] - ETA: 2:24:55 - loss: 0.3903 - regression_loss: 0.3470 - classification_loss: 0.0433
  202/10000 [..............................] - ETA: 2:24:54 - loss: 0.3922 - regression_loss: 0.3490 - classification_loss: 0.0431
  203/10000 [..............................] - ETA: 2:24:54 - loss: 0.3902 - regression_loss: 0.3473 - classification_loss: 0.0429
  204/10000 [..............................] - ETA: 2:24:53 - loss: 0.3896 - regression_loss: 0.3468 - classification_loss: 0.0429
  205/10000 [..............................] - ETA: 2:24:52 - loss: 0.3891 - regression_loss: 0.3464 - classification_loss: 0.0428
  206/10000 [..............................] - ETA: 2:24:51 - loss: 0.3873 - regression_loss: 0.3447 - classification_loss: 0.0425
  207/10000 [..............................] - ETA: 2:24:50 - loss: 0.3865 - regression_loss: 0.3441 - classification_loss: 0.0424
  208/10000 [..............................] - ETA: 2:24:49 - loss: 0.3859 - regression_loss: 0.3436 - classification_loss: 0.0423
  209/10000 [..............................] - ETA: 2:24:48 - loss: 0.3849 - regression_loss: 0.3426 - classification_loss: 0.0423
  210/10000 [..............................] - ETA: 2:24:47 - loss: 0.3863 - regression_loss: 0.3439 - classification_loss: 0.0424
  211/10000 [..............................] - ETA: 2:24:46 - loss: 0.3858 - regression_loss: 0.3435 - classification_loss: 0.0423
  212/10000 [..............................] - ETA: 2:24:45 - loss: 0.3858 - regression_loss: 0.3437 - classification_loss: 0.0421
  213/10000 [..............................] - ETA: 2:24:44 - loss: 0.3875 - regression_loss: 0.3450 - classification_loss: 0.0425
  214/10000 [..............................] - ETA: 2:24:44 - loss: 0.3864 - regression_loss: 0.3440 - classification_loss: 0.0424
  215/10000 [..............................] - ETA: 2:24:43 - loss: 0.3868 - regression_loss: 0.3443 - classification_loss: 0.0426
  216/10000 [..............................] - ETA: 2:24:42 - loss: 0.3865 - regression_loss: 0.3441 - classification_loss: 0.0425
  217/10000 [..............................] - ETA: 2:24:41 - loss: 0.3862 - regression_loss: 0.3438 - classification_loss: 0.0424
  218/10000 [..............................] - ETA: 2:24:40 - loss: 0.3844 - regression_loss: 0.3422 - classification_loss: 0.0422
  219/10000 [..............................] - ETA: 2:24:39 - loss: 0.3858 - regression_loss: 0.3437 - classification_loss: 0.0421
  220/10000 [..............................] - ETA: 2:24:39 - loss: 0.3858 - regression_loss: 0.3437 - classification_loss: 0.0421
  221/10000 [..............................] - ETA: 2:24:38 - loss: 0.3851 - regression_loss: 0.3431 - classification_loss: 0.0420
  222/10000 [..............................] - ETA: 2:24:37 - loss: 0.3844 - regression_loss: 0.3426 - classification_loss: 0.0418
  223/10000 [..............................] - ETA: 2:24:36 - loss: 0.3837 - regression_loss: 0.3418 - classification_loss: 0.0419
  224/10000 [..............................] - ETA: 2:24:36 - loss: 0.3838 - regression_loss: 0.3418 - classification_loss: 0.0419
  225/10000 [..............................] - ETA: 2:24:35 - loss: 0.3831 - regression_loss: 0.3413 - classification_loss: 0.0418
  226/10000 [..............................] - ETA: 2:24:34 - loss: 0.3842 - regression_loss: 0.3424 - classification_loss: 0.0418
  227/10000 [..............................] - ETA: 2:24:33 - loss: 0.3857 - regression_loss: 0.3438 - classification_loss: 0.0419
  228/10000 [..............................] - ETA: 2:24:32 - loss: 0.3866 - regression_loss: 0.3448 - classification_loss: 0.0418
  229/10000 [..............................] - ETA: 2:24:31 - loss: 0.3889 - regression_loss: 0.3468 - classification_loss: 0.0422
  230/10000 [..............................] - ETA: 2:24:30 - loss: 0.3888 - regression_loss: 0.3467 - classification_loss: 0.0421
  231/10000 [..............................] - ETA: 2:24:30 - loss: 0.3885 - regression_loss: 0.3466 - classification_loss: 0.0420
  232/10000 [..............................] - ETA: 2:24:29 - loss: 0.3884 - regression_loss: 0.3465 - classification_loss: 0.0419
  233/10000 [..............................] - ETA: 2:24:28 - loss: 0.3886 - regression_loss: 0.3468 - classification_loss: 0.0418
  234/10000 [..............................] - ETA: 2:24:27 - loss: 0.3884 - regression_loss: 0.3466 - classification_loss: 0.0418
  235/10000 [..............................] - ETA: 2:24:26 - loss: 0.3883 - regression_loss: 0.3465 - classification_loss: 0.0418
  236/10000 [..............................] - ETA: 2:24:25 - loss: 0.3895 - regression_loss: 0.3477 - classification_loss: 0.0418
  237/10000 [..............................] - ETA: 2:24:24 - loss: 0.3897 - regression_loss: 0.3480 - classification_loss: 0.0417
  238/10000 [..............................] - ETA: 2:24:23 - loss: 0.3889 - regression_loss: 0.3471 - classification_loss: 0.0418
  239/10000 [..............................] - ETA: 2:24:23 - loss: 0.3892 - regression_loss: 0.3474 - classification_loss: 0.0417
  240/10000 [..............................] - ETA: 2:24:22 - loss: 0.3888 - regression_loss: 0.3472 - classification_loss: 0.0416
  241/10000 [..............................] - ETA: 2:24:21 - loss: 0.3898 - regression_loss: 0.3480 - classification_loss: 0.0418
  242/10000 [..............................] - ETA: 2:24:20 - loss: 0.3908 - regression_loss: 0.3491 - classification_loss: 0.0417
  243/10000 [..............................] - ETA: 2:24:19 - loss: 0.3921 - regression_loss: 0.3504 - classification_loss: 0.0417
  244/10000 [..............................] - ETA: 2:24:18 - loss: 0.3922 - regression_loss: 0.3505 - classification_loss: 0.0416
  245/10000 [..............................] - ETA: 2:24:17 - loss: 0.3917 - regression_loss: 0.3502 - classification_loss: 0.0415
  246/10000 [..............................] - ETA: 2:24:16 - loss: 0.3909 - regression_loss: 0.3495 - classification_loss: 0.0414
  247/10000 [..............................] - ETA: 2:24:15 - loss: 0.3919 - regression_loss: 0.3504 - classification_loss: 0.0415
  248/10000 [..............................] - ETA: 2:24:14 - loss: 0.3916 - regression_loss: 0.3502 - classification_loss: 0.0414
  249/10000 [..............................] - ETA: 2:24:13 - loss: 0.3912 - regression_loss: 0.3499 - classification_loss: 0.0412
  250/10000 [..............................] - ETA: 2:24:12 - loss: 0.3908 - regression_loss: 0.3497 - classification_loss: 0.0411
  251/10000 [..............................] - ETA: 2:24:11 - loss: 0.3917 - regression_loss: 0.3506 - classification_loss: 0.0410
  252/10000 [..............................] - ETA: 2:24:11 - loss: 0.3909 - regression_loss: 0.3499 - classification_loss: 0.0409
  253/10000 [..............................] - ETA: 2:24:10 - loss: 0.3902 - regression_loss: 0.3493 - classification_loss: 0.0409
  254/10000 [..............................] - ETA: 2:24:09 - loss: 0.3897 - regression_loss: 0.3490 - classification_loss: 0.0407
  255/10000 [..............................] - ETA: 2:24:08 - loss: 0.3891 - regression_loss: 0.3484 - classification_loss: 0.0407
  256/10000 [..............................] - ETA: 2:24:07 - loss: 0.3897 - regression_loss: 0.3491 - classification_loss: 0.0407
  257/10000 [..............................] - ETA: 2:24:07 - loss: 0.3896 - regression_loss: 0.3490 - classification_loss: 0.0406
  258/10000 [..............................] - ETA: 2:24:06 - loss: 0.3897 - regression_loss: 0.3491 - classification_loss: 0.0406
  259/10000 [..............................] - ETA: 2:24:05 - loss: 0.3904 - regression_loss: 0.3498 - classification_loss: 0.0406
  260/10000 [..............................] - ETA: 2:24:04 - loss: 0.3894 - regression_loss: 0.3488 - classification_loss: 0.0405
  261/10000 [..............................] - ETA: 2:24:03 - loss: 0.3887 - regression_loss: 0.3483 - classification_loss: 0.0404
  262/10000 [..............................] - ETA: 2:24:02 - loss: 0.3888 - regression_loss: 0.3485 - classification_loss: 0.0403
  263/10000 [..............................] - ETA: 2:24:01 - loss: 0.3889 - regression_loss: 0.3487 - classification_loss: 0.0402
  264/10000 [..............................] - ETA: 2:24:00 - loss: 0.3882 - regression_loss: 0.3482 - classification_loss: 0.0401
  265/10000 [..............................] - ETA: 2:23:59 - loss: 0.3883 - regression_loss: 0.3482 - classification_loss: 0.0401
  266/10000 [..............................] - ETA: 2:23:58 - loss: 0.3888 - regression_loss: 0.3488 - classification_loss: 0.0400
  267/10000 [..............................] - ETA: 2:23:57 - loss: 0.3883 - regression_loss: 0.3485 - classification_loss: 0.0398
  268/10000 [..............................] - ETA: 2:23:57 - loss: 0.3879 - regression_loss: 0.3481 - classification_loss: 0.0398
  269/10000 [..............................] - ETA: 2:23:56 - loss: 0.3877 - regression_loss: 0.3478 - classification_loss: 0.0399
  270/10000 [..............................] - ETA: 2:23:55 - loss: 0.3888 - regression_loss: 0.3490 - classification_loss: 0.0399
  271/10000 [..............................] - ETA: 2:23:54 - loss: 0.3894 - regression_loss: 0.3495 - classification_loss: 0.0399
  272/10000 [..............................] - ETA: 2:23:53 - loss: 0.3890 - regression_loss: 0.3492 - classification_loss: 0.0398
  273/10000 [..............................] - ETA: 2:23:52 - loss: 0.3890 - regression_loss: 0.3491 - classification_loss: 0.0399
  274/10000 [..............................] - ETA: 2:23:51 - loss: 0.3876 - regression_loss: 0.3478 - classification_loss: 0.0397
  275/10000 [..............................] - ETA: 2:23:50 - loss: 0.3870 - regression_loss: 0.3473 - classification_loss: 0.0396
  276/10000 [..............................] - ETA: 2:23:49 - loss: 0.3863 - regression_loss: 0.3466 - classification_loss: 0.0396
  277/10000 [..............................] - ETA: 2:23:48 - loss: 0.3856 - regression_loss: 0.3461 - classification_loss: 0.0395
  278/10000 [..............................] - ETA: 2:23:47 - loss: 0.3866 - regression_loss: 0.3471 - classification_loss: 0.0395
  279/10000 [..............................] - ETA: 2:23:47 - loss: 0.3882 - regression_loss: 0.3486 - classification_loss: 0.0396
  280/10000 [..............................] - ETA: 2:23:46 - loss: 0.3883 - regression_loss: 0.3487 - classification_loss: 0.0396
  281/10000 [..............................] - ETA: 2:23:45 - loss: 0.3901 - regression_loss: 0.3504 - classification_loss: 0.0397
  282/10000 [..............................] - ETA: 2:23:44 - loss: 0.3903 - regression_loss: 0.3507 - classification_loss: 0.0397
  283/10000 [..............................] - ETA: 2:23:43 - loss: 0.3935 - regression_loss: 0.3538 - classification_loss: 0.0397
  284/10000 [..............................] - ETA: 2:23:42 - loss: 0.3949 - regression_loss: 0.3549 - classification_loss: 0.0400
  285/10000 [..............................] - ETA: 2:23:42 - loss: 0.3943 - regression_loss: 0.3545 - classification_loss: 0.0399
  286/10000 [..............................] - ETA: 2:23:41 - loss: 0.3963 - regression_loss: 0.3562 - classification_loss: 0.0400
  287/10000 [..............................] - ETA: 2:23:40 - loss: 0.3986 - regression_loss: 0.3583 - classification_loss: 0.0402
  288/10000 [..............................] - ETA: 2:23:39 - loss: 0.3979 - regression_loss: 0.3577 - classification_loss: 0.0401
  289/10000 [..............................] - ETA: 2:23:38 - loss: 0.3970 - regression_loss: 0.3570 - classification_loss: 0.0400
  290/10000 [..............................] - ETA: 2:23:37 - loss: 0.3968 - regression_loss: 0.3569 - classification_loss: 0.0399
  291/10000 [..............................] - ETA: 2:23:37 - loss: 0.3964 - regression_loss: 0.3566 - classification_loss: 0.0398
  292/10000 [..............................] - ETA: 2:23:36 - loss: 0.3975 - regression_loss: 0.3576 - classification_loss: 0.0399
  293/10000 [..............................] - ETA: 2:23:35 - loss: 0.3978 - regression_loss: 0.3580 - classification_loss: 0.0398
  294/10000 [..............................] - ETA: 2:23:34 - loss: 0.3978 - regression_loss: 0.3580 - classification_loss: 0.0398
  295/10000 [..............................] - ETA: 2:23:33 - loss: 0.3979 - regression_loss: 0.3581 - classification_loss: 0.0398
  296/10000 [..............................] - ETA: 2:23:32 - loss: 0.3977 - regression_loss: 0.3580 - classification_loss: 0.0397
  297/10000 [..............................] - ETA: 2:23:31 - loss: 0.4000 - regression_loss: 0.3575 - classification_loss: 0.0425
  298/10000 [..............................] - ETA: 2:23:31 - loss: 0.3992 - regression_loss: 0.3567 - classification_loss: 0.0425
  299/10000 [..............................] - ETA: 2:23:30 - loss: 0.3996 - regression_loss: 0.3570 - classification_loss: 0.0426
  300/10000 [..............................] - ETA: 2:23:29 - loss: 0.3993 - regression_loss: 0.3568 - classification_loss: 0.0425
  301/10000 [..............................] - ETA: 2:23:28 - loss: 0.3987 - regression_loss: 0.3562 - classification_loss: 0.0424
  302/10000 [..............................] - ETA: 2:23:27 - loss: 0.4023 - regression_loss: 0.3568 - classification_loss: 0.0455
  303/10000 [..............................] - ETA: 2:23:26 - loss: 0.4021 - regression_loss: 0.3566 - classification_loss: 0.0455
  304/10000 [..............................] - ETA: 2:23:25 - loss: 0.4023 - regression_loss: 0.3567 - classification_loss: 0.0455
  305/10000 [..............................] - ETA: 2:23:25 - loss: 0.4014 - regression_loss: 0.3560 - classification_loss: 0.0454
  306/10000 [..............................] - ETA: 2:23:24 - loss: 0.4010 - regression_loss: 0.3557 - classification_loss: 0.0453
  307/10000 [..............................] - ETA: 2:23:23 - loss: 0.4000 - regression_loss: 0.3548 - classification_loss: 0.0452
  308/10000 [..............................] - ETA: 2:23:22 - loss: 0.4006 - regression_loss: 0.3555 - classification_loss: 0.0451
  309/10000 [..............................] - ETA: 2:23:21 - loss: 0.4007 - regression_loss: 0.3558 - classification_loss: 0.0449
  310/10000 [..............................] - ETA: 2:23:20 - loss: 0.4003 - regression_loss: 0.3555 - classification_loss: 0.0449
  311/10000 [..............................] - ETA: 2:23:19 - loss: 0.4008 - regression_loss: 0.3556 - classification_loss: 0.0452
  312/10000 [..............................] - ETA: 2:23:18 - loss: 0.4010 - regression_loss: 0.3558 - classification_loss: 0.0452
  313/10000 [..............................] - ETA: 2:23:17 - loss: 0.4004 - regression_loss: 0.3553 - classification_loss: 0.0451
  314/10000 [..............................] - ETA: 2:23:16 - loss: 0.4006 - regression_loss: 0.3555 - classification_loss: 0.0451
  315/10000 [..............................] - ETA: 2:23:16 - loss: 0.3993 - regression_loss: 0.3544 - classification_loss: 0.0449
  316/10000 [..............................] - ETA: 2:23:15 - loss: 0.3985 - regression_loss: 0.3537 - classification_loss: 0.0448
  317/10000 [..............................] - ETA: 2:23:14 - loss: 0.3982 - regression_loss: 0.3534 - classification_loss: 0.0448
  318/10000 [..............................] - ETA: 2:23:13 - loss: 0.3982 - regression_loss: 0.3536 - classification_loss: 0.0447
  319/10000 [..............................] - ETA: 2:23:12 - loss: 0.3984 - regression_loss: 0.3538 - classification_loss: 0.0446
  320/10000 [..............................] - ETA: 2:23:11 - loss: 0.3982 - regression_loss: 0.3536 - classification_loss: 0.0446
  321/10000 [..............................] - ETA: 2:23:10 - loss: 0.3979 - regression_loss: 0.3533 - classification_loss: 0.0446
  322/10000 [..............................] - ETA: 2:23:09 - loss: 0.3974 - regression_loss: 0.3529 - classification_loss: 0.0445
  323/10000 [..............................] - ETA: 2:23:08 - loss: 0.3967 - regression_loss: 0.3522 - classification_loss: 0.0445
  324/10000 [..............................] - ETA: 2:23:07 - loss: 0.3969 - regression_loss: 0.3523 - classification_loss: 0.0446
  325/10000 [..............................] - ETA: 2:23:07 - loss: 0.3972 - regression_loss: 0.3527 - classification_loss: 0.0444
  326/10000 [..............................] - ETA: 2:23:06 - loss: 0.3969 - regression_loss: 0.3526 - classification_loss: 0.0443
  327/10000 [..............................] - ETA: 2:23:05 - loss: 0.3961 - regression_loss: 0.3519 - classification_loss: 0.0442
  328/10000 [..............................] - ETA: 2:23:04 - loss: 0.3949 - regression_loss: 0.3508 - classification_loss: 0.0441
  329/10000 [..............................] - ETA: 2:23:03 - loss: 0.3943 - regression_loss: 0.3503 - classification_loss: 0.0440
  330/10000 [..............................] - ETA: 2:23:02 - loss: 0.3945 - regression_loss: 0.3505 - classification_loss: 0.0440
  331/10000 [..............................] - ETA: 2:23:01 - loss: 0.3962 - regression_loss: 0.3519 - classification_loss: 0.0443
  332/10000 [..............................] - ETA: 2:23:00 - loss: 0.3963 - regression_loss: 0.3520 - classification_loss: 0.0443
  333/10000 [..............................] - ETA: 2:22:59 - loss: 0.3965 - regression_loss: 0.3522 - classification_loss: 0.0443
  334/10000 [>.............................] - ETA: 2:22:58 - loss: 0.3963 - regression_loss: 0.3521 - classification_loss: 0.0442
  335/10000 [>.............................] - ETA: 2:22:58 - loss: 0.3971 - regression_loss: 0.3528 - classification_loss: 0.0443
  336/10000 [>.............................] - ETA: 2:22:57 - loss: 0.3965 - regression_loss: 0.3522 - classification_loss: 0.0443
  337/10000 [>.............................] - ETA: 2:22:56 - loss: 0.3960 - regression_loss: 0.3518 - classification_loss: 0.0442
  338/10000 [>.............................] - ETA: 2:22:55 - loss: 0.4011 - regression_loss: 0.3530 - classification_loss: 0.0481
  339/10000 [>.............................] - ETA: 2:22:54 - loss: 0.4012 - regression_loss: 0.3532 - classification_loss: 0.0480
  340/10000 [>.............................] - ETA: 2:22:53 - loss: 0.4019 - regression_loss: 0.3539 - classification_loss: 0.0479
  341/10000 [>.............................] - ETA: 2:22:52 - loss: 0.4022 - regression_loss: 0.3543 - classification_loss: 0.0479
  342/10000 [>.............................] - ETA: 2:22:52 - loss: 0.4011 - regression_loss: 0.3533 - classification_loss: 0.0478
  343/10000 [>.............................] - ETA: 2:22:51 - loss: 0.4016 - regression_loss: 0.3537 - classification_loss: 0.0479
  344/10000 [>.............................] - ETA: 2:22:50 - loss: 0.4020 - regression_loss: 0.3542 - classification_loss: 0.0478
  345/10000 [>.............................] - ETA: 2:22:49 - loss: 0.4025 - regression_loss: 0.3547 - classification_loss: 0.0478
  346/10000 [>.............................] - ETA: 2:22:48 - loss: 0.4038 - regression_loss: 0.3559 - classification_loss: 0.0479
  347/10000 [>.............................] - ETA: 2:22:47 - loss: 0.4045 - regression_loss: 0.3564 - classification_loss: 0.0481
  348/10000 [>.............................] - ETA: 2:22:47 - loss: 0.4040 - regression_loss: 0.3560 - classification_loss: 0.0480
  349/10000 [>.............................] - ETA: 2:22:46 - loss: 0.4036 - regression_loss: 0.3556 - classification_loss: 0.0480
  350/10000 [>.............................] - ETA: 2:22:45 - loss: 0.4033 - regression_loss: 0.3553 - classification_loss: 0.0479
  351/10000 [>.............................] - ETA: 2:22:44 - loss: 0.4031 - regression_loss: 0.3552 - classification_loss: 0.0479
  352/10000 [>.............................] - ETA: 2:22:43 - loss: 0.4029 - regression_loss: 0.3552 - classification_loss: 0.0478
  353/10000 [>.............................] - ETA: 2:22:42 - loss: 0.4024 - regression_loss: 0.3547 - classification_loss: 0.0477
  354/10000 [>.............................] - ETA: 2:22:42 - loss: 0.4025 - regression_loss: 0.3549 - classification_loss: 0.0476
  355/10000 [>.............................] - ETA: 2:22:41 - loss: 0.4026 - regression_loss: 0.3551 - classification_loss: 0.0475
  356/10000 [>.............................] - ETA: 2:22:40 - loss: 0.4030 - regression_loss: 0.3553 - classification_loss: 0.0477
  357/10000 [>.............................] - ETA: 2:22:39 - loss: 0.4025 - regression_loss: 0.3549 - classification_loss: 0.0476
  358/10000 [>.............................] - ETA: 2:22:38 - loss: 0.4026 - regression_loss: 0.3551 - classification_loss: 0.0475
  359/10000 [>.............................] - ETA: 2:22:37 - loss: 0.4023 - regression_loss: 0.3549 - classification_loss: 0.0474
  360/10000 [>.............................] - ETA: 2:22:36 - loss: 0.4018 - regression_loss: 0.3543 - classification_loss: 0.0474
  361/10000 [>.............................] - ETA: 2:22:36 - loss: 0.4014 - regression_loss: 0.3540 - classification_loss: 0.0474
  362/10000 [>.............................] - ETA: 2:22:35 - loss: 0.4014 - regression_loss: 0.3540 - classification_loss: 0.0473
  363/10000 [>.............................] - ETA: 2:22:34 - loss: 0.4010 - regression_loss: 0.3537 - classification_loss: 0.0472
  364/10000 [>.............................] - ETA: 2:22:33 - loss: 0.4014 - regression_loss: 0.3541 - classification_loss: 0.0473
  365/10000 [>.............................] - ETA: 2:22:32 - loss: 0.4017 - regression_loss: 0.3543 - classification_loss: 0.0474
  366/10000 [>.............................] - ETA: 2:22:31 - loss: 0.4014 - regression_loss: 0.3541 - classification_loss: 0.0473
  367/10000 [>.............................] - ETA: 2:22:30 - loss: 0.4029 - regression_loss: 0.3556 - classification_loss: 0.0473
  368/10000 [>.............................] - ETA: 2:22:29 - loss: 0.4030 - regression_loss: 0.3557 - classification_loss: 0.0473
  369/10000 [>.............................] - ETA: 2:22:28 - loss: 0.4026 - regression_loss: 0.3554 - classification_loss: 0.0472
  370/10000 [>.............................] - ETA: 2:22:28 - loss: 0.4018 - regression_loss: 0.3546 - classification_loss: 0.0472
  371/10000 [>.............................] - ETA: 2:22:27 - loss: 0.4017 - regression_loss: 0.3545 - classification_loss: 0.0471
  372/10000 [>.............................] - ETA: 2:22:26 - loss: 0.4009 - regression_loss: 0.3538 - classification_loss: 0.0471
  373/10000 [>.............................] - ETA: 2:22:25 - loss: 0.3998 - regression_loss: 0.3528 - classification_loss: 0.0469
  374/10000 [>.............................] - ETA: 2:22:24 - loss: 0.4007 - regression_loss: 0.3537 - classification_loss: 0.0470
  375/10000 [>.............................] - ETA: 2:22:23 - loss: 0.4012 - regression_loss: 0.3542 - classification_loss: 0.0470
  376/10000 [>.............................] - ETA: 2:22:22 - loss: 0.4010 - regression_loss: 0.3539 - classification_loss: 0.0470
  377/10000 [>.............................] - ETA: 2:22:21 - loss: 0.4013 - regression_loss: 0.3543 - classification_loss: 0.0470
  378/10000 [>.............................] - ETA: 2:22:20 - loss: 0.4008 - regression_loss: 0.3540 - classification_loss: 0.0469
  379/10000 [>.............................] - ETA: 2:22:19 - loss: 0.4003 - regression_loss: 0.3535 - classification_loss: 0.0468
  380/10000 [>.............................] - ETA: 2:22:18 - loss: 0.4003 - regression_loss: 0.3536 - classification_loss: 0.0468
  381/10000 [>.............................] - ETA: 2:22:18 - loss: 0.4000 - regression_loss: 0.3533 - classification_loss: 0.0467
  382/10000 [>.............................] - ETA: 2:22:17 - loss: 0.3995 - regression_loss: 0.3529 - classification_loss: 0.0466
  383/10000 [>.............................] - ETA: 2:22:16 - loss: 0.4001 - regression_loss: 0.3536 - classification_loss: 0.0465
  384/10000 [>.............................] - ETA: 2:22:15 - loss: 0.4001 - regression_loss: 0.3535 - classification_loss: 0.0465
  385/10000 [>.............................] - ETA: 2:22:15 - loss: 0.3998 - regression_loss: 0.3534 - classification_loss: 0.0465
  386/10000 [>.............................] - ETA: 2:22:14 - loss: 0.3996 - regression_loss: 0.3532 - classification_loss: 0.0464
  387/10000 [>.............................] - ETA: 2:22:13 - loss: 0.3988 - regression_loss: 0.3525 - classification_loss: 0.0463
  388/10000 [>.............................] - ETA: 2:22:12 - loss: 0.3981 - regression_loss: 0.3519 - classification_loss: 0.0462
  389/10000 [>.............................] - ETA: 2:22:11 - loss: 0.3982 - regression_loss: 0.3521 - classification_loss: 0.0462
  390/10000 [>.............................] - ETA: 2:22:10 - loss: 0.3986 - regression_loss: 0.3525 - classification_loss: 0.0461
  391/10000 [>.............................] - ETA: 2:22:09 - loss: 0.3976 - regression_loss: 0.3516 - classification_loss: 0.0460
  392/10000 [>.............................] - ETA: 2:22:08 - loss: 0.3977 - regression_loss: 0.3518 - classification_loss: 0.0459
  393/10000 [>.............................] - ETA: 2:22:08 - loss: 0.3970 - regression_loss: 0.3512 - classification_loss: 0.0458
  394/10000 [>.............................] - ETA: 2:22:07 - loss: 0.3972 - regression_loss: 0.3515 - classification_loss: 0.0457
  395/10000 [>.............................] - ETA: 2:22:06 - loss: 0.3962 - regression_loss: 0.3506 - classification_loss: 0.0456
  396/10000 [>.............................] - ETA: 2:22:05 - loss: 0.3959 - regression_loss: 0.3504 - classification_loss: 0.0456
  397/10000 [>.............................] - ETA: 2:22:04 - loss: 0.3959 - regression_loss: 0.3503 - classification_loss: 0.0456
  398/10000 [>.............................] - ETA: 2:22:03 - loss: 0.3955 - regression_loss: 0.3500 - classification_loss: 0.0455
  399/10000 [>.............................] - ETA: 2:22:02 - loss: 0.3954 - regression_loss: 0.3499 - classification_loss: 0.0454
  400/10000 [>.............................] - ETA: 2:22:01 - loss: 0.3954 - regression_loss: 0.3500 - classification_loss: 0.0454
  401/10000 [>.............................] - ETA: 2:22:00 - loss: 0.3949 - regression_loss: 0.3496 - classification_loss: 0.0453
  402/10000 [>.............................] - ETA: 2:22:00 - loss: 0.3942 - regression_loss: 0.3490 - classification_loss: 0.0452
  403/10000 [>.............................] - ETA: 2:21:59 - loss: 0.3940 - regression_loss: 0.3489 - classification_loss: 0.0452
  404/10000 [>.............................] - ETA: 2:21:58 - loss: 0.3932 - regression_loss: 0.3481 - classification_loss: 0.0451
  405/10000 [>.............................] - ETA: 2:21:57 - loss: 0.3940 - regression_loss: 0.3489 - classification_loss: 0.0451
  406/10000 [>.............................] - ETA: 2:21:56 - loss: 0.3953 - regression_loss: 0.3501 - classification_loss: 0.0452
  407/10000 [>.............................] - ETA: 2:21:55 - loss: 0.3960 - regression_loss: 0.3508 - classification_loss: 0.0452
  408/10000 [>.............................] - ETA: 2:21:54 - loss: 0.3959 - regression_loss: 0.3508 - classification_loss: 0.0451
  409/10000 [>.............................] - ETA: 2:21:53 - loss: 0.3951 - regression_loss: 0.3501 - classification_loss: 0.0450
  410/10000 [>.............................] - ETA: 2:21:52 - loss: 0.3948 - regression_loss: 0.3497 - classification_loss: 0.0451
  411/10000 [>.............................] - ETA: 2:21:51 - loss: 0.3950 - regression_loss: 0.3500 - classification_loss: 0.0451
  412/10000 [>.............................] - ETA: 2:21:51 - loss: 0.3959 - regression_loss: 0.3508 - classification_loss: 0.0451
  413/10000 [>.............................] - ETA: 2:21:50 - loss: 0.3967 - regression_loss: 0.3516 - classification_loss: 0.0451
  414/10000 [>.............................] - ETA: 2:21:49 - loss: 0.3969 - regression_loss: 0.3518 - classification_loss: 0.0451
  415/10000 [>.............................] - ETA: 2:21:48 - loss: 0.3971 - regression_loss: 0.3520 - classification_loss: 0.0451
  416/10000 [>.............................] - ETA: 2:21:47 - loss: 0.3975 - regression_loss: 0.3523 - classification_loss: 0.0452
  417/10000 [>.............................] - ETA: 2:21:46 - loss: 0.3979 - regression_loss: 0.3528 - classification_loss: 0.0451
  418/10000 [>.............................] - ETA: 2:21:45 - loss: 0.3976 - regression_loss: 0.3525 - classification_loss: 0.0451
  419/10000 [>.............................] - ETA: 2:21:44 - loss: 0.3979 - regression_loss: 0.3529 - classification_loss: 0.0451
  420/10000 [>.............................] - ETA: 2:21:44 - loss: 0.3976 - regression_loss: 0.3526 - classification_loss: 0.0450
  421/10000 [>.............................] - ETA: 2:21:43 - loss: 0.3975 - regression_loss: 0.3525 - classification_loss: 0.0450
  422/10000 [>.............................] - ETA: 2:21:42 - loss: 0.3981 - regression_loss: 0.3531 - classification_loss: 0.0450
  423/10000 [>.............................] - ETA: 2:21:41 - loss: 0.4013 - regression_loss: 0.3562 - classification_loss: 0.0451
  424/10000 [>.............................] - ETA: 2:21:40 - loss: 0.4020 - regression_loss: 0.3568 - classification_loss: 0.0452
  425/10000 [>.............................] - ETA: 2:21:39 - loss: 0.4020 - regression_loss: 0.3569 - classification_loss: 0.0451
  426/10000 [>.............................] - ETA: 2:21:38 - loss: 0.4011 - regression_loss: 0.3561 - classification_loss: 0.0450
  427/10000 [>.............................] - ETA: 2:21:37 - loss: 0.4009 - regression_loss: 0.3559 - classification_loss: 0.0449
  428/10000 [>.............................] - ETA: 2:21:36 - loss: 0.3999 - regression_loss: 0.3551 - classification_loss: 0.0448
  429/10000 [>.............................] - ETA: 2:21:36 - loss: 0.4012 - regression_loss: 0.3562 - classification_loss: 0.0450
  430/10000 [>.............................] - ETA: 2:21:35 - loss: 0.4012 - regression_loss: 0.3562 - classification_loss: 0.0450
  431/10000 [>.............................] - ETA: 2:21:34 - loss: 0.4005 - regression_loss: 0.3556 - classification_loss: 0.0449
  432/10000 [>.............................] - ETA: 2:21:33 - loss: 0.4005 - regression_loss: 0.3555 - classification_loss: 0.0450
  433/10000 [>.............................] - ETA: 2:21:32 - loss: 0.4007 - regression_loss: 0.3557 - classification_loss: 0.0450
  434/10000 [>.............................] - ETA: 2:21:31 - loss: 0.4012 - regression_loss: 0.3559 - classification_loss: 0.0453
  435/10000 [>.............................] - ETA: 2:21:30 - loss: 0.4015 - regression_loss: 0.3563 - classification_loss: 0.0452
  436/10000 [>.............................] - ETA: 2:21:29 - loss: 0.4013 - regression_loss: 0.3562 - classification_loss: 0.0451
  437/10000 [>.............................] - ETA: 2:21:28 - loss: 0.4012 - regression_loss: 0.3561 - classification_loss: 0.0451
  438/10000 [>.............................] - ETA: 2:21:27 - loss: 0.4013 - regression_loss: 0.3562 - classification_loss: 0.0451
  439/10000 [>.............................] - ETA: 2:21:27 - loss: 0.4014 - regression_loss: 0.3564 - classification_loss: 0.0450
  440/10000 [>.............................] - ETA: 2:21:26 - loss: 0.4010 - regression_loss: 0.3561 - classification_loss: 0.0449
  441/10000 [>.............................] - ETA: 2:21:25 - loss: 0.4009 - regression_loss: 0.3560 - classification_loss: 0.0449
  442/10000 [>.............................] - ETA: 2:21:24 - loss: 0.4000 - regression_loss: 0.3552 - classification_loss: 0.0448
  443/10000 [>.............................] - ETA: 2:21:23 - loss: 0.4003 - regression_loss: 0.3555 - classification_loss: 0.0448
  444/10000 [>.............................] - ETA: 2:21:22 - loss: 0.3999 - regression_loss: 0.3552 - classification_loss: 0.0447
  445/10000 [>.............................] - ETA: 2:21:21 - loss: 0.4004 - regression_loss: 0.3556 - classification_loss: 0.0447
  446/10000 [>.............................] - ETA: 2:21:20 - loss: 0.4004 - regression_loss: 0.3553 - classification_loss: 0.0451
  447/10000 [>.............................] - ETA: 2:21:20 - loss: 0.3995 - regression_loss: 0.3545 - classification_loss: 0.0450
  448/10000 [>.............................] - ETA: 2:21:19 - loss: 0.3999 - regression_loss: 0.3549 - classification_loss: 0.0450
  449/10000 [>.............................] - ETA: 2:21:18 - loss: 0.4001 - regression_loss: 0.3550 - classification_loss: 0.0450
  450/10000 [>.............................] - ETA: 2:21:17 - loss: 0.4004 - regression_loss: 0.3554 - classification_loss: 0.0450
  451/10000 [>.............................] - ETA: 2:21:16 - loss: 0.4002 - regression_loss: 0.3553 - classification_loss: 0.0450
  452/10000 [>.............................] - ETA: 2:21:15 - loss: 0.3999 - regression_loss: 0.3550 - classification_loss: 0.0449
  453/10000 [>.............................] - ETA: 2:21:14 - loss: 0.3999 - regression_loss: 0.3551 - classification_loss: 0.0448
  454/10000 [>.............................] - ETA: 2:21:13 - loss: 0.3998 - regression_loss: 0.3550 - classification_loss: 0.0448
  455/10000 [>.............................] - ETA: 2:21:12 - loss: 0.3995 - regression_loss: 0.3547 - classification_loss: 0.0448
  456/10000 [>.............................] - ETA: 2:21:12 - loss: 0.3999 - regression_loss: 0.3552 - classification_loss: 0.0447
  457/10000 [>.............................] - ETA: 2:21:11 - loss: 0.3990 - regression_loss: 0.3544 - classification_loss: 0.0446
  458/10000 [>.............................] - ETA: 2:21:10 - loss: 0.3988 - regression_loss: 0.3541 - classification_loss: 0.0446
  459/10000 [>.............................] - ETA: 2:21:09 - loss: 0.3982 - regression_loss: 0.3536 - classification_loss: 0.0446
  460/10000 [>.............................] - ETA: 2:21:08 - loss: 0.3996 - regression_loss: 0.3549 - classification_loss: 0.0447
  461/10000 [>.............................] - ETA: 2:21:07 - loss: 0.3989 - regression_loss: 0.3543 - classification_loss: 0.0446
  462/10000 [>.............................] - ETA: 2:21:06 - loss: 0.3989 - regression_loss: 0.3543 - classification_loss: 0.0446
  463/10000 [>.............................] - ETA: 2:21:05 - loss: 0.3986 - regression_loss: 0.3540 - classification_loss: 0.0445
  464/10000 [>.............................] - ETA: 2:21:04 - loss: 0.3996 - regression_loss: 0.3552 - classification_loss: 0.0444
  465/10000 [>.............................] - ETA: 2:21:04 - loss: 0.3999 - regression_loss: 0.3554 - classification_loss: 0.0444
  466/10000 [>.............................] - ETA: 2:21:03 - loss: 0.4002 - regression_loss: 0.3557 - classification_loss: 0.0444
  467/10000 [>.............................] - ETA: 2:21:02 - loss: 0.4000 - regression_loss: 0.3556 - classification_loss: 0.0444
  468/10000 [>.............................] - ETA: 2:21:01 - loss: 0.3997 - regression_loss: 0.3554 - classification_loss: 0.0443
  469/10000 [>.............................] - ETA: 2:21:00 - loss: 0.3994 - regression_loss: 0.3551 - classification_loss: 0.0443
  470/10000 [>.............................] - ETA: 2:20:59 - loss: 0.3990 - regression_loss: 0.3547 - classification_loss: 0.0443
  471/10000 [>.............................] - ETA: 2:20:58 - loss: 0.3991 - regression_loss: 0.3549 - classification_loss: 0.0442
  472/10000 [>.............................] - ETA: 2:20:57 - loss: 0.3988 - regression_loss: 0.3546 - classification_loss: 0.0442
  473/10000 [>.............................] - ETA: 2:20:56 - loss: 0.3990 - regression_loss: 0.3549 - classification_loss: 0.0441
  474/10000 [>.............................] - ETA: 2:20:55 - loss: 0.3985 - regression_loss: 0.3544 - classification_loss: 0.0440
  475/10000 [>.............................] - ETA: 2:20:55 - loss: 0.3988 - regression_loss: 0.3549 - classification_loss: 0.0440
  476/10000 [>.............................] - ETA: 2:20:54 - loss: 0.3984 - regression_loss: 0.3545 - classification_loss: 0.0439
  477/10000 [>.............................] - ETA: 2:20:53 - loss: 0.3981 - regression_loss: 0.3542 - classification_loss: 0.0439
  478/10000 [>.............................] - ETA: 2:20:52 - loss: 0.3973 - regression_loss: 0.3535 - classification_loss: 0.0438
  479/10000 [>.............................] - ETA: 2:20:51 - loss: 0.3968 - regression_loss: 0.3531 - classification_loss: 0.0437
  480/10000 [>.............................] - ETA: 2:20:50 - loss: 0.3959 - regression_loss: 0.3523 - classification_loss: 0.0436
  481/10000 [>.............................] - ETA: 2:20:49 - loss: 0.3955 - regression_loss: 0.3519 - classification_loss: 0.0436
  482/10000 [>.............................] - ETA: 2:20:48 - loss: 0.3952 - regression_loss: 0.3516 - classification_loss: 0.0436
  483/10000 [>.............................] - ETA: 2:20:47 - loss: 0.3954 - regression_loss: 0.3518 - classification_loss: 0.0436
  484/10000 [>.............................] - ETA: 2:20:47 - loss: 0.3951 - regression_loss: 0.3515 - classification_loss: 0.0436
  485/10000 [>.............................] - ETA: 2:20:46 - loss: 0.3950 - regression_loss: 0.3514 - classification_loss: 0.0436
  486/10000 [>.............................] - ETA: 2:20:45 - loss: 0.3955 - regression_loss: 0.3520 - classification_loss: 0.0435
  487/10000 [>.............................] - ETA: 2:20:44 - loss: 0.3962 - regression_loss: 0.3528 - classification_loss: 0.0434
  488/10000 [>.............................] - ETA: 2:20:43 - loss: 0.3965 - regression_loss: 0.3531 - classification_loss: 0.0435
  489/10000 [>.............................] - ETA: 2:20:42 - loss: 0.3964 - regression_loss: 0.3529 - classification_loss: 0.0435
  490/10000 [>.............................] - ETA: 2:20:41 - loss: 0.3958 - regression_loss: 0.3524 - classification_loss: 0.0434
  491/10000 [>.............................] - ETA: 2:20:40 - loss: 0.3955 - regression_loss: 0.3521 - classification_loss: 0.0434
  492/10000 [>.............................] - ETA: 2:20:40 - loss: 0.3960 - regression_loss: 0.3526 - classification_loss: 0.0434
  493/10000 [>.............................] - ETA: 2:20:39 - loss: 0.3959 - regression_loss: 0.3526 - classification_loss: 0.0433
  494/10000 [>.............................] - ETA: 2:20:38 - loss: 0.3956 - regression_loss: 0.3523 - classification_loss: 0.0433
  495/10000 [>.............................] - ETA: 2:20:37 - loss: 0.3948 - regression_loss: 0.3516 - classification_loss: 0.0432
  496/10000 [>.............................] - ETA: 2:20:36 - loss: 0.3968 - regression_loss: 0.3534 - classification_loss: 0.0433
  497/10000 [>.............................] - ETA: 2:20:35 - loss: 0.3976 - regression_loss: 0.3542 - classification_loss: 0.0434
  498/10000 [>.............................] - ETA: 2:20:34 - loss: 0.3981 - regression_loss: 0.3546 - classification_loss: 0.0434
  499/10000 [>.............................] - ETA: 2:20:33 - loss: 0.3982 - regression_loss: 0.3547 - classification_loss: 0.0435
  500/10000 [>.............................] - ETA: 2:20:32 - loss: 0.3991 - regression_loss: 0.3556 - classification_loss: 0.0436
  501/10000 [>.............................] - ETA: 2:20:31 - loss: 0.3992 - regression_loss: 0.3557 - classification_loss: 0.0435
  502/10000 [>.............................] - ETA: 2:20:31 - loss: 0.3988 - regression_loss: 0.3554 - classification_loss: 0.0434
  503/10000 [>.............................] - ETA: 2:20:30 - loss: 0.3986 - regression_loss: 0.3552 - classification_loss: 0.0433
  504/10000 [>.............................] - ETA: 2:20:29 - loss: 0.3982 - regression_loss: 0.3549 - classification_loss: 0.0433
  505/10000 [>.............................] - ETA: 2:20:28 - loss: 0.3991 - regression_loss: 0.3559 - classification_loss: 0.0432
  506/10000 [>.............................] - ETA: 2:20:27 - loss: 0.3992 - regression_loss: 0.3559 - classification_loss: 0.0433
  507/10000 [>.............................] - ETA: 2:20:26 - loss: 0.3991 - regression_loss: 0.3558 - classification_loss: 0.0432
  508/10000 [>.............................] - ETA: 2:20:25 - loss: 0.3986 - regression_loss: 0.3554 - classification_loss: 0.0432
  509/10000 [>.............................] - ETA: 2:20:24 - loss: 0.3987 - regression_loss: 0.3556 - classification_loss: 0.0431
  510/10000 [>.............................] - ETA: 2:20:24 - loss: 0.3983 - regression_loss: 0.3552 - classification_loss: 0.0430
  511/10000 [>.............................] - ETA: 2:20:23 - loss: 0.3987 - regression_loss: 0.3556 - classification_loss: 0.0431
  512/10000 [>.............................] - ETA: 2:20:22 - loss: 0.3979 - regression_loss: 0.3549 - classification_loss: 0.0430
  513/10000 [>.............................] - ETA: 2:20:21 - loss: 0.3980 - regression_loss: 0.3550 - classification_loss: 0.0430
  514/10000 [>.............................] - ETA: 2:20:20 - loss: 0.3972 - regression_loss: 0.3543 - classification_loss: 0.0429
  515/10000 [>.............................] - ETA: 2:20:19 - loss: 0.3993 - regression_loss: 0.3564 - classification_loss: 0.0429
  516/10000 [>.............................] - ETA: 2:20:18 - loss: 0.3992 - regression_loss: 0.3563 - classification_loss: 0.0429
  517/10000 [>.............................] - ETA: 2:20:17 - loss: 0.3989 - regression_loss: 0.3561 - classification_loss: 0.0428
  518/10000 [>.............................] - ETA: 2:20:16 - loss: 0.3995 - regression_loss: 0.3567 - classification_loss: 0.0428
  519/10000 [>.............................] - ETA: 2:20:16 - loss: 0.3997 - regression_loss: 0.3569 - classification_loss: 0.0428
  520/10000 [>.............................] - ETA: 2:20:15 - loss: 0.4005 - regression_loss: 0.3577 - classification_loss: 0.0428
  521/10000 [>.............................] - ETA: 2:20:14 - loss: 0.4002 - regression_loss: 0.3574 - classification_loss: 0.0428
  522/10000 [>.............................] - ETA: 2:20:13 - loss: 0.4000 - regression_loss: 0.3573 - classification_loss: 0.0427
  523/10000 [>.............................] - ETA: 2:20:12 - loss: 0.3999 - regression_loss: 0.3570 - classification_loss: 0.0429
  524/10000 [>.............................] - ETA: 2:20:11 - loss: 0.3994 - regression_loss: 0.3566 - classification_loss: 0.0428
  525/10000 [>.............................] - ETA: 2:20:10 - loss: 0.3994 - regression_loss: 0.3566 - classification_loss: 0.0427
  526/10000 [>.............................] - ETA: 2:20:09 - loss: 0.3991 - regression_loss: 0.3565 - classification_loss: 0.0427
  527/10000 [>.............................] - ETA: 2:20:09 - loss: 0.3990 - regression_loss: 0.3563 - classification_loss: 0.0427
  528/10000 [>.............................] - ETA: 2:20:08 - loss: 0.3996 - regression_loss: 0.3569 - classification_loss: 0.0427
  529/10000 [>.............................] - ETA: 2:20:07 - loss: 0.3993 - regression_loss: 0.3567 - classification_loss: 0.0426
  530/10000 [>.............................] - ETA: 2:20:06 - loss: 0.3993 - regression_loss: 0.3566 - classification_loss: 0.0426
  531/10000 [>.............................] - ETA: 2:20:05 - loss: 0.3991 - regression_loss: 0.3565 - classification_loss: 0.0426
  532/10000 [>.............................] - ETA: 2:20:04 - loss: 0.3989 - regression_loss: 0.3563 - classification_loss: 0.0425
  533/10000 [>.............................] - ETA: 2:20:03 - loss: 0.3985 - regression_loss: 0.3560 - classification_loss: 0.0425
  534/10000 [>.............................] - ETA: 2:20:02 - loss: 0.3987 - regression_loss: 0.3563 - classification_loss: 0.0424
  535/10000 [>.............................] - ETA: 2:20:02 - loss: 0.3986 - regression_loss: 0.3561 - classification_loss: 0.0424
  536/10000 [>.............................] - ETA: 2:20:01 - loss: 0.3984 - regression_loss: 0.3560 - classification_loss: 0.0424
  537/10000 [>.............................] - ETA: 2:20:00 - loss: 0.3983 - regression_loss: 0.3559 - classification_loss: 0.0424
  538/10000 [>.............................] - ETA: 2:19:59 - loss: 0.4011 - regression_loss: 0.3585 - classification_loss: 0.0426
  539/10000 [>.............................] - ETA: 2:19:58 - loss: 0.4005 - regression_loss: 0.3580 - classification_loss: 0.0425
  540/10000 [>.............................] - ETA: 2:19:57 - loss: 0.4000 - regression_loss: 0.3575 - classification_loss: 0.0425
  541/10000 [>.............................] - ETA: 2:19:56 - loss: 0.4000 - regression_loss: 0.3576 - classification_loss: 0.0424
  542/10000 [>.............................] - ETA: 2:19:55 - loss: 0.3996 - regression_loss: 0.3572 - classification_loss: 0.0424
  543/10000 [>.............................] - ETA: 2:19:54 - loss: 0.3996 - regression_loss: 0.3572 - classification_loss: 0.0425
  544/10000 [>.............................] - ETA: 2:19:54 - loss: 0.3989 - regression_loss: 0.3565 - classification_loss: 0.0424
  545/10000 [>.............................] - ETA: 2:19:53 - loss: 0.3985 - regression_loss: 0.3562 - classification_loss: 0.0423
  546/10000 [>.............................] - ETA: 2:19:52 - loss: 0.3983 - regression_loss: 0.3559 - classification_loss: 0.0424
  547/10000 [>.............................] - ETA: 2:19:51 - loss: 0.3977 - regression_loss: 0.3554 - classification_loss: 0.0423
  548/10000 [>.............................] - ETA: 2:19:50 - loss: 0.3972 - regression_loss: 0.3550 - classification_loss: 0.0423
  549/10000 [>.............................] - ETA: 2:19:49 - loss: 0.3969 - regression_loss: 0.3547 - classification_loss: 0.0422
  550/10000 [>.............................] - ETA: 2:19:48 - loss: 0.3969 - regression_loss: 0.3547 - classification_loss: 0.0422
  551/10000 [>.............................] - ETA: 2:19:47 - loss: 0.3967 - regression_loss: 0.3545 - classification_loss: 0.0422
  552/10000 [>.............................] - ETA: 2:19:47 - loss: 0.3965 - regression_loss: 0.3543 - classification_loss: 0.0422
  553/10000 [>.............................] - ETA: 2:19:46 - loss: 0.3963 - regression_loss: 0.3541 - classification_loss: 0.0421
  554/10000 [>.............................] - ETA: 2:19:45 - loss: 0.3956 - regression_loss: 0.3535 - classification_loss: 0.0421
  555/10000 [>.............................] - ETA: 2:19:44 - loss: 0.3953 - regression_loss: 0.3533 - classification_loss: 0.0420
  556/10000 [>.............................] - ETA: 2:19:43 - loss: 0.3974 - regression_loss: 0.3550 - classification_loss: 0.0424
  557/10000 [>.............................] - ETA: 2:19:42 - loss: 0.3967 - regression_loss: 0.3544 - classification_loss: 0.0423
  558/10000 [>.............................] - ETA: 2:19:41 - loss: 0.3960 - regression_loss: 0.3538 - classification_loss: 0.0422
  559/10000 [>.............................] - ETA: 2:19:40 - loss: 0.3974 - regression_loss: 0.3551 - classification_loss: 0.0424
  560/10000 [>.............................] - ETA: 2:19:39 - loss: 0.3971 - regression_loss: 0.3548 - classification_loss: 0.0423
  561/10000 [>.............................] - ETA: 2:19:39 - loss: 0.3971 - regression_loss: 0.3548 - classification_loss: 0.0423
  562/10000 [>.............................] - ETA: 2:19:38 - loss: 0.3964 - regression_loss: 0.3542 - classification_loss: 0.0422
  563/10000 [>.............................] - ETA: 2:19:37 - loss: 0.3961 - regression_loss: 0.3540 - classification_loss: 0.0421
  564/10000 [>.............................] - ETA: 2:19:36 - loss: 0.3960 - regression_loss: 0.3539 - classification_loss: 0.0421
  565/10000 [>.............................] - ETA: 2:19:35 - loss: 0.3961 - regression_loss: 0.3540 - classification_loss: 0.0421
  566/10000 [>.............................] - ETA: 2:19:34 - loss: 0.3958 - regression_loss: 0.3538 - classification_loss: 0.0420
  567/10000 [>.............................] - ETA: 2:19:33 - loss: 0.3956 - regression_loss: 0.3535 - classification_loss: 0.0420
  568/10000 [>.............................] - ETA: 2:19:32 - loss: 0.3954 - regression_loss: 0.3534 - classification_loss: 0.0420
  569/10000 [>.............................] - ETA: 2:19:31 - loss: 0.3959 - regression_loss: 0.3538 - classification_loss: 0.0421
  570/10000 [>.............................] - ETA: 2:19:30 - loss: 0.3960 - regression_loss: 0.3540 - classification_loss: 0.0420
  571/10000 [>.............................] - ETA: 2:19:29 - loss: 0.3958 - regression_loss: 0.3538 - classification_loss: 0.0420
  572/10000 [>.............................] - ETA: 2:19:29 - loss: 0.3955 - regression_loss: 0.3534 - classification_loss: 0.0421
  573/10000 [>.............................] - ETA: 2:19:28 - loss: 0.3951 - regression_loss: 0.3530 - classification_loss: 0.0420
  574/10000 [>.............................] - ETA: 2:19:27 - loss: 0.3951 - regression_loss: 0.3531 - classification_loss: 0.0420
  575/10000 [>.............................] - ETA: 2:19:26 - loss: 0.3953 - regression_loss: 0.3533 - classification_loss: 0.0420
  576/10000 [>.............................] - ETA: 2:19:25 - loss: 0.3949 - regression_loss: 0.3530 - classification_loss: 0.0420
  577/10000 [>.............................] - ETA: 2:19:24 - loss: 0.3944 - regression_loss: 0.3525 - classification_loss: 0.0419
  578/10000 [>.............................] - ETA: 2:19:23 - loss: 0.3943 - regression_loss: 0.3523 - classification_loss: 0.0419
  579/10000 [>.............................] - ETA: 2:19:23 - loss: 0.3945 - regression_loss: 0.3526 - classification_loss: 0.0419
  580/10000 [>.............................] - ETA: 2:19:22 - loss: 0.3945 - regression_loss: 0.3526 - classification_loss: 0.0419
  581/10000 [>.............................] - ETA: 2:19:21 - loss: 0.3945 - regression_loss: 0.3526 - classification_loss: 0.0419
  582/10000 [>.............................] - ETA: 2:19:20 - loss: 0.3942 - regression_loss: 0.3523 - classification_loss: 0.0418
  583/10000 [>.............................] - ETA: 2:19:19 - loss: 0.3946 - regression_loss: 0.3528 - classification_loss: 0.0418
  584/10000 [>.............................] - ETA: 2:19:18 - loss: 0.3943 - regression_loss: 0.3526 - classification_loss: 0.0418
  585/10000 [>.............................] - ETA: 2:19:17 - loss: 0.3940 - regression_loss: 0.3522 - classification_loss: 0.0417
  586/10000 [>.............................] - ETA: 2:19:16 - loss: 0.3939 - regression_loss: 0.3522 - classification_loss: 0.0417
  587/10000 [>.............................] - ETA: 2:19:16 - loss: 0.3943 - regression_loss: 0.3525 - classification_loss: 0.0417
  588/10000 [>.............................] - ETA: 2:19:15 - loss: 0.3941 - regression_loss: 0.3523 - classification_loss: 0.0417
  589/10000 [>.............................] - ETA: 2:19:14 - loss: 0.3940 - regression_loss: 0.3524 - classification_loss: 0.0417
  590/10000 [>.............................] - ETA: 2:19:13 - loss: 0.3942 - regression_loss: 0.3526 - classification_loss: 0.0416
  591/10000 [>.............................] - ETA: 2:19:12 - loss: 0.3942 - regression_loss: 0.3526 - classification_loss: 0.0416
  592/10000 [>.............................] - ETA: 2:19:11 - loss: 0.3940 - regression_loss: 0.3524 - classification_loss: 0.0416
  593/10000 [>.............................] - ETA: 2:19:10 - loss: 0.3938 - regression_loss: 0.3523 - classification_loss: 0.0416
  594/10000 [>.............................] - ETA: 2:19:09 - loss: 0.3937 - regression_loss: 0.3521 - classification_loss: 0.0415
  595/10000 [>.............................] - ETA: 2:19:09 - loss: 0.3936 - regression_loss: 0.3521 - classification_loss: 0.0415
  596/10000 [>.............................] - ETA: 2:19:08 - loss: 0.3938 - regression_loss: 0.3523 - classification_loss: 0.0415
  597/10000 [>.............................] - ETA: 2:19:07 - loss: 0.3938 - regression_loss: 0.3523 - classification_loss: 0.0415
  598/10000 [>.............................] - ETA: 2:19:06 - loss: 0.3938 - regression_loss: 0.3523 - classification_loss: 0.0415
  599/10000 [>.............................] - ETA: 2:19:05 - loss: 0.3933 - regression_loss: 0.3519 - classification_loss: 0.0415
  600/10000 [>.............................] - ETA: 2:19:04 - loss: 0.3931 - regression_loss: 0.3517 - classification_loss: 0.0414
  601/10000 [>.............................] - ETA: 2:19:03 - loss: 0.3928 - regression_loss: 0.3515 - classification_loss: 0.0414
  602/10000 [>.............................] - ETA: 2:19:02 - loss: 0.3934 - regression_loss: 0.3520 - classification_loss: 0.0414
  603/10000 [>.............................] - ETA: 2:19:02 - loss: 0.3938 - regression_loss: 0.3524 - classification_loss: 0.0414
  604/10000 [>.............................] - ETA: 2:19:01 - loss: 0.3940 - regression_loss: 0.3527 - classification_loss: 0.0413
  605/10000 [>.............................] - ETA: 2:19:00 - loss: 0.3940 - regression_loss: 0.3527 - classification_loss: 0.0413
  606/10000 [>.............................] - ETA: 2:18:59 - loss: 0.3938 - regression_loss: 0.3526 - classification_loss: 0.0413
  607/10000 [>.............................] - ETA: 2:18:58 - loss: 0.3935 - regression_loss: 0.3523 - classification_loss: 0.0413
  608/10000 [>.............................] - ETA: 2:18:57 - loss: 0.3931 - regression_loss: 0.3519 - classification_loss: 0.0412
  609/10000 [>.............................] - ETA: 2:18:56 - loss: 0.3931 - regression_loss: 0.3519 - classification_loss: 0.0412
  610/10000 [>.............................] - ETA: 2:18:55 - loss: 0.3935 - regression_loss: 0.3523 - classification_loss: 0.0412
  611/10000 [>.............................] - ETA: 2:18:54 - loss: 0.3940 - regression_loss: 0.3528 - classification_loss: 0.0412
  612/10000 [>.............................] - ETA: 2:18:53 - loss: 0.3937 - regression_loss: 0.3525 - classification_loss: 0.0412
  613/10000 [>.............................] - ETA: 2:18:52 - loss: 0.3934 - regression_loss: 0.3523 - classification_loss: 0.0411
  614/10000 [>.............................] - ETA: 2:18:52 - loss: 0.3935 - regression_loss: 0.3524 - classification_loss: 0.0411
  615/10000 [>.............................] - ETA: 2:18:51 - loss: 0.3933 - regression_loss: 0.3522 - classification_loss: 0.0411
  616/10000 [>.............................] - ETA: 2:18:50 - loss: 0.3929 - regression_loss: 0.3518 - classification_loss: 0.0411
  617/10000 [>.............................] - ETA: 2:18:49 - loss: 0.3929 - regression_loss: 0.3518 - classification_loss: 0.0411
  618/10000 [>.............................] - ETA: 2:18:48 - loss: 0.3928 - regression_loss: 0.3517 - classification_loss: 0.0411
  619/10000 [>.............................] - ETA: 2:18:47 - loss: 0.3930 - regression_loss: 0.3518 - classification_loss: 0.0412
  620/10000 [>.............................] - ETA: 2:18:46 - loss: 0.3927 - regression_loss: 0.3516 - classification_loss: 0.0411
  621/10000 [>.............................] - ETA: 2:18:45 - loss: 0.3926 - regression_loss: 0.3515 - classification_loss: 0.0411
  622/10000 [>.............................] - ETA: 2:18:44 - loss: 0.3924 - regression_loss: 0.3513 - classification_loss: 0.0411
  623/10000 [>.............................] - ETA: 2:18:44 - loss: 0.3930 - regression_loss: 0.3516 - classification_loss: 0.0413
  624/10000 [>.............................] - ETA: 2:18:43 - loss: 0.3937 - regression_loss: 0.3519 - classification_loss: 0.0418
  625/10000 [>.............................] - ETA: 2:18:42 - loss: 0.3936 - regression_loss: 0.3518 - classification_loss: 0.0418
  626/10000 [>.............................] - ETA: 2:18:41 - loss: 0.3947 - regression_loss: 0.3529 - classification_loss: 0.0418
  627/10000 [>.............................] - ETA: 2:18:40 - loss: 0.3945 - regression_loss: 0.3526 - classification_loss: 0.0419
  628/10000 [>.............................] - ETA: 2:18:39 - loss: 0.3948 - regression_loss: 0.3530 - classification_loss: 0.0419
  629/10000 [>.............................] - ETA: 2:18:38 - loss: 0.3947 - regression_loss: 0.3528 - classification_loss: 0.0419
  630/10000 [>.............................] - ETA: 2:18:37 - loss: 0.3943 - regression_loss: 0.3524 - classification_loss: 0.0418
  631/10000 [>.............................] - ETA: 2:18:36 - loss: 0.3938 - regression_loss: 0.3521 - classification_loss: 0.0418
  632/10000 [>.............................] - ETA: 2:18:36 - loss: 0.3937 - regression_loss: 0.3520 - classification_loss: 0.0418
  633/10000 [>.............................] - ETA: 2:18:35 - loss: 0.3939 - regression_loss: 0.3520 - classification_loss: 0.0419
  634/10000 [>.............................] - ETA: 2:18:34 - loss: 0.3942 - regression_loss: 0.3522 - classification_loss: 0.0419
  635/10000 [>.............................] - ETA: 2:18:33 - loss: 0.3942 - regression_loss: 0.3523 - classification_loss: 0.0419
  636/10000 [>.............................] - ETA: 2:18:32 - loss: 0.3940 - regression_loss: 0.3520 - classification_loss: 0.0419
  637/10000 [>.............................] - ETA: 2:18:31 - loss: 0.3935 - regression_loss: 0.3516 - classification_loss: 0.0419
  638/10000 [>.............................] - ETA: 2:18:30 - loss: 0.3931 - regression_loss: 0.3512 - classification_loss: 0.0419
  639/10000 [>.............................] - ETA: 2:18:29 - loss: 0.3927 - regression_loss: 0.3508 - classification_loss: 0.0418
  640/10000 [>.............................] - ETA: 2:18:29 - loss: 0.3925 - regression_loss: 0.3506 - classification_loss: 0.0418
  641/10000 [>.............................] - ETA: 2:18:28 - loss: 0.3923 - regression_loss: 0.3505 - classification_loss: 0.0418
  642/10000 [>.............................] - ETA: 2:18:27 - loss: 0.3920 - regression_loss: 0.3502 - classification_loss: 0.0417
  643/10000 [>.............................] - ETA: 2:18:26 - loss: 0.3917 - regression_loss: 0.3500 - classification_loss: 0.0417
  644/10000 [>.............................] - ETA: 2:18:25 - loss: 0.3914 - regression_loss: 0.3498 - classification_loss: 0.0417
  645/10000 [>.............................] - ETA: 2:18:24 - loss: 0.3911 - regression_loss: 0.3495 - classification_loss: 0.0416
  646/10000 [>.............................] - ETA: 2:18:23 - loss: 0.3918 - regression_loss: 0.3500 - classification_loss: 0.0417
  647/10000 [>.............................] - ETA: 2:18:22 - loss: 0.3918 - regression_loss: 0.3501 - classification_loss: 0.0417
  648/10000 [>.............................] - ETA: 2:18:21 - loss: 0.3915 - regression_loss: 0.3498 - classification_loss: 0.0417
  649/10000 [>.............................] - ETA: 2:18:20 - loss: 0.3912 - regression_loss: 0.3496 - classification_loss: 0.0417
  650/10000 [>.............................] - ETA: 2:18:20 - loss: 0.3914 - regression_loss: 0.3498 - classification_loss: 0.0417
  651/10000 [>.............................] - ETA: 2:18:19 - loss: 0.3912 - regression_loss: 0.3495 - classification_loss: 0.0416
  652/10000 [>.............................] - ETA: 2:18:18 - loss: 0.3912 - regression_loss: 0.3495 - classification_loss: 0.0416
  653/10000 [>.............................] - ETA: 2:18:17 - loss: 0.3907 - regression_loss: 0.3492 - classification_loss: 0.0416
  654/10000 [>.............................] - ETA: 2:18:16 - loss: 0.3908 - regression_loss: 0.3492 - classification_loss: 0.0417
  655/10000 [>.............................] - ETA: 2:18:15 - loss: 0.3906 - regression_loss: 0.3490 - classification_loss: 0.0417
  656/10000 [>.............................] - ETA: 2:18:14 - loss: 0.3905 - regression_loss: 0.3489 - classification_loss: 0.0417
  657/10000 [>.............................] - ETA: 2:18:13 - loss: 0.3904 - regression_loss: 0.3488 - classification_loss: 0.0417
  658/10000 [>.............................] - ETA: 2:18:13 - loss: 0.3903 - regression_loss: 0.3486 - classification_loss: 0.0417
  659/10000 [>.............................] - ETA: 2:18:12 - loss: 0.3902 - regression_loss: 0.3485 - classification_loss: 0.0417
  660/10000 [>.............................] - ETA: 2:18:11 - loss: 0.3905 - regression_loss: 0.3488 - classification_loss: 0.0417
  661/10000 [>.............................] - ETA: 2:18:10 - loss: 0.3904 - regression_loss: 0.3487 - classification_loss: 0.0417
  662/10000 [>.............................] - ETA: 2:18:09 - loss: 0.3905 - regression_loss: 0.3488 - classification_loss: 0.0417
  663/10000 [>.............................] - ETA: 2:18:08 - loss: 0.3902 - regression_loss: 0.3486 - classification_loss: 0.0417
  664/10000 [>.............................] - ETA: 2:18:07 - loss: 0.3900 - regression_loss: 0.3484 - classification_loss: 0.0417
  665/10000 [>.............................] - ETA: 2:18:06 - loss: 0.3899 - regression_loss: 0.3482 - classification_loss: 0.0417
  666/10000 [>.............................] - ETA: 2:18:05 - loss: 0.3899 - regression_loss: 0.3483 - classification_loss: 0.0416
  667/10000 [=>............................] - ETA: 2:18:05 - loss: 0.3896 - regression_loss: 0.3480 - classification_loss: 0.0416
  668/10000 [=>............................] - ETA: 2:18:04 - loss: 0.3898 - regression_loss: 0.3483 - classification_loss: 0.0416
  669/10000 [=>............................] - ETA: 2:18:03 - loss: 0.3895 - regression_loss: 0.3479 - classification_loss: 0.0415
  670/10000 [=>............................] - ETA: 2:18:02 - loss: 0.3897 - regression_loss: 0.3481 - classification_loss: 0.0416
  671/10000 [=>............................] - ETA: 2:18:01 - loss: 0.3898 - regression_loss: 0.3482 - classification_loss: 0.0415
  672/10000 [=>............................] - ETA: 2:18:00 - loss: 0.3902 - regression_loss: 0.3487 - classification_loss: 0.0416
  673/10000 [=>............................] - ETA: 2:17:59 - loss: 0.3901 - regression_loss: 0.3486 - classification_loss: 0.0415
  674/10000 [=>............................] - ETA: 2:17:58 - loss: 0.3900 - regression_loss: 0.3485 - classification_loss: 0.0415
  675/10000 [=>............................] - ETA: 2:17:58 - loss: 0.3902 - regression_loss: 0.3487 - classification_loss: 0.0415
  676/10000 [=>............................] - ETA: 2:17:57 - loss: 0.3907 - regression_loss: 0.3490 - classification_loss: 0.0417
  677/10000 [=>............................] - ETA: 2:17:56 - loss: 0.3905 - regression_loss: 0.3489 - classification_loss: 0.0417
  678/10000 [=>............................] - ETA: 2:17:55 - loss: 0.3909 - regression_loss: 0.3492 - classification_loss: 0.0417
  679/10000 [=>............................] - ETA: 2:17:54 - loss: 0.3907 - regression_loss: 0.3491 - classification_loss: 0.0416
  680/10000 [=>............................] - ETA: 2:17:53 - loss: 0.3910 - regression_loss: 0.3493 - classification_loss: 0.0417
  681/10000 [=>............................] - ETA: 2:17:52 - loss: 0.3909 - regression_loss: 0.3492 - classification_loss: 0.0417
  682/10000 [=>............................] - ETA: 2:17:51 - loss: 0.3908 - regression_loss: 0.3491 - classification_loss: 0.0416
  683/10000 [=>............................] - ETA: 2:17:50 - loss: 0.3913 - regression_loss: 0.3496 - classification_loss: 0.0417
  684/10000 [=>............................] - ETA: 2:17:50 - loss: 0.3912 - regression_loss: 0.3496 - classification_loss: 0.0417
  685/10000 [=>............................] - ETA: 2:17:49 - loss: 0.3907 - regression_loss: 0.3491 - classification_loss: 0.0416
  686/10000 [=>............................] - ETA: 2:17:48 - loss: 0.3905 - regression_loss: 0.3490 - classification_loss: 0.0416
  687/10000 [=>............................] - ETA: 2:17:47 - loss: 0.3914 - regression_loss: 0.3496 - classification_loss: 0.0417
  688/10000 [=>............................] - ETA: 2:17:46 - loss: 0.3916 - regression_loss: 0.3498 - classification_loss: 0.0418
  689/10000 [=>............................] - ETA: 2:17:45 - loss: 0.3914 - regression_loss: 0.3497 - classification_loss: 0.0417
  690/10000 [=>............................] - ETA: 2:17:44 - loss: 0.3929 - regression_loss: 0.3511 - classification_loss: 0.0418
  691/10000 [=>............................] - ETA: 2:17:43 - loss: 0.3928 - regression_loss: 0.3511 - classification_loss: 0.0418
  692/10000 [=>............................] - ETA: 2:17:42 - loss: 0.3927 - regression_loss: 0.3508 - classification_loss: 0.0418
  693/10000 [=>............................] - ETA: 2:17:42 - loss: 0.3928 - regression_loss: 0.3510 - classification_loss: 0.0418
  694/10000 [=>............................] - ETA: 2:17:41 - loss: 0.3926 - regression_loss: 0.3508 - classification_loss: 0.0418
  695/10000 [=>............................] - ETA: 2:17:40 - loss: 0.3925 - regression_loss: 0.3507 - classification_loss: 0.0418
  696/10000 [=>............................] - ETA: 2:17:39 - loss: 0.3923 - regression_loss: 0.3505 - classification_loss: 0.0418
  697/10000 [=>............................] - ETA: 2:17:38 - loss: 0.3922 - regression_loss: 0.3504 - classification_loss: 0.0418
  698/10000 [=>............................] - ETA: 2:17:37 - loss: 0.3919 - regression_loss: 0.3501 - classification_loss: 0.0418
  699/10000 [=>............................] - ETA: 2:17:36 - loss: 0.3918 - regression_loss: 0.3500 - classification_loss: 0.0417
  700/10000 [=>............................] - ETA: 2:17:35 - loss: 0.3914 - regression_loss: 0.3497 - classification_loss: 0.0417
  701/10000 [=>............................] - ETA: 2:17:34 - loss: 0.3910 - regression_loss: 0.3494 - classification_loss: 0.0417
  702/10000 [=>............................] - ETA: 2:17:33 - loss: 0.3908 - regression_loss: 0.3492 - classification_loss: 0.0416
  703/10000 [=>............................] - ETA: 2:17:33 - loss: 0.3905 - regression_loss: 0.3489 - classification_loss: 0.0416
  704/10000 [=>............................] - ETA: 2:17:32 - loss: 0.3905 - regression_loss: 0.3489 - classification_loss: 0.0416
  705/10000 [=>............................] - ETA: 2:17:31 - loss: 0.3903 - regression_loss: 0.3488 - classification_loss: 0.0415
  706/10000 [=>............................] - ETA: 2:17:30 - loss: 0.3901 - regression_loss: 0.3486 - classification_loss: 0.0415
  707/10000 [=>............................] - ETA: 2:17:29 - loss: 0.3898 - regression_loss: 0.3483 - classification_loss: 0.0415
  708/10000 [=>............................] - ETA: 2:17:28 - loss: 0.3895 - regression_loss: 0.3481 - classification_loss: 0.0415
  709/10000 [=>............................] - ETA: 2:17:27 - loss: 0.3900 - regression_loss: 0.3486 - classification_loss: 0.0414
  710/10000 [=>............................] - ETA: 2:17:26 - loss: 0.3898 - regression_loss: 0.3484 - classification_loss: 0.0414
  711/10000 [=>............................] - ETA: 2:17:25 - loss: 0.3902 - regression_loss: 0.3488 - classification_loss: 0.0414
  712/10000 [=>............................] - ETA: 2:17:25 - loss: 0.3903 - regression_loss: 0.3489 - classification_loss: 0.0414
  713/10000 [=>............................] - ETA: 2:17:24 - loss: 0.3902 - regression_loss: 0.3488 - classification_loss: 0.0414
  714/10000 [=>............................] - ETA: 2:17:23 - loss: 0.3899 - regression_loss: 0.3486 - classification_loss: 0.0414
  715/10000 [=>............................] - ETA: 2:17:22 - loss: 0.3905 - regression_loss: 0.3492 - classification_loss: 0.0413
  716/10000 [=>............................] - ETA: 2:17:21 - loss: 0.3901 - regression_loss: 0.3489 - classification_loss: 0.0413
  717/10000 [=>............................] - ETA: 2:17:20 - loss: 0.3910 - regression_loss: 0.3495 - classification_loss: 0.0416
  718/10000 [=>............................] - ETA: 2:17:19 - loss: 0.3912 - regression_loss: 0.3495 - classification_loss: 0.0417
  719/10000 [=>............................] - ETA: 2:17:18 - loss: 0.3910 - regression_loss: 0.3494 - classification_loss: 0.0416
  720/10000 [=>............................] - ETA: 2:17:17 - loss: 0.3911 - regression_loss: 0.3494 - classification_loss: 0.0416
  721/10000 [=>............................] - ETA: 2:17:17 - loss: 0.3919 - regression_loss: 0.3503 - classification_loss: 0.0416
  722/10000 [=>............................] - ETA: 2:17:16 - loss: 0.3917 - regression_loss: 0.3501 - classification_loss: 0.0416
  723/10000 [=>............................] - ETA: 2:17:15 - loss: 0.3920 - regression_loss: 0.3504 - classification_loss: 0.0415
  724/10000 [=>............................] - ETA: 2:17:14 - loss: 0.3922 - regression_loss: 0.3508 - classification_loss: 0.0415
  725/10000 [=>............................] - ETA: 2:17:13 - loss: 0.3923 - regression_loss: 0.3508 - classification_loss: 0.0414
  726/10000 [=>............................] - ETA: 2:17:12 - loss: 0.3923 - regression_loss: 0.3508 - classification_loss: 0.0414
  727/10000 [=>............................] - ETA: 2:17:11 - loss: 0.3924 - regression_loss: 0.3510 - classification_loss: 0.0414
  728/10000 [=>............................] - ETA: 2:17:10 - loss: 0.3920 - regression_loss: 0.3506 - classification_loss: 0.0414
  729/10000 [=>............................] - ETA: 2:17:09 - loss: 0.3920 - regression_loss: 0.3506 - classification_loss: 0.0414
  730/10000 [=>............................] - ETA: 2:17:08 - loss: 0.3918 - regression_loss: 0.3505 - classification_loss: 0.0414
  731/10000 [=>............................] - ETA: 2:17:08 - loss: 0.3921 - regression_loss: 0.3507 - classification_loss: 0.0413
  732/10000 [=>............................] - ETA: 2:17:07 - loss: 0.3918 - regression_loss: 0.3505 - classification_loss: 0.0413
  733/10000 [=>............................] - ETA: 2:17:06 - loss: 0.3914 - regression_loss: 0.3502 - classification_loss: 0.0413
  734/10000 [=>............................] - ETA: 2:17:05 - loss: 0.3919 - regression_loss: 0.3506 - classification_loss: 0.0413
  735/10000 [=>............................] - ETA: 2:17:04 - loss: 0.3917 - regression_loss: 0.3504 - classification_loss: 0.0413
  736/10000 [=>............................] - ETA: 2:17:03 - loss: 0.3914 - regression_loss: 0.3502 - classification_loss: 0.0412
  737/10000 [=>............................] - ETA: 2:17:02 - loss: 0.3915 - regression_loss: 0.3503 - classification_loss: 0.0412
  738/10000 [=>............................] - ETA: 2:17:01 - loss: 0.3915 - regression_loss: 0.3503 - classification_loss: 0.0412
  739/10000 [=>............................] - ETA: 2:17:00 - loss: 0.3919 - regression_loss: 0.3507 - classification_loss: 0.0412
  740/10000 [=>............................] - ETA: 2:17:00 - loss: 0.3916 - regression_loss: 0.3504 - classification_loss: 0.0412
  741/10000 [=>............................] - ETA: 2:16:59 - loss: 0.3919 - regression_loss: 0.3507 - classification_loss: 0.0412
  742/10000 [=>............................] - ETA: 2:16:58 - loss: 0.3925 - regression_loss: 0.3512 - classification_loss: 0.0412
  743/10000 [=>............................] - ETA: 2:16:57 - loss: 0.3922 - regression_loss: 0.3510 - classification_loss: 0.0412
  744/10000 [=>............................] - ETA: 2:16:56 - loss: 0.3930 - regression_loss: 0.3517 - classification_loss: 0.0413
  745/10000 [=>............................] - ETA: 2:16:55 - loss: 0.3931 - regression_loss: 0.3518 - classification_loss: 0.0413
  746/10000 [=>............................] - ETA: 2:16:54 - loss: 0.3930 - regression_loss: 0.3517 - classification_loss: 0.0413
  747/10000 [=>............................] - ETA: 2:16:53 - loss: 0.3931 - regression_loss: 0.3518 - classification_loss: 0.0413
  748/10000 [=>............................] - ETA: 2:16:53 - loss: 0.3935 - regression_loss: 0.3522 - classification_loss: 0.0413
  749/10000 [=>............................] - ETA: 2:16:52 - loss: 0.3936 - regression_loss: 0.3523 - classification_loss: 0.0413
  750/10000 [=>............................] - ETA: 2:16:51 - loss: 0.3933 - regression_loss: 0.3520 - classification_loss: 0.0413
  751/10000 [=>............................] - ETA: 2:16:50 - loss: 0.3932 - regression_loss: 0.3519 - classification_loss: 0.0412
  752/10000 [=>............................] - ETA: 2:16:49 - loss: 0.3933 - regression_loss: 0.3521 - classification_loss: 0.0412
  753/10000 [=>............................] - ETA: 2:16:48 - loss: 0.3930 - regression_loss: 0.3518 - classification_loss: 0.0412
  754/10000 [=>............................] - ETA: 2:16:47 - loss: 0.3932 - regression_loss: 0.3520 - classification_loss: 0.0412
  755/10000 [=>............................] - ETA: 2:16:46 - loss: 0.3930 - regression_loss: 0.3518 - classification_loss: 0.0411
  756/10000 [=>............................] - ETA: 2:16:45 - loss: 0.3928 - regression_loss: 0.3517 - classification_loss: 0.0411
  757/10000 [=>............................] - ETA: 2:16:45 - loss: 0.3928 - regression_loss: 0.3517 - classification_loss: 0.0411
  758/10000 [=>............................] - ETA: 2:16:44 - loss: 0.3929 - regression_loss: 0.3518 - classification_loss: 0.0411
  759/10000 [=>............................] - ETA: 2:16:43 - loss: 0.3935 - regression_loss: 0.3523 - classification_loss: 0.0412
  760/10000 [=>............................] - ETA: 2:16:42 - loss: 0.3933 - regression_loss: 0.3522 - classification_loss: 0.0411
  761/10000 [=>............................] - ETA: 2:16:41 - loss: 0.3934 - regression_loss: 0.3522 - classification_loss: 0.0411
  762/10000 [=>............................] - ETA: 2:16:40 - loss: 0.3936 - regression_loss: 0.3524 - classification_loss: 0.0411
  763/10000 [=>............................] - ETA: 2:16:39 - loss: 0.3931 - regression_loss: 0.3520 - classification_loss: 0.0411
  764/10000 [=>............................] - ETA: 2:16:39 - loss: 0.3939 - regression_loss: 0.3528 - classification_loss: 0.0411
  765/10000 [=>............................] - ETA: 2:16:38 - loss: 0.3939 - regression_loss: 0.3528 - classification_loss: 0.0411
  766/10000 [=>............................] - ETA: 2:16:37 - loss: 0.3934 - regression_loss: 0.3523 - classification_loss: 0.0411
  767/10000 [=>............................] - ETA: 2:16:36 - loss: 0.3932 - regression_loss: 0.3522 - classification_loss: 0.0411
  768/10000 [=>............................] - ETA: 2:16:35 - loss: 0.3928 - regression_loss: 0.3518 - classification_loss: 0.0410
  769/10000 [=>............................] - ETA: 2:16:34 - loss: 0.3927 - regression_loss: 0.3517 - classification_loss: 0.0410
  770/10000 [=>............................] - ETA: 2:16:33 - loss: 0.3924 - regression_loss: 0.3515 - classification_loss: 0.0410
  771/10000 [=>............................] - ETA: 2:16:32 - loss: 0.3924 - regression_loss: 0.3514 - classification_loss: 0.0410
  772/10000 [=>............................] - ETA: 2:16:32 - loss: 0.3925 - regression_loss: 0.3515 - classification_loss: 0.0410
  773/10000 [=>............................] - ETA: 2:16:31 - loss: 0.3926 - regression_loss: 0.3515 - classification_loss: 0.0410
  774/10000 [=>............................] - ETA: 2:16:30 - loss: 0.3920 - regression_loss: 0.3511 - classification_loss: 0.0410
  775/10000 [=>............................] - ETA: 2:16:29 - loss: 0.3918 - regression_loss: 0.3508 - classification_loss: 0.0409
  776/10000 [=>............................] - ETA: 2:16:28 - loss: 0.3922 - regression_loss: 0.3513 - classification_loss: 0.0409
  777/10000 [=>............................] - ETA: 2:16:27 - loss: 0.3925 - regression_loss: 0.3516 - classification_loss: 0.0410
  778/10000 [=>............................] - ETA: 2:16:26 - loss: 0.3923 - regression_loss: 0.3514 - classification_loss: 0.0409
  779/10000 [=>............................] - ETA: 2:16:25 - loss: 0.3935 - regression_loss: 0.3524 - classification_loss: 0.0411
  780/10000 [=>............................] - ETA: 2:16:24 - loss: 0.3932 - regression_loss: 0.3522 - classification_loss: 0.0411
  781/10000 [=>............................] - ETA: 2:16:24 - loss: 0.3933 - regression_loss: 0.3523 - classification_loss: 0.0410
  782/10000 [=>............................] - ETA: 2:16:23 - loss: 0.3932 - regression_loss: 0.3522 - classification_loss: 0.0410
  783/10000 [=>............................] - ETA: 2:16:22 - loss: 0.3927 - regression_loss: 0.3517 - classification_loss: 0.0410
  784/10000 [=>............................] - ETA: 2:16:21 - loss: 0.3927 - regression_loss: 0.3517 - classification_loss: 0.0410
  785/10000 [=>............................] - ETA: 2:16:20 - loss: 0.3924 - regression_loss: 0.3515 - classification_loss: 0.0409
  786/10000 [=>............................] - ETA: 2:16:19 - loss: 0.3922 - regression_loss: 0.3513 - classification_loss: 0.0409
  787/10000 [=>............................] - ETA: 2:16:18 - loss: 0.3921 - regression_loss: 0.3512 - classification_loss: 0.0409
  788/10000 [=>............................] - ETA: 2:16:18 - loss: 0.3921 - regression_loss: 0.3512 - classification_loss: 0.0409
  789/10000 [=>............................] - ETA: 2:16:17 - loss: 0.3925 - regression_loss: 0.3516 - classification_loss: 0.0409
  790/10000 [=>............................] - ETA: 2:16:16 - loss: 0.3941 - regression_loss: 0.3527 - classification_loss: 0.0413
  791/10000 [=>............................] - ETA: 2:16:15 - loss: 0.3943 - regression_loss: 0.3529 - classification_loss: 0.0414
  792/10000 [=>............................] - ETA: 2:16:14 - loss: 0.3941 - regression_loss: 0.3528 - classification_loss: 0.0413
  793/10000 [=>............................] - ETA: 2:16:13 - loss: 0.3936 - regression_loss: 0.3523 - classification_loss: 0.0413
  794/10000 [=>............................] - ETA: 2:16:12 - loss: 0.3936 - regression_loss: 0.3524 - classification_loss: 0.0413
  795/10000 [=>............................] - ETA: 2:16:11 - loss: 0.3934 - regression_loss: 0.3522 - classification_loss: 0.0412
  796/10000 [=>............................] - ETA: 2:16:11 - loss: 0.3935 - regression_loss: 0.3522 - classification_loss: 0.0413
  797/10000 [=>............................] - ETA: 2:16:10 - loss: 0.3936 - regression_loss: 0.3523 - classification_loss: 0.0413
  798/10000 [=>............................] - ETA: 2:16:09 - loss: 0.3935 - regression_loss: 0.3522 - classification_loss: 0.0413
  799/10000 [=>............................] - ETA: 2:16:08 - loss: 0.3934 - regression_loss: 0.3521 - classification_loss: 0.0413
  800/10000 [=>............................] - ETA: 2:16:07 - loss: 0.3944 - regression_loss: 0.3531 - classification_loss: 0.0413
  801/10000 [=>............................] - ETA: 2:16:06 - loss: 0.3946 - regression_loss: 0.3533 - classification_loss: 0.0413
  802/10000 [=>............................] - ETA: 2:16:05 - loss: 0.3947 - regression_loss: 0.3534 - classification_loss: 0.0413
  803/10000 [=>............................] - ETA: 2:16:04 - loss: 0.3947 - regression_loss: 0.3534 - classification_loss: 0.0413
  804/10000 [=>............................] - ETA: 2:16:03 - loss: 0.3946 - regression_loss: 0.3533 - classification_loss: 0.0413
  805/10000 [=>............................] - ETA: 2:16:03 - loss: 0.3948 - regression_loss: 0.3535 - classification_loss: 0.0413
  806/10000 [=>............................] - ETA: 2:16:02 - loss: 0.3946 - regression_loss: 0.3533 - classification_loss: 0.0413
  807/10000 [=>............................] - ETA: 2:16:01 - loss: 0.3944 - regression_loss: 0.3531 - classification_loss: 0.0412
  808/10000 [=>............................] - ETA: 2:16:00 - loss: 0.3944 - regression_loss: 0.3532 - classification_loss: 0.0412
  809/10000 [=>............................] - ETA: 2:15:59 - loss: 0.3942 - regression_loss: 0.3531 - classification_loss: 0.0412
  810/10000 [=>............................] - ETA: 2:15:58 - loss: 0.3941 - regression_loss: 0.3529 - classification_loss: 0.0411
  811/10000 [=>............................] - ETA: 2:15:57 - loss: 0.3938 - regression_loss: 0.3527 - classification_loss: 0.0411
  812/10000 [=>............................] - ETA: 2:15:56 - loss: 0.3939 - regression_loss: 0.3528 - classification_loss: 0.0411
  813/10000 [=>............................] - ETA: 2:15:55 - loss: 0.3940 - regression_loss: 0.3529 - classification_loss: 0.0411
  814/10000 [=>............................] - ETA: 2:15:54 - loss: 0.3940 - regression_loss: 0.3529 - classification_loss: 0.0411
  815/10000 [=>............................] - ETA: 2:15:54 - loss: 0.3944 - regression_loss: 0.3533 - classification_loss: 0.0410
  816/10000 [=>............................] - ETA: 2:15:53 - loss: 0.3941 - regression_loss: 0.3531 - classification_loss: 0.0410
  817/10000 [=>............................] - ETA: 2:15:52 - loss: 0.3938 - regression_loss: 0.3529 - classification_loss: 0.0410
  818/10000 [=>............................] - ETA: 2:15:51 - loss: 0.3936 - regression_loss: 0.3527 - classification_loss: 0.0409
  819/10000 [=>............................] - ETA: 2:15:50 - loss: 0.3943 - regression_loss: 0.3533 - classification_loss: 0.0410
  820/10000 [=>............................] - ETA: 2:15:49 - loss: 0.3941 - regression_loss: 0.3531 - classification_loss: 0.0410
  821/10000 [=>............................] - ETA: 2:15:48 - loss: 0.3954 - regression_loss: 0.3540 - classification_loss: 0.0414
  822/10000 [=>............................] - ETA: 2:15:47 - loss: 0.3952 - regression_loss: 0.3539 - classification_loss: 0.0413
  823/10000 [=>............................] - ETA: 2:15:46 - loss: 0.3947 - regression_loss: 0.3534 - classification_loss: 0.0413
  824/10000 [=>............................] - ETA: 2:15:46 - loss: 0.3942 - regression_loss: 0.3530 - classification_loss: 0.0412
  825/10000 [=>............................] - ETA: 2:15:45 - loss: 0.3942 - regression_loss: 0.3529 - classification_loss: 0.0413
  826/10000 [=>............................] - ETA: 2:15:44 - loss: 0.3945 - regression_loss: 0.3533 - classification_loss: 0.0413
  827/10000 [=>............................] - ETA: 2:15:43 - loss: 0.3944 - regression_loss: 0.3531 - classification_loss: 0.0412
  828/10000 [=>............................] - ETA: 2:15:42 - loss: 0.3942 - regression_loss: 0.3530 - classification_loss: 0.0412
  829/10000 [=>............................] - ETA: 2:15:41 - loss: 0.3942 - regression_loss: 0.3529 - classification_loss: 0.0412
  830/10000 [=>............................] - ETA: 2:15:40 - loss: 0.3941 - regression_loss: 0.3529 - classification_loss: 0.0412
  831/10000 [=>............................] - ETA: 2:15:39 - loss: 0.3942 - regression_loss: 0.3530 - classification_loss: 0.0412
  832/10000 [=>............................] - ETA: 2:15:38 - loss: 0.3941 - regression_loss: 0.3530 - classification_loss: 0.0412
  833/10000 [=>............................] - ETA: 2:15:38 - loss: 0.3938 - regression_loss: 0.3527 - classification_loss: 0.0412
  834/10000 [=>............................] - ETA: 2:15:37 - loss: 0.3938 - regression_loss: 0.3527 - classification_loss: 0.0411
  835/10000 [=>............................] - ETA: 2:15:36 - loss: 0.3933 - regression_loss: 0.3523 - classification_loss: 0.0411
  836/10000 [=>............................] - ETA: 2:15:35 - loss: 0.3931 - regression_loss: 0.3521 - classification_loss: 0.0410
  837/10000 [=>............................] - ETA: 2:15:34 - loss: 0.3931 - regression_loss: 0.3520 - classification_loss: 0.0410
  838/10000 [=>............................] - ETA: 2:15:33 - loss: 0.3929 - regression_loss: 0.3519 - classification_loss: 0.0410
  839/10000 [=>............................] - ETA: 2:15:32 - loss: 0.3932 - regression_loss: 0.3522 - classification_loss: 0.0410
  840/10000 [=>............................] - ETA: 2:15:31 - loss: 0.3929 - regression_loss: 0.3520 - classification_loss: 0.0410
  841/10000 [=>............................] - ETA: 2:15:31 - loss: 0.3928 - regression_loss: 0.3519 - classification_loss: 0.0409
  842/10000 [=>............................] - ETA: 2:15:30 - loss: 0.3925 - regression_loss: 0.3516 - classification_loss: 0.0409
  843/10000 [=>............................] - ETA: 2:15:29 - loss: 0.3923 - regression_loss: 0.3514 - classification_loss: 0.0409
  844/10000 [=>............................] - ETA: 2:15:28 - loss: 0.3923 - regression_loss: 0.3515 - classification_loss: 0.0409
  845/10000 [=>............................] - ETA: 2:15:27 - loss: 0.3929 - regression_loss: 0.3520 - classification_loss: 0.0409
  846/10000 [=>............................] - ETA: 2:15:26 - loss: 0.3928 - regression_loss: 0.3520 - classification_loss: 0.0409
  847/10000 [=>............................] - ETA: 2:15:25 - loss: 0.3928 - regression_loss: 0.3519 - classification_loss: 0.0409
  848/10000 [=>............................] - ETA: 2:15:24 - loss: 0.3926 - regression_loss: 0.3517 - classification_loss: 0.0408
  849/10000 [=>............................] - ETA: 2:15:23 - loss: 0.3924 - regression_loss: 0.3515 - classification_loss: 0.0408
  850/10000 [=>............................] - ETA: 2:15:22 - loss: 0.3921 - regression_loss: 0.3513 - classification_loss: 0.0408
  851/10000 [=>............................] - ETA: 2:15:22 - loss: 0.3923 - regression_loss: 0.3516 - classification_loss: 0.0408
  852/10000 [=>............................] - ETA: 2:15:21 - loss: 0.3927 - regression_loss: 0.3520 - classification_loss: 0.0407
  853/10000 [=>............................] - ETA: 2:15:20 - loss: 0.3927 - regression_loss: 0.3520 - classification_loss: 0.0408
  854/10000 [=>............................] - ETA: 2:15:19 - loss: 0.3928 - regression_loss: 0.3521 - classification_loss: 0.0407
  855/10000 [=>............................] - ETA: 2:15:18 - loss: 0.3926 - regression_loss: 0.3519 - classification_loss: 0.0407
  856/10000 [=>............................] - ETA: 2:15:17 - loss: 0.3930 - regression_loss: 0.3522 - classification_loss: 0.0407
  857/10000 [=>............................] - ETA: 2:15:16 - loss: 0.3927 - regression_loss: 0.3520 - classification_loss: 0.0407
  858/10000 [=>............................] - ETA: 2:15:15 - loss: 0.3929 - regression_loss: 0.3522 - classification_loss: 0.0407
  859/10000 [=>............................] - ETA: 2:15:14 - loss: 0.3932 - regression_loss: 0.3525 - classification_loss: 0.0407
  860/10000 [=>............................] - ETA: 2:15:13 - loss: 0.3931 - regression_loss: 0.3524 - classification_loss: 0.0407
  861/10000 [=>............................] - ETA: 2:15:13 - loss: 0.3930 - regression_loss: 0.3523 - classification_loss: 0.0406
  862/10000 [=>............................] - ETA: 2:15:12 - loss: 0.3928 - regression_loss: 0.3521 - classification_loss: 0.0406
  863/10000 [=>............................] - ETA: 2:15:11 - loss: 0.3924 - regression_loss: 0.3518 - classification_loss: 0.0406
  864/10000 [=>............................] - ETA: 2:15:10 - loss: 0.3923 - regression_loss: 0.3518 - classification_loss: 0.0406
  865/10000 [=>............................] - ETA: 2:15:09 - loss: 0.3930 - regression_loss: 0.3522 - classification_loss: 0.0408
  866/10000 [=>............................] - ETA: 2:15:08 - loss: 0.3927 - regression_loss: 0.3519 - classification_loss: 0.0408
  867/10000 [=>............................] - ETA: 2:15:07 - loss: 0.3924 - regression_loss: 0.3516 - classification_loss: 0.0407
  868/10000 [=>............................] - ETA: 2:15:06 - loss: 0.3929 - regression_loss: 0.3520 - classification_loss: 0.0409
  869/10000 [=>............................] - ETA: 2:15:05 - loss: 0.3930 - regression_loss: 0.3521 - classification_loss: 0.0408
  870/10000 [=>............................] - ETA: 2:15:05 - loss: 0.3929 - regression_loss: 0.3521 - classification_loss: 0.0408
  871/10000 [=>............................] - ETA: 2:15:04 - loss: 0.3930 - regression_loss: 0.3522 - classification_loss: 0.0407
  872/10000 [=>............................] - ETA: 2:15:03 - loss: 0.3933 - regression_loss: 0.3525 - classification_loss: 0.0408
  873/10000 [=>............................] - ETA: 2:15:02 - loss: 0.3931 - regression_loss: 0.3524 - classification_loss: 0.0408
  874/10000 [=>............................] - ETA: 2:15:01 - loss: 0.3929 - regression_loss: 0.3522 - classification_loss: 0.0407
  875/10000 [=>............................] - ETA: 2:15:00 - loss: 0.3930 - regression_loss: 0.3523 - classification_loss: 0.0407
  876/10000 [=>............................] - ETA: 2:14:59 - loss: 0.3929 - regression_loss: 0.3522 - classification_loss: 0.0407
  877/10000 [=>............................] - ETA: 2:14:58 - loss: 0.3927 - regression_loss: 0.3520 - classification_loss: 0.0408
  878/10000 [=>............................] - ETA: 2:14:57 - loss: 0.3925 - regression_loss: 0.3518 - classification_loss: 0.0407
  879/10000 [=>............................] - ETA: 2:14:56 - loss: 0.3925 - regression_loss: 0.3517 - classification_loss: 0.0407
  880/10000 [=>............................] - ETA: 2:14:56 - loss: 0.3922 - regression_loss: 0.3515 - classification_loss: 0.0407
  881/10000 [=>............................] - ETA: 2:14:55 - loss: 0.3921 - regression_loss: 0.3514 - classification_loss: 0.0407
  882/10000 [=>............................] - ETA: 2:14:54 - loss: 0.3928 - regression_loss: 0.3521 - classification_loss: 0.0407
  883/10000 [=>............................] - ETA: 2:14:53 - loss: 0.3927 - regression_loss: 0.3520 - classification_loss: 0.0407
  884/10000 [=>............................] - ETA: 2:14:52 - loss: 0.3929 - regression_loss: 0.3522 - classification_loss: 0.0407
  885/10000 [=>............................] - ETA: 2:14:51 - loss: 0.3926 - regression_loss: 0.3520 - classification_loss: 0.0406
  886/10000 [=>............................] - ETA: 2:14:50 - loss: 0.3926 - regression_loss: 0.3519 - classification_loss: 0.0406
  887/10000 [=>............................] - ETA: 2:14:49 - loss: 0.3926 - regression_loss: 0.3520 - classification_loss: 0.0406
  888/10000 [=>............................] - ETA: 2:14:49 - loss: 0.3923 - regression_loss: 0.3517 - classification_loss: 0.0406
  889/10000 [=>............................] - ETA: 2:14:48 - loss: 0.3924 - regression_loss: 0.3518 - classification_loss: 0.0406
  890/10000 [=>............................] - ETA: 2:14:47 - loss: 0.3926 - regression_loss: 0.3520 - classification_loss: 0.0406
  891/10000 [=>............................] - ETA: 2:14:46 - loss: 0.3925 - regression_loss: 0.3520 - classification_loss: 0.0405
  892/10000 [=>............................] - ETA: 2:14:45 - loss: 0.3928 - regression_loss: 0.3523 - classification_loss: 0.0405
  893/10000 [=>............................] - ETA: 2:14:44 - loss: 0.3928 - regression_loss: 0.3523 - classification_loss: 0.0405
  894/10000 [=>............................] - ETA: 2:14:43 - loss: 0.3928 - regression_loss: 0.3524 - classification_loss: 0.0405
  895/10000 [=>............................] - ETA: 2:14:42 - loss: 0.3936 - regression_loss: 0.3530 - classification_loss: 0.0406
  896/10000 [=>............................] - ETA: 2:14:41 - loss: 0.3935 - regression_loss: 0.3530 - classification_loss: 0.0406
  897/10000 [=>............................] - ETA: 2:14:41 - loss: 0.3934 - regression_loss: 0.3528 - classification_loss: 0.0406
  898/10000 [=>............................] - ETA: 2:14:40 - loss: 0.3936 - regression_loss: 0.3530 - classification_loss: 0.0405
  899/10000 [=>............................] - ETA: 2:14:39 - loss: 0.3936 - regression_loss: 0.3531 - classification_loss: 0.0405
  900/10000 [=>............................] - ETA: 2:14:38 - loss: 0.3935 - regression_loss: 0.3530 - classification_loss: 0.0405
  901/10000 [=>............................] - ETA: 2:14:37 - loss: 0.3933 - regression_loss: 0.3528 - classification_loss: 0.0405
  902/10000 [=>............................] - ETA: 2:14:36 - loss: 0.3933 - regression_loss: 0.3528 - classification_loss: 0.0405
  903/10000 [=>............................] - ETA: 2:14:35 - loss: 0.3932 - regression_loss: 0.3528 - classification_loss: 0.0405
  904/10000 [=>............................] - ETA: 2:14:34 - loss: 0.3930 - regression_loss: 0.3526 - classification_loss: 0.0404
  905/10000 [=>............................] - ETA: 2:14:33 - loss: 0.3927 - regression_loss: 0.3524 - classification_loss: 0.0404
  906/10000 [=>............................] - ETA: 2:14:33 - loss: 0.3928 - regression_loss: 0.3524 - classification_loss: 0.0404
  907/10000 [=>............................] - ETA: 2:14:32 - loss: 0.3930 - regression_loss: 0.3526 - classification_loss: 0.0404
  908/10000 [=>............................] - ETA: 2:14:31 - loss: 0.3931 - regression_loss: 0.3527 - classification_loss: 0.0404
  909/10000 [=>............................] - ETA: 2:14:30 - loss: 0.3931 - regression_loss: 0.3527 - classification_loss: 0.0404
  910/10000 [=>............................] - ETA: 2:14:29 - loss: 0.3933 - regression_loss: 0.3529 - classification_loss: 0.0404
  911/10000 [=>............................] - ETA: 2:14:28 - loss: 0.3934 - regression_loss: 0.3530 - classification_loss: 0.0404
  912/10000 [=>............................] - ETA: 2:14:27 - loss: 0.3931 - regression_loss: 0.3527 - classification_loss: 0.0404
  913/10000 [=>............................] - ETA: 2:14:26 - loss: 0.3928 - regression_loss: 0.3525 - classification_loss: 0.0403
  914/10000 [=>............................] - ETA: 2:14:25 - loss: 0.3927 - regression_loss: 0.3524 - classification_loss: 0.0403
  915/10000 [=>............................] - ETA: 2:14:25 - loss: 0.3924 - regression_loss: 0.3522 - classification_loss: 0.0403
  916/10000 [=>............................] - ETA: 2:14:24 - loss: 0.3922 - regression_loss: 0.3519 - classification_loss: 0.0402
  917/10000 [=>............................] - ETA: 2:14:23 - loss: 0.3921 - regression_loss: 0.3519 - classification_loss: 0.0402
  918/10000 [=>............................] - ETA: 2:14:22 - loss: 0.3920 - regression_loss: 0.3518 - classification_loss: 0.0402
  919/10000 [=>............................] - ETA: 2:14:21 - loss: 0.3919 - regression_loss: 0.3517 - classification_loss: 0.0402
  920/10000 [=>............................] - ETA: 2:14:20 - loss: 0.3924 - regression_loss: 0.3522 - classification_loss: 0.0401
  921/10000 [=>............................] - ETA: 2:14:19 - loss: 0.3920 - regression_loss: 0.3519 - classification_loss: 0.0401
  922/10000 [=>............................] - ETA: 2:14:18 - loss: 0.3917 - regression_loss: 0.3517 - classification_loss: 0.0401
  923/10000 [=>............................] - ETA: 2:14:17 - loss: 0.3921 - regression_loss: 0.3520 - classification_loss: 0.0401
  924/10000 [=>............................] - ETA: 2:14:17 - loss: 0.3919 - regression_loss: 0.3518 - classification_loss: 0.0401
  925/10000 [=>............................] - ETA: 2:14:16 - loss: 0.3915 - regression_loss: 0.3515 - classification_loss: 0.0400
  926/10000 [=>............................] - ETA: 2:14:15 - loss: 0.3916 - regression_loss: 0.3516 - classification_loss: 0.0400
  927/10000 [=>............................] - ETA: 2:14:14 - loss: 0.3915 - regression_loss: 0.3515 - classification_loss: 0.0400
  928/10000 [=>............................] - ETA: 2:14:13 - loss: 0.3915 - regression_loss: 0.3515 - classification_loss: 0.0400
  929/10000 [=>............................] - ETA: 2:14:12 - loss: 0.3924 - regression_loss: 0.3523 - classification_loss: 0.0401
  930/10000 [=>............................] - ETA: 2:14:11 - loss: 0.3920 - regression_loss: 0.3520 - classification_loss: 0.0400
  931/10000 [=>............................] - ETA: 2:14:10 - loss: 0.3920 - regression_loss: 0.3520 - classification_loss: 0.0400
  932/10000 [=>............................] - ETA: 2:14:09 - loss: 0.3919 - regression_loss: 0.3519 - classification_loss: 0.0400
  933/10000 [=>............................] - ETA: 2:14:09 - loss: 0.3920 - regression_loss: 0.3520 - classification_loss: 0.0400
  934/10000 [=>............................] - ETA: 2:14:08 - loss: 0.3915 - regression_loss: 0.3516 - classification_loss: 0.0399
  935/10000 [=>............................] - ETA: 2:14:07 - loss: 0.3922 - regression_loss: 0.3513 - classification_loss: 0.0409
  936/10000 [=>............................] - ETA: 2:14:06 - loss: 0.3925 - regression_loss: 0.3513 - classification_loss: 0.0412
  937/10000 [=>............................] - ETA: 2:14:05 - loss: 0.3923 - regression_loss: 0.3511 - classification_loss: 0.0412
  938/10000 [=>............................] - ETA: 2:14:04 - loss: 0.3921 - regression_loss: 0.3510 - classification_loss: 0.0412
  939/10000 [=>............................] - ETA: 2:14:03 - loss: 0.3917 - regression_loss: 0.3506 - classification_loss: 0.0411
  940/10000 [=>............................] - ETA: 2:14:02 - loss: 0.3917 - regression_loss: 0.3505 - classification_loss: 0.0412
  941/10000 [=>............................] - ETA: 2:14:02 - loss: 0.3915 - regression_loss: 0.3503 - classification_loss: 0.0412
  942/10000 [=>............................] - ETA: 2:14:01 - loss: 0.3914 - regression_loss: 0.3503 - classification_loss: 0.0411
  943/10000 [=>............................] - ETA: 2:14:00 - loss: 0.3912 - regression_loss: 0.3501 - classification_loss: 0.0411
  944/10000 [=>............................] - ETA: 2:13:59 - loss: 0.3910 - regression_loss: 0.3500 - classification_loss: 0.0411
  945/10000 [=>............................] - ETA: 2:13:58 - loss: 0.3914 - regression_loss: 0.3503 - classification_loss: 0.0411
  946/10000 [=>............................] - ETA: 2:13:57 - loss: 0.3913 - regression_loss: 0.3502 - classification_loss: 0.0411
  947/10000 [=>............................] - ETA: 2:13:56 - loss: 0.3914 - regression_loss: 0.3503 - classification_loss: 0.0411
  948/10000 [=>............................] - ETA: 2:13:55 - loss: 0.3913 - regression_loss: 0.3503 - classification_loss: 0.0411
  949/10000 [=>............................] - ETA: 2:13:54 - loss: 0.3912 - regression_loss: 0.3502 - classification_loss: 0.0410
  950/10000 [=>............................] - ETA: 2:13:54 - loss: 0.3909 - regression_loss: 0.3499 - classification_loss: 0.0410
  951/10000 [=>............................] - ETA: 2:13:53 - loss: 0.3908 - regression_loss: 0.3497 - classification_loss: 0.0411
  952/10000 [=>............................] - ETA: 2:13:52 - loss: 0.3907 - regression_loss: 0.3497 - classification_loss: 0.0410
  953/10000 [=>............................] - ETA: 2:13:51 - loss: 0.3905 - regression_loss: 0.3495 - classification_loss: 0.0410
  954/10000 [=>............................] - ETA: 2:13:50 - loss: 0.3901 - regression_loss: 0.3491 - classification_loss: 0.0409
  955/10000 [=>............................] - ETA: 2:13:49 - loss: 0.3898 - regression_loss: 0.3489 - classification_loss: 0.0409
  956/10000 [=>............................] - ETA: 2:13:48 - loss: 0.3897 - regression_loss: 0.3487 - classification_loss: 0.0410
  957/10000 [=>............................] - ETA: 2:13:47 - loss: 0.3895 - regression_loss: 0.3486 - classification_loss: 0.0409
  958/10000 [=>............................] - ETA: 2:13:47 - loss: 0.3895 - regression_loss: 0.3486 - classification_loss: 0.0409
  959/10000 [=>............................] - ETA: 2:13:46 - loss: 0.3894 - regression_loss: 0.3485 - classification_loss: 0.0409
  960/10000 [=>............................] - ETA: 2:13:45 - loss: 0.3892 - regression_loss: 0.3483 - classification_loss: 0.0409
  961/10000 [=>............................] - ETA: 2:13:44 - loss: 0.3894 - regression_loss: 0.3484 - classification_loss: 0.0409
  962/10000 [=>............................] - ETA: 2:13:43 - loss: 0.3892 - regression_loss: 0.3483 - classification_loss: 0.0409
  963/10000 [=>............................] - ETA: 2:13:42 - loss: 0.3889 - regression_loss: 0.3480 - classification_loss: 0.0409
  964/10000 [=>............................] - ETA: 2:13:41 - loss: 0.3886 - regression_loss: 0.3477 - classification_loss: 0.0409
  965/10000 [=>............................] - ETA: 2:13:40 - loss: 0.3885 - regression_loss: 0.3476 - classification_loss: 0.0409
  966/10000 [=>............................] - ETA: 2:13:39 - loss: 0.3882 - regression_loss: 0.3474 - classification_loss: 0.0408
  967/10000 [=>............................] - ETA: 2:13:38 - loss: 0.3884 - regression_loss: 0.3476 - classification_loss: 0.0409
  968/10000 [=>............................] - ETA: 2:13:38 - loss: 0.3888 - regression_loss: 0.3478 - classification_loss: 0.0409
  969/10000 [=>............................] - ETA: 2:13:37 - loss: 0.3888 - regression_loss: 0.3479 - classification_loss: 0.0409
  970/10000 [=>............................] - ETA: 2:13:36 - loss: 0.3893 - regression_loss: 0.3483 - classification_loss: 0.0410
  971/10000 [=>............................] - ETA: 2:13:35 - loss: 0.3890 - regression_loss: 0.3480 - classification_loss: 0.0410
  972/10000 [=>............................] - ETA: 2:13:34 - loss: 0.3891 - regression_loss: 0.3481 - classification_loss: 0.0410
  973/10000 [=>............................] - ETA: 2:13:33 - loss: 0.3892 - regression_loss: 0.3482 - classification_loss: 0.0410
  974/10000 [=>............................] - ETA: 2:13:32 - loss: 0.3892 - regression_loss: 0.3483 - classification_loss: 0.0409
  975/10000 [=>............................] - ETA: 2:13:31 - loss: 0.3896 - regression_loss: 0.3486 - classification_loss: 0.0409
  976/10000 [=>............................] - ETA: 2:13:30 - loss: 0.3895 - regression_loss: 0.3486 - classification_loss: 0.0409
  977/10000 [=>............................] - ETA: 2:13:30 - loss: 0.3900 - regression_loss: 0.3491 - classification_loss: 0.0409
  978/10000 [=>............................] - ETA: 2:13:29 - loss: 0.3901 - regression_loss: 0.3491 - classification_loss: 0.0409
  979/10000 [=>............................] - ETA: 2:13:28 - loss: 0.3899 - regression_loss: 0.3490 - classification_loss: 0.0409
  980/10000 [=>............................] - ETA: 2:13:27 - loss: 0.3899 - regression_loss: 0.3490 - classification_loss: 0.0409
  981/10000 [=>............................] - ETA: 2:13:26 - loss: 0.3898 - regression_loss: 0.3489 - classification_loss: 0.0409
  982/10000 [=>............................] - ETA: 2:13:25 - loss: 0.3900 - regression_loss: 0.3491 - classification_loss: 0.0409
  983/10000 [=>............................] - ETA: 2:13:24 - loss: 0.3902 - regression_loss: 0.3493 - classification_loss: 0.0409
  984/10000 [=>............................] - ETA: 2:13:23 - loss: 0.3902 - regression_loss: 0.3493 - classification_loss: 0.0409
  985/10000 [=>............................] - ETA: 2:13:22 - loss: 0.3902 - regression_loss: 0.3493 - classification_loss: 0.0409
  986/10000 [=>............................] - ETA: 2:13:21 - loss: 0.3901 - regression_loss: 0.3492 - classification_loss: 0.0408
  987/10000 [=>............................] - ETA: 2:13:21 - loss: 0.3897 - regression_loss: 0.3489 - classification_loss: 0.0408
  988/10000 [=>............................] - ETA: 2:13:20 - loss: 0.3895 - regression_loss: 0.3488 - classification_loss: 0.0408
  989/10000 [=>............................] - ETA: 2:13:19 - loss: 0.3895 - regression_loss: 0.3488 - classification_loss: 0.0408
  990/10000 [=>............................] - ETA: 2:13:18 - loss: 0.3894 - regression_loss: 0.3486 - classification_loss: 0.0407
  991/10000 [=>............................] - ETA: 2:13:17 - loss: 0.3891 - regression_loss: 0.3483 - classification_loss: 0.0407
  992/10000 [=>............................] - ETA: 2:13:16 - loss: 0.3890 - regression_loss: 0.3483 - classification_loss: 0.0407
  993/10000 [=>............................] - ETA: 2:13:15 - loss: 0.3889 - regression_loss: 0.3482 - classification_loss: 0.0407
  994/10000 [=>............................] - ETA: 2:13:14 - loss: 0.3887 - regression_loss: 0.3481 - classification_loss: 0.0406
  995/10000 [=>............................] - ETA: 2:13:13 - loss: 0.3890 - regression_loss: 0.3483 - classification_loss: 0.0407
  996/10000 [=>............................] - ETA: 2:13:13 - loss: 0.3889 - regression_loss: 0.3482 - classification_loss: 0.0407
  997/10000 [=>............................] - ETA: 2:13:12 - loss: 0.3888 - regression_loss: 0.3481 - classification_loss: 0.0407
  998/10000 [=>............................] - ETA: 2:13:11 - loss: 0.3888 - regression_loss: 0.3481 - classification_loss: 0.0407
  999/10000 [=>............................] - ETA: 2:13:10 - loss: 0.3891 - regression_loss: 0.3484 - classification_loss: 0.0407
 1000/10000 [==>...........................] - ETA: 2:13:09 - loss: 0.3889 - regression_loss: 0.3482 - classification_loss: 0.0407
 1001/10000 [==>...........................] - ETA: 2:13:08 - loss: 0.3889 - regression_loss: 0.3483 - classification_loss: 0.0407
 1002/10000 [==>...........................] - ETA: 2:13:07 - loss: 0.3892 - regression_loss: 0.3485 - classification_loss: 0.0407
 1003/10000 [==>...........................] - ETA: 2:13:06 - loss: 0.3891 - regression_loss: 0.3484 - classification_loss: 0.0407
 1004/10000 [==>...........................] - ETA: 2:13:05 - loss: 0.3895 - regression_loss: 0.3488 - classification_loss: 0.0407
 1005/10000 [==>...........................] - ETA: 2:13:04 - loss: 0.3893 - regression_loss: 0.3486 - classification_loss: 0.0407
 1006/10000 [==>...........................] - ETA: 2:13:04 - loss: 0.3895 - regression_loss: 0.3488 - classification_loss: 0.0407
 1007/10000 [==>...........................] - ETA: 2:13:03 - loss: 0.3896 - regression_loss: 0.3489 - classification_loss: 0.0407
 1008/10000 [==>...........................] - ETA: 2:13:02 - loss: 0.3900 - regression_loss: 0.3490 - classification_loss: 0.0410
 1009/10000 [==>...........................] - ETA: 2:13:01 - loss: 0.3901 - regression_loss: 0.3491 - classification_loss: 0.0410
 1010/10000 [==>...........................] - ETA: 2:13:00 - loss: 0.3902 - regression_loss: 0.3491 - classification_loss: 0.0410
 1011/10000 [==>...........................] - ETA: 2:12:59 - loss: 0.3902 - regression_loss: 0.3492 - classification_loss: 0.0411
 1012/10000 [==>...........................] - ETA: 2:12:58 - loss: 0.3902 - regression_loss: 0.3492 - classification_loss: 0.0411
 1013/10000 [==>...........................] - ETA: 2:12:57 - loss: 0.3901 - regression_loss: 0.3491 - classification_loss: 0.0410
 1014/10000 [==>...........................] - ETA: 2:12:56 - loss: 0.3901 - regression_loss: 0.3490 - classification_loss: 0.0410
 1015/10000 [==>...........................] - ETA: 2:12:56 - loss: 0.3898 - regression_loss: 0.3488 - classification_loss: 0.0410
 1016/10000 [==>...........................] - ETA: 2:12:55 - loss: 0.3898 - regression_loss: 0.3488 - classification_loss: 0.0410
 1017/10000 [==>...........................] - ETA: 2:12:54 - loss: 0.3897 - regression_loss: 0.3488 - classification_loss: 0.0410
 1018/10000 [==>...........................] - ETA: 2:12:53 - loss: 0.3900 - regression_loss: 0.3490 - classification_loss: 0.0410
 1019/10000 [==>...........................] - ETA: 2:12:52 - loss: 0.3898 - regression_loss: 0.3488 - classification_loss: 0.0410
 1020/10000 [==>...........................] - ETA: 2:12:51 - loss: 0.3897 - regression_loss: 0.3487 - classification_loss: 0.0410
 1021/10000 [==>...........................] - ETA: 2:12:50 - loss: 0.3895 - regression_loss: 0.3485 - classification_loss: 0.0410
 1022/10000 [==>...........................] - ETA: 2:12:49 - loss: 0.3892 - regression_loss: 0.3482 - classification_loss: 0.0409
 1023/10000 [==>...........................] - ETA: 2:12:48 - loss: 0.3891 - regression_loss: 0.3481 - classification_loss: 0.0409
 1024/10000 [==>...........................] - ETA: 2:12:48 - loss: 0.3890 - regression_loss: 0.3481 - classification_loss: 0.0409
 1025/10000 [==>...........................] - ETA: 2:12:47 - loss: 0.3888 - regression_loss: 0.3479 - classification_loss: 0.0409
 1026/10000 [==>...........................] - ETA: 2:12:46 - loss: 0.3892 - regression_loss: 0.3483 - classification_loss: 0.0408
 1027/10000 [==>...........................] - ETA: 2:12:45 - loss: 0.3891 - regression_loss: 0.3483 - classification_loss: 0.0408
 1028/10000 [==>...........................] - ETA: 2:12:44 - loss: 0.3890 - regression_loss: 0.3482 - classification_loss: 0.0408
 1029/10000 [==>...........................] - ETA: 2:12:43 - loss: 0.3892 - regression_loss: 0.3481 - classification_loss: 0.0410
 1030/10000 [==>...........................] - ETA: 2:12:42 - loss: 0.3888 - regression_loss: 0.3478 - classification_loss: 0.0410
 1031/10000 [==>...........................] - ETA: 2:12:41 - loss: 0.3890 - regression_loss: 0.3480 - classification_loss: 0.0410
 1032/10000 [==>...........................] - ETA: 2:12:40 - loss: 0.3886 - regression_loss: 0.3477 - classification_loss: 0.0409
 1033/10000 [==>...........................] - ETA: 2:12:40 - loss: 0.3898 - regression_loss: 0.3488 - classification_loss: 0.0410
 1034/10000 [==>...........................] - ETA: 2:12:39 - loss: 0.3898 - regression_loss: 0.3489 - classification_loss: 0.0409
 1035/10000 [==>...........................] - ETA: 2:12:38 - loss: 0.3897 - regression_loss: 0.3488 - classification_loss: 0.0409
 1036/10000 [==>...........................] - ETA: 2:12:37 - loss: 0.3896 - regression_loss: 0.3487 - classification_loss: 0.0409
 1037/10000 [==>...........................] - ETA: 2:12:36 - loss: 0.3895 - regression_loss: 0.3486 - classification_loss: 0.0409
 1038/10000 [==>...........................] - ETA: 2:12:35 - loss: 0.3895 - regression_loss: 0.3486 - classification_loss: 0.0409
 1039/10000 [==>...........................] - ETA: 2:12:34 - loss: 0.3895 - regression_loss: 0.3487 - classification_loss: 0.0409
 1040/10000 [==>...........................] - ETA: 2:12:33 - loss: 0.3895 - regression_loss: 0.3487 - classification_loss: 0.0408
 1041/10000 [==>...........................] - ETA: 2:12:32 - loss: 0.3893 - regression_loss: 0.3485 - classification_loss: 0.0408
 1042/10000 [==>...........................] - ETA: 2:12:32 - loss: 0.3889 - regression_loss: 0.3482 - classification_loss: 0.0408
 1043/10000 [==>...........................] - ETA: 2:12:31 - loss: 0.3891 - regression_loss: 0.3483 - classification_loss: 0.0408
 1044/10000 [==>...........................] - ETA: 2:12:30 - loss: 0.3889 - regression_loss: 0.3482 - classification_loss: 0.0408
 1045/10000 [==>...........................] - ETA: 2:12:29 - loss: 0.3889 - regression_loss: 0.3481 - classification_loss: 0.0407
 1046/10000 [==>...........................] - ETA: 2:12:28 - loss: 0.3887 - regression_loss: 0.3480 - classification_loss: 0.0407
 1047/10000 [==>...........................] - ETA: 2:12:27 - loss: 0.3885 - regression_loss: 0.3479 - classification_loss: 0.0407
 1048/10000 [==>...........................] - ETA: 2:12:26 - loss: 0.3885 - regression_loss: 0.3479 - classification_loss: 0.0406
 1049/10000 [==>...........................] - ETA: 2:12:25 - loss: 0.3881 - regression_loss: 0.3475 - classification_loss: 0.0406
 1050/10000 [==>...........................] - ETA: 2:12:24 - loss: 0.3884 - regression_loss: 0.3477 - classification_loss: 0.0407
 1051/10000 [==>...........................] - ETA: 2:12:24 - loss: 0.3882 - regression_loss: 0.3475 - classification_loss: 0.0407
 1052/10000 [==>...........................] - ETA: 2:12:23 - loss: 0.3883 - regression_loss: 0.3476 - classification_loss: 0.0407
 1053/10000 [==>...........................] - ETA: 2:12:22 - loss: 0.3884 - regression_loss: 0.3476 - classification_loss: 0.0407
 1054/10000 [==>...........................] - ETA: 2:12:21 - loss: 0.3882 - regression_loss: 0.3475 - classification_loss: 0.0407
 1055/10000 [==>...........................] - ETA: 2:12:20 - loss: 0.3884 - regression_loss: 0.3476 - classification_loss: 0.0407
 1056/10000 [==>...........................] - ETA: 2:12:19 - loss: 0.3884 - regression_loss: 0.3477 - classification_loss: 0.0407
 1057/10000 [==>...........................] - ETA: 2:12:18 - loss: 0.3882 - regression_loss: 0.3475 - classification_loss: 0.0407
 1058/10000 [==>...........................] - ETA: 2:12:17 - loss: 0.3880 - regression_loss: 0.3474 - classification_loss: 0.0407
 1059/10000 [==>...........................] - ETA: 2:12:16 - loss: 0.3880 - regression_loss: 0.3473 - classification_loss: 0.0407
 1060/10000 [==>...........................] - ETA: 2:12:15 - loss: 0.3878 - regression_loss: 0.3471 - classification_loss: 0.0406
 1061/10000 [==>...........................] - ETA: 2:12:15 - loss: 0.3877 - regression_loss: 0.3471 - classification_loss: 0.0406
 1062/10000 [==>...........................] - ETA: 2:12:14 - loss: 0.3876 - regression_loss: 0.3470 - classification_loss: 0.0406
 1063/10000 [==>...........................] - ETA: 2:12:13 - loss: 0.3876 - regression_loss: 0.3470 - classification_loss: 0.0406
 1064/10000 [==>...........................] - ETA: 2:12:12 - loss: 0.3877 - regression_loss: 0.3471 - classification_loss: 0.0405
 1065/10000 [==>...........................] - ETA: 2:12:11 - loss: 0.3879 - regression_loss: 0.3473 - classification_loss: 0.0406
 1066/10000 [==>...........................] - ETA: 2:12:10 - loss: 0.3877 - regression_loss: 0.3471 - classification_loss: 0.0406
 1067/10000 [==>...........................] - ETA: 2:12:09 - loss: 0.3876 - regression_loss: 0.3470 - classification_loss: 0.0405
 1068/10000 [==>...........................] - ETA: 2:12:08 - loss: 0.3874 - regression_loss: 0.3468 - classification_loss: 0.0405
 1069/10000 [==>...........................] - ETA: 2:12:07 - loss: 0.3872 - regression_loss: 0.3467 - classification_loss: 0.0405
 1070/10000 [==>...........................] - ETA: 2:12:07 - loss: 0.3870 - regression_loss: 0.3465 - classification_loss: 0.0405
 1071/10000 [==>...........................] - ETA: 2:12:06 - loss: 0.3870 - regression_loss: 0.3465 - classification_loss: 0.0405
 1072/10000 [==>...........................] - ETA: 2:12:05 - loss: 0.3868 - regression_loss: 0.3464 - classification_loss: 0.0405
 1073/10000 [==>...........................] - ETA: 2:12:04 - loss: 0.3867 - regression_loss: 0.3463 - classification_loss: 0.0405
 1074/10000 [==>...........................] - ETA: 2:12:03 - loss: 0.3875 - regression_loss: 0.3469 - classification_loss: 0.0406
 1075/10000 [==>...........................] - ETA: 2:12:02 - loss: 0.3875 - regression_loss: 0.3470 - classification_loss: 0.0406
 1076/10000 [==>...........................] - ETA: 2:12:01 - loss: 0.3875 - regression_loss: 0.3469 - classification_loss: 0.0405
 1077/10000 [==>...........................] - ETA: 2:12:00 - loss: 0.3874 - regression_loss: 0.3469 - classification_loss: 0.0406
 1078/10000 [==>...........................] - ETA: 2:11:59 - loss: 0.3876 - regression_loss: 0.3470 - classification_loss: 0.0406
 1079/10000 [==>...........................] - ETA: 2:11:59 - loss: 0.3874 - regression_loss: 0.3469 - classification_loss: 0.0405
 1080/10000 [==>...........................] - ETA: 2:11:58 - loss: 0.3871 - regression_loss: 0.3466 - classification_loss: 0.0405
 1081/10000 [==>...........................] - ETA: 2:11:57 - loss: 0.3871 - regression_loss: 0.3466 - classification_loss: 0.0405
 1082/10000 [==>...........................] - ETA: 2:11:56 - loss: 0.3871 - regression_loss: 0.3466 - classification_loss: 0.0405
 1083/10000 [==>...........................] - ETA: 2:11:55 - loss: 0.3867 - regression_loss: 0.3463 - classification_loss: 0.0405
 1084/10000 [==>...........................] - ETA: 2:11:54 - loss: 0.3865 - regression_loss: 0.3461 - classification_loss: 0.0404
 1085/10000 [==>...........................] - ETA: 2:11:53 - loss: 0.3863 - regression_loss: 0.3459 - classification_loss: 0.0404
 1086/10000 [==>...........................] - ETA: 2:11:52 - loss: 0.3863 - regression_loss: 0.3459 - classification_loss: 0.0404
 1087/10000 [==>...........................] - ETA: 2:11:51 - loss: 0.3865 - regression_loss: 0.3460 - classification_loss: 0.0404
 1088/10000 [==>...........................] - ETA: 2:11:51 - loss: 0.3861 - regression_loss: 0.3457 - classification_loss: 0.0404
 1089/10000 [==>...........................] - ETA: 2:11:50 - loss: 0.3861 - regression_loss: 0.3457 - classification_loss: 0.0404
 1090/10000 [==>...........................] - ETA: 2:11:49 - loss: 0.3861 - regression_loss: 0.3457 - classification_loss: 0.0404
 1091/10000 [==>...........................] - ETA: 2:11:48 - loss: 0.3860 - regression_loss: 0.3456 - classification_loss: 0.0404
 1092/10000 [==>...........................] - ETA: 2:11:47 - loss: 0.3864 - regression_loss: 0.3461 - classification_loss: 0.0404
 1093/10000 [==>...........................] - ETA: 2:11:46 - loss: 0.3862 - regression_loss: 0.3459 - classification_loss: 0.0403
 1094/10000 [==>...........................] - ETA: 2:11:45 - loss: 0.3865 - regression_loss: 0.3461 - classification_loss: 0.0404
 1095/10000 [==>...........................] - ETA: 2:11:44 - loss: 0.3865 - regression_loss: 0.3461 - classification_loss: 0.0403
 1096/10000 [==>...........................] - ETA: 2:11:44 - loss: 0.3865 - regression_loss: 0.3461 - classification_loss: 0.0403
 1097/10000 [==>...........................] - ETA: 2:11:43 - loss: 0.3863 - regression_loss: 0.3460 - classification_loss: 0.0403
 1098/10000 [==>...........................] - ETA: 2:11:42 - loss: 0.3868 - regression_loss: 0.3462 - classification_loss: 0.0405
 1099/10000 [==>...........................] - ETA: 2:11:41 - loss: 0.3866 - regression_loss: 0.3461 - classification_loss: 0.0405
 1100/10000 [==>...........................] - ETA: 2:11:40 - loss: 0.3865 - regression_loss: 0.3460 - classification_loss: 0.0405
 1101/10000 [==>...........................] - ETA: 2:11:39 - loss: 0.3865 - regression_loss: 0.3461 - classification_loss: 0.0405
 1102/10000 [==>...........................] - ETA: 2:11:38 - loss: 0.3867 - regression_loss: 0.3462 - classification_loss: 0.0405
 1103/10000 [==>...........................] - ETA: 2:11:37 - loss: 0.3867 - regression_loss: 0.3462 - classification_loss: 0.0405
 1104/10000 [==>...........................] - ETA: 2:11:36 - loss: 0.3867 - regression_loss: 0.3462 - classification_loss: 0.0405
 1105/10000 [==>...........................] - ETA: 2:11:35 - loss: 0.3867 - regression_loss: 0.3462 - classification_loss: 0.0405
 1106/10000 [==>...........................] - ETA: 2:11:35 - loss: 0.3867 - regression_loss: 0.3462 - classification_loss: 0.0404
 1107/10000 [==>...........................] - ETA: 2:11:34 - loss: 0.3863 - regression_loss: 0.3459 - classification_loss: 0.0404
 1108/10000 [==>...........................] - ETA: 2:11:33 - loss: 0.3862 - regression_loss: 0.3458 - classification_loss: 0.0404
 1109/10000 [==>...........................] - ETA: 2:11:32 - loss: 0.3858 - regression_loss: 0.3455 - classification_loss: 0.0404
 1110/10000 [==>...........................] - ETA: 2:11:31 - loss: 0.3856 - regression_loss: 0.3452 - classification_loss: 0.0403
 1111/10000 [==>...........................] - ETA: 2:11:30 - loss: 0.3852 - regression_loss: 0.3449 - classification_loss: 0.0403
 1112/10000 [==>...........................] - ETA: 2:11:29 - loss: 0.3852 - regression_loss: 0.3449 - classification_loss: 0.0403
 1113/10000 [==>...........................] - ETA: 2:11:28 - loss: 0.3849 - regression_loss: 0.3447 - classification_loss: 0.0402
 1114/10000 [==>...........................] - ETA: 2:11:27 - loss: 0.3847 - regression_loss: 0.3445 - classification_loss: 0.0402
 1115/10000 [==>...........................] - ETA: 2:11:27 - loss: 0.3846 - regression_loss: 0.3444 - classification_loss: 0.0402
 1116/10000 [==>...........................] - ETA: 2:11:26 - loss: 0.3843 - regression_loss: 0.3442 - classification_loss: 0.0402
 1117/10000 [==>...........................] - ETA: 2:11:25 - loss: 0.3844 - regression_loss: 0.3443 - classification_loss: 0.0402
 1118/10000 [==>...........................] - ETA: 2:11:24 - loss: 0.3844 - regression_loss: 0.3442 - classification_loss: 0.0402
 1119/10000 [==>...........................] - ETA: 2:11:23 - loss: 0.3841 - regression_loss: 0.3440 - classification_loss: 0.0402
 1120/10000 [==>...........................] - ETA: 2:11:22 - loss: 0.3841 - regression_loss: 0.3439 - classification_loss: 0.0402
 1121/10000 [==>...........................] - ETA: 2:11:21 - loss: 0.3842 - regression_loss: 0.3440 - classification_loss: 0.0402
 1122/10000 [==>...........................] - ETA: 2:11:20 - loss: 0.3841 - regression_loss: 0.3440 - classification_loss: 0.0401
 1123/10000 [==>...........................] - ETA: 2:11:19 - loss: 0.3839 - regression_loss: 0.3438 - classification_loss: 0.0401
 1124/10000 [==>...........................] - ETA: 2:11:19 - loss: 0.3844 - regression_loss: 0.3443 - classification_loss: 0.0401
 1125/10000 [==>...........................] - ETA: 2:11:18 - loss: 0.3845 - regression_loss: 0.3444 - classification_loss: 0.0401
 1126/10000 [==>...........................] - ETA: 2:11:17 - loss: 0.3844 - regression_loss: 0.3443 - classification_loss: 0.0401
 1127/10000 [==>...........................] - ETA: 2:11:16 - loss: 0.3852 - regression_loss: 0.3446 - classification_loss: 0.0406
 1128/10000 [==>...........................] - ETA: 2:11:15 - loss: 0.3851 - regression_loss: 0.3445 - classification_loss: 0.0406
 1129/10000 [==>...........................] - ETA: 2:11:14 - loss: 0.3850 - regression_loss: 0.3445 - classification_loss: 0.0406
 1130/10000 [==>...........................] - ETA: 2:11:13 - loss: 0.3849 - regression_loss: 0.3444 - classification_loss: 0.0405
 1131/10000 [==>...........................] - ETA: 2:11:12 - loss: 0.3848 - regression_loss: 0.3443 - classification_loss: 0.0405
 1132/10000 [==>...........................] - ETA: 2:11:12 - loss: 0.3852 - regression_loss: 0.3447 - classification_loss: 0.0405
 1133/10000 [==>...........................] - ETA: 2:11:11 - loss: 0.3853 - regression_loss: 0.3448 - classification_loss: 0.0405
 1134/10000 [==>...........................] - ETA: 2:11:10 - loss: 0.3853 - regression_loss: 0.3447 - classification_loss: 0.0406
 1135/10000 [==>...........................] - ETA: 2:11:09 - loss: 0.3851 - regression_loss: 0.3445 - classification_loss: 0.0405
 1136/10000 [==>...........................] - ETA: 2:11:08 - loss: 0.3856 - regression_loss: 0.3451 - classification_loss: 0.0405
 1137/10000 [==>...........................] - ETA: 2:11:07 - loss: 0.3858 - regression_loss: 0.3453 - classification_loss: 0.0405
 1138/10000 [==>...........................] - ETA: 2:11:06 - loss: 0.3856 - regression_loss: 0.3451 - classification_loss: 0.0405
 1139/10000 [==>...........................] - ETA: 2:11:05 - loss: 0.3855 - regression_loss: 0.3450 - classification_loss: 0.0405
 1140/10000 [==>...........................] - ETA: 2:11:05 - loss: 0.3852 - regression_loss: 0.3447 - classification_loss: 0.0404
 1141/10000 [==>...........................] - ETA: 2:11:04 - loss: 0.3853 - regression_loss: 0.3448 - classification_loss: 0.0404
 1142/10000 [==>...........................] - ETA: 2:11:03 - loss: 0.3851 - regression_loss: 0.3447 - classification_loss: 0.0404
 1143/10000 [==>...........................] - ETA: 2:11:02 - loss: 0.3849 - regression_loss: 0.3445 - classification_loss: 0.0404
 1144/10000 [==>...........................] - ETA: 2:11:01 - loss: 0.3856 - regression_loss: 0.3451 - classification_loss: 0.0404
 1145/10000 [==>...........................] - ETA: 2:11:00 - loss: 0.3856 - regression_loss: 0.3452 - classification_loss: 0.0404
 1146/10000 [==>...........................] - ETA: 2:10:59 - loss: 0.3857 - regression_loss: 0.3453 - classification_loss: 0.0404
 1147/10000 [==>...........................] - ETA: 2:10:58 - loss: 0.3855 - regression_loss: 0.3451 - classification_loss: 0.0404
 1148/10000 [==>...........................] - ETA: 2:10:57 - loss: 0.3854 - regression_loss: 0.3450 - classification_loss: 0.0404
 1149/10000 [==>...........................] - ETA: 2:10:57 - loss: 0.3853 - regression_loss: 0.3450 - classification_loss: 0.0404
 1150/10000 [==>...........................] - ETA: 2:10:56 - loss: 0.3853 - regression_loss: 0.3449 - classification_loss: 0.0403
 1151/10000 [==>...........................] - ETA: 2:10:55 - loss: 0.3851 - regression_loss: 0.3448 - classification_loss: 0.0403
 1152/10000 [==>...........................] - ETA: 2:10:54 - loss: 0.3849 - regression_loss: 0.3446 - classification_loss: 0.0403
 1153/10000 [==>...........................] - ETA: 2:10:53 - loss: 0.3845 - regression_loss: 0.3443 - classification_loss: 0.0403
 1154/10000 [==>...........................] - ETA: 2:10:52 - loss: 0.3845 - regression_loss: 0.3442 - classification_loss: 0.0403
 1155/10000 [==>...........................] - ETA: 2:10:51 - loss: 0.3844 - regression_loss: 0.3442 - classification_loss: 0.0402
 1156/10000 [==>...........................] - ETA: 2:10:50 - loss: 0.3849 - regression_loss: 0.3446 - classification_loss: 0.0403
 1157/10000 [==>...........................] - ETA: 2:10:49 - loss: 0.3852 - regression_loss: 0.3446 - classification_loss: 0.0406
 1158/10000 [==>...........................] - ETA: 2:10:49 - loss: 0.3852 - regression_loss: 0.3446 - classification_loss: 0.0406
 1159/10000 [==>...........................] - ETA: 2:10:48 - loss: 0.3851 - regression_loss: 0.3445 - classification_loss: 0.0406
 1160/10000 [==>...........................] - ETA: 2:10:47 - loss: 0.3850 - regression_loss: 0.3444 - classification_loss: 0.0406
 1161/10000 [==>...........................] - ETA: 2:10:46 - loss: 0.3849 - regression_loss: 0.3443 - classification_loss: 0.0406
 1162/10000 [==>...........................] - ETA: 2:10:45 - loss: 0.3849 - regression_loss: 0.3443 - classification_loss: 0.0406
 1163/10000 [==>...........................] - ETA: 2:10:44 - loss: 0.3848 - regression_loss: 0.3443 - classification_loss: 0.0406
 1164/10000 [==>...........................] - ETA: 2:10:43 - loss: 0.3848 - regression_loss: 0.3442 - classification_loss: 0.0406
 1165/10000 [==>...........................] - ETA: 2:10:42 - loss: 0.3847 - regression_loss: 0.3441 - classification_loss: 0.0406
 1166/10000 [==>...........................] - ETA: 2:10:41 - loss: 0.3844 - regression_loss: 0.3438 - classification_loss: 0.0405
 1167/10000 [==>...........................] - ETA: 2:10:41 - loss: 0.3840 - regression_loss: 0.3436 - classification_loss: 0.0405
 1168/10000 [==>...........................] - ETA: 2:10:40 - loss: 0.3840 - regression_loss: 0.3435 - classification_loss: 0.0405
 1169/10000 [==>...........................] - ETA: 2:10:39 - loss: 0.3840 - regression_loss: 0.3435 - classification_loss: 0.0405
 1170/10000 [==>...........................] - ETA: 2:10:38 - loss: 0.3836 - regression_loss: 0.3432 - classification_loss: 0.0405
 1171/10000 [==>...........................] - ETA: 2:10:37 - loss: 0.3836 - regression_loss: 0.3431 - classification_loss: 0.0405
 1172/10000 [==>...........................] - ETA: 2:10:36 - loss: 0.3835 - regression_loss: 0.3430 - classification_loss: 0.0405
 1173/10000 [==>...........................] - ETA: 2:10:35 - loss: 0.3835 - regression_loss: 0.3430 - classification_loss: 0.0405
 1174/10000 [==>...........................] - ETA: 2:10:34 - loss: 0.3836 - regression_loss: 0.3431 - classification_loss: 0.0405
 1175/10000 [==>...........................] - ETA: 2:10:34 - loss: 0.3836 - regression_loss: 0.3431 - classification_loss: 0.0405
 1176/10000 [==>...........................] - ETA: 2:10:33 - loss: 0.3841 - regression_loss: 0.3436 - classification_loss: 0.0405
 1177/10000 [==>...........................] - ETA: 2:10:32 - loss: 0.3839 - regression_loss: 0.3434 - classification_loss: 0.0405
 1178/10000 [==>...........................] - ETA: 2:10:31 - loss: 0.3838 - regression_loss: 0.3433 - classification_loss: 0.0405
 1179/10000 [==>...........................] - ETA: 2:10:30 - loss: 0.3836 - regression_loss: 0.3431 - classification_loss: 0.0405
 1180/10000 [==>...........................] - ETA: 2:10:29 - loss: 0.3837 - regression_loss: 0.3432 - classification_loss: 0.0405
 1181/10000 [==>...........................] - ETA: 2:10:28 - loss: 0.3837 - regression_loss: 0.3432 - classification_loss: 0.0405
 1182/10000 [==>...........................] - ETA: 2:10:27 - loss: 0.3837 - regression_loss: 0.3432 - classification_loss: 0.0405
 1183/10000 [==>...........................] - ETA: 2:10:26 - loss: 0.3835 - regression_loss: 0.3430 - classification_loss: 0.0404
 1184/10000 [==>...........................] - ETA: 2:10:26 - loss: 0.3833 - regression_loss: 0.3429 - classification_loss: 0.0404
 1185/10000 [==>...........................] - ETA: 2:10:25 - loss: 0.3832 - regression_loss: 0.3428 - classification_loss: 0.0404
 1186/10000 [==>...........................] - ETA: 2:10:24 - loss: 0.3835 - regression_loss: 0.3431 - classification_loss: 0.0404
 1187/10000 [==>...........................] - ETA: 2:10:23 - loss: 0.3836 - regression_loss: 0.3432 - classification_loss: 0.0404
 1188/10000 [==>...........................] - ETA: 2:10:22 - loss: 0.3835 - regression_loss: 0.3431 - classification_loss: 0.0404
 1189/10000 [==>...........................] - ETA: 2:10:21 - loss: 0.3837 - regression_loss: 0.3433 - classification_loss: 0.0404
 1190/10000 [==>...........................] - ETA: 2:10:20 - loss: 0.3838 - regression_loss: 0.3435 - classification_loss: 0.0404
 1191/10000 [==>...........................] - ETA: 2:10:19 - loss: 0.3835 - regression_loss: 0.3432 - classification_loss: 0.0404
 1192/10000 [==>...........................] - ETA: 2:10:18 - loss: 0.3832 - regression_loss: 0.3429 - classification_loss: 0.0403
 1193/10000 [==>...........................] - ETA: 2:10:18 - loss: 0.3833 - regression_loss: 0.3429 - classification_loss: 0.0404
 1194/10000 [==>...........................] - ETA: 2:10:17 - loss: 0.3835 - regression_loss: 0.3432 - classification_loss: 0.0404
 1195/10000 [==>...........................] - ETA: 2:10:16 - loss: 0.3834 - regression_loss: 0.3430 - classification_loss: 0.0403
 1196/10000 [==>...........................] - ETA: 2:10:15 - loss: 0.3836 - regression_loss: 0.3432 - classification_loss: 0.0403
 1197/10000 [==>...........................] - ETA: 2:10:14 - loss: 0.3836 - regression_loss: 0.3433 - classification_loss: 0.0403
 1198/10000 [==>...........................] - ETA: 2:10:13 - loss: 0.3835 - regression_loss: 0.3432 - classification_loss: 0.0403
 1199/10000 [==>...........................] - ETA: 2:10:12 - loss: 0.3834 - regression_loss: 0.3431 - classification_loss: 0.0403
 1200/10000 [==>...........................] - ETA: 2:10:11 - loss: 0.3832 - regression_loss: 0.3429 - classification_loss: 0.0403
 1201/10000 [==>...........................] - ETA: 2:10:10 - loss: 0.3835 - regression_loss: 0.3432 - classification_loss: 0.0403
 1202/10000 [==>...........................] - ETA: 2:10:10 - loss: 0.3835 - regression_loss: 0.3433 - classification_loss: 0.0403
 1203/10000 [==>...........................] - ETA: 2:10:09 - loss: 0.3836 - regression_loss: 0.3434 - classification_loss: 0.0403
 1204/10000 [==>...........................] - ETA: 2:10:08 - loss: 0.3835 - regression_loss: 0.3433 - classification_loss: 0.0403
 1205/10000 [==>...........................] - ETA: 2:10:07 - loss: 0.3834 - regression_loss: 0.3432 - classification_loss: 0.0402
 1206/10000 [==>...........................] - ETA: 2:10:06 - loss: 0.3834 - regression_loss: 0.3432 - classification_loss: 0.0402
 1207/10000 [==>...........................] - ETA: 2:10:05 - loss: 0.3831 - regression_loss: 0.3429 - classification_loss: 0.0402
 1208/10000 [==>...........................] - ETA: 2:10:04 - loss: 0.3831 - regression_loss: 0.3429 - classification_loss: 0.0402
 1209/10000 [==>...........................] - ETA: 2:10:03 - loss: 0.3831 - regression_loss: 0.3429 - classification_loss: 0.0402
 1210/10000 [==>...........................] - ETA: 2:10:02 - loss: 0.3831 - regression_loss: 0.3429 - classification_loss: 0.0402
 1211/10000 [==>...........................] - ETA: 2:10:02 - loss: 0.3831 - regression_loss: 0.3429 - classification_loss: 0.0402
 1212/10000 [==>...........................] - ETA: 2:10:01 - loss: 0.3831 - regression_loss: 0.3429 - classification_loss: 0.0402
 1213/10000 [==>...........................] - ETA: 2:10:00 - loss: 0.3828 - regression_loss: 0.3426 - classification_loss: 0.0402
 1214/10000 [==>...........................] - ETA: 2:09:59 - loss: 0.3828 - regression_loss: 0.3426 - classification_loss: 0.0402
 1215/10000 [==>...........................] - ETA: 2:09:58 - loss: 0.3829 - regression_loss: 0.3427 - classification_loss: 0.0402
 1216/10000 [==>...........................] - ETA: 2:09:57 - loss: 0.3826 - regression_loss: 0.3425 - classification_loss: 0.0402
 1217/10000 [==>...........................] - ETA: 2:09:56 - loss: 0.3831 - regression_loss: 0.3428 - classification_loss: 0.0402
 1218/10000 [==>...........................] - ETA: 2:09:55 - loss: 0.3831 - regression_loss: 0.3429 - classification_loss: 0.0402
 1219/10000 [==>...........................] - ETA: 2:09:54 - loss: 0.3833 - regression_loss: 0.3431 - classification_loss: 0.0402
 1220/10000 [==>...........................] - ETA: 2:09:54 - loss: 0.3830 - regression_loss: 0.3428 - classification_loss: 0.0402
 1221/10000 [==>...........................] - ETA: 2:09:53 - loss: 0.3832 - regression_loss: 0.3430 - classification_loss: 0.0402
 1222/10000 [==>...........................] - ETA: 2:09:52 - loss: 0.3834 - regression_loss: 0.3432 - classification_loss: 0.0402
 1223/10000 [==>...........................] - ETA: 2:09:51 - loss: 0.3835 - regression_loss: 0.3433 - classification_loss: 0.0402
 1224/10000 [==>...........................] - ETA: 2:09:50 - loss: 0.3835 - regression_loss: 0.3434 - classification_loss: 0.0402
 1225/10000 [==>...........................] - ETA: 2:09:49 - loss: 0.3837 - regression_loss: 0.3435 - classification_loss: 0.0402
 1226/10000 [==>...........................] - ETA: 2:09:48 - loss: 0.3836 - regression_loss: 0.3434 - classification_loss: 0.0402
 1227/10000 [==>...........................] - ETA: 2:09:47 - loss: 0.3835 - regression_loss: 0.3433 - classification_loss: 0.0402
 1228/10000 [==>...........................] - ETA: 2:09:46 - loss: 0.3832 - regression_loss: 0.3430 - classification_loss: 0.0401
 1229/10000 [==>...........................] - ETA: 2:09:46 - loss: 0.3839 - regression_loss: 0.3437 - classification_loss: 0.0403
 1230/10000 [==>...........................] - ETA: 2:09:45 - loss: 0.3846 - regression_loss: 0.3442 - classification_loss: 0.0404
 1231/10000 [==>...........................] - ETA: 2:09:44 - loss: 0.3846 - regression_loss: 0.3442 - classification_loss: 0.0404
 1232/10000 [==>...........................] - ETA: 2:09:43 - loss: 0.3845 - regression_loss: 0.3441 - classification_loss: 0.0404
 1233/10000 [==>...........................] - ETA: 2:09:42 - loss: 0.3843 - regression_loss: 0.3439 - classification_loss: 0.0404
 1234/10000 [==>...........................] - ETA: 2:09:41 - loss: 0.3848 - regression_loss: 0.3444 - classification_loss: 0.0405
 1235/10000 [==>...........................] - ETA: 2:09:40 - loss: 0.3848 - regression_loss: 0.3443 - classification_loss: 0.0405
 1236/10000 [==>...........................] - ETA: 2:09:39 - loss: 0.3851 - regression_loss: 0.3446 - classification_loss: 0.0405
 1237/10000 [==>...........................] - ETA: 2:09:38 - loss: 0.3850 - regression_loss: 0.3445 - classification_loss: 0.0405
 1238/10000 [==>...........................] - ETA: 2:09:38 - loss: 0.3851 - regression_loss: 0.3447 - classification_loss: 0.0405
 1239/10000 [==>...........................] - ETA: 2:09:37 - loss: 0.3850 - regression_loss: 0.3445 - classification_loss: 0.0405
 1240/10000 [==>...........................] - ETA: 2:09:36 - loss: 0.3847 - regression_loss: 0.3443 - classification_loss: 0.0405
 1241/10000 [==>...........................] - ETA: 2:09:35 - loss: 0.3848 - regression_loss: 0.3444 - classification_loss: 0.0405
 1242/10000 [==>...........................] - ETA: 2:09:34 - loss: 0.3847 - regression_loss: 0.3442 - classification_loss: 0.0404
 1243/10000 [==>...........................] - ETA: 2:09:33 - loss: 0.3845 - regression_loss: 0.3441 - classification_loss: 0.0404
 1244/10000 [==>...........................] - ETA: 2:09:32 - loss: 0.3846 - regression_loss: 0.3442 - classification_loss: 0.0404
 1245/10000 [==>...........................] - ETA: 2:09:31 - loss: 0.3851 - regression_loss: 0.3446 - classification_loss: 0.0405
 1246/10000 [==>...........................] - ETA: 2:09:30 - loss: 0.3851 - regression_loss: 0.3446 - classification_loss: 0.0405
 1247/10000 [==>...........................] - ETA: 2:09:30 - loss: 0.3852 - regression_loss: 0.3446 - classification_loss: 0.0406
 1248/10000 [==>...........................] - ETA: 2:09:29 - loss: 0.3853 - regression_loss: 0.3447 - classification_loss: 0.0406
 1249/10000 [==>...........................] - ETA: 2:09:28 - loss: 0.3852 - regression_loss: 0.3447 - classification_loss: 0.0406
 1250/10000 [==>...........................] - ETA: 2:09:27 - loss: 0.3852 - regression_loss: 0.3446 - classification_loss: 0.0405
 1251/10000 [==>...........................] - ETA: 2:09:26 - loss: 0.3852 - regression_loss: 0.3447 - classification_loss: 0.0406
 1252/10000 [==>...........................] - ETA: 2:09:25 - loss: 0.3851 - regression_loss: 0.3446 - classification_loss: 0.0405
 1253/10000 [==>...........................] - ETA: 2:09:24 - loss: 0.3849 - regression_loss: 0.3444 - classification_loss: 0.0405
 1254/10000 [==>...........................] - ETA: 2:09:23 - loss: 0.3847 - regression_loss: 0.3442 - classification_loss: 0.0405
 1255/10000 [==>...........................] - ETA: 2:09:22 - loss: 0.3846 - regression_loss: 0.3441 - classification_loss: 0.0405
 1256/10000 [==>...........................] - ETA: 2:09:22 - loss: 0.3850 - regression_loss: 0.3444 - classification_loss: 0.0406
 1257/10000 [==>...........................] - ETA: 2:09:21 - loss: 0.3849 - regression_loss: 0.3444 - classification_loss: 0.0405
 1258/10000 [==>...........................] - ETA: 2:09:20 - loss: 0.3852 - regression_loss: 0.3447 - classification_loss: 0.0405
 1259/10000 [==>...........................] - ETA: 2:09:19 - loss: 0.3855 - regression_loss: 0.3449 - classification_loss: 0.0406
 1260/10000 [==>...........................] - ETA: 2:09:18 - loss: 0.3854 - regression_loss: 0.3449 - classification_loss: 0.0406
 1261/10000 [==>...........................] - ETA: 2:09:17 - loss: 0.3853 - regression_loss: 0.3448 - classification_loss: 0.0405
 1262/10000 [==>...........................] - ETA: 2:09:16 - loss: 0.3850 - regression_loss: 0.3445 - classification_loss: 0.0405
 1263/10000 [==>...........................] - ETA: 2:09:15 - loss: 0.3850 - regression_loss: 0.3445 - classification_loss: 0.0405
 1264/10000 [==>...........................] - ETA: 2:09:15 - loss: 0.3848 - regression_loss: 0.3443 - classification_loss: 0.0405
 1265/10000 [==>...........................] - ETA: 2:09:14 - loss: 0.3846 - regression_loss: 0.3442 - classification_loss: 0.0404
 1266/10000 [==>...........................] - ETA: 2:09:13 - loss: 0.3847 - regression_loss: 0.3442 - classification_loss: 0.0405
 1267/10000 [==>...........................] - ETA: 2:09:12 - loss: 0.3851 - regression_loss: 0.3446 - classification_loss: 0.0405
 1268/10000 [==>...........................] - ETA: 2:09:11 - loss: 0.3851 - regression_loss: 0.3446 - classification_loss: 0.0405
 1269/10000 [==>...........................] - ETA: 2:09:10 - loss: 0.3856 - regression_loss: 0.3451 - classification_loss: 0.0405
 1270/10000 [==>...........................] - ETA: 2:09:09 - loss: 0.3854 - regression_loss: 0.3449 - classification_loss: 0.0405
 1271/10000 [==>...........................] - ETA: 2:09:08 - loss: 0.3856 - regression_loss: 0.3451 - classification_loss: 0.0406
 1272/10000 [==>...........................] - ETA: 2:09:07 - loss: 0.3855 - regression_loss: 0.3449 - classification_loss: 0.0405
 1273/10000 [==>...........................] - ETA: 2:09:06 - loss: 0.3854 - regression_loss: 0.3448 - classification_loss: 0.0405
 1274/10000 [==>...........................] - ETA: 2:09:06 - loss: 0.3855 - regression_loss: 0.3449 - classification_loss: 0.0405
 1275/10000 [==>...........................] - ETA: 2:09:05 - loss: 0.3853 - regression_loss: 0.3448 - classification_loss: 0.0405
 1276/10000 [==>...........................] - ETA: 2:09:04 - loss: 0.3853 - regression_loss: 0.3448 - classification_loss: 0.0405
 1277/10000 [==>...........................] - ETA: 2:09:03 - loss: 0.3851 - regression_loss: 0.3447 - classification_loss: 0.0405
 1278/10000 [==>...........................] - ETA: 2:09:02 - loss: 0.3850 - regression_loss: 0.3446 - classification_loss: 0.0405
 1279/10000 [==>...........................] - ETA: 2:09:01 - loss: 0.3850 - regression_loss: 0.3445 - classification_loss: 0.0404
 1280/10000 [==>...........................] - ETA: 2:09:00 - loss: 0.3850 - regression_loss: 0.3445 - classification_loss: 0.0405
 1281/10000 [==>...........................] - ETA: 2:08:59 - loss: 0.3851 - regression_loss: 0.3447 - classification_loss: 0.0405
 1282/10000 [==>...........................] - ETA: 2:08:59 - loss: 0.3850 - regression_loss: 0.3446 - classification_loss: 0.0404
 1283/10000 [==>...........................] - ETA: 2:08:58 - loss: 0.3851 - regression_loss: 0.3446 - classification_loss: 0.0404
 1284/10000 [==>...........................] - ETA: 2:08:57 - loss: 0.3849 - regression_loss: 0.3445 - classification_loss: 0.0404
 1285/10000 [==>...........................] - ETA: 2:08:56 - loss: 0.3849 - regression_loss: 0.3446 - classification_loss: 0.0404
 1286/10000 [==>...........................] - ETA: 2:08:55 - loss: 0.3850 - regression_loss: 0.3446 - classification_loss: 0.0404
 1287/10000 [==>...........................] - ETA: 2:08:54 - loss: 0.3849 - regression_loss: 0.3445 - classification_loss: 0.0403
 1288/10000 [==>...........................] - ETA: 2:08:53 - loss: 0.3848 - regression_loss: 0.3444 - classification_loss: 0.0403
 1289/10000 [==>...........................] - ETA: 2:08:52 - loss: 0.3845 - regression_loss: 0.3442 - classification_loss: 0.0403
 1290/10000 [==>...........................] - ETA: 2:08:51 - loss: 0.3844 - regression_loss: 0.3441 - classification_loss: 0.0403
 1291/10000 [==>...........................] - ETA: 2:08:50 - loss: 0.3843 - regression_loss: 0.3439 - classification_loss: 0.0404
 1292/10000 [==>...........................] - ETA: 2:08:50 - loss: 0.3844 - regression_loss: 0.3441 - classification_loss: 0.0404
 1293/10000 [==>...........................] - ETA: 2:08:49 - loss: 0.3844 - regression_loss: 0.3441 - classification_loss: 0.0403
 1294/10000 [==>...........................] - ETA: 2:08:48 - loss: 0.3846 - regression_loss: 0.3442 - classification_loss: 0.0403
 1295/10000 [==>...........................] - ETA: 2:08:47 - loss: 0.3847 - regression_loss: 0.3443 - classification_loss: 0.0403
 1296/10000 [==>...........................] - ETA: 2:08:46 - loss: 0.3845 - regression_loss: 0.3442 - classification_loss: 0.0403
 1297/10000 [==>...........................] - ETA: 2:08:45 - loss: 0.3846 - regression_loss: 0.3443 - classification_loss: 0.0403
 1298/10000 [==>...........................] - ETA: 2:08:44 - loss: 0.3847 - regression_loss: 0.3444 - classification_loss: 0.0403
 1299/10000 [==>...........................] - ETA: 2:08:43 - loss: 0.3846 - regression_loss: 0.3443 - classification_loss: 0.0403
 1300/10000 [==>...........................] - ETA: 2:08:42 - loss: 0.3845 - regression_loss: 0.3443 - classification_loss: 0.0403
 1301/10000 [==>...........................] - ETA: 2:08:42 - loss: 0.3845 - regression_loss: 0.3443 - classification_loss: 0.0402
 1302/10000 [==>...........................] - ETA: 2:08:41 - loss: 0.3842 - regression_loss: 0.3440 - classification_loss: 0.0402
 1303/10000 [==>...........................] - ETA: 2:08:40 - loss: 0.3841 - regression_loss: 0.3439 - classification_loss: 0.0402
 1304/10000 [==>...........................] - ETA: 2:08:39 - loss: 0.3841 - regression_loss: 0.3439 - classification_loss: 0.0401
 1305/10000 [==>...........................] - ETA: 2:08:38 - loss: 0.3840 - regression_loss: 0.3438 - classification_loss: 0.0401
 1306/10000 [==>...........................] - ETA: 2:08:37 - loss: 0.3840 - regression_loss: 0.3439 - classification_loss: 0.0401
 1307/10000 [==>...........................] - ETA: 2:08:36 - loss: 0.3839 - regression_loss: 0.3438 - classification_loss: 0.0401
 1308/10000 [==>...........................] - ETA: 2:08:35 - loss: 0.3838 - regression_loss: 0.3437 - classification_loss: 0.0401
 1309/10000 [==>...........................] - ETA: 2:08:35 - loss: 0.3838 - regression_loss: 0.3437 - classification_loss: 0.0401
 1310/10000 [==>...........................] - ETA: 2:08:34 - loss: 0.3837 - regression_loss: 0.3437 - classification_loss: 0.0400
 1311/10000 [==>...........................] - ETA: 2:08:33 - loss: 0.3834 - regression_loss: 0.3434 - classification_loss: 0.0400
 1312/10000 [==>...........................] - ETA: 2:08:32 - loss: 0.3835 - regression_loss: 0.3435 - classification_loss: 0.0400
 1313/10000 [==>...........................] - ETA: 2:08:31 - loss: 0.3833 - regression_loss: 0.3433 - classification_loss: 0.0400
 1314/10000 [==>...........................] - ETA: 2:08:30 - loss: 0.3834 - regression_loss: 0.3434 - classification_loss: 0.0400
 1315/10000 [==>...........................] - ETA: 2:08:29 - loss: 0.3833 - regression_loss: 0.3433 - classification_loss: 0.0400
 1316/10000 [==>...........................] - ETA: 2:08:28 - loss: 0.3832 - regression_loss: 0.3433 - classification_loss: 0.0399
 1317/10000 [==>...........................] - ETA: 2:08:27 - loss: 0.3831 - regression_loss: 0.3432 - classification_loss: 0.0399
 1318/10000 [==>...........................] - ETA: 2:08:27 - loss: 0.3830 - regression_loss: 0.3431 - classification_loss: 0.0399
 1319/10000 [==>...........................] - ETA: 2:08:26 - loss: 0.3830 - regression_loss: 0.3431 - classification_loss: 0.0399
 1320/10000 [==>...........................] - ETA: 2:08:25 - loss: 0.3832 - regression_loss: 0.3433 - classification_loss: 0.0399
 1321/10000 [==>...........................] - ETA: 2:08:24 - loss: 0.3832 - regression_loss: 0.3433 - classification_loss: 0.0399
 1322/10000 [==>...........................] - ETA: 2:08:23 - loss: 0.3834 - regression_loss: 0.3435 - classification_loss: 0.0399
 1323/10000 [==>...........................] - ETA: 2:08:22 - loss: 0.3831 - regression_loss: 0.3433 - classification_loss: 0.0398
 1324/10000 [==>...........................] - ETA: 2:08:21 - loss: 0.3828 - regression_loss: 0.3430 - classification_loss: 0.0398
 1325/10000 [==>...........................] - ETA: 2:08:20 - loss: 0.3827 - regression_loss: 0.3429 - classification_loss: 0.0398
 1326/10000 [==>...........................] - ETA: 2:08:19 - loss: 0.3834 - regression_loss: 0.3435 - classification_loss: 0.0399
 1327/10000 [==>...........................] - ETA: 2:08:19 - loss: 0.3834 - regression_loss: 0.3435 - classification_loss: 0.0399
 1328/10000 [==>...........................] - ETA: 2:08:18 - loss: 0.3832 - regression_loss: 0.3434 - classification_loss: 0.0399
 1329/10000 [==>...........................] - ETA: 2:08:17 - loss: 0.3831 - regression_loss: 0.3433 - classification_loss: 0.0398
 1330/10000 [==>...........................] - ETA: 2:08:16 - loss: 0.3830 - regression_loss: 0.3432 - classification_loss: 0.0398
 1331/10000 [==>...........................] - ETA: 2:08:15 - loss: 0.3832 - regression_loss: 0.3433 - classification_loss: 0.0399
 1332/10000 [==>...........................] - ETA: 2:08:14 - loss: 0.3834 - regression_loss: 0.3436 - classification_loss: 0.0398
 1333/10000 [==>...........................] - ETA: 2:08:13 - loss: 0.3836 - regression_loss: 0.3437 - classification_loss: 0.0398
 1334/10000 [===>..........................] - ETA: 2:08:12 - loss: 0.3841 - regression_loss: 0.3443 - classification_loss: 0.0399
 1335/10000 [===>..........................] - ETA: 2:08:12 - loss: 0.3844 - regression_loss: 0.3445 - classification_loss: 0.0399
 1336/10000 [===>..........................] - ETA: 2:08:11 - loss: 0.3841 - regression_loss: 0.3442 - classification_loss: 0.0398
 1337/10000 [===>..........................] - ETA: 2:08:10 - loss: 0.3840 - regression_loss: 0.3441 - classification_loss: 0.0398
 1338/10000 [===>..........................] - ETA: 2:08:09 - loss: 0.3841 - regression_loss: 0.3443 - classification_loss: 0.0398
 1339/10000 [===>..........................] - ETA: 2:08:08 - loss: 0.3840 - regression_loss: 0.3442 - classification_loss: 0.0398
 1340/10000 [===>..........................] - ETA: 2:08:07 - loss: 0.3838 - regression_loss: 0.3440 - classification_loss: 0.0398
 1341/10000 [===>..........................] - ETA: 2:08:06 - loss: 0.3837 - regression_loss: 0.3439 - classification_loss: 0.0398
 1342/10000 [===>..........................] - ETA: 2:08:05 - loss: 0.3834 - regression_loss: 0.3436 - classification_loss: 0.0397
 1343/10000 [===>..........................] - ETA: 2:08:04 - loss: 0.3836 - regression_loss: 0.3438 - classification_loss: 0.0397
 1344/10000 [===>..........................] - ETA: 2:08:03 - loss: 0.3834 - regression_loss: 0.3437 - classification_loss: 0.0398
 1345/10000 [===>..........................] - ETA: 2:08:03 - loss: 0.3833 - regression_loss: 0.3436 - classification_loss: 0.0398
 1346/10000 [===>..........................] - ETA: 2:08:02 - loss: 0.3831 - regression_loss: 0.3434 - classification_loss: 0.0397
 1347/10000 [===>..........................] - ETA: 2:08:01 - loss: 0.3831 - regression_loss: 0.3434 - classification_loss: 0.0397
 1348/10000 [===>..........................] - ETA: 2:08:00 - loss: 0.3831 - regression_loss: 0.3434 - classification_loss: 0.0397
 1349/10000 [===>..........................] - ETA: 2:07:59 - loss: 0.3831 - regression_loss: 0.3434 - classification_loss: 0.0397
 1350/10000 [===>..........................] - ETA: 2:07:58 - loss: 0.3832 - regression_loss: 0.3436 - classification_loss: 0.0397
 1351/10000 [===>..........................] - ETA: 2:07:57 - loss: 0.3836 - regression_loss: 0.3438 - classification_loss: 0.0397
 1352/10000 [===>..........................] - ETA: 2:07:56 - loss: 0.3843 - regression_loss: 0.3442 - classification_loss: 0.0401
 1353/10000 [===>..........................] - ETA: 2:07:56 - loss: 0.3846 - regression_loss: 0.3445 - classification_loss: 0.0401
 1354/10000 [===>..........................] - ETA: 2:07:55 - loss: 0.3845 - regression_loss: 0.3444 - classification_loss: 0.0401
 1355/10000 [===>..........................] - ETA: 2:07:54 - loss: 0.3845 - regression_loss: 0.3444 - classification_loss: 0.0401
 1356/10000 [===>..........................] - ETA: 2:07:53 - loss: 0.3844 - regression_loss: 0.3443 - classification_loss: 0.0401
 1357/10000 [===>..........................] - ETA: 2:07:52 - loss: 0.3842 - regression_loss: 0.3441 - classification_loss: 0.0401
 1358/10000 [===>..........................] - ETA: 2:07:51 - loss: 0.3844 - regression_loss: 0.3444 - classification_loss: 0.0400
 1359/10000 [===>..........................] - ETA: 2:07:50 - loss: 0.3843 - regression_loss: 0.3443 - classification_loss: 0.0400
 1360/10000 [===>..........................] - ETA: 2:07:49 - loss: 0.3852 - regression_loss: 0.3451 - classification_loss: 0.0401
 1361/10000 [===>..........................] - ETA: 2:07:48 - loss: 0.3851 - regression_loss: 0.3450 - classification_loss: 0.0400
 1362/10000 [===>..........................] - ETA: 2:07:47 - loss: 0.3849 - regression_loss: 0.3449 - classification_loss: 0.0400
 1363/10000 [===>..........................] - ETA: 2:07:47 - loss: 0.3850 - regression_loss: 0.3450 - classification_loss: 0.0400
 1364/10000 [===>..........................] - ETA: 2:07:46 - loss: 0.3849 - regression_loss: 0.3449 - classification_loss: 0.0400
 1365/10000 [===>..........................] - ETA: 2:07:45 - loss: 0.3848 - regression_loss: 0.3448 - classification_loss: 0.0400
 1366/10000 [===>..........................] - ETA: 2:07:44 - loss: 0.3847 - regression_loss: 0.3447 - classification_loss: 0.0400
 1367/10000 [===>..........................] - ETA: 2:07:43 - loss: 0.3846 - regression_loss: 0.3447 - classification_loss: 0.0400
 1368/10000 [===>..........................] - ETA: 2:07:42 - loss: 0.3847 - regression_loss: 0.3447 - classification_loss: 0.0400
 1369/10000 [===>..........................] - ETA: 2:07:41 - loss: 0.3854 - regression_loss: 0.3454 - classification_loss: 0.0400
 1370/10000 [===>..........................] - ETA: 2:07:40 - loss: 0.3854 - regression_loss: 0.3454 - classification_loss: 0.0400
 1371/10000 [===>..........................] - ETA: 2:07:39 - loss: 0.3853 - regression_loss: 0.3453 - classification_loss: 0.0400
 1372/10000 [===>..........................] - ETA: 2:07:39 - loss: 0.3856 - regression_loss: 0.3456 - classification_loss: 0.0400
 1373/10000 [===>..........................] - ETA: 2:07:38 - loss: 0.3853 - regression_loss: 0.3453 - classification_loss: 0.0400
 1374/10000 [===>..........................] - ETA: 2:07:37 - loss: 0.3852 - regression_loss: 0.3452 - classification_loss: 0.0400
 1375/10000 [===>..........................] - ETA: 2:07:36 - loss: 0.3852 - regression_loss: 0.3452 - classification_loss: 0.0400
 1376/10000 [===>..........................] - ETA: 2:07:35 - loss: 0.3852 - regression_loss: 0.3452 - classification_loss: 0.0400
 1377/10000 [===>..........................] - ETA: 2:07:34 - loss: 0.3850 - regression_loss: 0.3450 - classification_loss: 0.0400
 1378/10000 [===>..........................] - ETA: 2:07:33 - loss: 0.3850 - regression_loss: 0.3450 - classification_loss: 0.0400
 1379/10000 [===>..........................] - ETA: 2:07:32 - loss: 0.3849 - regression_loss: 0.3449 - classification_loss: 0.0400
 1380/10000 [===>..........................] - ETA: 2:07:31 - loss: 0.3849 - regression_loss: 0.3450 - classification_loss: 0.0399
 1381/10000 [===>..........................] - ETA: 2:07:31 - loss: 0.3850 - regression_loss: 0.3450 - classification_loss: 0.0400
 1382/10000 [===>..........................] - ETA: 2:07:30 - loss: 0.3850 - regression_loss: 0.3450 - classification_loss: 0.0400
 1383/10000 [===>..........................] - ETA: 2:07:29 - loss: 0.3848 - regression_loss: 0.3449 - classification_loss: 0.0399
 1384/10000 [===>..........................] - ETA: 2:07:28 - loss: 0.3848 - regression_loss: 0.3448 - classification_loss: 0.0399
 1385/10000 [===>..........................] - ETA: 2:07:27 - loss: 0.3849 - regression_loss: 0.3450 - classification_loss: 0.0399
 1386/10000 [===>..........................] - ETA: 2:07:26 - loss: 0.3851 - regression_loss: 0.3451 - classification_loss: 0.0399
 1387/10000 [===>..........................] - ETA: 2:07:25 - loss: 0.3848 - regression_loss: 0.3449 - classification_loss: 0.0399
 1388/10000 [===>..........................] - ETA: 2:07:24 - loss: 0.3847 - regression_loss: 0.3448 - classification_loss: 0.0399
 1389/10000 [===>..........................] - ETA: 2:07:23 - loss: 0.3844 - regression_loss: 0.3445 - classification_loss: 0.0399
 1390/10000 [===>..........................] - ETA: 2:07:23 - loss: 0.3848 - regression_loss: 0.3449 - classification_loss: 0.0399
 1391/10000 [===>..........................] - ETA: 2:07:22 - loss: 0.3848 - regression_loss: 0.3449 - classification_loss: 0.0399
 1392/10000 [===>..........................] - ETA: 2:07:21 - loss: 0.3845 - regression_loss: 0.3447 - classification_loss: 0.0399
 1393/10000 [===>..........................] - ETA: 2:07:20 - loss: 0.3846 - regression_loss: 0.3447 - classification_loss: 0.0399
 1394/10000 [===>..........................] - ETA: 2:07:19 - loss: 0.3846 - regression_loss: 0.3447 - classification_loss: 0.0399
 1395/10000 [===>..........................] - ETA: 2:07:18 - loss: 0.3848 - regression_loss: 0.3449 - classification_loss: 0.0399
 1396/10000 [===>..........................] - ETA: 2:07:17 - loss: 0.3845 - regression_loss: 0.3447 - classification_loss: 0.0398
 1397/10000 [===>..........................] - ETA: 2:07:16 - loss: 0.3845 - regression_loss: 0.3447 - classification_loss: 0.0398
 1398/10000 [===>..........................] - ETA: 2:07:15 - loss: 0.3845 - regression_loss: 0.3447 - classification_loss: 0.0398
 1399/10000 [===>..........................] - ETA: 2:07:15 - loss: 0.3845 - regression_loss: 0.3446 - classification_loss: 0.0398
 1400/10000 [===>..........................] - ETA: 2:07:14 - loss: 0.3843 - regression_loss: 0.3444 - classification_loss: 0.0398
 1401/10000 [===>..........................] - ETA: 2:07:13 - loss: 0.3844 - regression_loss: 0.3445 - classification_loss: 0.0398
 1402/10000 [===>..........................] - ETA: 2:07:12 - loss: 0.3842 - regression_loss: 0.3444 - classification_loss: 0.0398
 1403/10000 [===>..........................] - ETA: 2:07:11 - loss: 0.3841 - regression_loss: 0.3443 - classification_loss: 0.0398
 1404/10000 [===>..........................] - ETA: 2:07:10 - loss: 0.3842 - regression_loss: 0.3444 - classification_loss: 0.0398
 1405/10000 [===>..........................] - ETA: 2:07:09 - loss: 0.3842 - regression_loss: 0.3444 - classification_loss: 0.0398
 1406/10000 [===>..........................] - ETA: 2:07:08 - loss: 0.3841 - regression_loss: 0.3443 - classification_loss: 0.0398
 1407/10000 [===>..........................] - ETA: 2:07:07 - loss: 0.3840 - regression_loss: 0.3443 - classification_loss: 0.0398
 1408/10000 [===>..........................] - ETA: 2:07:07 - loss: 0.3844 - regression_loss: 0.3446 - classification_loss: 0.0398
 1409/10000 [===>..........................] - ETA: 2:07:06 - loss: 0.3845 - regression_loss: 0.3447 - classification_loss: 0.0398
 1410/10000 [===>..........................] - ETA: 2:07:05 - loss: 0.3844 - regression_loss: 0.3446 - classification_loss: 0.0398
 1411/10000 [===>..........................] - ETA: 2:07:04 - loss: 0.3841 - regression_loss: 0.3444 - classification_loss: 0.0397
 1412/10000 [===>..........................] - ETA: 2:07:03 - loss: 0.3841 - regression_loss: 0.3444 - classification_loss: 0.0397
 1413/10000 [===>..........................] - ETA: 2:07:02 - loss: 0.3838 - regression_loss: 0.3441 - classification_loss: 0.0397
 1414/10000 [===>..........................] - ETA: 2:07:01 - loss: 0.3836 - regression_loss: 0.3439 - classification_loss: 0.0397
 1415/10000 [===>..........................] - ETA: 2:07:00 - loss: 0.3833 - regression_loss: 0.3436 - classification_loss: 0.0396
 1416/10000 [===>..........................] - ETA: 2:06:59 - loss: 0.3834 - regression_loss: 0.3438 - classification_loss: 0.0397
 1417/10000 [===>..........................] - ETA: 2:06:59 - loss: 0.3837 - regression_loss: 0.3440 - classification_loss: 0.0397
 1418/10000 [===>..........................] - ETA: 2:06:58 - loss: 0.3841 - regression_loss: 0.3444 - classification_loss: 0.0397
 1419/10000 [===>..........................] - ETA: 2:06:57 - loss: 0.3839 - regression_loss: 0.3443 - classification_loss: 0.0397
 1420/10000 [===>..........................] - ETA: 2:06:56 - loss: 0.3842 - regression_loss: 0.3445 - classification_loss: 0.0397
 1421/10000 [===>..........................] - ETA: 2:06:55 - loss: 0.3842 - regression_loss: 0.3445 - classification_loss: 0.0397
 1422/10000 [===>..........................] - ETA: 2:06:54 - loss: 0.3841 - regression_loss: 0.3444 - classification_loss: 0.0397
 1423/10000 [===>..........................] - ETA: 2:06:53 - loss: 0.3838 - regression_loss: 0.3442 - classification_loss: 0.0396
 1424/10000 [===>..........................] - ETA: 2:06:52 - loss: 0.3837 - regression_loss: 0.3441 - classification_loss: 0.0396
 1425/10000 [===>..........................] - ETA: 2:06:51 - loss: 0.3836 - regression_loss: 0.3440 - classification_loss: 0.0396
 1426/10000 [===>..........................] - ETA: 2:06:51 - loss: 0.3835 - regression_loss: 0.3439 - classification_loss: 0.0396
 1427/10000 [===>..........................] - ETA: 2:06:50 - loss: 0.3835 - regression_loss: 0.3439 - classification_loss: 0.0396
 1428/10000 [===>..........................] - ETA: 2:06:49 - loss: 0.3834 - regression_loss: 0.3438 - classification_loss: 0.0396
 1429/10000 [===>..........................] - ETA: 2:06:48 - loss: 0.3835 - regression_loss: 0.3439 - classification_loss: 0.0396
 1430/10000 [===>..........................] - ETA: 2:06:47 - loss: 0.3835 - regression_loss: 0.3439 - classification_loss: 0.0396
 1431/10000 [===>..........................] - ETA: 2:06:46 - loss: 0.3835 - regression_loss: 0.3439 - classification_loss: 0.0396
 1432/10000 [===>..........................] - ETA: 2:06:45 - loss: 0.3835 - regression_loss: 0.3439 - classification_loss: 0.0396
 1433/10000 [===>..........................] - ETA: 2:06:44 - loss: 0.3835 - regression_loss: 0.3440 - classification_loss: 0.0396
 1434/10000 [===>..........................] - ETA: 2:06:43 - loss: 0.3834 - regression_loss: 0.3439 - classification_loss: 0.0395
 1435/10000 [===>..........................] - ETA: 2:06:43 - loss: 0.3837 - regression_loss: 0.3441 - classification_loss: 0.0396
 1436/10000 [===>..........................] - ETA: 2:06:42 - loss: 0.3836 - regression_loss: 0.3441 - classification_loss: 0.0395
 1437/10000 [===>..........................] - ETA: 2:06:41 - loss: 0.3837 - regression_loss: 0.3441 - classification_loss: 0.0395
 1438/10000 [===>..........................] - ETA: 2:06:40 - loss: 0.3834 - regression_loss: 0.3439 - classification_loss: 0.0395
 1439/10000 [===>..........................] - ETA: 2:06:39 - loss: 0.3833 - regression_loss: 0.3438 - classification_loss: 0.0395
 1440/10000 [===>..........................] - ETA: 2:06:38 - loss: 0.3833 - regression_loss: 0.3438 - classification_loss: 0.0395
 1441/10000 [===>..........................] - ETA: 2:06:37 - loss: 0.3832 - regression_loss: 0.3437 - classification_loss: 0.0395
 1442/10000 [===>..........................] - ETA: 2:06:36 - loss: 0.3831 - regression_loss: 0.3437 - classification_loss: 0.0395
 1443/10000 [===>..........................] - ETA: 2:06:35 - loss: 0.3831 - regression_loss: 0.3437 - classification_loss: 0.0394
 1444/10000 [===>..........................] - ETA: 2:06:35 - loss: 0.3830 - regression_loss: 0.3436 - classification_loss: 0.0394
 1445/10000 [===>..........................] - ETA: 2:06:34 - loss: 0.3830 - regression_loss: 0.3436 - classification_loss: 0.0394
 1446/10000 [===>..........................] - ETA: 2:06:33 - loss: 0.3829 - regression_loss: 0.3435 - classification_loss: 0.0394
 1447/10000 [===>..........................] - ETA: 2:06:32 - loss: 0.3829 - regression_loss: 0.3435 - classification_loss: 0.0394
 1448/10000 [===>..........................] - ETA: 2:06:31 - loss: 0.3832 - regression_loss: 0.3437 - classification_loss: 0.0394
 1449/10000 [===>..........................] - ETA: 2:06:30 - loss: 0.3831 - regression_loss: 0.3437 - classification_loss: 0.0394
 1450/10000 [===>..........................] - ETA: 2:06:29 - loss: 0.3834 - regression_loss: 0.3440 - classification_loss: 0.0395
 1451/10000 [===>..........................] - ETA: 2:06:28 - loss: 0.3833 - regression_loss: 0.3439 - classification_loss: 0.0394
 1452/10000 [===>..........................] - ETA: 2:06:27 - loss: 0.3833 - regression_loss: 0.3439 - classification_loss: 0.0394
 1453/10000 [===>..........................] - ETA: 2:06:27 - loss: 0.3833 - regression_loss: 0.3439 - classification_loss: 0.0394
 1454/10000 [===>..........................] - ETA: 2:06:26 - loss: 0.3833 - regression_loss: 0.3439 - classification_loss: 0.0394
 1455/10000 [===>..........................] - ETA: 2:06:25 - loss: 0.3835 - regression_loss: 0.3441 - classification_loss: 0.0394
 1456/10000 [===>..........................] - ETA: 2:06:24 - loss: 0.3835 - regression_loss: 0.3441 - classification_loss: 0.0394
 1457/10000 [===>..........................] - ETA: 2:06:23 - loss: 0.3836 - regression_loss: 0.3442 - classification_loss: 0.0394
 1458/10000 [===>..........................] - ETA: 2:06:22 - loss: 0.3836 - regression_loss: 0.3442 - classification_loss: 0.0394
 1459/10000 [===>..........................] - ETA: 2:06:21 - loss: 0.3835 - regression_loss: 0.3441 - classification_loss: 0.0394
 1460/10000 [===>..........................] - ETA: 2:06:20 - loss: 0.3834 - regression_loss: 0.3441 - classification_loss: 0.0393
 1461/10000 [===>..........................] - ETA: 2:06:20 - loss: 0.3834 - regression_loss: 0.3441 - classification_loss: 0.0393
 1462/10000 [===>..........................] - ETA: 2:06:19 - loss: 0.3832 - regression_loss: 0.3439 - classification_loss: 0.0393
 1463/10000 [===>..........................] - ETA: 2:06:18 - loss: 0.3831 - regression_loss: 0.3438 - classification_loss: 0.0393
 1464/10000 [===>..........................] - ETA: 2:06:17 - loss: 0.3832 - regression_loss: 0.3439 - classification_loss: 0.0393
 1465/10000 [===>..........................] - ETA: 2:06:16 - loss: 0.3830 - regression_loss: 0.3437 - classification_loss: 0.0393
 1466/10000 [===>..........................] - ETA: 2:06:15 - loss: 0.3830 - regression_loss: 0.3437 - classification_loss: 0.0393
 1467/10000 [===>..........................] - ETA: 2:06:14 - loss: 0.3832 - regression_loss: 0.3439 - classification_loss: 0.0393
 1468/10000 [===>..........................] - ETA: 2:06:13 - loss: 0.3834 - regression_loss: 0.3441 - classification_loss: 0.0393
 1469/10000 [===>..........................] - ETA: 2:06:12 - loss: 0.3835 - regression_loss: 0.3442 - classification_loss: 0.0393
 1470/10000 [===>..........................] - ETA: 2:06:11 - loss: 0.3834 - regression_loss: 0.3440 - classification_loss: 0.0393
 1471/10000 [===>..........................] - ETA: 2:06:11 - loss: 0.3832 - regression_loss: 0.3439 - classification_loss: 0.0393
 1472/10000 [===>..........................] - ETA: 2:06:10 - loss: 0.3832 - regression_loss: 0.3439 - classification_loss: 0.0393
 1473/10000 [===>..........................] - ETA: 2:06:09 - loss: 0.3831 - regression_loss: 0.3438 - classification_loss: 0.0393
 1474/10000 [===>..........................] - ETA: 2:06:08 - loss: 0.3832 - regression_loss: 0.3439 - classification_loss: 0.0393
 1475/10000 [===>..........................] - ETA: 2:06:07 - loss: 0.3831 - regression_loss: 0.3438 - classification_loss: 0.0393
 1476/10000 [===>..........................] - ETA: 2:06:06 - loss: 0.3831 - regression_loss: 0.3437 - classification_loss: 0.0393
 1477/10000 [===>..........................] - ETA: 2:06:05 - loss: 0.3829 - regression_loss: 0.3436 - classification_loss: 0.0393
 1478/10000 [===>..........................] - ETA: 2:06:04 - loss: 0.3830 - regression_loss: 0.3438 - classification_loss: 0.0393
 1479/10000 [===>..........................] - ETA: 2:06:04 - loss: 0.3830 - regression_loss: 0.3437 - classification_loss: 0.0393
 1480/10000 [===>..........................] - ETA: 2:06:03 - loss: 0.3829 - regression_loss: 0.3436 - classification_loss: 0.0393
 1481/10000 [===>..........................] - ETA: 2:06:02 - loss: 0.3830 - regression_loss: 0.3437 - classification_loss: 0.0393
 1482/10000 [===>..........................] - ETA: 2:06:01 - loss: 0.3829 - regression_loss: 0.3436 - classification_loss: 0.0393
 1483/10000 [===>..........................] - ETA: 2:06:00 - loss: 0.3829 - regression_loss: 0.3436 - classification_loss: 0.0393
 1484/10000 [===>..........................] - ETA: 2:05:59 - loss: 0.3827 - regression_loss: 0.3434 - classification_loss: 0.0393
 1485/10000 [===>..........................] - ETA: 2:05:58 - loss: 0.3829 - regression_loss: 0.3436 - classification_loss: 0.0393
 1486/10000 [===>..........................] - ETA: 2:05:57 - loss: 0.3832 - regression_loss: 0.3439 - classification_loss: 0.0393
 1487/10000 [===>..........................] - ETA: 2:05:56 - loss: 0.3829 - regression_loss: 0.3436 - classification_loss: 0.0393
 1488/10000 [===>..........................] - ETA: 2:05:56 - loss: 0.3838 - regression_loss: 0.3441 - classification_loss: 0.0397
 1489/10000 [===>..........................] - ETA: 2:05:55 - loss: 0.3840 - regression_loss: 0.3442 - classification_loss: 0.0397
 1490/10000 [===>..........................] - ETA: 2:05:54 - loss: 0.3840 - regression_loss: 0.3443 - classification_loss: 0.0397
 1491/10000 [===>..........................] - ETA: 2:05:53 - loss: 0.3841 - regression_loss: 0.3443 - classification_loss: 0.0397
 1492/10000 [===>..........................] - ETA: 2:05:52 - loss: 0.3841 - regression_loss: 0.3443 - classification_loss: 0.0397
 1493/10000 [===>..........................] - ETA: 2:05:51 - loss: 0.3841 - regression_loss: 0.3444 - classification_loss: 0.0397
 1494/10000 [===>..........................] - ETA: 2:05:50 - loss: 0.3841 - regression_loss: 0.3444 - classification_loss: 0.0397
 1495/10000 [===>..........................] - ETA: 2:05:49 - loss: 0.3841 - regression_loss: 0.3444 - classification_loss: 0.0397
 1496/10000 [===>..........................] - ETA: 2:05:48 - loss: 0.3840 - regression_loss: 0.3443 - classification_loss: 0.0397
 1497/10000 [===>..........................] - ETA: 2:05:48 - loss: 0.3843 - regression_loss: 0.3445 - classification_loss: 0.0397
 1498/10000 [===>..........................] - ETA: 2:05:47 - loss: 0.3844 - regression_loss: 0.3446 - classification_loss: 0.0397
 1499/10000 [===>..........................] - ETA: 2:05:46 - loss: 0.3843 - regression_loss: 0.3446 - classification_loss: 0.0397
 1500/10000 [===>..........................] - ETA: 2:05:45 - loss: 0.3843 - regression_loss: 0.3446 - classification_loss: 0.0397
 1501/10000 [===>..........................] - ETA: 2:05:44 - loss: 0.3843 - regression_loss: 0.3445 - classification_loss: 0.0397
 1502/10000 [===>..........................] - ETA: 2:05:43 - loss: 0.3844 - regression_loss: 0.3447 - classification_loss: 0.0397
 1503/10000 [===>..........................] - ETA: 2:05:42 - loss: 0.3844 - regression_loss: 0.3447 - classification_loss: 0.0397
 1504/10000 [===>..........................] - ETA: 2:05:41 - loss: 0.3843 - regression_loss: 0.3446 - classification_loss: 0.0397
 1505/10000 [===>..........................] - ETA: 2:05:41 - loss: 0.3845 - regression_loss: 0.3448 - classification_loss: 0.0397
 1506/10000 [===>..........................] - ETA: 2:05:40 - loss: 0.3848 - regression_loss: 0.3451 - classification_loss: 0.0397
 1507/10000 [===>..........................] - ETA: 2:05:39 - loss: 0.3850 - regression_loss: 0.3453 - classification_loss: 0.0397
 1508/10000 [===>..........................] - ETA: 2:05:38 - loss: 0.3849 - regression_loss: 0.3453 - classification_loss: 0.0397
 1509/10000 [===>..........................] - ETA: 2:05:37 - loss: 0.3850 - regression_loss: 0.3453 - classification_loss: 0.0397
 1510/10000 [===>..........................] - ETA: 2:05:36 - loss: 0.3852 - regression_loss: 0.3454 - classification_loss: 0.0399
 1511/10000 [===>..........................] - ETA: 2:05:35 - loss: 0.3851 - regression_loss: 0.3453 - classification_loss: 0.0398
 1512/10000 [===>..........................] - ETA: 2:05:34 - loss: 0.3852 - regression_loss: 0.3453 - classification_loss: 0.0398
 1513/10000 [===>..........................] - ETA: 2:05:33 - loss: 0.3850 - regression_loss: 0.3452 - classification_loss: 0.0398
 1514/10000 [===>..........................] - ETA: 2:05:33 - loss: 0.3854 - regression_loss: 0.3456 - classification_loss: 0.0398
 1515/10000 [===>..........................] - ETA: 2:05:32 - loss: 0.3854 - regression_loss: 0.3456 - classification_loss: 0.0398
 1516/10000 [===>..........................] - ETA: 2:05:31 - loss: 0.3853 - regression_loss: 0.3455 - classification_loss: 0.0398
 1517/10000 [===>..........................] - ETA: 2:05:30 - loss: 0.3853 - regression_loss: 0.3455 - classification_loss: 0.0398
 1518/10000 [===>..........................] - ETA: 2:05:29 - loss: 0.3852 - regression_loss: 0.3454 - classification_loss: 0.0398
 1519/10000 [===>..........................] - ETA: 2:05:28 - loss: 0.3849 - regression_loss: 0.3452 - classification_loss: 0.0398
 1520/10000 [===>..........................] - ETA: 2:05:27 - loss: 0.3849 - regression_loss: 0.3451 - classification_loss: 0.0397
 1521/10000 [===>..........................] - ETA: 2:05:26 - loss: 0.3848 - regression_loss: 0.3450 - classification_loss: 0.0397
 1522/10000 [===>..........................] - ETA: 2:05:25 - loss: 0.3845 - regression_loss: 0.3448 - classification_loss: 0.0397
 1523/10000 [===>..........................] - ETA: 2:05:25 - loss: 0.3845 - regression_loss: 0.3448 - classification_loss: 0.0397
 1524/10000 [===>..........................] - ETA: 2:05:24 - loss: 0.3844 - regression_loss: 0.3447 - classification_loss: 0.0397
 1525/10000 [===>..........................] - ETA: 2:05:23 - loss: 0.3844 - regression_loss: 0.3448 - classification_loss: 0.0397
 1526/10000 [===>..........................] - ETA: 2:05:22 - loss: 0.3843 - regression_loss: 0.3446 - classification_loss: 0.0396
 1527/10000 [===>..........................] - ETA: 2:05:21 - loss: 0.3843 - regression_loss: 0.3447 - classification_loss: 0.0397
 1528/10000 [===>..........................] - ETA: 2:05:20 - loss: 0.3842 - regression_loss: 0.3446 - classification_loss: 0.0397
 1529/10000 [===>..........................] - ETA: 2:05:19 - loss: 0.3841 - regression_loss: 0.3445 - classification_loss: 0.0397
 1530/10000 [===>..........................] - ETA: 2:05:18 - loss: 0.3841 - regression_loss: 0.3445 - classification_loss: 0.0396
 1531/10000 [===>..........................] - ETA: 2:05:18 - loss: 0.3840 - regression_loss: 0.3444 - classification_loss: 0.0396
 1532/10000 [===>..........................] - ETA: 2:05:17 - loss: 0.3837 - regression_loss: 0.3441 - classification_loss: 0.0396
 1533/10000 [===>..........................] - ETA: 2:05:16 - loss: 0.3837 - regression_loss: 0.3441 - classification_loss: 0.0396
 1534/10000 [===>..........................] - ETA: 2:05:15 - loss: 0.3834 - regression_loss: 0.3439 - classification_loss: 0.0396
 1535/10000 [===>..........................] - ETA: 2:05:14 - loss: 0.3833 - regression_loss: 0.3437 - classification_loss: 0.0396
 1536/10000 [===>..........................] - ETA: 2:05:13 - loss: 0.3838 - regression_loss: 0.3441 - classification_loss: 0.0397
 1537/10000 [===>..........................] - ETA: 2:05:12 - loss: 0.3836 - regression_loss: 0.3439 - classification_loss: 0.0397
 1538/10000 [===>..........................] - ETA: 2:05:11 - loss: 0.3838 - regression_loss: 0.3441 - classification_loss: 0.0397
 1539/10000 [===>..........................] - ETA: 2:05:10 - loss: 0.3836 - regression_loss: 0.3439 - classification_loss: 0.0397
 1540/10000 [===>..........................] - ETA: 2:05:09 - loss: 0.3835 - regression_loss: 0.3438 - classification_loss: 0.0397
 1541/10000 [===>..........................] - ETA: 2:05:09 - loss: 0.3833 - regression_loss: 0.3437 - classification_loss: 0.0397
 1542/10000 [===>..........................] - ETA: 2:05:08 - loss: 0.3837 - regression_loss: 0.3440 - classification_loss: 0.0397
 1543/10000 [===>..........................] - ETA: 2:05:07 - loss: 0.3838 - regression_loss: 0.3441 - classification_loss: 0.0397
 1544/10000 [===>..........................] - ETA: 2:05:06 - loss: 0.3837 - regression_loss: 0.3441 - classification_loss: 0.0397
 1545/10000 [===>..........................] - ETA: 2:05:05 - loss: 0.3835 - regression_loss: 0.3438 - classification_loss: 0.0397
 1546/10000 [===>..........................] - ETA: 2:05:04 - loss: 0.3835 - regression_loss: 0.3438 - classification_loss: 0.0396
 1547/10000 [===>..........................] - ETA: 2:05:03 - loss: 0.3832 - regression_loss: 0.3436 - classification_loss: 0.0396
 1548/10000 [===>..........................] - ETA: 2:05:02 - loss: 0.3832 - regression_loss: 0.3436 - classification_loss: 0.0396
 1549/10000 [===>..........................] - ETA: 2:05:02 - loss: 0.3831 - regression_loss: 0.3435 - classification_loss: 0.0396
 1550/10000 [===>..........................] - ETA: 2:05:01 - loss: 0.3834 - regression_loss: 0.3438 - classification_loss: 0.0396
 1551/10000 [===>..........................] - ETA: 2:05:00 - loss: 0.3832 - regression_loss: 0.3436 - classification_loss: 0.0396
 1552/10000 [===>..........................] - ETA: 2:04:59 - loss: 0.3831 - regression_loss: 0.3435 - classification_loss: 0.0396
 1553/10000 [===>..........................] - ETA: 2:04:58 - loss: 0.3830 - regression_loss: 0.3434 - classification_loss: 0.0396
 1554/10000 [===>..........................] - ETA: 2:04:57 - loss: 0.3829 - regression_loss: 0.3434 - classification_loss: 0.0396
 1555/10000 [===>..........................] - ETA: 2:04:56 - loss: 0.3830 - regression_loss: 0.3434 - classification_loss: 0.0396
 1556/10000 [===>..........................] - ETA: 2:04:55 - loss: 0.3831 - regression_loss: 0.3436 - classification_loss: 0.0395
 1557/10000 [===>..........................] - ETA: 2:04:54 - loss: 0.3831 - regression_loss: 0.3436 - classification_loss: 0.0396
 1558/10000 [===>..........................] - ETA: 2:04:54 - loss: 0.3830 - regression_loss: 0.3435 - classification_loss: 0.0395
 1559/10000 [===>..........................] - ETA: 2:04:53 - loss: 0.3829 - regression_loss: 0.3433 - classification_loss: 0.0396
 1560/10000 [===>..........................] - ETA: 2:04:52 - loss: 0.3829 - regression_loss: 0.3433 - classification_loss: 0.0395
 1561/10000 [===>..........................] - ETA: 2:04:51 - loss: 0.3829 - regression_loss: 0.3434 - classification_loss: 0.0395
 1562/10000 [===>..........................] - ETA: 2:04:50 - loss: 0.3839 - regression_loss: 0.3442 - classification_loss: 0.0397
 1563/10000 [===>..........................] - ETA: 2:04:49 - loss: 0.3841 - regression_loss: 0.3444 - classification_loss: 0.0397
 1564/10000 [===>..........................] - ETA: 2:04:48 - loss: 0.3842 - regression_loss: 0.3445 - classification_loss: 0.0397
 1565/10000 [===>..........................] - ETA: 2:04:47 - loss: 0.3842 - regression_loss: 0.3444 - classification_loss: 0.0397
 1566/10000 [===>..........................] - ETA: 2:04:47 - loss: 0.3840 - regression_loss: 0.3443 - classification_loss: 0.0397
 1567/10000 [===>..........................] - ETA: 2:04:46 - loss: 0.3840 - regression_loss: 0.3443 - classification_loss: 0.0397
 1568/10000 [===>..........................] - ETA: 2:04:45 - loss: 0.3840 - regression_loss: 0.3443 - classification_loss: 0.0397
 1569/10000 [===>..........................] - ETA: 2:04:44 - loss: 0.3838 - regression_loss: 0.3442 - classification_loss: 0.0397
 1570/10000 [===>..........................] - ETA: 2:04:43 - loss: 0.3839 - regression_loss: 0.3442 - classification_loss: 0.0396
 1571/10000 [===>..........................] - ETA: 2:04:42 - loss: 0.3838 - regression_loss: 0.3442 - classification_loss: 0.0396
 1572/10000 [===>..........................] - ETA: 2:04:41 - loss: 0.3839 - regression_loss: 0.3443 - classification_loss: 0.0396
 1573/10000 [===>..........................] - ETA: 2:04:40 - loss: 0.3838 - regression_loss: 0.3442 - classification_loss: 0.0396
 1574/10000 [===>..........................] - ETA: 2:04:39 - loss: 0.3843 - regression_loss: 0.3447 - classification_loss: 0.0396
 1575/10000 [===>..........................] - ETA: 2:04:39 - loss: 0.3844 - regression_loss: 0.3447 - classification_loss: 0.0396
 1576/10000 [===>..........................] - ETA: 2:04:38 - loss: 0.3843 - regression_loss: 0.3447 - classification_loss: 0.0396
 1577/10000 [===>..........................] - ETA: 2:04:37 - loss: 0.3843 - regression_loss: 0.3447 - classification_loss: 0.0396
 1578/10000 [===>..........................] - ETA: 2:04:36 - loss: 0.3843 - regression_loss: 0.3448 - classification_loss: 0.0396
 1579/10000 [===>..........................] - ETA: 2:04:35 - loss: 0.3842 - regression_loss: 0.3446 - classification_loss: 0.0396
 1580/10000 [===>..........................] - ETA: 2:04:34 - loss: 0.3843 - regression_loss: 0.3447 - classification_loss: 0.0396
 1581/10000 [===>..........................] - ETA: 2:04:33 - loss: 0.3845 - regression_loss: 0.3449 - classification_loss: 0.0396
 1582/10000 [===>..........................] - ETA: 2:04:32 - loss: 0.3845 - regression_loss: 0.3449 - classification_loss: 0.0396
 1583/10000 [===>..........................] - ETA: 2:04:31 - loss: 0.3847 - regression_loss: 0.3451 - classification_loss: 0.0397
 1584/10000 [===>..........................] - ETA: 2:04:31 - loss: 0.3846 - regression_loss: 0.3450 - classification_loss: 0.0397
 1585/10000 [===>..........................] - ETA: 2:04:30 - loss: 0.3847 - regression_loss: 0.3451 - classification_loss: 0.0397
 1586/10000 [===>..........................] - ETA: 2:04:29 - loss: 0.3846 - regression_loss: 0.3450 - classification_loss: 0.0397
 1587/10000 [===>..........................] - ETA: 2:04:28 - loss: 0.3846 - regression_loss: 0.3450 - classification_loss: 0.0396
 1588/10000 [===>..........................] - ETA: 2:04:27 - loss: 0.3845 - regression_loss: 0.3449 - classification_loss: 0.0396
 1589/10000 [===>..........................] - ETA: 2:04:26 - loss: 0.3844 - regression_loss: 0.3448 - classification_loss: 0.0396
 1590/10000 [===>..........................] - ETA: 2:04:25 - loss: 0.3846 - regression_loss: 0.3450 - classification_loss: 0.0396
 1591/10000 [===>..........................] - ETA: 2:04:24 - loss: 0.3848 - regression_loss: 0.3452 - classification_loss: 0.0396
 1592/10000 [===>..........................] - ETA: 2:04:23 - loss: 0.3847 - regression_loss: 0.3451 - classification_loss: 0.0396
 1593/10000 [===>..........................] - ETA: 2:04:23 - loss: 0.3849 - regression_loss: 0.3453 - classification_loss: 0.0396
 1594/10000 [===>..........................] - ETA: 2:04:22 - loss: 0.3846 - regression_loss: 0.3451 - classification_loss: 0.0395
 1595/10000 [===>..........................] - ETA: 2:04:21 - loss: 0.3845 - regression_loss: 0.3449 - classification_loss: 0.0395
 1596/10000 [===>..........................] - ETA: 2:04:20 - loss: 0.3847 - regression_loss: 0.3452 - classification_loss: 0.0395
 1597/10000 [===>..........................] - ETA: 2:04:19 - loss: 0.3848 - regression_loss: 0.3453 - classification_loss: 0.0395
 1598/10000 [===>..........................] - ETA: 2:04:18 - loss: 0.3848 - regression_loss: 0.3453 - classification_loss: 0.0395
 1599/10000 [===>..........................] - ETA: 2:04:17 - loss: 0.3850 - regression_loss: 0.3455 - classification_loss: 0.0395
 1600/10000 [===>..........................] - ETA: 2:04:16 - loss: 0.3850 - regression_loss: 0.3455 - classification_loss: 0.0395
 1601/10000 [===>..........................] - ETA: 2:04:15 - loss: 0.3850 - regression_loss: 0.3456 - classification_loss: 0.0395
 1602/10000 [===>..........................] - ETA: 2:04:15 - loss: 0.3849 - regression_loss: 0.3454 - classification_loss: 0.0394
 1603/10000 [===>..........................] - ETA: 2:04:14 - loss: 0.3848 - regression_loss: 0.3454 - classification_loss: 0.0394
 1604/10000 [===>..........................] - ETA: 2:04:13 - loss: 0.3847 - regression_loss: 0.3453 - classification_loss: 0.0394
 1605/10000 [===>..........................] - ETA: 2:04:12 - loss: 0.3845 - regression_loss: 0.3451 - classification_loss: 0.0394
 1606/10000 [===>..........................] - ETA: 2:04:11 - loss: 0.3843 - regression_loss: 0.3449 - classification_loss: 0.0394
 1607/10000 [===>..........................] - ETA: 2:04:10 - loss: 0.3844 - regression_loss: 0.3450 - classification_loss: 0.0394
 1608/10000 [===>..........................] - ETA: 2:04:09 - loss: 0.3845 - regression_loss: 0.3451 - classification_loss: 0.0394
 1609/10000 [===>..........................] - ETA: 2:04:08 - loss: 0.3843 - regression_loss: 0.3449 - classification_loss: 0.0394
 1610/10000 [===>..........................] - ETA: 2:04:08 - loss: 0.3842 - regression_loss: 0.3448 - classification_loss: 0.0394
 1611/10000 [===>..........................] - ETA: 2:04:07 - loss: 0.3840 - regression_loss: 0.3446 - classification_loss: 0.0394
 1612/10000 [===>..........................] - ETA: 2:04:06 - loss: 0.3841 - regression_loss: 0.3447 - classification_loss: 0.0394
 1613/10000 [===>..........................] - ETA: 2:04:05 - loss: 0.3849 - regression_loss: 0.3454 - classification_loss: 0.0394
 1614/10000 [===>..........................] - ETA: 2:04:04 - loss: 0.3848 - regression_loss: 0.3454 - classification_loss: 0.0394
 1615/10000 [===>..........................] - ETA: 2:04:03 - loss: 0.3848 - regression_loss: 0.3454 - classification_loss: 0.0394
 1616/10000 [===>..........................] - ETA: 2:04:02 - loss: 0.3848 - regression_loss: 0.3453 - classification_loss: 0.0394
 1617/10000 [===>..........................] - ETA: 2:04:01 - loss: 0.3852 - regression_loss: 0.3457 - classification_loss: 0.0395
 1618/10000 [===>..........................] - ETA: 2:04:00 - loss: 0.3854 - regression_loss: 0.3459 - classification_loss: 0.0395
 1619/10000 [===>..........................] - ETA: 2:04:00 - loss: 0.3853 - regression_loss: 0.3458 - classification_loss: 0.0395
 1620/10000 [===>..........................] - ETA: 2:03:59 - loss: 0.3852 - regression_loss: 0.3457 - classification_loss: 0.0395
 1621/10000 [===>..........................] - ETA: 2:03:58 - loss: 0.3852 - regression_loss: 0.3457 - classification_loss: 0.0395
 1622/10000 [===>..........................] - ETA: 2:03:57 - loss: 0.3851 - regression_loss: 0.3457 - classification_loss: 0.0395
 1623/10000 [===>..........................] - ETA: 2:03:56 - loss: 0.3853 - regression_loss: 0.3457 - classification_loss: 0.0396
 1624/10000 [===>..........................] - ETA: 2:03:55 - loss: 0.3856 - regression_loss: 0.3460 - classification_loss: 0.0396
 1625/10000 [===>..........................] - ETA: 2:03:54 - loss: 0.3855 - regression_loss: 0.3459 - classification_loss: 0.0396
 1626/10000 [===>..........................] - ETA: 2:03:53 - loss: 0.3854 - regression_loss: 0.3458 - classification_loss: 0.0396
 1627/10000 [===>..........................] - ETA: 2:03:52 - loss: 0.3854 - regression_loss: 0.3458 - classification_loss: 0.0396
 1628/10000 [===>..........................] - ETA: 2:03:52 - loss: 0.3858 - regression_loss: 0.3462 - classification_loss: 0.0396
 1629/10000 [===>..........................] - ETA: 2:03:51 - loss: 0.3858 - regression_loss: 0.3462 - classification_loss: 0.0397
 1630/10000 [===>..........................] - ETA: 2:03:50 - loss: 0.3859 - regression_loss: 0.3463 - classification_loss: 0.0397
 1631/10000 [===>..........................] - ETA: 2:03:49 - loss: 0.3858 - regression_loss: 0.3462 - classification_loss: 0.0397
 1632/10000 [===>..........................] - ETA: 2:03:48 - loss: 0.3859 - regression_loss: 0.3463 - classification_loss: 0.0396
 1633/10000 [===>..........................] - ETA: 2:03:47 - loss: 0.3858 - regression_loss: 0.3462 - classification_loss: 0.0396
 1634/10000 [===>..........................] - ETA: 2:03:46 - loss: 0.3861 - regression_loss: 0.3464 - classification_loss: 0.0397
 1635/10000 [===>..........................] - ETA: 2:03:45 - loss: 0.3865 - regression_loss: 0.3463 - classification_loss: 0.0402
 1636/10000 [===>..........................] - ETA: 2:03:45 - loss: 0.3863 - regression_loss: 0.3462 - classification_loss: 0.0401
 1637/10000 [===>..........................] - ETA: 2:03:44 - loss: 0.3862 - regression_loss: 0.3461 - classification_loss: 0.0401
 1638/10000 [===>..........................] - ETA: 2:03:43 - loss: 0.3862 - regression_loss: 0.3461 - classification_loss: 0.0401
 1639/10000 [===>..........................] - ETA: 2:03:42 - loss: 0.3864 - regression_loss: 0.3462 - classification_loss: 0.0401
 1640/10000 [===>..........................] - ETA: 2:03:41 - loss: 0.3861 - regression_loss: 0.3460 - classification_loss: 0.0401
 1641/10000 [===>..........................] - ETA: 2:03:40 - loss: 0.3860 - regression_loss: 0.3459 - classification_loss: 0.0401
 1642/10000 [===>..........................] - ETA: 2:03:39 - loss: 0.3860 - regression_loss: 0.3459 - classification_loss: 0.0401
 1643/10000 [===>..........................] - ETA: 2:03:38 - loss: 0.3862 - regression_loss: 0.3461 - classification_loss: 0.0401
 1644/10000 [===>..........................] - ETA: 2:03:37 - loss: 0.3862 - regression_loss: 0.3461 - classification_loss: 0.0401
 1645/10000 [===>..........................] - ETA: 2:03:37 - loss: 0.3864 - regression_loss: 0.3462 - classification_loss: 0.0401
 1646/10000 [===>..........................] - ETA: 2:03:36 - loss: 0.3864 - regression_loss: 0.3463 - classification_loss: 0.0402
 1647/10000 [===>..........................] - ETA: 2:03:35 - loss: 0.3864 - regression_loss: 0.3463 - classification_loss: 0.0402
 1648/10000 [===>..........................] - ETA: 2:03:34 - loss: 0.3866 - regression_loss: 0.3464 - classification_loss: 0.0402
 1649/10000 [===>..........................] - ETA: 2:03:33 - loss: 0.3868 - regression_loss: 0.3466 - classification_loss: 0.0402
 1650/10000 [===>..........................] - ETA: 2:03:32 - loss: 0.3869 - regression_loss: 0.3467 - classification_loss: 0.0402
 1651/10000 [===>..........................] - ETA: 2:03:31 - loss: 0.3867 - regression_loss: 0.3466 - classification_loss: 0.0402
 1652/10000 [===>..........................] - ETA: 2:03:30 - loss: 0.3866 - regression_loss: 0.3465 - classification_loss: 0.0402
 1653/10000 [===>..........................] - ETA: 2:03:29 - loss: 0.3866 - regression_loss: 0.3464 - classification_loss: 0.0402
 1654/10000 [===>..........................] - ETA: 2:03:29 - loss: 0.3866 - regression_loss: 0.3465 - classification_loss: 0.0401
 1655/10000 [===>..........................] - ETA: 2:03:28 - loss: 0.3867 - regression_loss: 0.3465 - classification_loss: 0.0401
 1656/10000 [===>..........................] - ETA: 2:03:27 - loss: 0.3865 - regression_loss: 0.3464 - classification_loss: 0.0401
 1657/10000 [===>..........................] - ETA: 2:03:26 - loss: 0.3865 - regression_loss: 0.3463 - classification_loss: 0.0401
 1658/10000 [===>..........................] - ETA: 2:03:25 - loss: 0.3863 - regression_loss: 0.3462 - classification_loss: 0.0401
 1659/10000 [===>..........................] - ETA: 2:03:24 - loss: 0.3867 - regression_loss: 0.3466 - classification_loss: 0.0401
 1660/10000 [===>..........................] - ETA: 2:03:23 - loss: 0.3869 - regression_loss: 0.3467 - classification_loss: 0.0401
 1661/10000 [===>..........................] - ETA: 2:03:22 - loss: 0.3871 - regression_loss: 0.3470 - classification_loss: 0.0402
 1662/10000 [===>..........................] - ETA: 2:03:21 - loss: 0.3871 - regression_loss: 0.3470 - classification_loss: 0.0402
 1663/10000 [===>..........................] - ETA: 2:03:21 - loss: 0.3870 - regression_loss: 0.3469 - classification_loss: 0.0401
 1664/10000 [===>..........................] - ETA: 2:03:20 - loss: 0.3868 - regression_loss: 0.3467 - classification_loss: 0.0401
 1665/10000 [===>..........................] - ETA: 2:03:19 - loss: 0.3875 - regression_loss: 0.3470 - classification_loss: 0.0405
 1666/10000 [===>..........................] - ETA: 2:03:18 - loss: 0.3874 - regression_loss: 0.3470 - classification_loss: 0.0405
 1667/10000 [====>.........................] - ETA: 2:03:17 - loss: 0.3874 - regression_loss: 0.3469 - classification_loss: 0.0404
 1668/10000 [====>.........................] - ETA: 2:03:16 - loss: 0.3872 - regression_loss: 0.3468 - classification_loss: 0.0404
 1669/10000 [====>.........................] - ETA: 2:03:15 - loss: 0.3873 - regression_loss: 0.3468 - classification_loss: 0.0404
 1670/10000 [====>.........................] - ETA: 2:03:14 - loss: 0.3870 - regression_loss: 0.3466 - classification_loss: 0.0404
 1671/10000 [====>.........................] - ETA: 2:03:13 - loss: 0.3870 - regression_loss: 0.3466 - classification_loss: 0.0404
 1672/10000 [====>.........................] - ETA: 2:03:13 - loss: 0.3868 - regression_loss: 0.3464 - classification_loss: 0.0404
 1673/10000 [====>.........................] - ETA: 2:03:12 - loss: 0.3869 - regression_loss: 0.3465 - classification_loss: 0.0404
 1674/10000 [====>.........................] - ETA: 2:03:11 - loss: 0.3867 - regression_loss: 0.3464 - classification_loss: 0.0403
 1675/10000 [====>.........................] - ETA: 2:03:10 - loss: 0.3869 - regression_loss: 0.3464 - classification_loss: 0.0405
 1676/10000 [====>.........................] - ETA: 2:03:09 - loss: 0.3868 - regression_loss: 0.3463 - classification_loss: 0.0405
 1677/10000 [====>.........................] - ETA: 2:03:08 - loss: 0.3868 - regression_loss: 0.3463 - classification_loss: 0.0405
 1678/10000 [====>.........................] - ETA: 2:03:07 - loss: 0.3867 - regression_loss: 0.3462 - classification_loss: 0.0405
 1679/10000 [====>.........................] - ETA: 2:03:06 - loss: 0.3867 - regression_loss: 0.3462 - classification_loss: 0.0405
 1680/10000 [====>.........................] - ETA: 2:03:05 - loss: 0.3868 - regression_loss: 0.3463 - classification_loss: 0.0405
 1681/10000 [====>.........................] - ETA: 2:03:05 - loss: 0.3870 - regression_loss: 0.3466 - classification_loss: 0.0405
 1682/10000 [====>.........................] - ETA: 2:03:04 - loss: 0.3868 - regression_loss: 0.3464 - classification_loss: 0.0404
 1683/10000 [====>.........................] - ETA: 2:03:03 - loss: 0.3867 - regression_loss: 0.3463 - classification_loss: 0.0404
 1684/10000 [====>.........................] - ETA: 2:03:02 - loss: 0.3866 - regression_loss: 0.3462 - classification_loss: 0.0404
 1685/10000 [====>.........................] - ETA: 2:03:01 - loss: 0.3868 - regression_loss: 0.3464 - classification_loss: 0.0404
 1686/10000 [====>.........................] - ETA: 2:03:00 - loss: 0.3869 - regression_loss: 0.3463 - classification_loss: 0.0406
 1687/10000 [====>.........................] - ETA: 2:02:59 - loss: 0.3870 - regression_loss: 0.3464 - classification_loss: 0.0406
 1688/10000 [====>.........................] - ETA: 2:02:58 - loss: 0.3869 - regression_loss: 0.3463 - classification_loss: 0.0406
 1689/10000 [====>.........................] - ETA: 2:02:58 - loss: 0.3868 - regression_loss: 0.3462 - classification_loss: 0.0407
 1690/10000 [====>.........................] - ETA: 2:02:57 - loss: 0.3869 - regression_loss: 0.3463 - classification_loss: 0.0406
 1691/10000 [====>.........................] - ETA: 2:02:56 - loss: 0.3867 - regression_loss: 0.3461 - classification_loss: 0.0406
 1692/10000 [====>.........................] - ETA: 2:02:55 - loss: 0.3866 - regression_loss: 0.3460 - classification_loss: 0.0406
 1693/10000 [====>.........................] - ETA: 2:02:54 - loss: 0.3866 - regression_loss: 0.3460 - classification_loss: 0.0406
 1694/10000 [====>.........................] - ETA: 2:02:53 - loss: 0.3866 - regression_loss: 0.3460 - classification_loss: 0.0406
 1695/10000 [====>.........................] - ETA: 2:02:52 - loss: 0.3868 - regression_loss: 0.3462 - classification_loss: 0.0406
 1696/10000 [====>.........................] - ETA: 2:02:51 - loss: 0.3867 - regression_loss: 0.3462 - classification_loss: 0.0405
 1697/10000 [====>.........................] - ETA: 2:02:50 - loss: 0.3867 - regression_loss: 0.3462 - classification_loss: 0.0405
 1698/10000 [====>.........................] - ETA: 2:02:50 - loss: 0.3867 - regression_loss: 0.3462 - classification_loss: 0.0405
 1699/10000 [====>.........................] - ETA: 2:02:49 - loss: 0.3867 - regression_loss: 0.3462 - classification_loss: 0.0405
 1700/10000 [====>.........................] - ETA: 2:02:48 - loss: 0.3865 - regression_loss: 0.3461 - classification_loss: 0.0405
 1701/10000 [====>.........................] - ETA: 2:02:47 - loss: 0.3865 - regression_loss: 0.3461 - classification_loss: 0.0404
 1702/10000 [====>.........................] - ETA: 2:02:46 - loss: 0.3865 - regression_loss: 0.3461 - classification_loss: 0.0404
 1703/10000 [====>.........................] - ETA: 2:02:45 - loss: 0.3866 - regression_loss: 0.3461 - classification_loss: 0.0405
 1704/10000 [====>.........................] - ETA: 2:02:44 - loss: 0.3864 - regression_loss: 0.3459 - classification_loss: 0.0404
 1705/10000 [====>.........................] - ETA: 2:02:43 - loss: 0.3865 - regression_loss: 0.3460 - classification_loss: 0.0405
 1706/10000 [====>.........................] - ETA: 2:02:42 - loss: 0.3863 - regression_loss: 0.3459 - classification_loss: 0.0404
 1707/10000 [====>.........................] - ETA: 2:02:42 - loss: 0.3862 - regression_loss: 0.3458 - classification_loss: 0.0404
 1708/10000 [====>.........................] - ETA: 2:02:41 - loss: 0.3863 - regression_loss: 0.3459 - classification_loss: 0.0404
 1709/10000 [====>.........................] - ETA: 2:02:40 - loss: 0.3864 - regression_loss: 0.3460 - classification_loss: 0.0404
 1710/10000 [====>.........................] - ETA: 2:02:39 - loss: 0.3864 - regression_loss: 0.3460 - classification_loss: 0.0404
 1711/10000 [====>.........................] - ETA: 2:02:38 - loss: 0.3865 - regression_loss: 0.3461 - classification_loss: 0.0404
 1712/10000 [====>.........................] - ETA: 2:02:37 - loss: 0.3865 - regression_loss: 0.3461 - classification_loss: 0.0404
 1713/10000 [====>.........................] - ETA: 2:02:36 - loss: 0.3864 - regression_loss: 0.3460 - classification_loss: 0.0404
 1714/10000 [====>.........................] - ETA: 2:02:35 - loss: 0.3865 - regression_loss: 0.3461 - classification_loss: 0.0404
 1715/10000 [====>.........................] - ETA: 2:02:34 - loss: 0.3866 - regression_loss: 0.3462 - classification_loss: 0.0404
 1716/10000 [====>.........................] - ETA: 2:02:34 - loss: 0.3867 - regression_loss: 0.3463 - classification_loss: 0.0404
 1717/10000 [====>.........................] - ETA: 2:02:33 - loss: 0.3869 - regression_loss: 0.3465 - classification_loss: 0.0404
 1718/10000 [====>.........................] - ETA: 2:02:32 - loss: 0.3869 - regression_loss: 0.3465 - classification_loss: 0.0404
 1719/10000 [====>.........................] - ETA: 2:02:31 - loss: 0.3868 - regression_loss: 0.3464 - classification_loss: 0.0404
 1720/10000 [====>.........................] - ETA: 2:02:30 - loss: 0.3871 - regression_loss: 0.3466 - classification_loss: 0.0404
 1721/10000 [====>.........................] - ETA: 2:02:29 - loss: 0.3870 - regression_loss: 0.3466 - classification_loss: 0.0404
 1722/10000 [====>.........................] - ETA: 2:02:28 - loss: 0.3870 - regression_loss: 0.3466 - classification_loss: 0.0404
 1723/10000 [====>.........................] - ETA: 2:02:27 - loss: 0.3869 - regression_loss: 0.3465 - classification_loss: 0.0404
 1724/10000 [====>.........................] - ETA: 2:02:26 - loss: 0.3870 - regression_loss: 0.3466 - classification_loss: 0.0404
 1725/10000 [====>.........................] - ETA: 2:02:26 - loss: 0.3870 - regression_loss: 0.3466 - classification_loss: 0.0404
 1726/10000 [====>.........................] - ETA: 2:02:25 - loss: 0.3870 - regression_loss: 0.3466 - classification_loss: 0.0404
 1727/10000 [====>.........................] - ETA: 2:02:24 - loss: 0.3871 - regression_loss: 0.3466 - classification_loss: 0.0404
 1728/10000 [====>.........................] - ETA: 2:02:23 - loss: 0.3868 - regression_loss: 0.3464 - classification_loss: 0.0404
 1729/10000 [====>.........................] - ETA: 2:02:22 - loss: 0.3868 - regression_loss: 0.3464 - classification_loss: 0.0404
 1730/10000 [====>.........................] - ETA: 2:02:21 - loss: 0.3866 - regression_loss: 0.3462 - classification_loss: 0.0404
 1731/10000 [====>.........................] - ETA: 2:02:20 - loss: 0.3865 - regression_loss: 0.3462 - classification_loss: 0.0403
 1732/10000 [====>.........................] - ETA: 2:02:19 - loss: 0.3864 - regression_loss: 0.3461 - classification_loss: 0.0403
 1733/10000 [====>.........................] - ETA: 2:02:18 - loss: 0.3865 - regression_loss: 0.3462 - classification_loss: 0.0403
 1734/10000 [====>.........................] - ETA: 2:02:18 - loss: 0.3867 - regression_loss: 0.3463 - classification_loss: 0.0404
 1735/10000 [====>.........................] - ETA: 2:02:17 - loss: 0.3867 - regression_loss: 0.3464 - classification_loss: 0.0404
 1736/10000 [====>.........................] - ETA: 2:02:16 - loss: 0.3868 - regression_loss: 0.3464 - classification_loss: 0.0404
 1737/10000 [====>.........................] - ETA: 2:02:15 - loss: 0.3868 - regression_loss: 0.3464 - classification_loss: 0.0404
 1738/10000 [====>.........................] - ETA: 2:02:14 - loss: 0.3866 - regression_loss: 0.3462 - classification_loss: 0.0404
 1739/10000 [====>.........................] - ETA: 2:02:13 - loss: 0.3864 - regression_loss: 0.3461 - classification_loss: 0.0404
 1740/10000 [====>.........................] - ETA: 2:02:12 - loss: 0.3864 - regression_loss: 0.3460 - classification_loss: 0.0404
 1741/10000 [====>.........................] - ETA: 2:02:11 - loss: 0.3862 - regression_loss: 0.3459 - classification_loss: 0.0404
 1742/10000 [====>.........................] - ETA: 2:02:10 - loss: 0.3862 - regression_loss: 0.3459 - classification_loss: 0.0404
 1743/10000 [====>.........................] - ETA: 2:02:10 - loss: 0.3862 - regression_loss: 0.3459 - classification_loss: 0.0404
 1744/10000 [====>.........................] - ETA: 2:02:09 - loss: 0.3861 - regression_loss: 0.3458 - classification_loss: 0.0404
 1745/10000 [====>.........................] - ETA: 2:02:08 - loss: 0.3861 - regression_loss: 0.3458 - classification_loss: 0.0403
 1746/10000 [====>.........................] - ETA: 2:02:07 - loss: 0.3860 - regression_loss: 0.3457 - classification_loss: 0.0403
 1747/10000 [====>.........................] - ETA: 2:02:06 - loss: 0.3861 - regression_loss: 0.3457 - classification_loss: 0.0404
 1748/10000 [====>.........................] - ETA: 2:02:05 - loss: 0.3859 - regression_loss: 0.3456 - classification_loss: 0.0403
 1749/10000 [====>.........................] - ETA: 2:02:04 - loss: 0.3863 - regression_loss: 0.3460 - classification_loss: 0.0403
 1750/10000 [====>.........................] - ETA: 2:02:03 - loss: 0.3865 - regression_loss: 0.3461 - classification_loss: 0.0403
 1751/10000 [====>.........................] - ETA: 2:02:03 - loss: 0.3864 - regression_loss: 0.3461 - classification_loss: 0.0403
 1752/10000 [====>.........................] - ETA: 2:02:02 - loss: 0.3864 - regression_loss: 0.3460 - classification_loss: 0.0403
 1753/10000 [====>.........................] - ETA: 2:02:01 - loss: 0.3864 - regression_loss: 0.3460 - classification_loss: 0.0403
 1754/10000 [====>.........................] - ETA: 2:02:00 - loss: 0.3869 - regression_loss: 0.3465 - classification_loss: 0.0404
 1755/10000 [====>.........................] - ETA: 2:01:59 - loss: 0.3868 - regression_loss: 0.3464 - classification_loss: 0.0404
 1756/10000 [====>.........................] - ETA: 2:01:58 - loss: 0.3867 - regression_loss: 0.3463 - classification_loss: 0.0404
 1757/10000 [====>.........................] - ETA: 2:01:57 - loss: 0.3867 - regression_loss: 0.3464 - classification_loss: 0.0404
 1758/10000 [====>.........................] - ETA: 2:01:56 - loss: 0.3867 - regression_loss: 0.3463 - classification_loss: 0.0404
 1759/10000 [====>.........................] - ETA: 2:01:55 - loss: 0.3866 - regression_loss: 0.3462 - classification_loss: 0.0404
 1760/10000 [====>.........................] - ETA: 2:01:55 - loss: 0.3864 - regression_loss: 0.3460 - classification_loss: 0.0404
 1761/10000 [====>.........................] - ETA: 2:01:54 - loss: 0.3863 - regression_loss: 0.3459 - classification_loss: 0.0404
 1762/10000 [====>.........................] - ETA: 2:01:53 - loss: 0.3862 - regression_loss: 0.3459 - classification_loss: 0.0403
 1763/10000 [====>.........................] - ETA: 2:01:52 - loss: 0.3861 - regression_loss: 0.3458 - classification_loss: 0.0403
 1764/10000 [====>.........................] - ETA: 2:01:51 - loss: 0.3863 - regression_loss: 0.3460 - classification_loss: 0.0403
 1765/10000 [====>.........................] - ETA: 2:01:50 - loss: 0.3862 - regression_loss: 0.3459 - classification_loss: 0.0403
 1766/10000 [====>.........................] - ETA: 2:01:49 - loss: 0.3861 - regression_loss: 0.3458 - classification_loss: 0.0403
 1767/10000 [====>.........................] - ETA: 2:01:48 - loss: 0.3862 - regression_loss: 0.3459 - classification_loss: 0.0403
 1768/10000 [====>.........................] - ETA: 2:01:47 - loss: 0.3862 - regression_loss: 0.3459 - classification_loss: 0.0403
 1769/10000 [====>.........................] - ETA: 2:01:47 - loss: 0.3861 - regression_loss: 0.3459 - classification_loss: 0.0403
 1770/10000 [====>.........................] - ETA: 2:01:46 - loss: 0.3865 - regression_loss: 0.3461 - classification_loss: 0.0403
 1771/10000 [====>.........................] - ETA: 2:01:45 - loss: 0.3864 - regression_loss: 0.3461 - classification_loss: 0.0403
 1772/10000 [====>.........................] - ETA: 2:01:44 - loss: 0.3865 - regression_loss: 0.3462 - classification_loss: 0.0403
 1773/10000 [====>.........................] - ETA: 2:01:43 - loss: 0.3865 - regression_loss: 0.3462 - classification_loss: 0.0403
 1774/10000 [====>.........................] - ETA: 2:01:42 - loss: 0.3868 - regression_loss: 0.3465 - classification_loss: 0.0403
 1775/10000 [====>.........................] - ETA: 2:01:41 - loss: 0.3867 - regression_loss: 0.3464 - classification_loss: 0.0403
 1776/10000 [====>.........................] - ETA: 2:01:40 - loss: 0.3867 - regression_loss: 0.3464 - classification_loss: 0.0403
 1777/10000 [====>.........................] - ETA: 2:01:39 - loss: 0.3868 - regression_loss: 0.3465 - classification_loss: 0.0403
 1778/10000 [====>.........................] - ETA: 2:01:39 - loss: 0.3868 - regression_loss: 0.3465 - classification_loss: 0.0403
 1779/10000 [====>.........................] - ETA: 2:01:38 - loss: 0.3867 - regression_loss: 0.3464 - classification_loss: 0.0403
 1780/10000 [====>.........................] - ETA: 2:01:37 - loss: 0.3867 - regression_loss: 0.3464 - classification_loss: 0.0403
 1781/10000 [====>.........................] - ETA: 2:01:36 - loss: 0.3866 - regression_loss: 0.3463 - classification_loss: 0.0402
 1782/10000 [====>.........................] - ETA: 2:01:35 - loss: 0.3867 - regression_loss: 0.3464 - classification_loss: 0.0402
 1783/10000 [====>.........................] - ETA: 2:01:34 - loss: 0.3866 - regression_loss: 0.3464 - classification_loss: 0.0402
 1784/10000 [====>.........................] - ETA: 2:01:33 - loss: 0.3866 - regression_loss: 0.3464 - classification_loss: 0.0402
 1785/10000 [====>.........................] - ETA: 2:01:32 - loss: 0.3866 - regression_loss: 0.3464 - classification_loss: 0.0402
 1786/10000 [====>.........................] - ETA: 2:01:31 - loss: 0.3865 - regression_loss: 0.3464 - classification_loss: 0.0402
 1787/10000 [====>.........................] - ETA: 2:01:31 - loss: 0.3867 - regression_loss: 0.3465 - classification_loss: 0.0402
 1788/10000 [====>.........................] - ETA: 2:01:30 - loss: 0.3870 - regression_loss: 0.3467 - classification_loss: 0.0402
 1789/10000 [====>.........................] - ETA: 2:01:29 - loss: 0.3868 - regression_loss: 0.3466 - classification_loss: 0.0402
 1790/10000 [====>.........................] - ETA: 2:01:28 - loss: 0.3868 - regression_loss: 0.3465 - classification_loss: 0.0402
 1791/10000 [====>.........................] - ETA: 2:01:27 - loss: 0.3866 - regression_loss: 0.3464 - classification_loss: 0.0402
 1792/10000 [====>.........................] - ETA: 2:01:26 - loss: 0.3866 - regression_loss: 0.3464 - classification_loss: 0.0402
 1793/10000 [====>.........................] - ETA: 2:01:25 - loss: 0.3865 - regression_loss: 0.3463 - classification_loss: 0.0402
 1794/10000 [====>.........................] - ETA: 2:01:24 - loss: 0.3863 - regression_loss: 0.3461 - classification_loss: 0.0402
 1795/10000 [====>.........................] - ETA: 2:01:24 - loss: 0.3863 - regression_loss: 0.3461 - classification_loss: 0.0402
 1796/10000 [====>.........................] - ETA: 2:01:23 - loss: 0.3861 - regression_loss: 0.3459 - classification_loss: 0.0402
 1797/10000 [====>.........................] - ETA: 2:01:22 - loss: 0.3863 - regression_loss: 0.3460 - classification_loss: 0.0403
 1798/10000 [====>.........................] - ETA: 2:01:21 - loss: 0.3862 - regression_loss: 0.3459 - classification_loss: 0.0403
 1799/10000 [====>.........................] - ETA: 2:01:20 - loss: 0.3862 - regression_loss: 0.3459 - classification_loss: 0.0403
 1800/10000 [====>.........................] - ETA: 2:01:19 - loss: 0.3862 - regression_loss: 0.3459 - classification_loss: 0.0403
 1801/10000 [====>.........................] - ETA: 2:01:18 - loss: 0.3861 - regression_loss: 0.3458 - classification_loss: 0.0403
 1802/10000 [====>.........................] - ETA: 2:01:17 - loss: 0.3862 - regression_loss: 0.3459 - classification_loss: 0.0403
 1803/10000 [====>.........................] - ETA: 2:01:16 - loss: 0.3863 - regression_loss: 0.3460 - classification_loss: 0.0403
 1804/10000 [====>.........................] - ETA: 2:01:16 - loss: 0.3863 - regression_loss: 0.3460 - classification_loss: 0.0403
 1805/10000 [====>.........................] - ETA: 2:01:15 - loss: 0.3863 - regression_loss: 0.3460 - classification_loss: 0.0403
 1806/10000 [====>.........................] - ETA: 2:01:14 - loss: 0.3862 - regression_loss: 0.3459 - classification_loss: 0.0403
 1807/10000 [====>.........................] - ETA: 2:01:13 - loss: 0.3864 - regression_loss: 0.3461 - classification_loss: 0.0403
 1808/10000 [====>.........................] - ETA: 2:01:12 - loss: 0.3862 - regression_loss: 0.3459 - classification_loss: 0.0403
 1809/10000 [====>.........................] - ETA: 2:01:11 - loss: 0.3862 - regression_loss: 0.3459 - classification_loss: 0.0403
 1810/10000 [====>.........................] - ETA: 2:01:10 - loss: 0.3862 - regression_loss: 0.3459 - classification_loss: 0.0403
 1811/10000 [====>.........................] - ETA: 2:01:09 - loss: 0.3861 - regression_loss: 0.3458 - classification_loss: 0.0403
 1812/10000 [====>.........................] - ETA: 2:01:08 - loss: 0.3859 - regression_loss: 0.3457 - classification_loss: 0.0403
 1813/10000 [====>.........................] - ETA: 2:01:07 - loss: 0.3859 - regression_loss: 0.3456 - classification_loss: 0.0402
 1814/10000 [====>.........................] - ETA: 2:01:07 - loss: 0.3857 - regression_loss: 0.3454 - classification_loss: 0.0402
 1815/10000 [====>.........................] - ETA: 2:01:06 - loss: 0.3855 - regression_loss: 0.3453 - classification_loss: 0.0402
 1816/10000 [====>.........................] - ETA: 2:01:05 - loss: 0.3855 - regression_loss: 0.3453 - classification_loss: 0.0402
 1817/10000 [====>.........................] - ETA: 2:01:04 - loss: 0.3855 - regression_loss: 0.3453 - classification_loss: 0.0402
 1818/10000 [====>.........................] - ETA: 2:01:03 - loss: 0.3857 - regression_loss: 0.3455 - classification_loss: 0.0402
 1819/10000 [====>.........................] - ETA: 2:01:02 - loss: 0.3857 - regression_loss: 0.3455 - classification_loss: 0.0402
 1820/10000 [====>.........................] - ETA: 2:01:01 - loss: 0.3858 - regression_loss: 0.3456 - classification_loss: 0.0402
 1821/10000 [====>.........................] - ETA: 2:01:00 - loss: 0.3857 - regression_loss: 0.3455 - classification_loss: 0.0402
 1822/10000 [====>.........................] - ETA: 2:01:00 - loss: 0.3857 - regression_loss: 0.3456 - classification_loss: 0.0402
 1823/10000 [====>.........................] - ETA: 2:00:59 - loss: 0.3857 - regression_loss: 0.3455 - classification_loss: 0.0402
 1824/10000 [====>.........................] - ETA: 2:00:58 - loss: 0.3858 - regression_loss: 0.3456 - classification_loss: 0.0402
 1825/10000 [====>.........................] - ETA: 2:00:57 - loss: 0.3858 - regression_loss: 0.3456 - classification_loss: 0.0402
 1826/10000 [====>.........................] - ETA: 2:00:56 - loss: 0.3860 - regression_loss: 0.3458 - classification_loss: 0.0402
 1827/10000 [====>.........................] - ETA: 2:00:55 - loss: 0.3860 - regression_loss: 0.3458 - classification_loss: 0.0402
 1828/10000 [====>.........................] - ETA: 2:00:54 - loss: 0.3858 - regression_loss: 0.3456 - classification_loss: 0.0401
 1829/10000 [====>.........................] - ETA: 2:00:53 - loss: 0.3859 - regression_loss: 0.3457 - classification_loss: 0.0402
 1830/10000 [====>.........................] - ETA: 2:00:52 - loss: 0.3862 - regression_loss: 0.3460 - classification_loss: 0.0402
 1831/10000 [====>.........................] - ETA: 2:00:52 - loss: 0.3860 - regression_loss: 0.3458 - classification_loss: 0.0402
 1832/10000 [====>.........................] - ETA: 2:00:51 - loss: 0.3861 - regression_loss: 0.3459 - classification_loss: 0.0402
 1833/10000 [====>.........................] - ETA: 2:00:50 - loss: 0.3861 - regression_loss: 0.3459 - classification_loss: 0.0402
 1834/10000 [====>.........................] - ETA: 2:00:49 - loss: 0.3860 - regression_loss: 0.3458 - classification_loss: 0.0402
 1835/10000 [====>.........................] - ETA: 2:00:48 - loss: 0.3860 - regression_loss: 0.3458 - classification_loss: 0.0402
 1836/10000 [====>.........................] - ETA: 2:00:47 - loss: 0.3859 - regression_loss: 0.3458 - classification_loss: 0.0402
 1837/10000 [====>.........................] - ETA: 2:00:46 - loss: 0.3858 - regression_loss: 0.3457 - classification_loss: 0.0401
 1838/10000 [====>.........................] - ETA: 2:00:45 - loss: 0.3858 - regression_loss: 0.3456 - classification_loss: 0.0401
 1839/10000 [====>.........................] - ETA: 2:00:44 - loss: 0.3858 - regression_loss: 0.3457 - classification_loss: 0.0401
 1840/10000 [====>.........................] - ETA: 2:00:44 - loss: 0.3856 - regression_loss: 0.3455 - classification_loss: 0.0401
 1841/10000 [====>.........................] - ETA: 2:00:43 - loss: 0.3854 - regression_loss: 0.3453 - classification_loss: 0.0401
 1842/10000 [====>.........................] - ETA: 2:00:42 - loss: 0.3854 - regression_loss: 0.3453 - classification_loss: 0.0401
 1843/10000 [====>.........................] - ETA: 2:00:41 - loss: 0.3853 - regression_loss: 0.3452 - classification_loss: 0.0401
 1844/10000 [====>.........................] - ETA: 2:00:40 - loss: 0.3852 - regression_loss: 0.3451 - classification_loss: 0.0400
 1845/10000 [====>.........................] - ETA: 2:00:39 - loss: 0.3851 - regression_loss: 0.3450 - classification_loss: 0.0400
 1846/10000 [====>.........................] - ETA: 2:00:38 - loss: 0.3852 - regression_loss: 0.3451 - classification_loss: 0.0400
 1847/10000 [====>.........................] - ETA: 2:00:37 - loss: 0.3851 - regression_loss: 0.3451 - classification_loss: 0.0400
 1848/10000 [====>.........................] - ETA: 2:00:36 - loss: 0.3854 - regression_loss: 0.3453 - classification_loss: 0.0401
 1849/10000 [====>.........................] - ETA: 2:00:36 - loss: 0.3854 - regression_loss: 0.3453 - classification_loss: 0.0401
 1850/10000 [====>.........................] - ETA: 2:00:35 - loss: 0.3854 - regression_loss: 0.3452 - classification_loss: 0.0401
 1851/10000 [====>.........................] - ETA: 2:00:34 - loss: 0.3854 - regression_loss: 0.3452 - classification_loss: 0.0401
 1852/10000 [====>.........................] - ETA: 2:00:33 - loss: 0.3854 - regression_loss: 0.3452 - classification_loss: 0.0401
 1853/10000 [====>.........................] - ETA: 2:00:32 - loss: 0.3853 - regression_loss: 0.3452 - classification_loss: 0.0401
 1854/10000 [====>.........................] - ETA: 2:00:31 - loss: 0.3852 - regression_loss: 0.3451 - classification_loss: 0.0401
 1855/10000 [====>.........................] - ETA: 2:00:30 - loss: 0.3853 - regression_loss: 0.3452 - classification_loss: 0.0401
 1856/10000 [====>.........................] - ETA: 2:00:29 - loss: 0.3852 - regression_loss: 0.3452 - classification_loss: 0.0401
 1857/10000 [====>.........................] - ETA: 2:00:28 - loss: 0.3852 - regression_loss: 0.3452 - classification_loss: 0.0401
 1858/10000 [====>.........................] - ETA: 2:00:28 - loss: 0.3852 - regression_loss: 0.3452 - classification_loss: 0.0401
 1859/10000 [====>.........................] - ETA: 2:00:27 - loss: 0.3854 - regression_loss: 0.3453 - classification_loss: 0.0401
 1860/10000 [====>.........................] - ETA: 2:00:26 - loss: 0.3857 - regression_loss: 0.3456 - classification_loss: 0.0401
 1861/10000 [====>.........................] - ETA: 2:00:25 - loss: 0.3856 - regression_loss: 0.3455 - classification_loss: 0.0401
 1862/10000 [====>.........................] - ETA: 2:00:24 - loss: 0.3857 - regression_loss: 0.3456 - classification_loss: 0.0401
 1863/10000 [====>.........................] - ETA: 2:00:23 - loss: 0.3857 - regression_loss: 0.3456 - classification_loss: 0.0401
 1864/10000 [====>.........................] - ETA: 2:00:22 - loss: 0.3858 - regression_loss: 0.3457 - classification_loss: 0.0401
 1865/10000 [====>.........................] - ETA: 2:00:21 - loss: 0.3856 - regression_loss: 0.3455 - classification_loss: 0.0401
 1866/10000 [====>.........................] - ETA: 2:00:20 - loss: 0.3855 - regression_loss: 0.3454 - classification_loss: 0.0401
 1867/10000 [====>.........................] - ETA: 2:00:20 - loss: 0.3855 - regression_loss: 0.3454 - classification_loss: 0.0401
 1868/10000 [====>.........................] - ETA: 2:00:19 - loss: 0.3856 - regression_loss: 0.3454 - classification_loss: 0.0401
 1869/10000 [====>.........................] - ETA: 2:00:18 - loss: 0.3856 - regression_loss: 0.3455 - classification_loss: 0.0401
 1870/10000 [====>.........................] - ETA: 2:00:17 - loss: 0.3856 - regression_loss: 0.3455 - classification_loss: 0.0401
 1871/10000 [====>.........................] - ETA: 2:00:16 - loss: 0.3854 - regression_loss: 0.3453 - classification_loss: 0.0401
 1872/10000 [====>.........................] - ETA: 2:00:15 - loss: 0.3854 - regression_loss: 0.3453 - classification_loss: 0.0401
 1873/10000 [====>.........................] - ETA: 2:00:14 - loss: 0.3852 - regression_loss: 0.3451 - classification_loss: 0.0401
 1874/10000 [====>.........................] - ETA: 2:00:13 - loss: 0.3852 - regression_loss: 0.3451 - classification_loss: 0.0401
 1875/10000 [====>.........................] - ETA: 2:00:13 - loss: 0.3851 - regression_loss: 0.3451 - classification_loss: 0.0401
 1876/10000 [====>.........................] - ETA: 2:00:12 - loss: 0.3849 - regression_loss: 0.3449 - classification_loss: 0.0400
 1877/10000 [====>.........................] - ETA: 2:00:11 - loss: 0.3848 - regression_loss: 0.3448 - classification_loss: 0.0400
 1878/10000 [====>.........................] - ETA: 2:00:10 - loss: 0.3847 - regression_loss: 0.3447 - classification_loss: 0.0400
 1879/10000 [====>.........................] - ETA: 2:00:09 - loss: 0.3847 - regression_loss: 0.3447 - classification_loss: 0.0400
 1880/10000 [====>.........................] - ETA: 2:00:08 - loss: 0.3846 - regression_loss: 0.3446 - classification_loss: 0.0400
 1881/10000 [====>.........................] - ETA: 2:00:07 - loss: 0.3848 - regression_loss: 0.3448 - classification_loss: 0.0400
 1882/10000 [====>.........................] - ETA: 2:00:06 - loss: 0.3857 - regression_loss: 0.3448 - classification_loss: 0.0409
 1883/10000 [====>.........................] - ETA: 2:00:05 - loss: 0.3858 - regression_loss: 0.3449 - classification_loss: 0.0409
 1884/10000 [====>.........................] - ETA: 2:00:05 - loss: 0.3857 - regression_loss: 0.3448 - classification_loss: 0.0409
 1885/10000 [====>.........................] - ETA: 2:00:04 - loss: 0.3857 - regression_loss: 0.3448 - classification_loss: 0.0409
 1886/10000 [====>.........................] - ETA: 2:00:03 - loss: 0.3857 - regression_loss: 0.3448 - classification_loss: 0.0409
 1887/10000 [====>.........................] - ETA: 2:00:02 - loss: 0.3856 - regression_loss: 0.3448 - classification_loss: 0.0409
 1888/10000 [====>.........................] - ETA: 2:00:01 - loss: 0.3854 - regression_loss: 0.3446 - classification_loss: 0.0409
 1889/10000 [====>.........................] - ETA: 2:00:00 - loss: 0.3854 - regression_loss: 0.3445 - classification_loss: 0.0409
 1890/10000 [====>.........................] - ETA: 1:59:59 - loss: 0.3855 - regression_loss: 0.3446 - classification_loss: 0.0409
 1891/10000 [====>.........................] - ETA: 1:59:58 - loss: 0.3853 - regression_loss: 0.3444 - classification_loss: 0.0408
 1892/10000 [====>.........................] - ETA: 1:59:57 - loss: 0.3851 - regression_loss: 0.3443 - classification_loss: 0.0408
 1893/10000 [====>.........................] - ETA: 1:59:57 - loss: 0.3851 - regression_loss: 0.3443 - classification_loss: 0.0408
 1894/10000 [====>.........................] - ETA: 1:59:56 - loss: 0.3851 - regression_loss: 0.3443 - classification_loss: 0.0408
 1895/10000 [====>.........................] - ETA: 1:59:55 - loss: 0.3850 - regression_loss: 0.3443 - classification_loss: 0.0408
 1896/10000 [====>.........................] - ETA: 1:59:54 - loss: 0.3850 - regression_loss: 0.3443 - classification_loss: 0.0408
 1897/10000 [====>.........................] - ETA: 1:59:53 - loss: 0.3850 - regression_loss: 0.3443 - classification_loss: 0.0408
 1898/10000 [====>.........................] - ETA: 1:59:52 - loss: 0.3849 - regression_loss: 0.3441 - classification_loss: 0.0407
 1899/10000 [====>.........................] - ETA: 1:59:51 - loss: 0.3849 - regression_loss: 0.3441 - classification_loss: 0.0407
 1900/10000 [====>.........................] - ETA: 1:59:50 - loss: 0.3849 - regression_loss: 0.3442 - classification_loss: 0.0407
 1901/10000 [====>.........................] - ETA: 1:59:50 - loss: 0.3849 - regression_loss: 0.3441 - classification_loss: 0.0407
 1902/10000 [====>.........................] - ETA: 1:59:49 - loss: 0.3849 - regression_loss: 0.3441 - classification_loss: 0.0407
 1903/10000 [====>.........................] - ETA: 1:59:48 - loss: 0.3849 - regression_loss: 0.3442 - classification_loss: 0.0407
 1904/10000 [====>.........................] - ETA: 1:59:47 - loss: 0.3848 - regression_loss: 0.3441 - classification_loss: 0.0407
 1905/10000 [====>.........................] - ETA: 1:59:46 - loss: 0.3851 - regression_loss: 0.3443 - classification_loss: 0.0407
 1906/10000 [====>.........................] - ETA: 1:59:45 - loss: 0.3850 - regression_loss: 0.3443 - classification_loss: 0.0407
 1907/10000 [====>.........................] - ETA: 1:59:44 - loss: 0.3851 - regression_loss: 0.3443 - classification_loss: 0.0407
 1908/10000 [====>.........................] - ETA: 1:59:43 - loss: 0.3850 - regression_loss: 0.3442 - classification_loss: 0.0407
 1909/10000 [====>.........................] - ETA: 1:59:42 - loss: 0.3851 - regression_loss: 0.3444 - classification_loss: 0.0408
 1910/10000 [====>.........................] - ETA: 1:59:42 - loss: 0.3851 - regression_loss: 0.3444 - classification_loss: 0.0408
 1911/10000 [====>.........................] - ETA: 1:59:41 - loss: 0.3851 - regression_loss: 0.3443 - classification_loss: 0.0407
 1912/10000 [====>.........................] - ETA: 1:59:40 - loss: 0.3849 - regression_loss: 0.3441 - classification_loss: 0.0407
 1913/10000 [====>.........................] - ETA: 1:59:39 - loss: 0.3848 - regression_loss: 0.3441 - classification_loss: 0.0407
 1914/10000 [====>.........................] - ETA: 1:59:38 - loss: 0.3848 - regression_loss: 0.3441 - classification_loss: 0.0407
 1915/10000 [====>.........................] - ETA: 1:59:37 - loss: 0.3848 - regression_loss: 0.3441 - classification_loss: 0.0407
 1916/10000 [====>.........................] - ETA: 1:59:36 - loss: 0.3849 - regression_loss: 0.3442 - classification_loss: 0.0407
 1917/10000 [====>.........................] - ETA: 1:59:35 - loss: 0.3849 - regression_loss: 0.3442 - classification_loss: 0.0407
 1918/10000 [====>.........................] - ETA: 1:59:34 - loss: 0.3848 - regression_loss: 0.3442 - classification_loss: 0.0407
 1919/10000 [====>.........................] - ETA: 1:59:34 - loss: 0.3852 - regression_loss: 0.3445 - classification_loss: 0.0407
 1920/10000 [====>.........................] - ETA: 1:59:33 - loss: 0.3852 - regression_loss: 0.3445 - classification_loss: 0.0407
 1921/10000 [====>.........................] - ETA: 1:59:32 - loss: 0.3852 - regression_loss: 0.3445 - classification_loss: 0.0407
 1922/10000 [====>.........................] - ETA: 1:59:31 - loss: 0.3850 - regression_loss: 0.3443 - classification_loss: 0.0407
 1923/10000 [====>.........................] - ETA: 1:59:30 - loss: 0.3849 - regression_loss: 0.3442 - classification_loss: 0.0407
 1924/10000 [====>.........................] - ETA: 1:59:29 - loss: 0.3849 - regression_loss: 0.3442 - classification_loss: 0.0407
 1925/10000 [====>.........................] - ETA: 1:59:28 - loss: 0.3848 - regression_loss: 0.3441 - classification_loss: 0.0407
 1926/10000 [====>.........................] - ETA: 1:59:27 - loss: 0.3847 - regression_loss: 0.3441 - classification_loss: 0.0407
 1927/10000 [====>.........................] - ETA: 1:59:26 - loss: 0.3847 - regression_loss: 0.3440 - classification_loss: 0.0407
 1928/10000 [====>.........................] - ETA: 1:59:26 - loss: 0.3845 - regression_loss: 0.3439 - classification_loss: 0.0406
 1929/10000 [====>.........................] - ETA: 1:59:25 - loss: 0.3844 - regression_loss: 0.3438 - classification_loss: 0.0406
 1930/10000 [====>.........................] - ETA: 1:59:24 - loss: 0.3844 - regression_loss: 0.3438 - classification_loss: 0.0406
 1931/10000 [====>.........................] - ETA: 1:59:23 - loss: 0.3843 - regression_loss: 0.3437 - classification_loss: 0.0406
 1932/10000 [====>.........................] - ETA: 1:59:22 - loss: 0.3841 - regression_loss: 0.3436 - classification_loss: 0.0406
 1933/10000 [====>.........................] - ETA: 1:59:21 - loss: 0.3840 - regression_loss: 0.3435 - classification_loss: 0.0406
 1934/10000 [====>.........................] - ETA: 1:59:20 - loss: 0.3841 - regression_loss: 0.3435 - classification_loss: 0.0406
 1935/10000 [====>.........................] - ETA: 1:59:19 - loss: 0.3842 - regression_loss: 0.3436 - classification_loss: 0.0406
 1936/10000 [====>.........................] - ETA: 1:59:18 - loss: 0.3842 - regression_loss: 0.3437 - classification_loss: 0.0406
 1937/10000 [====>.........................] - ETA: 1:59:18 - loss: 0.3843 - regression_loss: 0.3437 - classification_loss: 0.0406
 1938/10000 [====>.........................] - ETA: 1:59:17 - loss: 0.3842 - regression_loss: 0.3436 - classification_loss: 0.0406
 1939/10000 [====>.........................] - ETA: 1:59:16 - loss: 0.3842 - regression_loss: 0.3437 - classification_loss: 0.0406
 1940/10000 [====>.........................] - ETA: 1:59:15 - loss: 0.3843 - regression_loss: 0.3437 - classification_loss: 0.0406
 1941/10000 [====>.........................] - ETA: 1:59:14 - loss: 0.3844 - regression_loss: 0.3438 - classification_loss: 0.0406
 1942/10000 [====>.........................] - ETA: 1:59:13 - loss: 0.3843 - regression_loss: 0.3437 - classification_loss: 0.0406
 1943/10000 [====>.........................] - ETA: 1:59:12 - loss: 0.3842 - regression_loss: 0.3436 - classification_loss: 0.0406
 1944/10000 [====>.........................] - ETA: 1:59:11 - loss: 0.3840 - regression_loss: 0.3434 - classification_loss: 0.0406
 1945/10000 [====>.........................] - ETA: 1:59:10 - loss: 0.3840 - regression_loss: 0.3434 - classification_loss: 0.0406
 1946/10000 [====>.........................] - ETA: 1:59:10 - loss: 0.3841 - regression_loss: 0.3435 - classification_loss: 0.0406
 1947/10000 [====>.........................] - ETA: 1:59:09 - loss: 0.3840 - regression_loss: 0.3435 - classification_loss: 0.0406
 1948/10000 [====>.........................] - ETA: 1:59:08 - loss: 0.3838 - regression_loss: 0.3433 - classification_loss: 0.0405
 1949/10000 [====>.........................] - ETA: 1:59:07 - loss: 0.3838 - regression_loss: 0.3433 - classification_loss: 0.0405
 1950/10000 [====>.........................] - ETA: 1:59:06 - loss: 0.3837 - regression_loss: 0.3432 - classification_loss: 0.0405
 1951/10000 [====>.........................] - ETA: 1:59:05 - loss: 0.3837 - regression_loss: 0.3432 - classification_loss: 0.0405
 1952/10000 [====>.........................] - ETA: 1:59:04 - loss: 0.3838 - regression_loss: 0.3432 - classification_loss: 0.0405
 1953/10000 [====>.........................] - ETA: 1:59:03 - loss: 0.3837 - regression_loss: 0.3432 - classification_loss: 0.0405
 1954/10000 [====>.........................] - ETA: 1:59:02 - loss: 0.3837 - regression_loss: 0.3432 - classification_loss: 0.0405
 1955/10000 [====>.........................] - ETA: 1:59:02 - loss: 0.3836 - regression_loss: 0.3431 - classification_loss: 0.0405
 1956/10000 [====>.........................] - ETA: 1:59:01 - loss: 0.3836 - regression_loss: 0.3431 - classification_loss: 0.0405
 1957/10000 [====>.........................] - ETA: 1:59:00 - loss: 0.3834 - regression_loss: 0.3430 - classification_loss: 0.0404
 1958/10000 [====>.........................] - ETA: 1:58:59 - loss: 0.3835 - regression_loss: 0.3431 - classification_loss: 0.0404
 1959/10000 [====>.........................] - ETA: 1:58:58 - loss: 0.3834 - regression_loss: 0.3430 - classification_loss: 0.0404
 1960/10000 [====>.........................] - ETA: 1:58:57 - loss: 0.3836 - regression_loss: 0.3432 - classification_loss: 0.0404
 1961/10000 [====>.........................] - ETA: 1:58:56 - loss: 0.3837 - regression_loss: 0.3433 - classification_loss: 0.0404
 1962/10000 [====>.........................] - ETA: 1:58:55 - loss: 0.3837 - regression_loss: 0.3433 - classification_loss: 0.0404
 1963/10000 [====>.........................] - ETA: 1:58:54 - loss: 0.3837 - regression_loss: 0.3433 - classification_loss: 0.0404
 1964/10000 [====>.........................] - ETA: 1:58:54 - loss: 0.3836 - regression_loss: 0.3432 - classification_loss: 0.0404
 1965/10000 [====>.........................] - ETA: 1:58:53 - loss: 0.3837 - regression_loss: 0.3433 - classification_loss: 0.0404
 1966/10000 [====>.........................] - ETA: 1:58:52 - loss: 0.3836 - regression_loss: 0.3432 - classification_loss: 0.0404
 1967/10000 [====>.........................] - ETA: 1:58:51 - loss: 0.3836 - regression_loss: 0.3432 - classification_loss: 0.0404
 1968/10000 [====>.........................] - ETA: 1:58:50 - loss: 0.3836 - regression_loss: 0.3432 - classification_loss: 0.0404
 1969/10000 [====>.........................] - ETA: 1:58:49 - loss: 0.3838 - regression_loss: 0.3433 - classification_loss: 0.0404
 1970/10000 [====>.........................] - ETA: 1:58:48 - loss: 0.3837 - regression_loss: 0.3433 - classification_loss: 0.0404
 1971/10000 [====>.........................] - ETA: 1:58:47 - loss: 0.3836 - regression_loss: 0.3432 - classification_loss: 0.0404
 1972/10000 [====>.........................] - ETA: 1:58:46 - loss: 0.3836 - regression_loss: 0.3433 - classification_loss: 0.0404
 1973/10000 [====>.........................] - ETA: 1:58:46 - loss: 0.3835 - regression_loss: 0.3432 - classification_loss: 0.0404
 1974/10000 [====>.........................] - ETA: 1:58:45 - loss: 0.3834 - regression_loss: 0.3431 - classification_loss: 0.0404
 1975/10000 [====>.........................] - ETA: 1:58:44 - loss: 0.3833 - regression_loss: 0.3430 - classification_loss: 0.0403
 1976/10000 [====>.........................] - ETA: 1:58:43 - loss: 0.3833 - regression_loss: 0.3430 - classification_loss: 0.0403
 1977/10000 [====>.........................] - ETA: 1:58:42 - loss: 0.3835 - regression_loss: 0.3432 - classification_loss: 0.0404
 1978/10000 [====>.........................] - ETA: 1:58:41 - loss: 0.3836 - regression_loss: 0.3432 - classification_loss: 0.0403
 1979/10000 [====>.........................] - ETA: 1:58:40 - loss: 0.3834 - regression_loss: 0.3431 - classification_loss: 0.0403
 1980/10000 [====>.........................] - ETA: 1:58:39 - loss: 0.3835 - regression_loss: 0.3431 - classification_loss: 0.0403
 1981/10000 [====>.........................] - ETA: 1:58:38 - loss: 0.3833 - regression_loss: 0.3430 - classification_loss: 0.0403
 1982/10000 [====>.........................] - ETA: 1:58:38 - loss: 0.3835 - regression_loss: 0.3432 - classification_loss: 0.0403
 1983/10000 [====>.........................] - ETA: 1:58:37 - loss: 0.3834 - regression_loss: 0.3431 - classification_loss: 0.0403
 1984/10000 [====>.........................] - ETA: 1:58:36 - loss: 0.3833 - regression_loss: 0.3430 - classification_loss: 0.0403
 1985/10000 [====>.........................] - ETA: 1:58:35 - loss: 0.3831 - regression_loss: 0.3428 - classification_loss: 0.0403
 1986/10000 [====>.........................] - ETA: 1:58:34 - loss: 0.3829 - regression_loss: 0.3427 - classification_loss: 0.0402
 1987/10000 [====>.........................] - ETA: 1:58:33 - loss: 0.3830 - regression_loss: 0.3427 - classification_loss: 0.0402
 1988/10000 [====>.........................] - ETA: 1:58:32 - loss: 0.3831 - regression_loss: 0.3428 - classification_loss: 0.0403
 1989/10000 [====>.........................] - ETA: 1:58:31 - loss: 0.3832 - regression_loss: 0.3429 - classification_loss: 0.0403
 1990/10000 [====>.........................] - ETA: 1:58:30 - loss: 0.3831 - regression_loss: 0.3429 - classification_loss: 0.0403
 1991/10000 [====>.........................] - ETA: 1:58:30 - loss: 0.3831 - regression_loss: 0.3428 - classification_loss: 0.0403
 1992/10000 [====>.........................] - ETA: 1:58:29 - loss: 0.3831 - regression_loss: 0.3429 - classification_loss: 0.0403
 1993/10000 [====>.........................] - ETA: 1:58:28 - loss: 0.3831 - regression_loss: 0.3429 - classification_loss: 0.0403
 1994/10000 [====>.........................] - ETA: 1:58:27 - loss: 0.3831 - regression_loss: 0.3429 - classification_loss: 0.0402
 1995/10000 [====>.........................] - ETA: 1:58:26 - loss: 0.3830 - regression_loss: 0.3428 - classification_loss: 0.0402
 1996/10000 [====>.........................] - ETA: 1:58:25 - loss: 0.3828 - regression_loss: 0.3426 - classification_loss: 0.0402
 1997/10000 [====>.........................] - ETA: 1:58:24 - loss: 0.3829 - regression_loss: 0.3427 - classification_loss: 0.0402
 1998/10000 [====>.........................] - ETA: 1:58:23 - loss: 0.3830 - regression_loss: 0.3428 - classification_loss: 0.0402
 1999/10000 [====>.........................] - ETA: 1:58:22 - loss: 0.3830 - regression_loss: 0.3428 - classification_loss: 0.0402
 2000/10000 [=====>........................] - ETA: 1:58:22 - loss: 0.3832 - regression_loss: 0.3430 - classification_loss: 0.0402
 2001/10000 [=====>........................] - ETA: 1:58:21 - loss: 0.3832 - regression_loss: 0.3430 - classification_loss: 0.0402
 2002/10000 [=====>........................] - ETA: 1:58:20 - loss: 0.3831 - regression_loss: 0.3429 - classification_loss: 0.0402
 2003/10000 [=====>........................] - ETA: 1:58:19 - loss: 0.3831 - regression_loss: 0.3429 - classification_loss: 0.0402
 2004/10000 [=====>........................] - ETA: 1:58:18 - loss: 0.3831 - regression_loss: 0.3429 - classification_loss: 0.0402
 2005/10000 [=====>........................] - ETA: 1:58:17 - loss: 0.3832 - regression_loss: 0.3430 - classification_loss: 0.0402
 2006/10000 [=====>........................] - ETA: 1:58:16 - loss: 0.3832 - regression_loss: 0.3430 - classification_loss: 0.0402
 2007/10000 [=====>........................] - ETA: 1:58:15 - loss: 0.3830 - regression_loss: 0.3429 - classification_loss: 0.0402
 2008/10000 [=====>........................] - ETA: 1:58:14 - loss: 0.3830 - regression_loss: 0.3428 - classification_loss: 0.0402
 2009/10000 [=====>........................] - ETA: 1:58:14 - loss: 0.3829 - regression_loss: 0.3427 - classification_loss: 0.0402
 2010/10000 [=====>........................] - ETA: 1:58:13 - loss: 0.3830 - regression_loss: 0.3428 - classification_loss: 0.0402
 2011/10000 [=====>........................] - ETA: 1:58:12 - loss: 0.3829 - regression_loss: 0.3428 - classification_loss: 0.0401
 2012/10000 [=====>........................] - ETA: 1:58:11 - loss: 0.3829 - regression_loss: 0.3428 - classification_loss: 0.0401
 2013/10000 [=====>........................] - ETA: 1:58:10 - loss: 0.3829 - regression_loss: 0.3427 - classification_loss: 0.0401
 2014/10000 [=====>........................] - ETA: 1:58:09 - loss: 0.3828 - regression_loss: 0.3427 - classification_loss: 0.0401
 2015/10000 [=====>........................] - ETA: 1:58:08 - loss: 0.3832 - regression_loss: 0.3430 - classification_loss: 0.0402
 2016/10000 [=====>........................] - ETA: 1:58:07 - loss: 0.3832 - regression_loss: 0.3430 - classification_loss: 0.0402
 2017/10000 [=====>........................] - ETA: 1:58:06 - loss: 0.3831 - regression_loss: 0.3429 - classification_loss: 0.0402
 2018/10000 [=====>........................] - ETA: 1:58:06 - loss: 0.3830 - regression_loss: 0.3428 - classification_loss: 0.0402
 2019/10000 [=====>........................] - ETA: 1:58:05 - loss: 0.3831 - regression_loss: 0.3429 - classification_loss: 0.0402
 2020/10000 [=====>........................] - ETA: 1:58:04 - loss: 0.3831 - regression_loss: 0.3428 - classification_loss: 0.0402
 2021/10000 [=====>........................] - ETA: 1:58:03 - loss: 0.3829 - regression_loss: 0.3427 - classification_loss: 0.0402
 2022/10000 [=====>........................] - ETA: 1:58:02 - loss: 0.3828 - regression_loss: 0.3426 - classification_loss: 0.0402
 2023/10000 [=====>........................] - ETA: 1:58:01 - loss: 0.3826 - regression_loss: 0.3424 - classification_loss: 0.0402
 2024/10000 [=====>........................] - ETA: 1:58:00 - loss: 0.3825 - regression_loss: 0.3424 - classification_loss: 0.0401
 2025/10000 [=====>........................] - ETA: 1:57:59 - loss: 0.3827 - regression_loss: 0.3425 - classification_loss: 0.0402
 2026/10000 [=====>........................] - ETA: 1:57:58 - loss: 0.3827 - regression_loss: 0.3426 - classification_loss: 0.0402
 2027/10000 [=====>........................] - ETA: 1:57:58 - loss: 0.3827 - regression_loss: 0.3426 - classification_loss: 0.0401
 2028/10000 [=====>........................] - ETA: 1:57:57 - loss: 0.3826 - regression_loss: 0.3424 - classification_loss: 0.0401
 2029/10000 [=====>........................] - ETA: 1:57:56 - loss: 0.3827 - regression_loss: 0.3426 - classification_loss: 0.0401
 2030/10000 [=====>........................] - ETA: 1:57:55 - loss: 0.3830 - regression_loss: 0.3429 - classification_loss: 0.0402
 2031/10000 [=====>........................] - ETA: 1:57:54 - loss: 0.3830 - regression_loss: 0.3428 - classification_loss: 0.0402
 2032/10000 [=====>........................] - ETA: 1:57:53 - loss: 0.3839 - regression_loss: 0.3434 - classification_loss: 0.0405
 2033/10000 [=====>........................] - ETA: 1:57:52 - loss: 0.3839 - regression_loss: 0.3434 - classification_loss: 0.0405
 2034/10000 [=====>........................] - ETA: 1:57:51 - loss: 0.3839 - regression_loss: 0.3434 - classification_loss: 0.0405
 2035/10000 [=====>........................] - ETA: 1:57:50 - loss: 0.3840 - regression_loss: 0.3434 - classification_loss: 0.0405
 2036/10000 [=====>........................] - ETA: 1:57:50 - loss: 0.3839 - regression_loss: 0.3434 - classification_loss: 0.0405
 2037/10000 [=====>........................] - ETA: 1:57:49 - loss: 0.3838 - regression_loss: 0.3433 - classification_loss: 0.0405
 2038/10000 [=====>........................] - ETA: 1:57:48 - loss: 0.3839 - regression_loss: 0.3434 - classification_loss: 0.0405
 2039/10000 [=====>........................] - ETA: 1:57:47 - loss: 0.3839 - regression_loss: 0.3433 - classification_loss: 0.0405
 2040/10000 [=====>........................] - ETA: 1:57:46 - loss: 0.3839 - regression_loss: 0.3434 - classification_loss: 0.0405
 2041/10000 [=====>........................] - ETA: 1:57:45 - loss: 0.3838 - regression_loss: 0.3433 - classification_loss: 0.0405
 2042/10000 [=====>........................] - ETA: 1:57:44 - loss: 0.3837 - regression_loss: 0.3432 - classification_loss: 0.0405
 2043/10000 [=====>........................] - ETA: 1:57:43 - loss: 0.3837 - regression_loss: 0.3432 - classification_loss: 0.0405
 2044/10000 [=====>........................] - ETA: 1:57:42 - loss: 0.3836 - regression_loss: 0.3432 - classification_loss: 0.0405
 2045/10000 [=====>........................] - ETA: 1:57:42 - loss: 0.3836 - regression_loss: 0.3431 - classification_loss: 0.0405
 2046/10000 [=====>........................] - ETA: 1:57:41 - loss: 0.3836 - regression_loss: 0.3432 - classification_loss: 0.0405
 2047/10000 [=====>........................] - ETA: 1:57:40 - loss: 0.3836 - regression_loss: 0.3432 - classification_loss: 0.0405
 2048/10000 [=====>........................] - ETA: 1:57:39 - loss: 0.3836 - regression_loss: 0.3431 - classification_loss: 0.0405
 2049/10000 [=====>........................] - ETA: 1:57:38 - loss: 0.3834 - regression_loss: 0.3430 - classification_loss: 0.0405
 2050/10000 [=====>........................] - ETA: 1:57:37 - loss: 0.3835 - regression_loss: 0.3431 - classification_loss: 0.0405
 2051/10000 [=====>........................] - ETA: 1:57:36 - loss: 0.3835 - regression_loss: 0.3431 - classification_loss: 0.0405
 2052/10000 [=====>........................] - ETA: 1:57:35 - loss: 0.3835 - regression_loss: 0.3430 - classification_loss: 0.0405
 2053/10000 [=====>........................] - ETA: 1:57:34 - loss: 0.3835 - regression_loss: 0.3430 - classification_loss: 0.0404
 2054/10000 [=====>........................] - ETA: 1:57:34 - loss: 0.3835 - regression_loss: 0.3430 - classification_loss: 0.0405
 2055/10000 [=====>........................] - ETA: 1:57:33 - loss: 0.3837 - regression_loss: 0.3432 - classification_loss: 0.0405
 2056/10000 [=====>........................] - ETA: 1:57:32 - loss: 0.3837 - regression_loss: 0.3432 - classification_loss: 0.0405
 2057/10000 [=====>........................] - ETA: 1:57:31 - loss: 0.3836 - regression_loss: 0.3432 - classification_loss: 0.0405
 2058/10000 [=====>........................] - ETA: 1:57:30 - loss: 0.3836 - regression_loss: 0.3431 - classification_loss: 0.0405
 2059/10000 [=====>........................] - ETA: 1:57:29 - loss: 0.3835 - regression_loss: 0.3430 - classification_loss: 0.0405
 2060/10000 [=====>........................] - ETA: 1:57:28 - loss: 0.3834 - regression_loss: 0.3429 - classification_loss: 0.0405
 2061/10000 [=====>........................] - ETA: 1:57:27 - loss: 0.3833 - regression_loss: 0.3429 - classification_loss: 0.0404
 2062/10000 [=====>........................] - ETA: 1:57:26 - loss: 0.3833 - regression_loss: 0.3428 - classification_loss: 0.0404
 2063/10000 [=====>........................] - ETA: 1:57:26 - loss: 0.3836 - regression_loss: 0.3431 - classification_loss: 0.0405
 2064/10000 [=====>........................] - ETA: 1:57:25 - loss: 0.3836 - regression_loss: 0.3431 - classification_loss: 0.0405
 2065/10000 [=====>........................] - ETA: 1:57:24 - loss: 0.3835 - regression_loss: 0.3430 - classification_loss: 0.0405
 2066/10000 [=====>........................] - ETA: 1:57:23 - loss: 0.3834 - regression_loss: 0.3429 - classification_loss: 0.0405
 2067/10000 [=====>........................] - ETA: 1:57:22 - loss: 0.3834 - regression_loss: 0.3429 - classification_loss: 0.0404
 2068/10000 [=====>........................] - ETA: 1:57:21 - loss: 0.3832 - regression_loss: 0.3428 - classification_loss: 0.0404
 2069/10000 [=====>........................] - ETA: 1:57:20 - loss: 0.3835 - regression_loss: 0.3428 - classification_loss: 0.0407
 2070/10000 [=====>........................] - ETA: 1:57:19 - loss: 0.3834 - regression_loss: 0.3427 - classification_loss: 0.0407
 2071/10000 [=====>........................] - ETA: 1:57:18 - loss: 0.3834 - regression_loss: 0.3428 - classification_loss: 0.0407
 2072/10000 [=====>........................] - ETA: 1:57:17 - loss: 0.3833 - regression_loss: 0.3426 - classification_loss: 0.0407
 2073/10000 [=====>........................] - ETA: 1:57:17 - loss: 0.3834 - regression_loss: 0.3427 - classification_loss: 0.0407
 2074/10000 [=====>........................] - ETA: 1:57:16 - loss: 0.3835 - regression_loss: 0.3428 - classification_loss: 0.0407
 2075/10000 [=====>........................] - ETA: 1:57:15 - loss: 0.3833 - regression_loss: 0.3427 - classification_loss: 0.0407
 2076/10000 [=====>........................] - ETA: 1:57:14 - loss: 0.3834 - regression_loss: 0.3427 - classification_loss: 0.0407
 2077/10000 [=====>........................] - ETA: 1:57:13 - loss: 0.3833 - regression_loss: 0.3426 - classification_loss: 0.0406
 2078/10000 [=====>........................] - ETA: 1:57:12 - loss: 0.3832 - regression_loss: 0.3426 - classification_loss: 0.0406
 2079/10000 [=====>........................] - ETA: 1:57:11 - loss: 0.3832 - regression_loss: 0.3426 - classification_loss: 0.0406
 2080/10000 [=====>........................] - ETA: 1:57:10 - loss: 0.3832 - regression_loss: 0.3426 - classification_loss: 0.0406
 2081/10000 [=====>........................] - ETA: 1:57:09 - loss: 0.3830 - regression_loss: 0.3425 - classification_loss: 0.0406
 2082/10000 [=====>........................] - ETA: 1:57:09 - loss: 0.3831 - regression_loss: 0.3425 - classification_loss: 0.0406
 2083/10000 [=====>........................] - ETA: 1:57:08 - loss: 0.3831 - regression_loss: 0.3425 - classification_loss: 0.0406
 2084/10000 [=====>........................] - ETA: 1:57:07 - loss: 0.3831 - regression_loss: 0.3425 - classification_loss: 0.0406
 2085/10000 [=====>........................] - ETA: 1:57:06 - loss: 0.3832 - regression_loss: 0.3426 - classification_loss: 0.0406
 2086/10000 [=====>........................] - ETA: 1:57:05 - loss: 0.3833 - regression_loss: 0.3427 - classification_loss: 0.0407
 2087/10000 [=====>........................] - ETA: 1:57:04 - loss: 0.3834 - regression_loss: 0.3428 - classification_loss: 0.0407
 2088/10000 [=====>........................] - ETA: 1:57:03 - loss: 0.3835 - regression_loss: 0.3428 - classification_loss: 0.0407
 2089/10000 [=====>........................] - ETA: 1:57:02 - loss: 0.3835 - regression_loss: 0.3428 - classification_loss: 0.0407
 2090/10000 [=====>........................] - ETA: 1:57:01 - loss: 0.3835 - regression_loss: 0.3428 - classification_loss: 0.0407
 2091/10000 [=====>........................] - ETA: 1:57:01 - loss: 0.3835 - regression_loss: 0.3427 - classification_loss: 0.0407
 2092/10000 [=====>........................] - ETA: 1:57:00 - loss: 0.3834 - regression_loss: 0.3427 - classification_loss: 0.0407
 2093/10000 [=====>........................] - ETA: 1:56:59 - loss: 0.3832 - regression_loss: 0.3425 - classification_loss: 0.0407
 2094/10000 [=====>........................] - ETA: 1:56:58 - loss: 0.3832 - regression_loss: 0.3425 - classification_loss: 0.0407
 2095/10000 [=====>........................] - ETA: 1:56:57 - loss: 0.3833 - regression_loss: 0.3426 - classification_loss: 0.0407
 2096/10000 [=====>........................] - ETA: 1:56:56 - loss: 0.3833 - regression_loss: 0.3426 - classification_loss: 0.0407
 2097/10000 [=====>........................] - ETA: 1:56:55 - loss: 0.3833 - regression_loss: 0.3426 - classification_loss: 0.0407
 2098/10000 [=====>........................] - ETA: 1:56:54 - loss: 0.3842 - regression_loss: 0.3435 - classification_loss: 0.0407
 2099/10000 [=====>........................] - ETA: 1:56:53 - loss: 0.3842 - regression_loss: 0.3434 - classification_loss: 0.0407
 2100/10000 [=====>........................] - ETA: 1:56:53 - loss: 0.3841 - regression_loss: 0.3434 - classification_loss: 0.0407
 2101/10000 [=====>........................] - ETA: 1:56:52 - loss: 0.3841 - regression_loss: 0.3434 - classification_loss: 0.0407
 2102/10000 [=====>........................] - ETA: 1:56:51 - loss: 0.3841 - regression_loss: 0.3434 - classification_loss: 0.0407
 2103/10000 [=====>........................] - ETA: 1:56:50 - loss: 0.3841 - regression_loss: 0.3433 - classification_loss: 0.0407
 2104/10000 [=====>........................] - ETA: 1:56:49 - loss: 0.3842 - regression_loss: 0.3435 - classification_loss: 0.0407
 2105/10000 [=====>........................] - ETA: 1:56:48 - loss: 0.3843 - regression_loss: 0.3436 - classification_loss: 0.0407
 2106/10000 [=====>........................] - ETA: 1:56:47 - loss: 0.3843 - regression_loss: 0.3436 - classification_loss: 0.0407
 2107/10000 [=====>........................] - ETA: 1:56:46 - loss: 0.3842 - regression_loss: 0.3435 - classification_loss: 0.0407
 2108/10000 [=====>........................] - ETA: 1:56:45 - loss: 0.3842 - regression_loss: 0.3435 - classification_loss: 0.0407
 2109/10000 [=====>........................] - ETA: 1:56:45 - loss: 0.3843 - regression_loss: 0.3436 - classification_loss: 0.0407
 2110/10000 [=====>........................] - ETA: 1:56:44 - loss: 0.3844 - regression_loss: 0.3437 - classification_loss: 0.0407
 2111/10000 [=====>........................] - ETA: 1:56:43 - loss: 0.3843 - regression_loss: 0.3436 - classification_loss: 0.0406
 2112/10000 [=====>........................] - ETA: 1:56:42 - loss: 0.3843 - regression_loss: 0.3437 - classification_loss: 0.0406
 2113/10000 [=====>........................] - ETA: 1:56:41 - loss: 0.3845 - regression_loss: 0.3438 - classification_loss: 0.0407
 2114/10000 [=====>........................] - ETA: 1:56:40 - loss: 0.3844 - regression_loss: 0.3438 - classification_loss: 0.0407
 2115/10000 [=====>........................] - ETA: 1:56:39 - loss: 0.3843 - regression_loss: 0.3436 - classification_loss: 0.0406
 2116/10000 [=====>........................] - ETA: 1:56:38 - loss: 0.3842 - regression_loss: 0.3436 - classification_loss: 0.0406
 2117/10000 [=====>........................] - ETA: 1:56:37 - loss: 0.3842 - regression_loss: 0.3436 - classification_loss: 0.0406
 2118/10000 [=====>........................] - ETA: 1:56:37 - loss: 0.3842 - regression_loss: 0.3436 - classification_loss: 0.0406
 2119/10000 [=====>........................] - ETA: 1:56:36 - loss: 0.3841 - regression_loss: 0.3435 - classification_loss: 0.0406
 2120/10000 [=====>........................] - ETA: 1:56:35 - loss: 0.3842 - regression_loss: 0.3436 - classification_loss: 0.0406
 2121/10000 [=====>........................] - ETA: 1:56:34 - loss: 0.3841 - regression_loss: 0.3436 - classification_loss: 0.0406
 2122/10000 [=====>........................] - ETA: 1:56:33 - loss: 0.3842 - regression_loss: 0.3436 - classification_loss: 0.0406
 2123/10000 [=====>........................] - ETA: 1:56:32 - loss: 0.3842 - regression_loss: 0.3436 - classification_loss: 0.0406
 2124/10000 [=====>........................] - ETA: 1:56:31 - loss: 0.3842 - regression_loss: 0.3436 - classification_loss: 0.0406
 2125/10000 [=====>........................] - ETA: 1:56:30 - loss: 0.3844 - regression_loss: 0.3438 - classification_loss: 0.0406
 2126/10000 [=====>........................] - ETA: 1:56:29 - loss: 0.3843 - regression_loss: 0.3437 - classification_loss: 0.0406
 2127/10000 [=====>........................] - ETA: 1:56:29 - loss: 0.3843 - regression_loss: 0.3438 - classification_loss: 0.0406
 2128/10000 [=====>........................] - ETA: 1:56:28 - loss: 0.3845 - regression_loss: 0.3439 - classification_loss: 0.0406
 2129/10000 [=====>........................] - ETA: 1:56:27 - loss: 0.3844 - regression_loss: 0.3438 - classification_loss: 0.0406
 2130/10000 [=====>........................] - ETA: 1:56:26 - loss: 0.3843 - regression_loss: 0.3437 - classification_loss: 0.0406
 2131/10000 [=====>........................] - ETA: 1:56:25 - loss: 0.3844 - regression_loss: 0.3438 - classification_loss: 0.0406
 2132/10000 [=====>........................] - ETA: 1:56:24 - loss: 0.3842 - regression_loss: 0.3436 - classification_loss: 0.0406
 2133/10000 [=====>........................] - ETA: 1:56:23 - loss: 0.3842 - regression_loss: 0.3436 - classification_loss: 0.0406
 2134/10000 [=====>........................] - ETA: 1:56:22 - loss: 0.3840 - regression_loss: 0.3435 - classification_loss: 0.0406
 2135/10000 [=====>........................] - ETA: 1:56:21 - loss: 0.3840 - regression_loss: 0.3434 - classification_loss: 0.0406
 2136/10000 [=====>........................] - ETA: 1:56:21 - loss: 0.3840 - regression_loss: 0.3434 - classification_loss: 0.0406
 2137/10000 [=====>........................] - ETA: 1:56:20 - loss: 0.3839 - regression_loss: 0.3433 - classification_loss: 0.0405
 2138/10000 [=====>........................] - ETA: 1:56:19 - loss: 0.3839 - regression_loss: 0.3434 - classification_loss: 0.0405
 2139/10000 [=====>........................] - ETA: 1:56:18 - loss: 0.3839 - regression_loss: 0.3434 - classification_loss: 0.0405
 2140/10000 [=====>........................] - ETA: 1:56:17 - loss: 0.3839 - regression_loss: 0.3434 - classification_loss: 0.0405
 2141/10000 [=====>........................] - ETA: 1:56:16 - loss: 0.3839 - regression_loss: 0.3434 - classification_loss: 0.0405
 2142/10000 [=====>........................] - ETA: 1:56:15 - loss: 0.3839 - regression_loss: 0.3434 - classification_loss: 0.0405
 2143/10000 [=====>........................] - ETA: 1:56:14 - loss: 0.3837 - regression_loss: 0.3432 - classification_loss: 0.0405
 2144/10000 [=====>........................] - ETA: 1:56:13 - loss: 0.3837 - regression_loss: 0.3432 - classification_loss: 0.0405
 2145/10000 [=====>........................] - ETA: 1:56:13 - loss: 0.3838 - regression_loss: 0.3433 - classification_loss: 0.0405
 2146/10000 [=====>........................] - ETA: 1:56:12 - loss: 0.3836 - regression_loss: 0.3432 - classification_loss: 0.0405
 2147/10000 [=====>........................] - ETA: 1:56:11 - loss: 0.3836 - regression_loss: 0.3431 - classification_loss: 0.0405
 2148/10000 [=====>........................] - ETA: 1:56:10 - loss: 0.3835 - regression_loss: 0.3430 - classification_loss: 0.0405
 2149/10000 [=====>........................] - ETA: 1:56:09 - loss: 0.3835 - regression_loss: 0.3430 - classification_loss: 0.0405
 2150/10000 [=====>........................] - ETA: 1:56:08 - loss: 0.3835 - regression_loss: 0.3431 - classification_loss: 0.0405
 2151/10000 [=====>........................] - ETA: 1:56:07 - loss: 0.3837 - regression_loss: 0.3432 - classification_loss: 0.0405
 2152/10000 [=====>........................] - ETA: 1:56:06 - loss: 0.3837 - regression_loss: 0.3432 - classification_loss: 0.0405
 2153/10000 [=====>........................] - ETA: 1:56:05 - loss: 0.3837 - regression_loss: 0.3432 - classification_loss: 0.0405
 2154/10000 [=====>........................] - ETA: 1:56:05 - loss: 0.3837 - regression_loss: 0.3432 - classification_loss: 0.0405
 2155/10000 [=====>........................] - ETA: 1:56:04 - loss: 0.3837 - regression_loss: 0.3432 - classification_loss: 0.0405
 2156/10000 [=====>........................] - ETA: 1:56:03 - loss: 0.3835 - regression_loss: 0.3430 - classification_loss: 0.0405
 2157/10000 [=====>........................] - ETA: 1:56:02 - loss: 0.3833 - regression_loss: 0.3429 - classification_loss: 0.0405
 2158/10000 [=====>........................] - ETA: 1:56:01 - loss: 0.3834 - regression_loss: 0.3429 - classification_loss: 0.0405
 2159/10000 [=====>........................] - ETA: 1:56:00 - loss: 0.3833 - regression_loss: 0.3428 - classification_loss: 0.0404
 2160/10000 [=====>........................] - ETA: 1:55:59 - loss: 0.3832 - regression_loss: 0.3428 - classification_loss: 0.0405
 2161/10000 [=====>........................] - ETA: 1:55:58 - loss: 0.3830 - regression_loss: 0.3426 - classification_loss: 0.0404
 2162/10000 [=====>........................] - ETA: 1:55:57 - loss: 0.3830 - regression_loss: 0.3426 - classification_loss: 0.0404
 2163/10000 [=====>........................] - ETA: 1:55:57 - loss: 0.3829 - regression_loss: 0.3425 - classification_loss: 0.0404
 2164/10000 [=====>........................] - ETA: 1:55:56 - loss: 0.3830 - regression_loss: 0.3425 - classification_loss: 0.0404
 2165/10000 [=====>........................] - ETA: 1:55:55 - loss: 0.3829 - regression_loss: 0.3425 - classification_loss: 0.0404
 2166/10000 [=====>........................] - ETA: 1:55:54 - loss: 0.3828 - regression_loss: 0.3424 - classification_loss: 0.0404
 2167/10000 [=====>........................] - ETA: 1:55:53 - loss: 0.3828 - regression_loss: 0.3424 - classification_loss: 0.0404
 2168/10000 [=====>........................] - ETA: 1:55:52 - loss: 0.3828 - regression_loss: 0.3424 - classification_loss: 0.0404
 2169/10000 [=====>........................] - ETA: 1:55:51 - loss: 0.3829 - regression_loss: 0.3424 - classification_loss: 0.0404
 2170/10000 [=====>........................] - ETA: 1:55:50 - loss: 0.3830 - regression_loss: 0.3425 - classification_loss: 0.0404
 2171/10000 [=====>........................] - ETA: 1:55:49 - loss: 0.3829 - regression_loss: 0.3425 - classification_loss: 0.0404
 2172/10000 [=====>........................] - ETA: 1:55:49 - loss: 0.3833 - regression_loss: 0.3429 - classification_loss: 0.0405
 2173/10000 [=====>........................] - ETA: 1:55:48 - loss: 0.3833 - regression_loss: 0.3428 - classification_loss: 0.0405
 2174/10000 [=====>........................] - ETA: 1:55:47 - loss: 0.3831 - regression_loss: 0.3427 - classification_loss: 0.0404
 2175/10000 [=====>........................] - ETA: 1:55:46 - loss: 0.3830 - regression_loss: 0.3425 - classification_loss: 0.0404
 2176/10000 [=====>........................] - ETA: 1:55:45 - loss: 0.3830 - regression_loss: 0.3425 - classification_loss: 0.0404
 2177/10000 [=====>........................] - ETA: 1:55:44 - loss: 0.3829 - regression_loss: 0.3425 - classification_loss: 0.0404
 2178/10000 [=====>........................] - ETA: 1:55:43 - loss: 0.3829 - regression_loss: 0.3425 - classification_loss: 0.0404
 2179/10000 [=====>........................] - ETA: 1:55:42 - loss: 0.3829 - regression_loss: 0.3425 - classification_loss: 0.0404
 2180/10000 [=====>........................] - ETA: 1:55:41 - loss: 0.3827 - regression_loss: 0.3423 - classification_loss: 0.0404
 2181/10000 [=====>........................] - ETA: 1:55:41 - loss: 0.3828 - regression_loss: 0.3424 - classification_loss: 0.0404
 2182/10000 [=====>........................] - ETA: 1:55:40 - loss: 0.3827 - regression_loss: 0.3423 - classification_loss: 0.0404
 2183/10000 [=====>........................] - ETA: 1:55:39 - loss: 0.3828 - regression_loss: 0.3425 - classification_loss: 0.0404
 2184/10000 [=====>........................] - ETA: 1:55:38 - loss: 0.3829 - regression_loss: 0.3425 - classification_loss: 0.0404
 2185/10000 [=====>........................] - ETA: 1:55:37 - loss: 0.3828 - regression_loss: 0.3424 - classification_loss: 0.0404
 2186/10000 [=====>........................] - ETA: 1:55:36 - loss: 0.3826 - regression_loss: 0.3423 - classification_loss: 0.0403
 2187/10000 [=====>........................] - ETA: 1:55:35 - loss: 0.3827 - regression_loss: 0.3423 - classification_loss: 0.0403
 2188/10000 [=====>........................] - ETA: 1:55:34 - loss: 0.3826 - regression_loss: 0.3422 - classification_loss: 0.0403
 2189/10000 [=====>........................] - ETA: 1:55:33 - loss: 0.3826 - regression_loss: 0.3422 - classification_loss: 0.0403
 2190/10000 [=====>........................] - ETA: 1:55:33 - loss: 0.3825 - regression_loss: 0.3422 - classification_loss: 0.0403
 2191/10000 [=====>........................] - ETA: 1:55:32 - loss: 0.3826 - regression_loss: 0.3423 - classification_loss: 0.0403
 2192/10000 [=====>........................] - ETA: 1:55:31 - loss: 0.3827 - regression_loss: 0.3424 - classification_loss: 0.0403
 2193/10000 [=====>........................] - ETA: 1:55:30 - loss: 0.3828 - regression_loss: 0.3424 - classification_loss: 0.0403
 2194/10000 [=====>........................] - ETA: 1:55:29 - loss: 0.3828 - regression_loss: 0.3424 - classification_loss: 0.0403
 2195/10000 [=====>........................] - ETA: 1:55:28 - loss: 0.3828 - regression_loss: 0.3425 - classification_loss: 0.0403
 2196/10000 [=====>........................] - ETA: 1:55:27 - loss: 0.3826 - regression_loss: 0.3423 - classification_loss: 0.0403
 2197/10000 [=====>........................] - ETA: 1:55:26 - loss: 0.3825 - regression_loss: 0.3423 - classification_loss: 0.0403
 2198/10000 [=====>........................] - ETA: 1:55:26 - loss: 0.3825 - regression_loss: 0.3423 - classification_loss: 0.0403
 2199/10000 [=====>........................] - ETA: 1:55:25 - loss: 0.3827 - regression_loss: 0.3424 - classification_loss: 0.0403
 2200/10000 [=====>........................] - ETA: 1:55:24 - loss: 0.3830 - regression_loss: 0.3426 - classification_loss: 0.0404
 2201/10000 [=====>........................] - ETA: 1:55:23 - loss: 0.3830 - regression_loss: 0.3426 - classification_loss: 0.0404
 2202/10000 [=====>........................] - ETA: 1:55:22 - loss: 0.3828 - regression_loss: 0.3424 - classification_loss: 0.0404
 2203/10000 [=====>........................] - ETA: 1:55:21 - loss: 0.3829 - regression_loss: 0.3425 - classification_loss: 0.0404
 2204/10000 [=====>........................] - ETA: 1:55:20 - loss: 0.3830 - regression_loss: 0.3426 - classification_loss: 0.0404
 2205/10000 [=====>........................] - ETA: 1:55:19 - loss: 0.3831 - regression_loss: 0.3427 - classification_loss: 0.0404
 2206/10000 [=====>........................] - ETA: 1:55:18 - loss: 0.3831 - regression_loss: 0.3427 - classification_loss: 0.0404
 2207/10000 [=====>........................] - ETA: 1:55:18 - loss: 0.3830 - regression_loss: 0.3426 - classification_loss: 0.0404
 2208/10000 [=====>........................] - ETA: 1:55:17 - loss: 0.3828 - regression_loss: 0.3424 - classification_loss: 0.0404
 2209/10000 [=====>........................] - ETA: 1:55:16 - loss: 0.3828 - regression_loss: 0.3424 - classification_loss: 0.0404
 2210/10000 [=====>........................] - ETA: 1:55:15 - loss: 0.3829 - regression_loss: 0.3424 - classification_loss: 0.0404
 2211/10000 [=====>........................] - ETA: 1:55:14 - loss: 0.3830 - regression_loss: 0.3425 - classification_loss: 0.0404
 2212/10000 [=====>........................] - ETA: 1:55:13 - loss: 0.3830 - regression_loss: 0.3426 - classification_loss: 0.0405
 2213/10000 [=====>........................] - ETA: 1:55:12 - loss: 0.3831 - regression_loss: 0.3426 - classification_loss: 0.0405
 2214/10000 [=====>........................] - ETA: 1:55:11 - loss: 0.3840 - regression_loss: 0.3433 - classification_loss: 0.0407
 2215/10000 [=====>........................] - ETA: 1:55:10 - loss: 0.3839 - regression_loss: 0.3432 - classification_loss: 0.0407
 2216/10000 [=====>........................] - ETA: 1:55:10 - loss: 0.3839 - regression_loss: 0.3432 - classification_loss: 0.0407
 2217/10000 [=====>........................] - ETA: 1:55:09 - loss: 0.3839 - regression_loss: 0.3433 - classification_loss: 0.0407
 2218/10000 [=====>........................] - ETA: 1:55:08 - loss: 0.3837 - regression_loss: 0.3431 - classification_loss: 0.0406
 2219/10000 [=====>........................] - ETA: 1:55:07 - loss: 0.3837 - regression_loss: 0.3431 - classification_loss: 0.0407
 2220/10000 [=====>........................] - ETA: 1:55:06 - loss: 0.3836 - regression_loss: 0.3430 - classification_loss: 0.0406
 2221/10000 [=====>........................] - ETA: 1:55:05 - loss: 0.3837 - regression_loss: 0.3431 - classification_loss: 0.0406
 2222/10000 [=====>........................] - ETA: 1:55:04 - loss: 0.3841 - regression_loss: 0.3434 - classification_loss: 0.0407
 2223/10000 [=====>........................] - ETA: 1:55:03 - loss: 0.3840 - regression_loss: 0.3434 - classification_loss: 0.0407
 2224/10000 [=====>........................] - ETA: 1:55:02 - loss: 0.3841 - regression_loss: 0.3434 - classification_loss: 0.0407
 2225/10000 [=====>........................] - ETA: 1:55:02 - loss: 0.3840 - regression_loss: 0.3434 - classification_loss: 0.0406
 2226/10000 [=====>........................] - ETA: 1:55:01 - loss: 0.3840 - regression_loss: 0.3433 - classification_loss: 0.0406
 2227/10000 [=====>........................] - ETA: 1:55:00 - loss: 0.3840 - regression_loss: 0.3434 - classification_loss: 0.0406
 2228/10000 [=====>........................] - ETA: 1:54:59 - loss: 0.3839 - regression_loss: 0.3433 - classification_loss: 0.0406
 2229/10000 [=====>........................] - ETA: 1:54:58 - loss: 0.3842 - regression_loss: 0.3435 - classification_loss: 0.0406
 2230/10000 [=====>........................] - ETA: 1:54:57 - loss: 0.3841 - regression_loss: 0.3435 - classification_loss: 0.0406
 2231/10000 [=====>........................] - ETA: 1:54:56 - loss: 0.3844 - regression_loss: 0.3437 - classification_loss: 0.0406
 2232/10000 [=====>........................] - ETA: 1:54:55 - loss: 0.3843 - regression_loss: 0.3436 - classification_loss: 0.0406
 2233/10000 [=====>........................] - ETA: 1:54:54 - loss: 0.3844 - regression_loss: 0.3437 - classification_loss: 0.0406
 2234/10000 [=====>........................] - ETA: 1:54:54 - loss: 0.3843 - regression_loss: 0.3437 - classification_loss: 0.0406
 2235/10000 [=====>........................] - ETA: 1:54:53 - loss: 0.3842 - regression_loss: 0.3436 - classification_loss: 0.0406
 2236/10000 [=====>........................] - ETA: 1:54:52 - loss: 0.3842 - regression_loss: 0.3435 - classification_loss: 0.0406
 2237/10000 [=====>........................] - ETA: 1:54:51 - loss: 0.3841 - regression_loss: 0.3435 - classification_loss: 0.0406
 2238/10000 [=====>........................] - ETA: 1:54:50 - loss: 0.3840 - regression_loss: 0.3434 - classification_loss: 0.0406
 2239/10000 [=====>........................] - ETA: 1:54:49 - loss: 0.3841 - regression_loss: 0.3435 - classification_loss: 0.0406
 2240/10000 [=====>........................] - ETA: 1:54:48 - loss: 0.3843 - regression_loss: 0.3437 - classification_loss: 0.0406
 2241/10000 [=====>........................] - ETA: 1:54:47 - loss: 0.3842 - regression_loss: 0.3437 - classification_loss: 0.0406
 2242/10000 [=====>........................] - ETA: 1:54:46 - loss: 0.3841 - regression_loss: 0.3436 - classification_loss: 0.0406
 2243/10000 [=====>........................] - ETA: 1:54:46 - loss: 0.3841 - regression_loss: 0.3435 - classification_loss: 0.0406
 2244/10000 [=====>........................] - ETA: 1:54:45 - loss: 0.3839 - regression_loss: 0.3434 - classification_loss: 0.0405
 2245/10000 [=====>........................] - ETA: 1:54:44 - loss: 0.3841 - regression_loss: 0.3435 - classification_loss: 0.0406
 2246/10000 [=====>........................] - ETA: 1:54:43 - loss: 0.3841 - regression_loss: 0.3436 - classification_loss: 0.0406
 2247/10000 [=====>........................] - ETA: 1:54:42 - loss: 0.3840 - regression_loss: 0.3434 - classification_loss: 0.0406
 2248/10000 [=====>........................] - ETA: 1:54:41 - loss: 0.3839 - regression_loss: 0.3434 - classification_loss: 0.0405
 2249/10000 [=====>........................] - ETA: 1:54:40 - loss: 0.3838 - regression_loss: 0.3433 - classification_loss: 0.0405
 2250/10000 [=====>........................] - ETA: 1:54:39 - loss: 0.3839 - regression_loss: 0.3434 - classification_loss: 0.0405
 2251/10000 [=====>........................] - ETA: 1:54:38 - loss: 0.3840 - regression_loss: 0.3435 - classification_loss: 0.0405
 2252/10000 [=====>........................] - ETA: 1:54:38 - loss: 0.3839 - regression_loss: 0.3434 - classification_loss: 0.0405
 2253/10000 [=====>........................] - ETA: 1:54:37 - loss: 0.3837 - regression_loss: 0.3433 - classification_loss: 0.0405
 2254/10000 [=====>........................] - ETA: 1:54:36 - loss: 0.3837 - regression_loss: 0.3432 - classification_loss: 0.0405
 2255/10000 [=====>........................] - ETA: 1:54:35 - loss: 0.3836 - regression_loss: 0.3431 - classification_loss: 0.0405
 2256/10000 [=====>........................] - ETA: 1:54:34 - loss: 0.3841 - regression_loss: 0.3436 - classification_loss: 0.0405
 2257/10000 [=====>........................] - ETA: 1:54:33 - loss: 0.3840 - regression_loss: 0.3435 - classification_loss: 0.0405
 2258/10000 [=====>........................] - ETA: 1:54:32 - loss: 0.3839 - regression_loss: 0.3434 - classification_loss: 0.0405
 2259/10000 [=====>........................] - ETA: 1:54:31 - loss: 0.3839 - regression_loss: 0.3434 - classification_loss: 0.0405
 2260/10000 [=====>........................] - ETA: 1:54:30 - loss: 0.3838 - regression_loss: 0.3433 - classification_loss: 0.0405
 2261/10000 [=====>........................] - ETA: 1:54:30 - loss: 0.3839 - regression_loss: 0.3434 - classification_loss: 0.0405
 2262/10000 [=====>........................] - ETA: 1:54:29 - loss: 0.3838 - regression_loss: 0.3434 - classification_loss: 0.0405
 2263/10000 [=====>........................] - ETA: 1:54:28 - loss: 0.3837 - regression_loss: 0.3432 - classification_loss: 0.0405
 2264/10000 [=====>........................] - ETA: 1:54:27 - loss: 0.3839 - regression_loss: 0.3434 - classification_loss: 0.0405
 2265/10000 [=====>........................] - ETA: 1:54:26 - loss: 0.3838 - regression_loss: 0.3433 - classification_loss: 0.0404
 2266/10000 [=====>........................] - ETA: 1:54:25 - loss: 0.3838 - regression_loss: 0.3434 - classification_loss: 0.0404
 2267/10000 [=====>........................] - ETA: 1:54:24 - loss: 0.3838 - regression_loss: 0.3433 - classification_loss: 0.0404
 2268/10000 [=====>........................] - ETA: 1:54:23 - loss: 0.3838 - regression_loss: 0.3433 - classification_loss: 0.0404
 2269/10000 [=====>........................] - ETA: 1:54:22 - loss: 0.3837 - regression_loss: 0.3433 - classification_loss: 0.0404
 2270/10000 [=====>........................] - ETA: 1:54:22 - loss: 0.3840 - regression_loss: 0.3435 - classification_loss: 0.0404
 2271/10000 [=====>........................] - ETA: 1:54:21 - loss: 0.3841 - regression_loss: 0.3436 - classification_loss: 0.0405
 2272/10000 [=====>........................] - ETA: 1:54:20 - loss: 0.3843 - regression_loss: 0.3437 - classification_loss: 0.0405
 2273/10000 [=====>........................] - ETA: 1:54:19 - loss: 0.3843 - regression_loss: 0.3437 - classification_loss: 0.0405
 2274/10000 [=====>........................] - ETA: 1:54:18 - loss: 0.3842 - regression_loss: 0.3436 - classification_loss: 0.0405
 2275/10000 [=====>........................] - ETA: 1:54:17 - loss: 0.3843 - regression_loss: 0.3438 - classification_loss: 0.0405
 2276/10000 [=====>........................] - ETA: 1:54:16 - loss: 0.3843 - regression_loss: 0.3438 - classification_loss: 0.0405
 2277/10000 [=====>........................] - ETA: 1:54:15 - loss: 0.3843 - regression_loss: 0.3437 - classification_loss: 0.0405
 2278/10000 [=====>........................] - ETA: 1:54:14 - loss: 0.3843 - regression_loss: 0.3438 - classification_loss: 0.0405
 2279/10000 [=====>........................] - ETA: 1:54:14 - loss: 0.3842 - regression_loss: 0.3437 - classification_loss: 0.0405
 2280/10000 [=====>........................] - ETA: 1:54:13 - loss: 0.3842 - regression_loss: 0.3437 - classification_loss: 0.0405
 2281/10000 [=====>........................] - ETA: 1:54:12 - loss: 0.3845 - regression_loss: 0.3440 - classification_loss: 0.0405
 2282/10000 [=====>........................] - ETA: 1:54:11 - loss: 0.3844 - regression_loss: 0.3439 - classification_loss: 0.0405
 2283/10000 [=====>........................] - ETA: 1:54:10 - loss: 0.3843 - regression_loss: 0.3438 - classification_loss: 0.0405
 2284/10000 [=====>........................] - ETA: 1:54:09 - loss: 0.3842 - regression_loss: 0.3437 - classification_loss: 0.0405
 2285/10000 [=====>........................] - ETA: 1:54:08 - loss: 0.3841 - regression_loss: 0.3436 - classification_loss: 0.0405
 2286/10000 [=====>........................] - ETA: 1:54:07 - loss: 0.3841 - regression_loss: 0.3436 - classification_loss: 0.0405
 2287/10000 [=====>........................] - ETA: 1:54:06 - loss: 0.3841 - regression_loss: 0.3436 - classification_loss: 0.0405
 2288/10000 [=====>........................] - ETA: 1:54:06 - loss: 0.3841 - regression_loss: 0.3436 - classification_loss: 0.0405
 2289/10000 [=====>........................] - ETA: 1:54:05 - loss: 0.3840 - regression_loss: 0.3435 - classification_loss: 0.0405
 2290/10000 [=====>........................] - ETA: 1:54:04 - loss: 0.3841 - regression_loss: 0.3436 - classification_loss: 0.0405
 2291/10000 [=====>........................] - ETA: 1:54:03 - loss: 0.3841 - regression_loss: 0.3436 - classification_loss: 0.0405
 2292/10000 [=====>........................] - ETA: 1:54:02 - loss: 0.3841 - regression_loss: 0.3436 - classification_loss: 0.0405
 2293/10000 [=====>........................] - ETA: 1:54:01 - loss: 0.3840 - regression_loss: 0.3435 - classification_loss: 0.0405
 2294/10000 [=====>........................] - ETA: 1:54:00 - loss: 0.3842 - regression_loss: 0.3437 - classification_loss: 0.0405
 2295/10000 [=====>........................] - ETA: 1:53:59 - loss: 0.3840 - regression_loss: 0.3435 - classification_loss: 0.0405
 2296/10000 [=====>........................] - ETA: 1:53:59 - loss: 0.3841 - regression_loss: 0.3436 - classification_loss: 0.0405
 2297/10000 [=====>........................] - ETA: 1:53:58 - loss: 0.3840 - regression_loss: 0.3436 - classification_loss: 0.0405
 2298/10000 [=====>........................] - ETA: 1:53:57 - loss: 0.3841 - regression_loss: 0.3436 - classification_loss: 0.0405
 2299/10000 [=====>........................] - ETA: 1:53:56 - loss: 0.3839 - regression_loss: 0.3435 - classification_loss: 0.0405
 2300/10000 [=====>........................] - ETA: 1:53:55 - loss: 0.3839 - regression_loss: 0.3435 - classification_loss: 0.0405
 2301/10000 [=====>........................] - ETA: 1:53:54 - loss: 0.3839 - regression_loss: 0.3434 - classification_loss: 0.0405
 2302/10000 [=====>........................] - ETA: 1:53:53 - loss: 0.3839 - regression_loss: 0.3434 - classification_loss: 0.0405
 2303/10000 [=====>........................] - ETA: 1:53:52 - loss: 0.3839 - regression_loss: 0.3435 - classification_loss: 0.0404
 2304/10000 [=====>........................] - ETA: 1:53:51 - loss: 0.3840 - regression_loss: 0.3436 - classification_loss: 0.0404
 2305/10000 [=====>........................] - ETA: 1:53:51 - loss: 0.3841 - regression_loss: 0.3437 - classification_loss: 0.0405
 2306/10000 [=====>........................] - ETA: 1:53:50 - loss: 0.3843 - regression_loss: 0.3438 - classification_loss: 0.0405
 2307/10000 [=====>........................] - ETA: 1:53:49 - loss: 0.3843 - regression_loss: 0.3438 - classification_loss: 0.0404
 2308/10000 [=====>........................] - ETA: 1:53:48 - loss: 0.3842 - regression_loss: 0.3438 - classification_loss: 0.0404
 2309/10000 [=====>........................] - ETA: 1:53:47 - loss: 0.3842 - regression_loss: 0.3438 - classification_loss: 0.0404
 2310/10000 [=====>........................] - ETA: 1:53:46 - loss: 0.3842 - regression_loss: 0.3438 - classification_loss: 0.0404
 2311/10000 [=====>........................] - ETA: 1:53:45 - loss: 0.3843 - regression_loss: 0.3439 - classification_loss: 0.0404
 2312/10000 [=====>........................] - ETA: 1:53:44 - loss: 0.3843 - regression_loss: 0.3439 - classification_loss: 0.0404
 2313/10000 [=====>........................] - ETA: 1:53:43 - loss: 0.3842 - regression_loss: 0.3437 - classification_loss: 0.0404
 2314/10000 [=====>........................] - ETA: 1:53:43 - loss: 0.3841 - regression_loss: 0.3437 - classification_loss: 0.0404
 2315/10000 [=====>........................] - ETA: 1:53:42 - loss: 0.3841 - regression_loss: 0.3437 - classification_loss: 0.0404
 2316/10000 [=====>........................] - ETA: 1:53:41 - loss: 0.3840 - regression_loss: 0.3436 - classification_loss: 0.0404
 2317/10000 [=====>........................] - ETA: 1:53:40 - loss: 0.3847 - regression_loss: 0.3443 - classification_loss: 0.0405
 2318/10000 [=====>........................] - ETA: 1:53:39 - loss: 0.3848 - regression_loss: 0.3443 - classification_loss: 0.0405
 2319/10000 [=====>........................] - ETA: 1:53:38 - loss: 0.3857 - regression_loss: 0.3450 - classification_loss: 0.0407
 2320/10000 [=====>........................] - ETA: 1:53:37 - loss: 0.3857 - regression_loss: 0.3450 - classification_loss: 0.0407
 2321/10000 [=====>........................] - ETA: 1:53:36 - loss: 0.3858 - regression_loss: 0.3451 - classification_loss: 0.0407
 2322/10000 [=====>........................] - ETA: 1:53:35 - loss: 0.3857 - regression_loss: 0.3450 - classification_loss: 0.0407
 2323/10000 [=====>........................] - ETA: 1:53:35 - loss: 0.3858 - regression_loss: 0.3450 - classification_loss: 0.0407
 2324/10000 [=====>........................] - ETA: 1:53:34 - loss: 0.3859 - regression_loss: 0.3451 - classification_loss: 0.0407
 2325/10000 [=====>........................] - ETA: 1:53:33 - loss: 0.3858 - regression_loss: 0.3451 - classification_loss: 0.0407
 2326/10000 [=====>........................] - ETA: 1:53:32 - loss: 0.3857 - regression_loss: 0.3450 - classification_loss: 0.0407
 2327/10000 [=====>........................] - ETA: 1:53:31 - loss: 0.3858 - regression_loss: 0.3450 - classification_loss: 0.0407
 2328/10000 [=====>........................] - ETA: 1:53:30 - loss: 0.3857 - regression_loss: 0.3450 - classification_loss: 0.0407
 2329/10000 [=====>........................] - ETA: 1:53:29 - loss: 0.3857 - regression_loss: 0.3450 - classification_loss: 0.0407
 2330/10000 [=====>........................] - ETA: 1:53:28 - loss: 0.3856 - regression_loss: 0.3449 - classification_loss: 0.0407
 2331/10000 [=====>........................] - ETA: 1:53:27 - loss: 0.3856 - regression_loss: 0.3449 - classification_loss: 0.0407
 2332/10000 [=====>........................] - ETA: 1:53:27 - loss: 0.3855 - regression_loss: 0.3448 - classification_loss: 0.0407
 2333/10000 [=====>........................] - ETA: 1:53:26 - loss: 0.3854 - regression_loss: 0.3447 - classification_loss: 0.0407
 2334/10000 [======>.......................] - ETA: 1:53:25 - loss: 0.3853 - regression_loss: 0.3446 - classification_loss: 0.0407
 2335/10000 [======>.......................] - ETA: 1:53:24 - loss: 0.3852 - regression_loss: 0.3446 - classification_loss: 0.0406
 2336/10000 [======>.......................] - ETA: 1:53:23 - loss: 0.3852 - regression_loss: 0.3446 - classification_loss: 0.0406
 2337/10000 [======>.......................] - ETA: 1:53:22 - loss: 0.3853 - regression_loss: 0.3446 - classification_loss: 0.0406
 2338/10000 [======>.......................] - ETA: 1:53:21 - loss: 0.3852 - regression_loss: 0.3446 - classification_loss: 0.0406
 2339/10000 [======>.......................] - ETA: 1:53:20 - loss: 0.3851 - regression_loss: 0.3445 - classification_loss: 0.0406
 2340/10000 [======>.......................] - ETA: 1:53:19 - loss: 0.3852 - regression_loss: 0.3446 - classification_loss: 0.0406
 2341/10000 [======>.......................] - ETA: 1:53:19 - loss: 0.3852 - regression_loss: 0.3446 - classification_loss: 0.0406
 2342/10000 [======>.......................] - ETA: 1:53:18 - loss: 0.3851 - regression_loss: 0.3445 - classification_loss: 0.0406
 2343/10000 [======>.......................] - ETA: 1:53:17 - loss: 0.3853 - regression_loss: 0.3447 - classification_loss: 0.0406
 2344/10000 [======>.......................] - ETA: 1:53:16 - loss: 0.3852 - regression_loss: 0.3446 - classification_loss: 0.0406
 2345/10000 [======>.......................] - ETA: 1:53:15 - loss: 0.3852 - regression_loss: 0.3446 - classification_loss: 0.0406
 2346/10000 [======>.......................] - ETA: 1:53:14 - loss: 0.3853 - regression_loss: 0.3447 - classification_loss: 0.0406
 2347/10000 [======>.......................] - ETA: 1:53:13 - loss: 0.3853 - regression_loss: 0.3447 - classification_loss: 0.0406
 2348/10000 [======>.......................] - ETA: 1:53:12 - loss: 0.3853 - regression_loss: 0.3447 - classification_loss: 0.0406
 2349/10000 [======>.......................] - ETA: 1:53:11 - loss: 0.3853 - regression_loss: 0.3447 - classification_loss: 0.0407
 2350/10000 [======>.......................] - ETA: 1:53:11 - loss: 0.3853 - regression_loss: 0.3446 - classification_loss: 0.0407
 2351/10000 [======>.......................] - ETA: 1:53:10 - loss: 0.3853 - regression_loss: 0.3446 - classification_loss: 0.0407
 2352/10000 [======>.......................] - ETA: 1:53:09 - loss: 0.3852 - regression_loss: 0.3445 - classification_loss: 0.0407
 2353/10000 [======>.......................] - ETA: 1:53:08 - loss: 0.3853 - regression_loss: 0.3446 - classification_loss: 0.0407
 2354/10000 [======>.......................] - ETA: 1:53:07 - loss: 0.3854 - regression_loss: 0.3447 - classification_loss: 0.0407
 2355/10000 [======>.......................] - ETA: 1:53:06 - loss: 0.3853 - regression_loss: 0.3446 - classification_loss: 0.0407
 2356/10000 [======>.......................] - ETA: 1:53:05 - loss: 0.3853 - regression_loss: 0.3446 - classification_loss: 0.0407
 2357/10000 [======>.......................] - ETA: 1:53:04 - loss: 0.3852 - regression_loss: 0.3445 - classification_loss: 0.0406
 2358/10000 [======>.......................] - ETA: 1:53:04 - loss: 0.3851 - regression_loss: 0.3445 - classification_loss: 0.0406
 2359/10000 [======>.......................] - ETA: 1:53:03 - loss: 0.3852 - regression_loss: 0.3445 - classification_loss: 0.0407
 2360/10000 [======>.......................] - ETA: 1:53:02 - loss: 0.3850 - regression_loss: 0.3443 - classification_loss: 0.0407
 2361/10000 [======>.......................] - ETA: 1:53:01 - loss: 0.3849 - regression_loss: 0.3442 - classification_loss: 0.0407
 2362/10000 [======>.......................] - ETA: 1:53:00 - loss: 0.3849 - regression_loss: 0.3442 - classification_loss: 0.0407
 2363/10000 [======>.......................] - ETA: 1:52:59 - loss: 0.3850 - regression_loss: 0.3443 - classification_loss: 0.0407
 2364/10000 [======>.......................] - ETA: 1:52:58 - loss: 0.3851 - regression_loss: 0.3444 - classification_loss: 0.0407
 2365/10000 [======>.......................] - ETA: 1:52:57 - loss: 0.3852 - regression_loss: 0.3445 - classification_loss: 0.0407
 2366/10000 [======>.......................] - ETA: 1:52:56 - loss: 0.3851 - regression_loss: 0.3445 - classification_loss: 0.0407
 2367/10000 [======>.......................] - ETA: 1:52:56 - loss: 0.3853 - regression_loss: 0.3445 - classification_loss: 0.0407
 2368/10000 [======>.......................] - ETA: 1:52:55 - loss: 0.3853 - regression_loss: 0.3446 - classification_loss: 0.0407
 2369/10000 [======>.......................] - ETA: 1:52:54 - loss: 0.3853 - regression_loss: 0.3446 - classification_loss: 0.0407
 2370/10000 [======>.......................] - ETA: 1:52:53 - loss: 0.3853 - regression_loss: 0.3446 - classification_loss: 0.0407
 2371/10000 [======>.......................] - ETA: 1:52:52 - loss: 0.3852 - regression_loss: 0.3445 - classification_loss: 0.0407
 2372/10000 [======>.......................] - ETA: 1:52:51 - loss: 0.3852 - regression_loss: 0.3445 - classification_loss: 0.0407
 2373/10000 [======>.......................] - ETA: 1:52:50 - loss: 0.3851 - regression_loss: 0.3445 - classification_loss: 0.0407
 2374/10000 [======>.......................] - ETA: 1:52:49 - loss: 0.3851 - regression_loss: 0.3444 - classification_loss: 0.0407
 2375/10000 [======>.......................] - ETA: 1:52:48 - loss: 0.3852 - regression_loss: 0.3445 - classification_loss: 0.0407
 2376/10000 [======>.......................] - ETA: 1:52:48 - loss: 0.3852 - regression_loss: 0.3445 - classification_loss: 0.0407
 2377/10000 [======>.......................] - ETA: 1:52:47 - loss: 0.3854 - regression_loss: 0.3447 - classification_loss: 0.0407
 2378/10000 [======>.......................] - ETA: 1:52:46 - loss: 0.3864 - regression_loss: 0.3452 - classification_loss: 0.0412
 2379/10000 [======>.......................] - ETA: 1:52:45 - loss: 0.3865 - regression_loss: 0.3453 - classification_loss: 0.0412
 2380/10000 [======>.......................] - ETA: 1:52:44 - loss: 0.3865 - regression_loss: 0.3453 - classification_loss: 0.0412
 2381/10000 [======>.......................] - ETA: 1:52:43 - loss: 0.3865 - regression_loss: 0.3453 - classification_loss: 0.0412
 2382/10000 [======>.......................] - ETA: 1:52:42 - loss: 0.3868 - regression_loss: 0.3456 - classification_loss: 0.0412
 2383/10000 [======>.......................] - ETA: 1:52:41 - loss: 0.3868 - regression_loss: 0.3456 - classification_loss: 0.0412
 2384/10000 [======>.......................] - ETA: 1:52:40 - loss: 0.3869 - regression_loss: 0.3456 - classification_loss: 0.0412
 2385/10000 [======>.......................] - ETA: 1:52:40 - loss: 0.3869 - regression_loss: 0.3457 - classification_loss: 0.0412
 2386/10000 [======>.......................] - ETA: 1:52:39 - loss: 0.3867 - regression_loss: 0.3455 - classification_loss: 0.0412
 2387/10000 [======>.......................] - ETA: 1:52:38 - loss: 0.3868 - regression_loss: 0.3456 - classification_loss: 0.0412
 2388/10000 [======>.......................] - ETA: 1:52:37 - loss: 0.3868 - regression_loss: 0.3457 - classification_loss: 0.0412
 2389/10000 [======>.......................] - ETA: 1:52:36 - loss: 0.3869 - regression_loss: 0.3457 - classification_loss: 0.0412
 2390/10000 [======>.......................] - ETA: 1:52:35 - loss: 0.3870 - regression_loss: 0.3458 - classification_loss: 0.0412
 2391/10000 [======>.......................] - ETA: 1:52:34 - loss: 0.3869 - regression_loss: 0.3457 - classification_loss: 0.0412
 2392/10000 [======>.......................] - ETA: 1:52:33 - loss: 0.3871 - regression_loss: 0.3459 - classification_loss: 0.0412
 2393/10000 [======>.......................] - ETA: 1:52:32 - loss: 0.3871 - regression_loss: 0.3460 - classification_loss: 0.0412
 2394/10000 [======>.......................] - ETA: 1:52:32 - loss: 0.3873 - regression_loss: 0.3461 - classification_loss: 0.0412
 2395/10000 [======>.......................] - ETA: 1:52:31 - loss: 0.3872 - regression_loss: 0.3461 - classification_loss: 0.0411
 2396/10000 [======>.......................] - ETA: 1:52:30 - loss: 0.3873 - regression_loss: 0.3462 - classification_loss: 0.0411
 2397/10000 [======>.......................] - ETA: 1:52:29 - loss: 0.3872 - regression_loss: 0.3461 - classification_loss: 0.0411
 2398/10000 [======>.......................] - ETA: 1:52:28 - loss: 0.3871 - regression_loss: 0.3460 - classification_loss: 0.0411
 2399/10000 [======>.......................] - ETA: 1:52:27 - loss: 0.3871 - regression_loss: 0.3460 - classification_loss: 0.0411
 2400/10000 [======>.......................] - ETA: 1:52:26 - loss: 0.3872 - regression_loss: 0.3461 - classification_loss: 0.0411
 2401/10000 [======>.......................] - ETA: 1:52:25 - loss: 0.3872 - regression_loss: 0.3462 - classification_loss: 0.0411
 2402/10000 [======>.......................] - ETA: 1:52:25 - loss: 0.3872 - regression_loss: 0.3461 - classification_loss: 0.0411
 2403/10000 [======>.......................] - ETA: 1:52:24 - loss: 0.3872 - regression_loss: 0.3461 - classification_loss: 0.0411
 2404/10000 [======>.......................] - ETA: 1:52:23 - loss: 0.3874 - regression_loss: 0.3463 - classification_loss: 0.0411
 2405/10000 [======>.......................] - ETA: 1:52:22 - loss: 0.3874 - regression_loss: 0.3462 - classification_loss: 0.0411
 2406/10000 [======>.......................] - ETA: 1:52:21 - loss: 0.3873 - regression_loss: 0.3462 - classification_loss: 0.0411
 2407/10000 [======>.......................] - ETA: 1:52:20 - loss: 0.3875 - regression_loss: 0.3464 - classification_loss: 0.0411
 2408/10000 [======>.......................] - ETA: 1:52:19 - loss: 0.3875 - regression_loss: 0.3464 - classification_loss: 0.0411
 2409/10000 [======>.......................] - ETA: 1:52:18 - loss: 0.3875 - regression_loss: 0.3464 - classification_loss: 0.0411
 2410/10000 [======>.......................] - ETA: 1:52:17 - loss: 0.3874 - regression_loss: 0.3463 - classification_loss: 0.0411
 2411/10000 [======>.......................] - ETA: 1:52:17 - loss: 0.3873 - regression_loss: 0.3463 - classification_loss: 0.0411
 2412/10000 [======>.......................] - ETA: 1:52:16 - loss: 0.3873 - regression_loss: 0.3463 - classification_loss: 0.0411
 2413/10000 [======>.......................] - ETA: 1:52:15 - loss: 0.3872 - regression_loss: 0.3462 - classification_loss: 0.0411
 2414/10000 [======>.......................] - ETA: 1:52:14 - loss: 0.3871 - regression_loss: 0.3460 - classification_loss: 0.0410
 2415/10000 [======>.......................] - ETA: 1:52:13 - loss: 0.3873 - regression_loss: 0.3463 - classification_loss: 0.0411
 2416/10000 [======>.......................] - ETA: 1:52:12 - loss: 0.3874 - regression_loss: 0.3463 - classification_loss: 0.0411
 2417/10000 [======>.......................] - ETA: 1:52:11 - loss: 0.3874 - regression_loss: 0.3463 - classification_loss: 0.0411
 2418/10000 [======>.......................] - ETA: 1:52:10 - loss: 0.3873 - regression_loss: 0.3462 - classification_loss: 0.0411
 2419/10000 [======>.......................] - ETA: 1:52:09 - loss: 0.3872 - regression_loss: 0.3462 - classification_loss: 0.0410
 2420/10000 [======>.......................] - ETA: 1:52:09 - loss: 0.3872 - regression_loss: 0.3462 - classification_loss: 0.0410
 2421/10000 [======>.......................] - ETA: 1:52:08 - loss: 0.3873 - regression_loss: 0.3462 - classification_loss: 0.0410
 2422/10000 [======>.......................] - ETA: 1:52:07 - loss: 0.3872 - regression_loss: 0.3462 - classification_loss: 0.0410
 2423/10000 [======>.......................] - ETA: 1:52:06 - loss: 0.3874 - regression_loss: 0.3464 - classification_loss: 0.0410
 2424/10000 [======>.......................] - ETA: 1:52:05 - loss: 0.3874 - regression_loss: 0.3464 - classification_loss: 0.0410
 2425/10000 [======>.......................] - ETA: 1:52:04 - loss: 0.3874 - regression_loss: 0.3464 - classification_loss: 0.0410
 2426/10000 [======>.......................] - ETA: 1:52:03 - loss: 0.3874 - regression_loss: 0.3464 - classification_loss: 0.0410
 2427/10000 [======>.......................] - ETA: 1:52:02 - loss: 0.3872 - regression_loss: 0.3462 - classification_loss: 0.0410
 2428/10000 [======>.......................] - ETA: 1:52:02 - loss: 0.3873 - regression_loss: 0.3463 - classification_loss: 0.0410
 2429/10000 [======>.......................] - ETA: 1:52:01 - loss: 0.3873 - regression_loss: 0.3462 - classification_loss: 0.0410
 2430/10000 [======>.......................] - ETA: 1:52:00 - loss: 0.3873 - regression_loss: 0.3463 - classification_loss: 0.0410
 2431/10000 [======>.......................] - ETA: 1:51:59 - loss: 0.3872 - regression_loss: 0.3462 - classification_loss: 0.0410
 2432/10000 [======>.......................] - ETA: 1:51:58 - loss: 0.3871 - regression_loss: 0.3461 - classification_loss: 0.0410
 2433/10000 [======>.......................] - ETA: 1:51:57 - loss: 0.3874 - regression_loss: 0.3463 - classification_loss: 0.0411
 2434/10000 [======>.......................] - ETA: 1:51:56 - loss: 0.3874 - regression_loss: 0.3463 - classification_loss: 0.0411
 2435/10000 [======>.......................] - ETA: 1:51:55 - loss: 0.3875 - regression_loss: 0.3464 - classification_loss: 0.0411
 2436/10000 [======>.......................] - ETA: 1:51:54 - loss: 0.3875 - regression_loss: 0.3464 - classification_loss: 0.0411
 2437/10000 [======>.......................] - ETA: 1:51:54 - loss: 0.3874 - regression_loss: 0.3463 - classification_loss: 0.0411
 2438/10000 [======>.......................] - ETA: 1:51:53 - loss: 0.3873 - regression_loss: 0.3463 - classification_loss: 0.0411
 2439/10000 [======>.......................] - ETA: 1:51:52 - loss: 0.3873 - regression_loss: 0.3462 - classification_loss: 0.0411
 2440/10000 [======>.......................] - ETA: 1:51:51 - loss: 0.3872 - regression_loss: 0.3462 - classification_loss: 0.0411
 2441/10000 [======>.......................] - ETA: 1:51:50 - loss: 0.3872 - regression_loss: 0.3461 - classification_loss: 0.0411
 2442/10000 [======>.......................] - ETA: 1:51:49 - loss: 0.3872 - regression_loss: 0.3461 - classification_loss: 0.0411
 2443/10000 [======>.......................] - ETA: 1:51:48 - loss: 0.3872 - regression_loss: 0.3461 - classification_loss: 0.0411
 2444/10000 [======>.......................] - ETA: 1:51:47 - loss: 0.3872 - regression_loss: 0.3461 - classification_loss: 0.0411
 2445/10000 [======>.......................] - ETA: 1:51:46 - loss: 0.3870 - regression_loss: 0.3459 - classification_loss: 0.0411
 2446/10000 [======>.......................] - ETA: 1:51:46 - loss: 0.3871 - regression_loss: 0.3460 - classification_loss: 0.0411
 2447/10000 [======>.......................] - ETA: 1:51:45 - loss: 0.3872 - regression_loss: 0.3460 - classification_loss: 0.0412
 2448/10000 [======>.......................] - ETA: 1:51:44 - loss: 0.3872 - regression_loss: 0.3460 - classification_loss: 0.0412
 2449/10000 [======>.......................] - ETA: 1:51:43 - loss: 0.3870 - regression_loss: 0.3459 - classification_loss: 0.0412
 2450/10000 [======>.......................] - ETA: 1:51:42 - loss: 0.3869 - regression_loss: 0.3458 - classification_loss: 0.0411
 2451/10000 [======>.......................] - ETA: 1:51:41 - loss: 0.3873 - regression_loss: 0.3461 - classification_loss: 0.0411
 2452/10000 [======>.......................] - ETA: 1:51:40 - loss: 0.3872 - regression_loss: 0.3461 - classification_loss: 0.0411
 2453/10000 [======>.......................] - ETA: 1:51:39 - loss: 0.3872 - regression_loss: 0.3461 - classification_loss: 0.0411
 2454/10000 [======>.......................] - ETA: 1:51:38 - loss: 0.3871 - regression_loss: 0.3460 - classification_loss: 0.0411
 2455/10000 [======>.......................] - ETA: 1:51:38 - loss: 0.3870 - regression_loss: 0.3459 - classification_loss: 0.0411
 2456/10000 [======>.......................] - ETA: 1:51:37 - loss: 0.3870 - regression_loss: 0.3459 - classification_loss: 0.0411
 2457/10000 [======>.......................] - ETA: 1:51:36 - loss: 0.3869 - regression_loss: 0.3458 - classification_loss: 0.0411
 2458/10000 [======>.......................] - ETA: 1:51:35 - loss: 0.3868 - regression_loss: 0.3457 - classification_loss: 0.0411
 2459/10000 [======>.......................] - ETA: 1:51:34 - loss: 0.3869 - regression_loss: 0.3459 - classification_loss: 0.0411
 2460/10000 [======>.......................] - ETA: 1:51:33 - loss: 0.3869 - regression_loss: 0.3458 - classification_loss: 0.0411
 2461/10000 [======>.......................] - ETA: 1:51:32 - loss: 0.3870 - regression_loss: 0.3459 - classification_loss: 0.0411
 2462/10000 [======>.......................] - ETA: 1:51:31 - loss: 0.3871 - regression_loss: 0.3460 - classification_loss: 0.0411
 2463/10000 [======>.......................] - ETA: 1:51:31 - loss: 0.3873 - regression_loss: 0.3462 - classification_loss: 0.0411
 2464/10000 [======>.......................] - ETA: 1:51:30 - loss: 0.3873 - regression_loss: 0.3462 - classification_loss: 0.0411
 2465/10000 [======>.......................] - ETA: 1:51:29 - loss: 0.3872 - regression_loss: 0.3461 - classification_loss: 0.0411
 2466/10000 [======>.......................] - ETA: 1:51:28 - loss: 0.3872 - regression_loss: 0.3461 - classification_loss: 0.0411
 2467/10000 [======>.......................] - ETA: 1:51:27 - loss: 0.3871 - regression_loss: 0.3461 - classification_loss: 0.0411
 2468/10000 [======>.......................] - ETA: 1:51:26 - loss: 0.3871 - regression_loss: 0.3460 - classification_loss: 0.0411
 2469/10000 [======>.......................] - ETA: 1:51:25 - loss: 0.3873 - regression_loss: 0.3462 - classification_loss: 0.0411
 2470/10000 [======>.......................] - ETA: 1:51:24 - loss: 0.3873 - regression_loss: 0.3462 - classification_loss: 0.0411
 2471/10000 [======>.......................] - ETA: 1:51:23 - loss: 0.3871 - regression_loss: 0.3460 - classification_loss: 0.0411
 2472/10000 [======>.......................] - ETA: 1:51:23 - loss: 0.3871 - regression_loss: 0.3460 - classification_loss: 0.0411
 2473/10000 [======>.......................] - ETA: 1:51:22 - loss: 0.3871 - regression_loss: 0.3460 - classification_loss: 0.0411
 2474/10000 [======>.......................] - ETA: 1:51:21 - loss: 0.3871 - regression_loss: 0.3460 - classification_loss: 0.0411
 2475/10000 [======>.......................] - ETA: 1:51:20 - loss: 0.3872 - regression_loss: 0.3461 - classification_loss: 0.0411
 2476/10000 [======>.......................] - ETA: 1:51:19 - loss: 0.3871 - regression_loss: 0.3460 - classification_loss: 0.0411
 2477/10000 [======>.......................] - ETA: 1:51:18 - loss: 0.3869 - regression_loss: 0.3458 - classification_loss: 0.0411
 2478/10000 [======>.......................] - ETA: 1:51:17 - loss: 0.3871 - regression_loss: 0.3460 - classification_loss: 0.0411
 2479/10000 [======>.......................] - ETA: 1:51:16 - loss: 0.3872 - regression_loss: 0.3461 - classification_loss: 0.0411
 2480/10000 [======>.......................] - ETA: 1:51:15 - loss: 0.3872 - regression_loss: 0.3461 - classification_loss: 0.0411
 2481/10000 [======>.......................] - ETA: 1:51:15 - loss: 0.3871 - regression_loss: 0.3460 - classification_loss: 0.0411
 2482/10000 [======>.......................] - ETA: 1:51:14 - loss: 0.3871 - regression_loss: 0.3460 - classification_loss: 0.0411
 2483/10000 [======>.......................] - ETA: 1:51:13 - loss: 0.3872 - regression_loss: 0.3461 - classification_loss: 0.0411
 2484/10000 [======>.......................] - ETA: 1:51:12 - loss: 0.3872 - regression_loss: 0.3461 - classification_loss: 0.0411
 2485/10000 [======>.......................] - ETA: 1:51:11 - loss: 0.3870 - regression_loss: 0.3459 - classification_loss: 0.0411
 2486/10000 [======>.......................] - ETA: 1:51:10 - loss: 0.3871 - regression_loss: 0.3460 - classification_loss: 0.0411
 2487/10000 [======>.......................] - ETA: 1:51:09 - loss: 0.3872 - regression_loss: 0.3461 - classification_loss: 0.0411
 2488/10000 [======>.......................] - ETA: 1:51:08 - loss: 0.3871 - regression_loss: 0.3460 - classification_loss: 0.0411
 2489/10000 [======>.......................] - ETA: 1:51:07 - loss: 0.3873 - regression_loss: 0.3462 - classification_loss: 0.0411
 2490/10000 [======>.......................] - ETA: 1:51:07 - loss: 0.3873 - regression_loss: 0.3462 - classification_loss: 0.0411
 2491/10000 [======>.......................] - ETA: 1:51:06 - loss: 0.3873 - regression_loss: 0.3462 - classification_loss: 0.0411
 2492/10000 [======>.......................] - ETA: 1:51:05 - loss: 0.3874 - regression_loss: 0.3463 - classification_loss: 0.0411
 2493/10000 [======>.......................] - ETA: 1:51:04 - loss: 0.3873 - regression_loss: 0.3462 - classification_loss: 0.0411
 2494/10000 [======>.......................] - ETA: 1:51:03 - loss: 0.3872 - regression_loss: 0.3461 - classification_loss: 0.0411
 2495/10000 [======>.......................] - ETA: 1:51:02 - loss: 0.3872 - regression_loss: 0.3461 - classification_loss: 0.0411
 2496/10000 [======>.......................] - ETA: 1:51:01 - loss: 0.3871 - regression_loss: 0.3460 - classification_loss: 0.0411
 2497/10000 [======>.......................] - ETA: 1:51:00 - loss: 0.3871 - regression_loss: 0.3460 - classification_loss: 0.0411
 2498/10000 [======>.......................] - ETA: 1:51:00 - loss: 0.3870 - regression_loss: 0.3460 - classification_loss: 0.0411
 2499/10000 [======>.......................] - ETA: 1:50:59 - loss: 0.3870 - regression_loss: 0.3459 - classification_loss: 0.0411
 2500/10000 [======>.......................] - ETA: 1:50:58 - loss: 0.3869 - regression_loss: 0.3459 - classification_loss: 0.0411
 2501/10000 [======>.......................] - ETA: 1:50:57 - loss: 0.3869 - regression_loss: 0.3458 - classification_loss: 0.0411
 2502/10000 [======>.......................] - ETA: 1:50:56 - loss: 0.3868 - regression_loss: 0.3457 - classification_loss: 0.0410
 2503/10000 [======>.......................] - ETA: 1:50:55 - loss: 0.3868 - regression_loss: 0.3458 - classification_loss: 0.0411
 2504/10000 [======>.......................] - ETA: 1:50:54 - loss: 0.3869 - regression_loss: 0.3458 - classification_loss: 0.0410
 2505/10000 [======>.......................] - ETA: 1:50:53 - loss: 0.3868 - regression_loss: 0.3458 - classification_loss: 0.0410
 2506/10000 [======>.......................] - ETA: 1:50:52 - loss: 0.3866 - regression_loss: 0.3456 - classification_loss: 0.0410
 2507/10000 [======>.......................] - ETA: 1:50:51 - loss: 0.3870 - regression_loss: 0.3459 - classification_loss: 0.0411
 2508/10000 [======>.......................] - ETA: 1:50:51 - loss: 0.3870 - regression_loss: 0.3459 - classification_loss: 0.0411
 2509/10000 [======>.......................] - ETA: 1:50:50 - loss: 0.3869 - regression_loss: 0.3458 - classification_loss: 0.0411
 2510/10000 [======>.......................] - ETA: 1:50:49 - loss: 0.3869 - regression_loss: 0.3459 - classification_loss: 0.0411
 2511/10000 [======>.......................] - ETA: 1:50:48 - loss: 0.3869 - regression_loss: 0.3459 - classification_loss: 0.0411
 2512/10000 [======>.......................] - ETA: 1:50:47 - loss: 0.3876 - regression_loss: 0.3464 - classification_loss: 0.0413
 2513/10000 [======>.......................] - ETA: 1:50:46 - loss: 0.3875 - regression_loss: 0.3463 - classification_loss: 0.0412
 2514/10000 [======>.......................] - ETA: 1:50:45 - loss: 0.3875 - regression_loss: 0.3462 - classification_loss: 0.0412
 2515/10000 [======>.......................] - ETA: 1:50:44 - loss: 0.3875 - regression_loss: 0.3462 - classification_loss: 0.0412
 2516/10000 [======>.......................] - ETA: 1:50:44 - loss: 0.3875 - regression_loss: 0.3462 - classification_loss: 0.0412
 2517/10000 [======>.......................] - ETA: 1:50:43 - loss: 0.3874 - regression_loss: 0.3462 - classification_loss: 0.0412
 2518/10000 [======>.......................] - ETA: 1:50:42 - loss: 0.3873 - regression_loss: 0.3461 - classification_loss: 0.0412
 2519/10000 [======>.......................] - ETA: 1:50:41 - loss: 0.3872 - regression_loss: 0.3460 - classification_loss: 0.0412
 2520/10000 [======>.......................] - ETA: 1:50:40 - loss: 0.3871 - regression_loss: 0.3459 - classification_loss: 0.0412
 2521/10000 [======>.......................] - ETA: 1:50:39 - loss: 0.3871 - regression_loss: 0.3460 - classification_loss: 0.0412
 2522/10000 [======>.......................] - ETA: 1:50:38 - loss: 0.3872 - regression_loss: 0.3460 - classification_loss: 0.0412
 2523/10000 [======>.......................] - ETA: 1:50:37 - loss: 0.3871 - regression_loss: 0.3459 - classification_loss: 0.0412
 2524/10000 [======>.......................] - ETA: 1:50:36 - loss: 0.3871 - regression_loss: 0.3459 - classification_loss: 0.0412
 2525/10000 [======>.......................] - ETA: 1:50:36 - loss: 0.3873 - regression_loss: 0.3461 - classification_loss: 0.0412
 2526/10000 [======>.......................] - ETA: 1:50:35 - loss: 0.3874 - regression_loss: 0.3462 - classification_loss: 0.0412
 2527/10000 [======>.......................] - ETA: 1:50:34 - loss: 0.3876 - regression_loss: 0.3463 - classification_loss: 0.0413
 2528/10000 [======>.......................] - ETA: 1:50:33 - loss: 0.3875 - regression_loss: 0.3462 - classification_loss: 0.0413
 2529/10000 [======>.......................] - ETA: 1:50:32 - loss: 0.3874 - regression_loss: 0.3461 - classification_loss: 0.0413
 2530/10000 [======>.......................] - ETA: 1:50:31 - loss: 0.3875 - regression_loss: 0.3462 - classification_loss: 0.0413
 2531/10000 [======>.......................] - ETA: 1:50:30 - loss: 0.3875 - regression_loss: 0.3462 - classification_loss: 0.0413
 2532/10000 [======>.......................] - ETA: 1:50:29 - loss: 0.3874 - regression_loss: 0.3461 - classification_loss: 0.0413
 2533/10000 [======>.......................] - ETA: 1:50:28 - loss: 0.3877 - regression_loss: 0.3464 - classification_loss: 0.0413
 2534/10000 [======>.......................] - ETA: 1:50:28 - loss: 0.3878 - regression_loss: 0.3465 - classification_loss: 0.0413
 2535/10000 [======>.......................] - ETA: 1:50:27 - loss: 0.3877 - regression_loss: 0.3464 - classification_loss: 0.0413
 2536/10000 [======>.......................] - ETA: 1:50:26 - loss: 0.3878 - regression_loss: 0.3465 - classification_loss: 0.0413
 2537/10000 [======>.......................] - ETA: 1:50:25 - loss: 0.3878 - regression_loss: 0.3464 - classification_loss: 0.0413
 2538/10000 [======>.......................] - ETA: 1:50:24 - loss: 0.3878 - regression_loss: 0.3464 - classification_loss: 0.0413
 2539/10000 [======>.......................] - ETA: 1:50:23 - loss: 0.3878 - regression_loss: 0.3465 - classification_loss: 0.0413
 2540/10000 [======>.......................] - ETA: 1:50:22 - loss: 0.3877 - regression_loss: 0.3464 - classification_loss: 0.0413
 2541/10000 [======>.......................] - ETA: 1:50:21 - loss: 0.3875 - regression_loss: 0.3462 - classification_loss: 0.0413
 2542/10000 [======>.......................] - ETA: 1:50:20 - loss: 0.3876 - regression_loss: 0.3462 - classification_loss: 0.0413
 2543/10000 [======>.......................] - ETA: 1:50:20 - loss: 0.3876 - regression_loss: 0.3463 - classification_loss: 0.0413
 2544/10000 [======>.......................] - ETA: 1:50:19 - loss: 0.3878 - regression_loss: 0.3464 - classification_loss: 0.0413
 2545/10000 [======>.......................] - ETA: 1:50:18 - loss: 0.3877 - regression_loss: 0.3464 - classification_loss: 0.0413
 2546/10000 [======>.......................] - ETA: 1:50:17 - loss: 0.3877 - regression_loss: 0.3464 - classification_loss: 0.0413
 2547/10000 [======>.......................] - ETA: 1:50:16 - loss: 0.3876 - regression_loss: 0.3463 - classification_loss: 0.0413
 2548/10000 [======>.......................] - ETA: 1:50:15 - loss: 0.3875 - regression_loss: 0.3462 - classification_loss: 0.0413
 2549/10000 [======>.......................] - ETA: 1:50:14 - loss: 0.3875 - regression_loss: 0.3462 - classification_loss: 0.0413
 2550/10000 [======>.......................] - ETA: 1:50:13 - loss: 0.3874 - regression_loss: 0.3461 - classification_loss: 0.0413
 2551/10000 [======>.......................] - ETA: 1:50:13 - loss: 0.3874 - regression_loss: 0.3461 - classification_loss: 0.0413
 2552/10000 [======>.......................] - ETA: 1:50:12 - loss: 0.3873 - regression_loss: 0.3460 - classification_loss: 0.0413
 2553/10000 [======>.......................] - ETA: 1:50:11 - loss: 0.3874 - regression_loss: 0.3461 - classification_loss: 0.0413
 2554/10000 [======>.......................] - ETA: 1:50:10 - loss: 0.3874 - regression_loss: 0.3461 - classification_loss: 0.0413
 2555/10000 [======>.......................] - ETA: 1:50:09 - loss: 0.3874 - regression_loss: 0.3461 - classification_loss: 0.0413
 2556/10000 [======>.......................] - ETA: 1:50:08 - loss: 0.3874 - regression_loss: 0.3461 - classification_loss: 0.0413
 2557/10000 [======>.......................] - ETA: 1:50:07 - loss: 0.3873 - regression_loss: 0.3461 - classification_loss: 0.0413
 2558/10000 [======>.......................] - ETA: 1:50:06 - loss: 0.3873 - regression_loss: 0.3461 - classification_loss: 0.0412
 2559/10000 [======>.......................] - ETA: 1:50:05 - loss: 0.3873 - regression_loss: 0.3461 - classification_loss: 0.0413
 2560/10000 [======>.......................] - ETA: 1:50:04 - loss: 0.3874 - regression_loss: 0.3461 - classification_loss: 0.0412
 2561/10000 [======>.......................] - ETA: 1:50:04 - loss: 0.3875 - regression_loss: 0.3462 - classification_loss: 0.0413
 2562/10000 [======>.......................] - ETA: 1:50:03 - loss: 0.3874 - regression_loss: 0.3462 - classification_loss: 0.0412
 2563/10000 [======>.......................] - ETA: 1:50:02 - loss: 0.3874 - regression_loss: 0.3462 - classification_loss: 0.0412
 2564/10000 [======>.......................] - ETA: 1:50:01 - loss: 0.3874 - regression_loss: 0.3461 - classification_loss: 0.0412
 2565/10000 [======>.......................] - ETA: 1:50:00 - loss: 0.3873 - regression_loss: 0.3461 - classification_loss: 0.0412
 2566/10000 [======>.......................] - ETA: 1:49:59 - loss: 0.3874 - regression_loss: 0.3462 - classification_loss: 0.0412
 2567/10000 [======>.......................] - ETA: 1:49:58 - loss: 0.3875 - regression_loss: 0.3462 - classification_loss: 0.0413
 2568/10000 [======>.......................] - ETA: 1:49:57 - loss: 0.3876 - regression_loss: 0.3463 - classification_loss: 0.0413
 2569/10000 [======>.......................] - ETA: 1:49:57 - loss: 0.3875 - regression_loss: 0.3462 - classification_loss: 0.0413
 2570/10000 [======>.......................] - ETA: 1:49:56 - loss: 0.3875 - regression_loss: 0.3462 - classification_loss: 0.0413
 2571/10000 [======>.......................] - ETA: 1:49:55 - loss: 0.3874 - regression_loss: 0.3462 - classification_loss: 0.0412
 2572/10000 [======>.......................] - ETA: 1:49:54 - loss: 0.3873 - regression_loss: 0.3461 - classification_loss: 0.0412
 2573/10000 [======>.......................] - ETA: 1:49:53 - loss: 0.3873 - regression_loss: 0.3461 - classification_loss: 0.0412
 2574/10000 [======>.......................] - ETA: 1:49:52 - loss: 0.3873 - regression_loss: 0.3460 - classification_loss: 0.0412
 2575/10000 [======>.......................] - ETA: 1:49:51 - loss: 0.3873 - regression_loss: 0.3461 - classification_loss: 0.0412
 2576/10000 [======>.......................] - ETA: 1:49:50 - loss: 0.3875 - regression_loss: 0.3463 - classification_loss: 0.0412
 2577/10000 [======>.......................] - ETA: 1:49:49 - loss: 0.3874 - regression_loss: 0.3462 - classification_loss: 0.0412
 2578/10000 [======>.......................] - ETA: 1:49:49 - loss: 0.3875 - regression_loss: 0.3463 - classification_loss: 0.0412
 2579/10000 [======>.......................] - ETA: 1:49:48 - loss: 0.3875 - regression_loss: 0.3462 - classification_loss: 0.0412
 2580/10000 [======>.......................] - ETA: 1:49:47 - loss: 0.3874 - regression_loss: 0.3462 - classification_loss: 0.0412
 2581/10000 [======>.......................] - ETA: 1:49:46 - loss: 0.3873 - regression_loss: 0.3461 - classification_loss: 0.0412
 2582/10000 [======>.......................] - ETA: 1:49:45 - loss: 0.3877 - regression_loss: 0.3465 - classification_loss: 0.0412
 2583/10000 [======>.......................] - ETA: 1:49:44 - loss: 0.3877 - regression_loss: 0.3465 - classification_loss: 0.0412
 2584/10000 [======>.......................] - ETA: 1:49:43 - loss: 0.3877 - regression_loss: 0.3465 - classification_loss: 0.0412
 2585/10000 [======>.......................] - ETA: 1:49:42 - loss: 0.3877 - regression_loss: 0.3465 - classification_loss: 0.0412
 2586/10000 [======>.......................] - ETA: 1:49:41 - loss: 0.3877 - regression_loss: 0.3465 - classification_loss: 0.0412
 2587/10000 [======>.......................] - ETA: 1:49:40 - loss: 0.3878 - regression_loss: 0.3466 - classification_loss: 0.0412
 2588/10000 [======>.......................] - ETA: 1:49:40 - loss: 0.3879 - regression_loss: 0.3466 - classification_loss: 0.0412
 2589/10000 [======>.......................] - ETA: 1:49:39 - loss: 0.3879 - regression_loss: 0.3467 - classification_loss: 0.0412
 2590/10000 [======>.......................] - ETA: 1:49:38 - loss: 0.3879 - regression_loss: 0.3466 - classification_loss: 0.0412
 2591/10000 [======>.......................] - ETA: 1:49:37 - loss: 0.3879 - regression_loss: 0.3467 - classification_loss: 0.0412
 2592/10000 [======>.......................] - ETA: 1:49:36 - loss: 0.3878 - regression_loss: 0.3466 - classification_loss: 0.0412
 2593/10000 [======>.......................] - ETA: 1:49:35 - loss: 0.3877 - regression_loss: 0.3466 - classification_loss: 0.0412
 2594/10000 [======>.......................] - ETA: 1:49:34 - loss: 0.3876 - regression_loss: 0.3464 - classification_loss: 0.0412
 2595/10000 [======>.......................] - ETA: 1:49:33 - loss: 0.3875 - regression_loss: 0.3464 - classification_loss: 0.0412
 2596/10000 [======>.......................] - ETA: 1:49:33 - loss: 0.3876 - regression_loss: 0.3465 - classification_loss: 0.0412
 2597/10000 [======>.......................] - ETA: 1:49:32 - loss: 0.3876 - regression_loss: 0.3464 - classification_loss: 0.0412
 2598/10000 [======>.......................] - ETA: 1:49:31 - loss: 0.3876 - regression_loss: 0.3464 - classification_loss: 0.0412
 2599/10000 [======>.......................] - ETA: 1:49:30 - loss: 0.3879 - regression_loss: 0.3465 - classification_loss: 0.0413
 2600/10000 [======>.......................] - ETA: 1:49:29 - loss: 0.3877 - regression_loss: 0.3464 - classification_loss: 0.0413
 2601/10000 [======>.......................] - ETA: 1:49:28 - loss: 0.3877 - regression_loss: 0.3464 - classification_loss: 0.0413
 2602/10000 [======>.......................] - ETA: 1:49:27 - loss: 0.3876 - regression_loss: 0.3463 - classification_loss: 0.0413
 2603/10000 [======>.......................] - ETA: 1:49:26 - loss: 0.3876 - regression_loss: 0.3463 - classification_loss: 0.0413
 2604/10000 [======>.......................] - ETA: 1:49:25 - loss: 0.3877 - regression_loss: 0.3464 - classification_loss: 0.0413
 2605/10000 [======>.......................] - ETA: 1:49:25 - loss: 0.3883 - regression_loss: 0.3468 - classification_loss: 0.0415
 2606/10000 [======>.......................] - ETA: 1:49:24 - loss: 0.3883 - regression_loss: 0.3468 - classification_loss: 0.0415
 2607/10000 [======>.......................] - ETA: 1:49:23 - loss: 0.3883 - regression_loss: 0.3468 - classification_loss: 0.0415
 2608/10000 [======>.......................] - ETA: 1:49:22 - loss: 0.3885 - regression_loss: 0.3470 - classification_loss: 0.0415
 2609/10000 [======>.......................] - ETA: 1:49:21 - loss: 0.3885 - regression_loss: 0.3470 - classification_loss: 0.0415
 2610/10000 [======>.......................] - ETA: 1:49:20 - loss: 0.3885 - regression_loss: 0.3470 - classification_loss: 0.0415
 2611/10000 [======>.......................] - ETA: 1:49:19 - loss: 0.3884 - regression_loss: 0.3469 - classification_loss: 0.0415
 2612/10000 [======>.......................] - ETA: 1:49:18 - loss: 0.3883 - regression_loss: 0.3469 - classification_loss: 0.0415
 2613/10000 [======>.......................] - ETA: 1:49:17 - loss: 0.3884 - regression_loss: 0.3469 - classification_loss: 0.0415
 2614/10000 [======>.......................] - ETA: 1:49:17 - loss: 0.3883 - regression_loss: 0.3468 - classification_loss: 0.0414
 2615/10000 [======>.......................] - ETA: 1:49:16 - loss: 0.3882 - regression_loss: 0.3467 - classification_loss: 0.0414
 2616/10000 [======>.......................] - ETA: 1:49:15 - loss: 0.3882 - regression_loss: 0.3468 - classification_loss: 0.0414
 2617/10000 [======>.......................] - ETA: 1:49:14 - loss: 0.3881 - regression_loss: 0.3467 - classification_loss: 0.0414
 2618/10000 [======>.......................] - ETA: 1:49:13 - loss: 0.3881 - regression_loss: 0.3466 - classification_loss: 0.0414
 2619/10000 [======>.......................] - ETA: 1:49:12 - loss: 0.3880 - regression_loss: 0.3466 - classification_loss: 0.0414
 2620/10000 [======>.......................] - ETA: 1:49:11 - loss: 0.3880 - regression_loss: 0.3466 - classification_loss: 0.0414
 2621/10000 [======>.......................] - ETA: 1:49:10 - loss: 0.3880 - regression_loss: 0.3466 - classification_loss: 0.0415
 2622/10000 [======>.......................] - ETA: 1:49:09 - loss: 0.3881 - regression_loss: 0.3466 - classification_loss: 0.0415
 2623/10000 [======>.......................] - ETA: 1:49:09 - loss: 0.3880 - regression_loss: 0.3466 - classification_loss: 0.0415
 2624/10000 [======>.......................] - ETA: 1:49:08 - loss: 0.3880 - regression_loss: 0.3465 - classification_loss: 0.0414
 2625/10000 [======>.......................] - ETA: 1:49:07 - loss: 0.3879 - regression_loss: 0.3465 - classification_loss: 0.0414
 2626/10000 [======>.......................] - ETA: 1:49:06 - loss: 0.3880 - regression_loss: 0.3465 - classification_loss: 0.0415
 2627/10000 [======>.......................] - ETA: 1:49:05 - loss: 0.3880 - regression_loss: 0.3465 - classification_loss: 0.0415
 2628/10000 [======>.......................] - ETA: 1:49:04 - loss: 0.3880 - regression_loss: 0.3465 - classification_loss: 0.0415
 2629/10000 [======>.......................] - ETA: 1:49:03 - loss: 0.3880 - regression_loss: 0.3464 - classification_loss: 0.0415
 2630/10000 [======>.......................] - ETA: 1:49:02 - loss: 0.3879 - regression_loss: 0.3463 - classification_loss: 0.0415
 2631/10000 [======>.......................] - ETA: 1:49:01 - loss: 0.3878 - regression_loss: 0.3463 - classification_loss: 0.0415
 2632/10000 [======>.......................] - ETA: 1:49:01 - loss: 0.3879 - regression_loss: 0.3463 - classification_loss: 0.0415
 2633/10000 [======>.......................] - ETA: 1:49:00 - loss: 0.3878 - regression_loss: 0.3463 - classification_loss: 0.0415
 2634/10000 [======>.......................] - ETA: 1:48:59 - loss: 0.3879 - regression_loss: 0.3464 - classification_loss: 0.0415
 2635/10000 [======>.......................] - ETA: 1:48:58 - loss: 0.3879 - regression_loss: 0.3464 - classification_loss: 0.0415
 2636/10000 [======>.......................] - ETA: 1:48:57 - loss: 0.3879 - regression_loss: 0.3464 - classification_loss: 0.0415
 2637/10000 [======>.......................] - ETA: 1:48:56 - loss: 0.3878 - regression_loss: 0.3463 - classification_loss: 0.0415
 2638/10000 [======>.......................] - ETA: 1:48:55 - loss: 0.3878 - regression_loss: 0.3463 - classification_loss: 0.0415
 2639/10000 [======>.......................] - ETA: 1:48:54 - loss: 0.3878 - regression_loss: 0.3462 - classification_loss: 0.0415
 2640/10000 [======>.......................] - ETA: 1:48:53 - loss: 0.3878 - regression_loss: 0.3463 - classification_loss: 0.0415
 2641/10000 [======>.......................] - ETA: 1:48:53 - loss: 0.3878 - regression_loss: 0.3463 - classification_loss: 0.0415
 2642/10000 [======>.......................] - ETA: 1:48:52 - loss: 0.3877 - regression_loss: 0.3462 - classification_loss: 0.0415
 2643/10000 [======>.......................] - ETA: 1:48:51 - loss: 0.3877 - regression_loss: 0.3462 - classification_loss: 0.0415
 2644/10000 [======>.......................] - ETA: 1:48:50 - loss: 0.3877 - regression_loss: 0.3462 - classification_loss: 0.0415
 2645/10000 [======>.......................] - ETA: 1:48:49 - loss: 0.3876 - regression_loss: 0.3461 - classification_loss: 0.0415
 2646/10000 [======>.......................] - ETA: 1:48:48 - loss: 0.3875 - regression_loss: 0.3460 - classification_loss: 0.0415
 2647/10000 [======>.......................] - ETA: 1:48:47 - loss: 0.3884 - regression_loss: 0.3465 - classification_loss: 0.0420
 2648/10000 [======>.......................] - ETA: 1:48:46 - loss: 0.3884 - regression_loss: 0.3464 - classification_loss: 0.0419
 2649/10000 [======>.......................] - ETA: 1:48:46 - loss: 0.3883 - regression_loss: 0.3464 - classification_loss: 0.0419
 2650/10000 [======>.......................] - ETA: 1:48:45 - loss: 0.3883 - regression_loss: 0.3463 - classification_loss: 0.0419
 2651/10000 [======>.......................] - ETA: 1:48:44 - loss: 0.3883 - regression_loss: 0.3464 - classification_loss: 0.0420
 2652/10000 [======>.......................] - ETA: 1:48:43 - loss: 0.3884 - regression_loss: 0.3464 - classification_loss: 0.0420
 2653/10000 [======>.......................] - ETA: 1:48:42 - loss: 0.3883 - regression_loss: 0.3464 - classification_loss: 0.0420
 2654/10000 [======>.......................] - ETA: 1:48:41 - loss: 0.3883 - regression_loss: 0.3464 - classification_loss: 0.0419
 2655/10000 [======>.......................] - ETA: 1:48:40 - loss: 0.3885 - regression_loss: 0.3466 - classification_loss: 0.0420
 2656/10000 [======>.......................] - ETA: 1:48:39 - loss: 0.3886 - regression_loss: 0.3466 - classification_loss: 0.0420
 2657/10000 [======>.......................] - ETA: 1:48:38 - loss: 0.3885 - regression_loss: 0.3465 - classification_loss: 0.0420
 2658/10000 [======>.......................] - ETA: 1:48:38 - loss: 0.3886 - regression_loss: 0.3466 - classification_loss: 0.0420
 2659/10000 [======>.......................] - ETA: 1:48:37 - loss: 0.3885 - regression_loss: 0.3465 - classification_loss: 0.0420
 2660/10000 [======>.......................] - ETA: 1:48:36 - loss: 0.3884 - regression_loss: 0.3465 - classification_loss: 0.0420
 2661/10000 [======>.......................] - ETA: 1:48:35 - loss: 0.3884 - regression_loss: 0.3465 - classification_loss: 0.0419
 2662/10000 [======>.......................] - ETA: 1:48:34 - loss: 0.3886 - regression_loss: 0.3466 - classification_loss: 0.0419
 2663/10000 [======>.......................] - ETA: 1:48:33 - loss: 0.3885 - regression_loss: 0.3465 - classification_loss: 0.0419
 2664/10000 [======>.......................] - ETA: 1:48:32 - loss: 0.3886 - regression_loss: 0.3466 - classification_loss: 0.0419
 2665/10000 [======>.......................] - ETA: 1:48:31 - loss: 0.3885 - regression_loss: 0.3466 - classification_loss: 0.0419
 2666/10000 [======>.......................] - ETA: 1:48:30 - loss: 0.3885 - regression_loss: 0.3466 - classification_loss: 0.0419
 2667/10000 [=======>......................] - ETA: 1:48:30 - loss: 0.3886 - regression_loss: 0.3466 - classification_loss: 0.0419
 2668/10000 [=======>......................] - ETA: 1:48:29 - loss: 0.3886 - regression_loss: 0.3467 - classification_loss: 0.0419
 2669/10000 [=======>......................] - ETA: 1:48:28 - loss: 0.3889 - regression_loss: 0.3469 - classification_loss: 0.0420
 2670/10000 [=======>......................] - ETA: 1:48:27 - loss: 0.3888 - regression_loss: 0.3469 - classification_loss: 0.0420
 2671/10000 [=======>......................] - ETA: 1:48:26 - loss: 0.3889 - regression_loss: 0.3469 - classification_loss: 0.0420
 2672/10000 [=======>......................] - ETA: 1:48:25 - loss: 0.3888 - regression_loss: 0.3469 - classification_loss: 0.0419
 2673/10000 [=======>......................] - ETA: 1:48:24 - loss: 0.3890 - regression_loss: 0.3470 - classification_loss: 0.0420
 2674/10000 [=======>......................] - ETA: 1:48:23 - loss: 0.3890 - regression_loss: 0.3471 - classification_loss: 0.0420
 2675/10000 [=======>......................] - ETA: 1:48:22 - loss: 0.3890 - regression_loss: 0.3471 - classification_loss: 0.0420
 2676/10000 [=======>......................] - ETA: 1:48:22 - loss: 0.3890 - regression_loss: 0.3470 - classification_loss: 0.0420
 2677/10000 [=======>......................] - ETA: 1:48:21 - loss: 0.3890 - regression_loss: 0.3470 - classification_loss: 0.0419
 2678/10000 [=======>......................] - ETA: 1:48:20 - loss: 0.3889 - regression_loss: 0.3470 - classification_loss: 0.0419
 2679/10000 [=======>......................] - ETA: 1:48:19 - loss: 0.3888 - regression_loss: 0.3469 - classification_loss: 0.0419
 2680/10000 [=======>......................] - ETA: 1:48:18 - loss: 0.3887 - regression_loss: 0.3468 - classification_loss: 0.0419
 2681/10000 [=======>......................] - ETA: 1:48:17 - loss: 0.3887 - regression_loss: 0.3468 - classification_loss: 0.0419
 2682/10000 [=======>......................] - ETA: 1:48:16 - loss: 0.3886 - regression_loss: 0.3467 - classification_loss: 0.0419
 2683/10000 [=======>......................] - ETA: 1:48:15 - loss: 0.3887 - regression_loss: 0.3467 - classification_loss: 0.0419
 2684/10000 [=======>......................] - ETA: 1:48:15 - loss: 0.3886 - regression_loss: 0.3467 - classification_loss: 0.0419
 2685/10000 [=======>......................] - ETA: 1:48:14 - loss: 0.3886 - regression_loss: 0.3467 - classification_loss: 0.0419
 2686/10000 [=======>......................] - ETA: 1:48:13 - loss: 0.3886 - regression_loss: 0.3467 - classification_loss: 0.0419
 2687/10000 [=======>......................] - ETA: 1:48:12 - loss: 0.3886 - regression_loss: 0.3467 - classification_loss: 0.0419
 2688/10000 [=======>......................] - ETA: 1:48:11 - loss: 0.3885 - regression_loss: 0.3466 - classification_loss: 0.0419
 2689/10000 [=======>......................] - ETA: 1:48:10 - loss: 0.3885 - regression_loss: 0.3466 - classification_loss: 0.0419
 2690/10000 [=======>......................] - ETA: 1:48:09 - loss: 0.3886 - regression_loss: 0.3467 - classification_loss: 0.0419
 2691/10000 [=======>......................] - ETA: 1:48:08 - loss: 0.3885 - regression_loss: 0.3466 - classification_loss: 0.0419
 2692/10000 [=======>......................] - ETA: 1:48:07 - loss: 0.3885 - regression_loss: 0.3466 - classification_loss: 0.0419
 2693/10000 [=======>......................] - ETA: 1:48:07 - loss: 0.3885 - regression_loss: 0.3466 - classification_loss: 0.0419
 2694/10000 [=======>......................] - ETA: 1:48:06 - loss: 0.3886 - regression_loss: 0.3467 - classification_loss: 0.0419
 2695/10000 [=======>......................] - ETA: 1:48:05 - loss: 0.3887 - regression_loss: 0.3468 - classification_loss: 0.0419
 2696/10000 [=======>......................] - ETA: 1:48:04 - loss: 0.3887 - regression_loss: 0.3468 - classification_loss: 0.0419
 2697/10000 [=======>......................] - ETA: 1:48:03 - loss: 0.3887 - regression_loss: 0.3468 - classification_loss: 0.0419
 2698/10000 [=======>......................] - ETA: 1:48:02 - loss: 0.3888 - regression_loss: 0.3469 - classification_loss: 0.0419
 2699/10000 [=======>......................] - ETA: 1:48:01 - loss: 0.3889 - regression_loss: 0.3469 - classification_loss: 0.0419
 2700/10000 [=======>......................] - ETA: 1:48:00 - loss: 0.3888 - regression_loss: 0.3469 - classification_loss: 0.0419
 2701/10000 [=======>......................] - ETA: 1:47:59 - loss: 0.3888 - regression_loss: 0.3469 - classification_loss: 0.0419
 2702/10000 [=======>......................] - ETA: 1:47:59 - loss: 0.3888 - regression_loss: 0.3469 - classification_loss: 0.0419
 2703/10000 [=======>......................] - ETA: 1:47:58 - loss: 0.3890 - regression_loss: 0.3470 - classification_loss: 0.0420
 2704/10000 [=======>......................] - ETA: 1:47:57 - loss: 0.3891 - regression_loss: 0.3470 - classification_loss: 0.0420
 2705/10000 [=======>......................] - ETA: 1:47:56 - loss: 0.3890 - regression_loss: 0.3470 - classification_loss: 0.0420
 2706/10000 [=======>......................] - ETA: 1:47:55 - loss: 0.3891 - regression_loss: 0.3471 - classification_loss: 0.0420
 2707/10000 [=======>......................] - ETA: 1:47:54 - loss: 0.3891 - regression_loss: 0.3471 - classification_loss: 0.0420
 2708/10000 [=======>......................] - ETA: 1:47:53 - loss: 0.3890 - regression_loss: 0.3470 - classification_loss: 0.0420
 2709/10000 [=======>......................] - ETA: 1:47:52 - loss: 0.3890 - regression_loss: 0.3470 - classification_loss: 0.0420
 2710/10000 [=======>......................] - ETA: 1:47:51 - loss: 0.3890 - regression_loss: 0.3469 - classification_loss: 0.0420
 2711/10000 [=======>......................] - ETA: 1:47:51 - loss: 0.3890 - regression_loss: 0.3469 - classification_loss: 0.0421
 2712/10000 [=======>......................] - ETA: 1:47:50 - loss: 0.3889 - regression_loss: 0.3468 - classification_loss: 0.0421
 2713/10000 [=======>......................] - ETA: 1:47:49 - loss: 0.3888 - regression_loss: 0.3468 - classification_loss: 0.0421
 2714/10000 [=======>......................] - ETA: 1:47:48 - loss: 0.3888 - regression_loss: 0.3468 - classification_loss: 0.0421
 2715/10000 [=======>......................] - ETA: 1:47:47 - loss: 0.3888 - regression_loss: 0.3467 - classification_loss: 0.0421
 2716/10000 [=======>......................] - ETA: 1:47:46 - loss: 0.3888 - regression_loss: 0.3467 - classification_loss: 0.0421
 2717/10000 [=======>......................] - ETA: 1:47:45 - loss: 0.3886 - regression_loss: 0.3466 - classification_loss: 0.0421
 2718/10000 [=======>......................] - ETA: 1:47:44 - loss: 0.3887 - regression_loss: 0.3466 - classification_loss: 0.0421
 2719/10000 [=======>......................] - ETA: 1:47:43 - loss: 0.3886 - regression_loss: 0.3466 - classification_loss: 0.0420
 2720/10000 [=======>......................] - ETA: 1:47:43 - loss: 0.3886 - regression_loss: 0.3465 - classification_loss: 0.0420
 2721/10000 [=======>......................] - ETA: 1:47:42 - loss: 0.3884 - regression_loss: 0.3464 - classification_loss: 0.0420
 2722/10000 [=======>......................] - ETA: 1:47:41 - loss: 0.3883 - regression_loss: 0.3463 - classification_loss: 0.0420
 2723/10000 [=======>......................] - ETA: 1:47:40 - loss: 0.3883 - regression_loss: 0.3463 - classification_loss: 0.0420
 2724/10000 [=======>......................] - ETA: 1:47:39 - loss: 0.3882 - regression_loss: 0.3463 - classification_loss: 0.0420
 2725/10000 [=======>......................] - ETA: 1:47:38 - loss: 0.3883 - regression_loss: 0.3463 - classification_loss: 0.0420
 2726/10000 [=======>......................] - ETA: 1:47:37 - loss: 0.3883 - regression_loss: 0.3463 - classification_loss: 0.0420
 2727/10000 [=======>......................] - ETA: 1:47:36 - loss: 0.3883 - regression_loss: 0.3463 - classification_loss: 0.0420
 2728/10000 [=======>......................] - ETA: 1:47:36 - loss: 0.3883 - regression_loss: 0.3463 - classification_loss: 0.0420
 2729/10000 [=======>......................] - ETA: 1:47:35 - loss: 0.3882 - regression_loss: 0.3463 - classification_loss: 0.0420
 2730/10000 [=======>......................] - ETA: 1:47:34 - loss: 0.3883 - regression_loss: 0.3463 - classification_loss: 0.0420
 2731/10000 [=======>......................] - ETA: 1:47:33 - loss: 0.3882 - regression_loss: 0.3463 - classification_loss: 0.0420
 2732/10000 [=======>......................] - ETA: 1:47:32 - loss: 0.3883 - regression_loss: 0.3463 - classification_loss: 0.0420
 2733/10000 [=======>......................] - ETA: 1:47:31 - loss: 0.3884 - regression_loss: 0.3464 - classification_loss: 0.0420
 2734/10000 [=======>......................] - ETA: 1:47:30 - loss: 0.3883 - regression_loss: 0.3464 - classification_loss: 0.0420
 2735/10000 [=======>......................] - ETA: 1:47:29 - loss: 0.3883 - regression_loss: 0.3463 - classification_loss: 0.0420
 2736/10000 [=======>......................] - ETA: 1:47:28 - loss: 0.3885 - regression_loss: 0.3466 - classification_loss: 0.0420
 2737/10000 [=======>......................] - ETA: 1:47:28 - loss: 0.3886 - regression_loss: 0.3467 - classification_loss: 0.0420
 2738/10000 [=======>......................] - ETA: 1:47:27 - loss: 0.3887 - regression_loss: 0.3467 - classification_loss: 0.0420
 2739/10000 [=======>......................] - ETA: 1:47:26 - loss: 0.3887 - regression_loss: 0.3466 - classification_loss: 0.0420
 2740/10000 [=======>......................] - ETA: 1:47:25 - loss: 0.3887 - regression_loss: 0.3467 - classification_loss: 0.0420
 2741/10000 [=======>......................] - ETA: 1:47:24 - loss: 0.3888 - regression_loss: 0.3468 - classification_loss: 0.0420
 2742/10000 [=======>......................] - ETA: 1:47:23 - loss: 0.3888 - regression_loss: 0.3468 - classification_loss: 0.0420
 2743/10000 [=======>......................] - ETA: 1:47:22 - loss: 0.3888 - regression_loss: 0.3468 - classification_loss: 0.0420
 2744/10000 [=======>......................] - ETA: 1:47:21 - loss: 0.3888 - regression_loss: 0.3467 - classification_loss: 0.0420
 2745/10000 [=======>......................] - ETA: 1:47:20 - loss: 0.3887 - regression_loss: 0.3467 - classification_loss: 0.0420
 2746/10000 [=======>......................] - ETA: 1:47:19 - loss: 0.3887 - regression_loss: 0.3467 - classification_loss: 0.0420
 2747/10000 [=======>......................] - ETA: 1:47:19 - loss: 0.3887 - regression_loss: 0.3467 - classification_loss: 0.0420
 2748/10000 [=======>......................] - ETA: 1:47:18 - loss: 0.3886 - regression_loss: 0.3465 - classification_loss: 0.0420
 2749/10000 [=======>......................] - ETA: 1:47:17 - loss: 0.3885 - regression_loss: 0.3465 - classification_loss: 0.0420
 2750/10000 [=======>......................] - ETA: 1:47:16 - loss: 0.3884 - regression_loss: 0.3464 - classification_loss: 0.0420
 2751/10000 [=======>......................] - ETA: 1:47:15 - loss: 0.3884 - regression_loss: 0.3464 - classification_loss: 0.0420
 2752/10000 [=======>......................] - ETA: 1:47:14 - loss: 0.3884 - regression_loss: 0.3464 - classification_loss: 0.0420
 2753/10000 [=======>......................] - ETA: 1:47:13 - loss: 0.3883 - regression_loss: 0.3463 - classification_loss: 0.0420
 2754/10000 [=======>......................] - ETA: 1:47:12 - loss: 0.3885 - regression_loss: 0.3465 - classification_loss: 0.0420
 2755/10000 [=======>......................] - ETA: 1:47:11 - loss: 0.3885 - regression_loss: 0.3465 - classification_loss: 0.0420
 2756/10000 [=======>......................] - ETA: 1:47:11 - loss: 0.3884 - regression_loss: 0.3464 - classification_loss: 0.0420
 2757/10000 [=======>......................] - ETA: 1:47:10 - loss: 0.3885 - regression_loss: 0.3466 - classification_loss: 0.0420
 2758/10000 [=======>......................] - ETA: 1:47:09 - loss: 0.3885 - regression_loss: 0.3466 - classification_loss: 0.0420
 2759/10000 [=======>......................] - ETA: 1:47:08 - loss: 0.3885 - regression_loss: 0.3465 - classification_loss: 0.0420
 2760/10000 [=======>......................] - ETA: 1:47:07 - loss: 0.3885 - regression_loss: 0.3465 - classification_loss: 0.0420
 2761/10000 [=======>......................] - ETA: 1:47:06 - loss: 0.3885 - regression_loss: 0.3465 - classification_loss: 0.0420
 2762/10000 [=======>......................] - ETA: 1:47:05 - loss: 0.3885 - regression_loss: 0.3465 - classification_loss: 0.0420
 2763/10000 [=======>......................] - ETA: 1:47:04 - loss: 0.3885 - regression_loss: 0.3465 - classification_loss: 0.0420
 2764/10000 [=======>......................] - ETA: 1:47:04 - loss: 0.3885 - regression_loss: 0.3465 - classification_loss: 0.0420
 2765/10000 [=======>......................] - ETA: 1:47:03 - loss: 0.3889 - regression_loss: 0.3468 - classification_loss: 0.0420
 2766/10000 [=======>......................] - ETA: 1:47:02 - loss: 0.3889 - regression_loss: 0.3468 - classification_loss: 0.0421
 2767/10000 [=======>......................] - ETA: 1:47:01 - loss: 0.3889 - regression_loss: 0.3468 - classification_loss: 0.0421
 2768/10000 [=======>......................] - ETA: 1:47:00 - loss: 0.3889 - regression_loss: 0.3468 - classification_loss: 0.0421
 2769/10000 [=======>......................] - ETA: 1:46:59 - loss: 0.3890 - regression_loss: 0.3469 - classification_loss: 0.0421
 2770/10000 [=======>......................] - ETA: 1:46:58 - loss: 0.3889 - regression_loss: 0.3468 - classification_loss: 0.0421
 2771/10000 [=======>......................] - ETA: 1:46:57 - loss: 0.3889 - regression_loss: 0.3468 - classification_loss: 0.0421
 2772/10000 [=======>......................] - ETA: 1:46:56 - loss: 0.3889 - regression_loss: 0.3468 - classification_loss: 0.0421
 2773/10000 [=======>......................] - ETA: 1:46:56 - loss: 0.3889 - regression_loss: 0.3469 - classification_loss: 0.0421
 2774/10000 [=======>......................] - ETA: 1:46:55 - loss: 0.3889 - regression_loss: 0.3468 - classification_loss: 0.0421
 2775/10000 [=======>......................] - ETA: 1:46:54 - loss: 0.3890 - regression_loss: 0.3469 - classification_loss: 0.0421
 2776/10000 [=======>......................] - ETA: 1:46:53 - loss: 0.3890 - regression_loss: 0.3469 - classification_loss: 0.0421
 2777/10000 [=======>......................] - ETA: 1:46:52 - loss: 0.3891 - regression_loss: 0.3469 - classification_loss: 0.0421
 2778/10000 [=======>......................] - ETA: 1:46:51 - loss: 0.3889 - regression_loss: 0.3468 - classification_loss: 0.0421
 2779/10000 [=======>......................] - ETA: 1:46:50 - loss: 0.3889 - regression_loss: 0.3468 - classification_loss: 0.0421
 2780/10000 [=======>......................] - ETA: 1:46:49 - loss: 0.3889 - regression_loss: 0.3468 - classification_loss: 0.0421
 2781/10000 [=======>......................] - ETA: 1:46:48 - loss: 0.3889 - regression_loss: 0.3468 - classification_loss: 0.0421
 2782/10000 [=======>......................] - ETA: 1:46:48 - loss: 0.3889 - regression_loss: 0.3468 - classification_loss: 0.0421
 2783/10000 [=======>......................] - ETA: 1:46:47 - loss: 0.3888 - regression_loss: 0.3467 - classification_loss: 0.0421
 2784/10000 [=======>......................] - ETA: 1:46:46 - loss: 0.3888 - regression_loss: 0.3467 - classification_loss: 0.0421
 2785/10000 [=======>......................] - ETA: 1:46:45 - loss: 0.3888 - regression_loss: 0.3467 - classification_loss: 0.0421
 2786/10000 [=======>......................] - ETA: 1:46:44 - loss: 0.3888 - regression_loss: 0.3467 - classification_loss: 0.0421
 2787/10000 [=======>......................] - ETA: 1:46:43 - loss: 0.3888 - regression_loss: 0.3467 - classification_loss: 0.0421
 2788/10000 [=======>......................] - ETA: 1:46:42 - loss: 0.3889 - regression_loss: 0.3468 - classification_loss: 0.0421
 2789/10000 [=======>......................] - ETA: 1:46:41 - loss: 0.3888 - regression_loss: 0.3468 - classification_loss: 0.0421
 2790/10000 [=======>......................] - ETA: 1:46:40 - loss: 0.3888 - regression_loss: 0.3467 - classification_loss: 0.0421
 2791/10000 [=======>......................] - ETA: 1:46:40 - loss: 0.3889 - regression_loss: 0.3468 - classification_loss: 0.0421
 2792/10000 [=======>......................] - ETA: 1:46:39 - loss: 0.3889 - regression_loss: 0.3468 - classification_loss: 0.0421
 2793/10000 [=======>......................] - ETA: 1:46:38 - loss: 0.3890 - regression_loss: 0.3469 - classification_loss: 0.0421
 2794/10000 [=======>......................] - ETA: 1:46:37 - loss: 0.3891 - regression_loss: 0.3469 - classification_loss: 0.0421
 2795/10000 [=======>......................] - ETA: 1:46:36 - loss: 0.3890 - regression_loss: 0.3469 - classification_loss: 0.0421
 2796/10000 [=======>......................] - ETA: 1:46:35 - loss: 0.3892 - regression_loss: 0.3470 - classification_loss: 0.0422
 2797/10000 [=======>......................] - ETA: 1:46:34 - loss: 0.3891 - regression_loss: 0.3469 - classification_loss: 0.0422
 2798/10000 [=======>......................] - ETA: 1:46:33 - loss: 0.3891 - regression_loss: 0.3469 - classification_loss: 0.0422
 2799/10000 [=======>......................] - ETA: 1:46:33 - loss: 0.3890 - regression_loss: 0.3469 - classification_loss: 0.0421
 2800/10000 [=======>......................] - ETA: 1:46:32 - loss: 0.3890 - regression_loss: 0.3468 - classification_loss: 0.0421
 2801/10000 [=======>......................] - ETA: 1:46:31 - loss: 0.3890 - regression_loss: 0.3468 - classification_loss: 0.0421
 2802/10000 [=======>......................] - ETA: 1:46:30 - loss: 0.3889 - regression_loss: 0.3468 - classification_loss: 0.0421
 2803/10000 [=======>......................] - ETA: 1:46:29 - loss: 0.3889 - regression_loss: 0.3467 - classification_loss: 0.0421
 2804/10000 [=======>......................] - ETA: 1:46:28 - loss: 0.3889 - regression_loss: 0.3468 - classification_loss: 0.0421
 2805/10000 [=======>......................] - ETA: 1:46:27 - loss: 0.3891 - regression_loss: 0.3469 - classification_loss: 0.0421
 2806/10000 [=======>......................] - ETA: 1:46:26 - loss: 0.3890 - regression_loss: 0.3469 - classification_loss: 0.0421
 2807/10000 [=======>......................] - ETA: 1:46:25 - loss: 0.3890 - regression_loss: 0.3469 - classification_loss: 0.0421
 2808/10000 [=======>......................] - ETA: 1:46:25 - loss: 0.3890 - regression_loss: 0.3469 - classification_loss: 0.0421
 2809/10000 [=======>......................] - ETA: 1:46:24 - loss: 0.3890 - regression_loss: 0.3469 - classification_loss: 0.0421
 2810/10000 [=======>......................] - ETA: 1:46:23 - loss: 0.3889 - regression_loss: 0.3468 - classification_loss: 0.0421
 2811/10000 [=======>......................] - ETA: 1:46:22 - loss: 0.3890 - regression_loss: 0.3468 - classification_loss: 0.0421
 2812/10000 [=======>......................] - ETA: 1:46:21 - loss: 0.3890 - regression_loss: 0.3468 - classification_loss: 0.0421
 2813/10000 [=======>......................] - ETA: 1:46:20 - loss: 0.3889 - regression_loss: 0.3467 - classification_loss: 0.0421
 2814/10000 [=======>......................] - ETA: 1:46:19 - loss: 0.3887 - regression_loss: 0.3466 - classification_loss: 0.0421
 2815/10000 [=======>......................] - ETA: 1:46:18 - loss: 0.3887 - regression_loss: 0.3466 - classification_loss: 0.0421
 2816/10000 [=======>......................] - ETA: 1:46:17 - loss: 0.3888 - regression_loss: 0.3467 - classification_loss: 0.0421
 2817/10000 [=======>......................] - ETA: 1:46:17 - loss: 0.3887 - regression_loss: 0.3466 - classification_loss: 0.0421
 2818/10000 [=======>......................] - ETA: 1:46:16 - loss: 0.3886 - regression_loss: 0.3465 - classification_loss: 0.0421
 2819/10000 [=======>......................] - ETA: 1:46:15 - loss: 0.3888 - regression_loss: 0.3467 - classification_loss: 0.0421
 2820/10000 [=======>......................] - ETA: 1:46:14 - loss: 0.3887 - regression_loss: 0.3466 - classification_loss: 0.0421
 2821/10000 [=======>......................] - ETA: 1:46:13 - loss: 0.3888 - regression_loss: 0.3467 - classification_loss: 0.0421
 2822/10000 [=======>......................] - ETA: 1:46:12 - loss: 0.3890 - regression_loss: 0.3469 - classification_loss: 0.0421
 2823/10000 [=======>......................] - ETA: 1:46:11 - loss: 0.3889 - regression_loss: 0.3468 - classification_loss: 0.0421
 2824/10000 [=======>......................] - ETA: 1:46:10 - loss: 0.3889 - regression_loss: 0.3468 - classification_loss: 0.0421
 2825/10000 [=======>......................] - ETA: 1:46:09 - loss: 0.3889 - regression_loss: 0.3468 - classification_loss: 0.0421
 2826/10000 [=======>......................] - ETA: 1:46:09 - loss: 0.3887 - regression_loss: 0.3466 - classification_loss: 0.0421
 2827/10000 [=======>......................] - ETA: 1:46:08 - loss: 0.3887 - regression_loss: 0.3466 - classification_loss: 0.0421
 2828/10000 [=======>......................] - ETA: 1:46:07 - loss: 0.3887 - regression_loss: 0.3467 - classification_loss: 0.0421
 2829/10000 [=======>......................] - ETA: 1:46:06 - loss: 0.3887 - regression_loss: 0.3466 - classification_loss: 0.0421
 2830/10000 [=======>......................] - ETA: 1:46:05 - loss: 0.3887 - regression_loss: 0.3466 - classification_loss: 0.0421
 2831/10000 [=======>......................] - ETA: 1:46:04 - loss: 0.3886 - regression_loss: 0.3466 - classification_loss: 0.0421
 2832/10000 [=======>......................] - ETA: 1:46:03 - loss: 0.3886 - regression_loss: 0.3465 - classification_loss: 0.0420
 2833/10000 [=======>......................] - ETA: 1:46:02 - loss: 0.3886 - regression_loss: 0.3465 - classification_loss: 0.0420
 2834/10000 [=======>......................] - ETA: 1:46:01 - loss: 0.3886 - regression_loss: 0.3465 - classification_loss: 0.0420
 2835/10000 [=======>......................] - ETA: 1:46:01 - loss: 0.3886 - regression_loss: 0.3465 - classification_loss: 0.0420
 2836/10000 [=======>......................] - ETA: 1:46:00 - loss: 0.3885 - regression_loss: 0.3465 - classification_loss: 0.0420
 2837/10000 [=======>......................] - ETA: 1:45:59 - loss: 0.3886 - regression_loss: 0.3466 - classification_loss: 0.0420
 2838/10000 [=======>......................] - ETA: 1:45:58 - loss: 0.3886 - regression_loss: 0.3466 - classification_loss: 0.0420
 2839/10000 [=======>......................] - ETA: 1:45:57 - loss: 0.3886 - regression_loss: 0.3466 - classification_loss: 0.0420
 2840/10000 [=======>......................] - ETA: 1:45:56 - loss: 0.3887 - regression_loss: 0.3466 - classification_loss: 0.0421
 2841/10000 [=======>......................] - ETA: 1:45:55 - loss: 0.3886 - regression_loss: 0.3465 - classification_loss: 0.0420
 2842/10000 [=======>......................] - ETA: 1:45:54 - loss: 0.3886 - regression_loss: 0.3465 - classification_loss: 0.0420
 2843/10000 [=======>......................] - ETA: 1:45:53 - loss: 0.3886 - regression_loss: 0.3465 - classification_loss: 0.0420
 2844/10000 [=======>......................] - ETA: 1:45:53 - loss: 0.3885 - regression_loss: 0.3465 - classification_loss: 0.0420
 2845/10000 [=======>......................] - ETA: 1:45:52 - loss: 0.3885 - regression_loss: 0.3465 - classification_loss: 0.0420
 2846/10000 [=======>......................] - ETA: 1:45:51 - loss: 0.3885 - regression_loss: 0.3464 - classification_loss: 0.0420
 2847/10000 [=======>......................] - ETA: 1:45:50 - loss: 0.3884 - regression_loss: 0.3464 - classification_loss: 0.0420
 2848/10000 [=======>......................] - ETA: 1:45:49 - loss: 0.3884 - regression_loss: 0.3464 - classification_loss: 0.0420
 2849/10000 [=======>......................] - ETA: 1:45:48 - loss: 0.3884 - regression_loss: 0.3464 - classification_loss: 0.0420
 2850/10000 [=======>......................] - ETA: 1:45:47 - loss: 0.3883 - regression_loss: 0.3463 - classification_loss: 0.0420
 2851/10000 [=======>......................] - ETA: 1:45:46 - loss: 0.3883 - regression_loss: 0.3463 - classification_loss: 0.0420
 2852/10000 [=======>......................] - ETA: 1:45:45 - loss: 0.3883 - regression_loss: 0.3462 - classification_loss: 0.0420
 2853/10000 [=======>......................] - ETA: 1:45:45 - loss: 0.3883 - regression_loss: 0.3463 - classification_loss: 0.0420
 2854/10000 [=======>......................] - ETA: 1:45:44 - loss: 0.3882 - regression_loss: 0.3462 - classification_loss: 0.0420
 2855/10000 [=======>......................] - ETA: 1:45:43 - loss: 0.3884 - regression_loss: 0.3464 - classification_loss: 0.0420
 2856/10000 [=======>......................] - ETA: 1:45:42 - loss: 0.3884 - regression_loss: 0.3464 - classification_loss: 0.0420
 2857/10000 [=======>......................] - ETA: 1:45:41 - loss: 0.3884 - regression_loss: 0.3464 - classification_loss: 0.0420
 2858/10000 [=======>......................] - ETA: 1:45:40 - loss: 0.3883 - regression_loss: 0.3464 - classification_loss: 0.0420
 2859/10000 [=======>......................] - ETA: 1:45:39 - loss: 0.3884 - regression_loss: 0.3464 - classification_loss: 0.0420
 2860/10000 [=======>......................] - ETA: 1:45:38 - loss: 0.3885 - regression_loss: 0.3465 - classification_loss: 0.0420
 2861/10000 [=======>......................] - ETA: 1:45:37 - loss: 0.3887 - regression_loss: 0.3467 - classification_loss: 0.0421
 2862/10000 [=======>......................] - ETA: 1:45:37 - loss: 0.3891 - regression_loss: 0.3470 - classification_loss: 0.0421
 2863/10000 [=======>......................] - ETA: 1:45:36 - loss: 0.3890 - regression_loss: 0.3469 - classification_loss: 0.0420
 2864/10000 [=======>......................] - ETA: 1:45:35 - loss: 0.3889 - regression_loss: 0.3469 - classification_loss: 0.0420
 2865/10000 [=======>......................] - ETA: 1:45:34 - loss: 0.3890 - regression_loss: 0.3469 - classification_loss: 0.0421
 2866/10000 [=======>......................] - ETA: 1:45:33 - loss: 0.3889 - regression_loss: 0.3468 - classification_loss: 0.0420
 2867/10000 [=======>......................] - ETA: 1:45:32 - loss: 0.3890 - regression_loss: 0.3469 - classification_loss: 0.0421
 2868/10000 [=======>......................] - ETA: 1:45:31 - loss: 0.3888 - regression_loss: 0.3468 - classification_loss: 0.0421
 2869/10000 [=======>......................] - ETA: 1:45:30 - loss: 0.3888 - regression_loss: 0.3467 - classification_loss: 0.0420
 2870/10000 [=======>......................] - ETA: 1:45:30 - loss: 0.3888 - regression_loss: 0.3468 - classification_loss: 0.0420
 2871/10000 [=======>......................] - ETA: 1:45:29 - loss: 0.3888 - regression_loss: 0.3468 - classification_loss: 0.0421
 2872/10000 [=======>......................] - ETA: 1:45:28 - loss: 0.3887 - regression_loss: 0.3467 - classification_loss: 0.0420
 2873/10000 [=======>......................] - ETA: 1:45:27 - loss: 0.3887 - regression_loss: 0.3466 - classification_loss: 0.0420
 2874/10000 [=======>......................] - ETA: 1:45:26 - loss: 0.3887 - regression_loss: 0.3466 - classification_loss: 0.0420
 2875/10000 [=======>......................] - ETA: 1:45:25 - loss: 0.3887 - regression_loss: 0.3467 - classification_loss: 0.0420
 2876/10000 [=======>......................] - ETA: 1:45:24 - loss: 0.3886 - regression_loss: 0.3466 - classification_loss: 0.0420
 2877/10000 [=======>......................] - ETA: 1:45:23 - loss: 0.3886 - regression_loss: 0.3466 - classification_loss: 0.0420
 2878/10000 [=======>......................] - ETA: 1:45:22 - loss: 0.3885 - regression_loss: 0.3465 - classification_loss: 0.0420
 2879/10000 [=======>......................] - ETA: 1:45:22 - loss: 0.3885 - regression_loss: 0.3465 - classification_loss: 0.0420
 2880/10000 [=======>......................] - ETA: 1:45:21 - loss: 0.3885 - regression_loss: 0.3465 - classification_loss: 0.0420
 2881/10000 [=======>......................] - ETA: 1:45:20 - loss: 0.3884 - regression_loss: 0.3465 - classification_loss: 0.0420
 2882/10000 [=======>......................] - ETA: 1:45:19 - loss: 0.3883 - regression_loss: 0.3463 - classification_loss: 0.0420
 2883/10000 [=======>......................] - ETA: 1:45:18 - loss: 0.3882 - regression_loss: 0.3463 - classification_loss: 0.0420
 2884/10000 [=======>......................] - ETA: 1:45:17 - loss: 0.3883 - regression_loss: 0.3464 - classification_loss: 0.0420
 2885/10000 [=======>......................] - ETA: 1:45:16 - loss: 0.3885 - regression_loss: 0.3465 - classification_loss: 0.0420
 2886/10000 [=======>......................] - ETA: 1:45:15 - loss: 0.3888 - regression_loss: 0.3468 - classification_loss: 0.0420
 2887/10000 [=======>......................] - ETA: 1:45:14 - loss: 0.3887 - regression_loss: 0.3467 - classification_loss: 0.0420
 2888/10000 [=======>......................] - ETA: 1:45:13 - loss: 0.3887 - regression_loss: 0.3467 - classification_loss: 0.0420
 2889/10000 [=======>......................] - ETA: 1:45:13 - loss: 0.3886 - regression_loss: 0.3466 - classification_loss: 0.0420
 2890/10000 [=======>......................] - ETA: 1:45:12 - loss: 0.3889 - regression_loss: 0.3468 - classification_loss: 0.0420
 2891/10000 [=======>......................] - ETA: 1:45:11 - loss: 0.3888 - regression_loss: 0.3468 - classification_loss: 0.0420
 2892/10000 [=======>......................] - ETA: 1:45:10 - loss: 0.3889 - regression_loss: 0.3469 - classification_loss: 0.0421
 2893/10000 [=======>......................] - ETA: 1:45:09 - loss: 0.3889 - regression_loss: 0.3468 - classification_loss: 0.0421
 2894/10000 [=======>......................] - ETA: 1:45:08 - loss: 0.3889 - regression_loss: 0.3468 - classification_loss: 0.0421
 2895/10000 [=======>......................] - ETA: 1:45:07 - loss: 0.3888 - regression_loss: 0.3467 - classification_loss: 0.0420
 2896/10000 [=======>......................] - ETA: 1:45:06 - loss: 0.3887 - regression_loss: 0.3467 - classification_loss: 0.0420
 2897/10000 [=======>......................] - ETA: 1:45:05 - loss: 0.3888 - regression_loss: 0.3468 - classification_loss: 0.0420
 2898/10000 [=======>......................] - ETA: 1:45:05 - loss: 0.3888 - regression_loss: 0.3467 - classification_loss: 0.0420
 2899/10000 [=======>......................] - ETA: 1:45:04 - loss: 0.3890 - regression_loss: 0.3469 - classification_loss: 0.0421
 2900/10000 [=======>......................] - ETA: 1:45:03 - loss: 0.3890 - regression_loss: 0.3469 - classification_loss: 0.0421
 2901/10000 [=======>......................] - ETA: 1:45:02 - loss: 0.3890 - regression_loss: 0.3469 - classification_loss: 0.0421
 2902/10000 [=======>......................] - ETA: 1:45:01 - loss: 0.3892 - regression_loss: 0.3471 - classification_loss: 0.0421
 2903/10000 [=======>......................] - ETA: 1:45:00 - loss: 0.3893 - regression_loss: 0.3472 - classification_loss: 0.0421
 2904/10000 [=======>......................] - ETA: 1:44:59 - loss: 0.3891 - regression_loss: 0.3471 - classification_loss: 0.0420
 2905/10000 [=======>......................] - ETA: 1:44:58 - loss: 0.3891 - regression_loss: 0.3470 - classification_loss: 0.0420
 2906/10000 [=======>......................] - ETA: 1:44:57 - loss: 0.3892 - regression_loss: 0.3471 - classification_loss: 0.0420
 2907/10000 [=======>......................] - ETA: 1:44:57 - loss: 0.3891 - regression_loss: 0.3471 - classification_loss: 0.0420
 2908/10000 [=======>......................] - ETA: 1:44:56 - loss: 0.3891 - regression_loss: 0.3471 - classification_loss: 0.0420
 2909/10000 [=======>......................] - ETA: 1:44:55 - loss: 0.3891 - regression_loss: 0.3471 - classification_loss: 0.0420
 2910/10000 [=======>......................] - ETA: 1:44:54 - loss: 0.3892 - regression_loss: 0.3472 - classification_loss: 0.0420
 2911/10000 [=======>......................] - ETA: 1:44:53 - loss: 0.3891 - regression_loss: 0.3472 - classification_loss: 0.0420
 2912/10000 [=======>......................] - ETA: 1:44:52 - loss: 0.3891 - regression_loss: 0.3471 - classification_loss: 0.0420
 2913/10000 [=======>......................] - ETA: 1:44:51 - loss: 0.3890 - regression_loss: 0.3470 - classification_loss: 0.0420
 2914/10000 [=======>......................] - ETA: 1:44:50 - loss: 0.3889 - regression_loss: 0.3470 - classification_loss: 0.0420
 2915/10000 [=======>......................] - ETA: 1:44:49 - loss: 0.3890 - regression_loss: 0.3470 - classification_loss: 0.0420
 2916/10000 [=======>......................] - ETA: 1:44:49 - loss: 0.3890 - regression_loss: 0.3470 - classification_loss: 0.0420
 2917/10000 [=======>......................] - ETA: 1:44:48 - loss: 0.3889 - regression_loss: 0.3469 - classification_loss: 0.0420
 2918/10000 [=======>......................] - ETA: 1:44:47 - loss: 0.3889 - regression_loss: 0.3469 - classification_loss: 0.0420
 2919/10000 [=======>......................] - ETA: 1:44:46 - loss: 0.3889 - regression_loss: 0.3469 - classification_loss: 0.0420
 2920/10000 [=======>......................] - ETA: 1:44:45 - loss: 0.3889 - regression_loss: 0.3469 - classification_loss: 0.0420
 2921/10000 [=======>......................] - ETA: 1:44:44 - loss: 0.3888 - regression_loss: 0.3469 - classification_loss: 0.0420
 2922/10000 [=======>......................] - ETA: 1:44:43 - loss: 0.3889 - regression_loss: 0.3470 - classification_loss: 0.0419
 2923/10000 [=======>......................] - ETA: 1:44:42 - loss: 0.3892 - regression_loss: 0.3472 - classification_loss: 0.0420
 2924/10000 [=======>......................] - ETA: 1:44:41 - loss: 0.3892 - regression_loss: 0.3472 - classification_loss: 0.0420
 2925/10000 [=======>......................] - ETA: 1:44:41 - loss: 0.3891 - regression_loss: 0.3472 - classification_loss: 0.0419
 2926/10000 [=======>......................] - ETA: 1:44:40 - loss: 0.3891 - regression_loss: 0.3471 - classification_loss: 0.0419
 2927/10000 [=======>......................] - ETA: 1:44:39 - loss: 0.3891 - regression_loss: 0.3471 - classification_loss: 0.0419
 2928/10000 [=======>......................] - ETA: 1:44:38 - loss: 0.3891 - regression_loss: 0.3471 - classification_loss: 0.0419
 2929/10000 [=======>......................] - ETA: 1:44:37 - loss: 0.3890 - regression_loss: 0.3471 - classification_loss: 0.0419
 2930/10000 [=======>......................] - ETA: 1:44:36 - loss: 0.3889 - regression_loss: 0.3470 - classification_loss: 0.0419
 2931/10000 [=======>......................] - ETA: 1:44:35 - loss: 0.3889 - regression_loss: 0.3470 - classification_loss: 0.0419
 2932/10000 [=======>......................] - ETA: 1:44:34 - loss: 0.3889 - regression_loss: 0.3470 - classification_loss: 0.0419
 2933/10000 [=======>......................] - ETA: 1:44:33 - loss: 0.3889 - regression_loss: 0.3470 - classification_loss: 0.0419
 2934/10000 [=======>......................] - ETA: 1:44:33 - loss: 0.3889 - regression_loss: 0.3470 - classification_loss: 0.0419
 2935/10000 [=======>......................] - ETA: 1:44:32 - loss: 0.3889 - regression_loss: 0.3470 - classification_loss: 0.0419
 2936/10000 [=======>......................] - ETA: 1:44:31 - loss: 0.3888 - regression_loss: 0.3469 - classification_loss: 0.0419
 2937/10000 [=======>......................] - ETA: 1:44:30 - loss: 0.3887 - regression_loss: 0.3468 - classification_loss: 0.0419
 2938/10000 [=======>......................] - ETA: 1:44:29 - loss: 0.3888 - regression_loss: 0.3469 - classification_loss: 0.0419
 2939/10000 [=======>......................] - ETA: 1:44:28 - loss: 0.3887 - regression_loss: 0.3468 - classification_loss: 0.0419
 2940/10000 [=======>......................] - ETA: 1:44:27 - loss: 0.3889 - regression_loss: 0.3470 - classification_loss: 0.0419
 2941/10000 [=======>......................] - ETA: 1:44:26 - loss: 0.3889 - regression_loss: 0.3470 - classification_loss: 0.0419
 2942/10000 [=======>......................] - ETA: 1:44:25 - loss: 0.3889 - regression_loss: 0.3470 - classification_loss: 0.0419
 2943/10000 [=======>......................] - ETA: 1:44:25 - loss: 0.3889 - regression_loss: 0.3470 - classification_loss: 0.0419
 2944/10000 [=======>......................] - ETA: 1:44:24 - loss: 0.3889 - regression_loss: 0.3470 - classification_loss: 0.0419
 2945/10000 [=======>......................] - ETA: 1:44:23 - loss: 0.3888 - regression_loss: 0.3469 - classification_loss: 0.0419
 2946/10000 [=======>......................] - ETA: 1:44:22 - loss: 0.3889 - regression_loss: 0.3470 - classification_loss: 0.0419
 2947/10000 [=======>......................] - ETA: 1:44:21 - loss: 0.3888 - regression_loss: 0.3469 - classification_loss: 0.0419
 2948/10000 [=======>......................] - ETA: 1:44:20 - loss: 0.3887 - regression_loss: 0.3468 - classification_loss: 0.0419
 2949/10000 [=======>......................] - ETA: 1:44:19 - loss: 0.3886 - regression_loss: 0.3467 - classification_loss: 0.0419
 2950/10000 [=======>......................] - ETA: 1:44:18 - loss: 0.3890 - regression_loss: 0.3470 - classification_loss: 0.0419
 2951/10000 [=======>......................] - ETA: 1:44:17 - loss: 0.3889 - regression_loss: 0.3470 - classification_loss: 0.0419
 2952/10000 [=======>......................] - ETA: 1:44:17 - loss: 0.3889 - regression_loss: 0.3470 - classification_loss: 0.0419
 2953/10000 [=======>......................] - ETA: 1:44:16 - loss: 0.3889 - regression_loss: 0.3469 - classification_loss: 0.0419
 2954/10000 [=======>......................] - ETA: 1:44:15 - loss: 0.3889 - regression_loss: 0.3470 - classification_loss: 0.0419
 2955/10000 [=======>......................] - ETA: 1:44:14 - loss: 0.3889 - regression_loss: 0.3470 - classification_loss: 0.0419
 2956/10000 [=======>......................] - ETA: 1:44:13 - loss: 0.3891 - regression_loss: 0.3471 - classification_loss: 0.0419
 2957/10000 [=======>......................] - ETA: 1:44:12 - loss: 0.3891 - regression_loss: 0.3472 - classification_loss: 0.0419
 2958/10000 [=======>......................] - ETA: 1:44:11 - loss: 0.3892 - regression_loss: 0.3472 - classification_loss: 0.0419
 2959/10000 [=======>......................] - ETA: 1:44:10 - loss: 0.3891 - regression_loss: 0.3472 - classification_loss: 0.0419
 2960/10000 [=======>......................] - ETA: 1:44:09 - loss: 0.3899 - regression_loss: 0.3479 - classification_loss: 0.0420
 2961/10000 [=======>......................] - ETA: 1:44:09 - loss: 0.3898 - regression_loss: 0.3478 - classification_loss: 0.0420
 2962/10000 [=======>......................] - ETA: 1:44:08 - loss: 0.3898 - regression_loss: 0.3478 - classification_loss: 0.0420
 2963/10000 [=======>......................] - ETA: 1:44:07 - loss: 0.3897 - regression_loss: 0.3477 - classification_loss: 0.0420
 2964/10000 [=======>......................] - ETA: 1:44:06 - loss: 0.3897 - regression_loss: 0.3477 - classification_loss: 0.0420
 2965/10000 [=======>......................] - ETA: 1:44:05 - loss: 0.3896 - regression_loss: 0.3477 - classification_loss: 0.0420
 2966/10000 [=======>......................] - ETA: 1:44:04 - loss: 0.3895 - regression_loss: 0.3475 - classification_loss: 0.0420
 2967/10000 [=======>......................] - ETA: 1:44:03 - loss: 0.3895 - regression_loss: 0.3475 - classification_loss: 0.0420
 2968/10000 [=======>......................] - ETA: 1:44:02 - loss: 0.3895 - regression_loss: 0.3476 - classification_loss: 0.0420
 2969/10000 [=======>......................] - ETA: 1:44:01 - loss: 0.3898 - regression_loss: 0.3478 - classification_loss: 0.0420
 2970/10000 [=======>......................] - ETA: 1:44:01 - loss: 0.3897 - regression_loss: 0.3478 - classification_loss: 0.0420
 2971/10000 [=======>......................] - ETA: 1:44:00 - loss: 0.3896 - regression_loss: 0.3477 - classification_loss: 0.0420
 2972/10000 [=======>......................] - ETA: 1:43:59 - loss: 0.3897 - regression_loss: 0.3478 - classification_loss: 0.0420
 2973/10000 [=======>......................] - ETA: 1:43:58 - loss: 0.3897 - regression_loss: 0.3477 - classification_loss: 0.0420
 2974/10000 [=======>......................] - ETA: 1:43:57 - loss: 0.3896 - regression_loss: 0.3477 - classification_loss: 0.0419
 2975/10000 [=======>......................] - ETA: 1:43:56 - loss: 0.3896 - regression_loss: 0.3477 - classification_loss: 0.0419
 2976/10000 [=======>......................] - ETA: 1:43:55 - loss: 0.3897 - regression_loss: 0.3477 - classification_loss: 0.0420
 2977/10000 [=======>......................] - ETA: 1:43:54 - loss: 0.3896 - regression_loss: 0.3477 - classification_loss: 0.0419
 2978/10000 [=======>......................] - ETA: 1:43:53 - loss: 0.3895 - regression_loss: 0.3475 - classification_loss: 0.0419
 2979/10000 [=======>......................] - ETA: 1:43:53 - loss: 0.3895 - regression_loss: 0.3476 - classification_loss: 0.0419
 2980/10000 [=======>......................] - ETA: 1:43:52 - loss: 0.3897 - regression_loss: 0.3477 - classification_loss: 0.0420
 2981/10000 [=======>......................] - ETA: 1:43:51 - loss: 0.3898 - regression_loss: 0.3479 - classification_loss: 0.0420
 2982/10000 [=======>......................] - ETA: 1:43:50 - loss: 0.3898 - regression_loss: 0.3478 - classification_loss: 0.0420
 2983/10000 [=======>......................] - ETA: 1:43:49 - loss: 0.3897 - regression_loss: 0.3477 - classification_loss: 0.0420
 2984/10000 [=======>......................] - ETA: 1:43:48 - loss: 0.3898 - regression_loss: 0.3478 - classification_loss: 0.0420
 2985/10000 [=======>......................] - ETA: 1:43:47 - loss: 0.3897 - regression_loss: 0.3478 - classification_loss: 0.0420
 2986/10000 [=======>......................] - ETA: 1:43:46 - loss: 0.3897 - regression_loss: 0.3477 - classification_loss: 0.0420
 2987/10000 [=======>......................] - ETA: 1:43:46 - loss: 0.3896 - regression_loss: 0.3477 - classification_loss: 0.0420
 2988/10000 [=======>......................] - ETA: 1:43:45 - loss: 0.3897 - regression_loss: 0.3477 - classification_loss: 0.0420
 2989/10000 [=======>......................] - ETA: 1:43:44 - loss: 0.3896 - regression_loss: 0.3476 - classification_loss: 0.0420
 2990/10000 [=======>......................] - ETA: 1:43:43 - loss: 0.3896 - regression_loss: 0.3476 - classification_loss: 0.0420
 2991/10000 [=======>......................] - ETA: 1:43:42 - loss: 0.3901 - regression_loss: 0.3479 - classification_loss: 0.0422
 2992/10000 [=======>......................] - ETA: 1:43:41 - loss: 0.3901 - regression_loss: 0.3479 - classification_loss: 0.0422
 2993/10000 [=======>......................] - ETA: 1:43:40 - loss: 0.3900 - regression_loss: 0.3478 - classification_loss: 0.0422
 2994/10000 [=======>......................] - ETA: 1:43:39 - loss: 0.3900 - regression_loss: 0.3478 - classification_loss: 0.0422
 2995/10000 [=======>......................] - ETA: 1:43:38 - loss: 0.3899 - regression_loss: 0.3477 - classification_loss: 0.0422
 2996/10000 [=======>......................] - ETA: 1:43:37 - loss: 0.3899 - regression_loss: 0.3477 - classification_loss: 0.0422
 2997/10000 [=======>......................] - ETA: 1:43:37 - loss: 0.3899 - regression_loss: 0.3477 - classification_loss: 0.0422
 2998/10000 [=======>......................] - ETA: 1:43:36 - loss: 0.3902 - regression_loss: 0.3479 - classification_loss: 0.0422
 2999/10000 [=======>......................] - ETA: 1:43:35 - loss: 0.3902 - regression_loss: 0.3479 - classification_loss: 0.0423
 3000/10000 [========>.....................] - ETA: 1:43:34 - loss: 0.3904 - regression_loss: 0.3481 - classification_loss: 0.0423
 3001/10000 [========>.....................] - ETA: 1:43:33 - loss: 0.3905 - regression_loss: 0.3482 - classification_loss: 0.0423
 3002/10000 [========>.....................] - ETA: 1:43:32 - loss: 0.3905 - regression_loss: 0.3482 - classification_loss: 0.0423
 3003/10000 [========>.....................] - ETA: 1:43:31 - loss: 0.3905 - regression_loss: 0.3482 - classification_loss: 0.0423
 3004/10000 [========>.....................] - ETA: 1:43:30 - loss: 0.3905 - regression_loss: 0.3482 - classification_loss: 0.0423
 3005/10000 [========>.....................] - ETA: 1:43:30 - loss: 0.3906 - regression_loss: 0.3483 - classification_loss: 0.0423
 3006/10000 [========>.....................] - ETA: 1:43:29 - loss: 0.3906 - regression_loss: 0.3483 - classification_loss: 0.0423
 3007/10000 [========>.....................] - ETA: 1:43:28 - loss: 0.3906 - regression_loss: 0.3482 - classification_loss: 0.0423
 3008/10000 [========>.....................] - ETA: 1:43:27 - loss: 0.3905 - regression_loss: 0.3482 - classification_loss: 0.0423
 3009/10000 [========>.....................] - ETA: 1:43:26 - loss: 0.3905 - regression_loss: 0.3481 - classification_loss: 0.0423
 3010/10000 [========>.....................] - ETA: 1:43:25 - loss: 0.3903 - regression_loss: 0.3480 - classification_loss: 0.0423
 3011/10000 [========>.....................] - ETA: 1:43:24 - loss: 0.3904 - regression_loss: 0.3480 - classification_loss: 0.0423
 3012/10000 [========>.....................] - ETA: 1:43:23 - loss: 0.3902 - regression_loss: 0.3479 - classification_loss: 0.0423
 3013/10000 [========>.....................] - ETA: 1:43:22 - loss: 0.3901 - regression_loss: 0.3478 - classification_loss: 0.0423
 3014/10000 [========>.....................] - ETA: 1:43:21 - loss: 0.3901 - regression_loss: 0.3478 - classification_loss: 0.0423
 3015/10000 [========>.....................] - ETA: 1:43:21 - loss: 0.3901 - regression_loss: 0.3479 - classification_loss: 0.0423
 3016/10000 [========>.....................] - ETA: 1:43:20 - loss: 0.3901 - regression_loss: 0.3478 - classification_loss: 0.0423
 3017/10000 [========>.....................] - ETA: 1:43:19 - loss: 0.3901 - regression_loss: 0.3478 - classification_loss: 0.0423
 3018/10000 [========>.....................] - ETA: 1:43:18 - loss: 0.3902 - regression_loss: 0.3479 - classification_loss: 0.0423
 3019/10000 [========>.....................] - ETA: 1:43:17 - loss: 0.3901 - regression_loss: 0.3479 - classification_loss: 0.0423
 3020/10000 [========>.....................] - ETA: 1:43:16 - loss: 0.3901 - regression_loss: 0.3478 - classification_loss: 0.0423
 3021/10000 [========>.....................] - ETA: 1:43:15 - loss: 0.3900 - regression_loss: 0.3477 - classification_loss: 0.0423
 3022/10000 [========>.....................] - ETA: 1:43:14 - loss: 0.3900 - regression_loss: 0.3478 - classification_loss: 0.0423
 3023/10000 [========>.....................] - ETA: 1:43:13 - loss: 0.3900 - regression_loss: 0.3477 - classification_loss: 0.0423
 3024/10000 [========>.....................] - ETA: 1:43:13 - loss: 0.3900 - regression_loss: 0.3477 - classification_loss: 0.0423
 3025/10000 [========>.....................] - ETA: 1:43:12 - loss: 0.3900 - regression_loss: 0.3477 - classification_loss: 0.0423
 3026/10000 [========>.....................] - ETA: 1:43:11 - loss: 0.3899 - regression_loss: 0.3476 - classification_loss: 0.0423
 3027/10000 [========>.....................] - ETA: 1:43:10 - loss: 0.3899 - regression_loss: 0.3476 - classification_loss: 0.0423
 3028/10000 [========>.....................] - ETA: 1:43:09 - loss: 0.3899 - regression_loss: 0.3476 - classification_loss: 0.0423
 3029/10000 [========>.....................] - ETA: 1:43:08 - loss: 0.3899 - regression_loss: 0.3477 - classification_loss: 0.0423
 3030/10000 [========>.....................] - ETA: 1:43:07 - loss: 0.3899 - regression_loss: 0.3477 - classification_loss: 0.0422
 3031/10000 [========>.....................] - ETA: 1:43:06 - loss: 0.3899 - regression_loss: 0.3476 - classification_loss: 0.0422
 3032/10000 [========>.....................] - ETA: 1:43:05 - loss: 0.3900 - regression_loss: 0.3477 - classification_loss: 0.0422
 3033/10000 [========>.....................] - ETA: 1:43:05 - loss: 0.3901 - regression_loss: 0.3479 - classification_loss: 0.0422
 3034/10000 [========>.....................] - ETA: 1:43:04 - loss: 0.3902 - regression_loss: 0.3480 - classification_loss: 0.0422
 3035/10000 [========>.....................] - ETA: 1:43:03 - loss: 0.3905 - regression_loss: 0.3483 - classification_loss: 0.0423
 3036/10000 [========>.....................] - ETA: 1:43:02 - loss: 0.3906 - regression_loss: 0.3483 - classification_loss: 0.0424
 3037/10000 [========>.....................] - ETA: 1:43:01 - loss: 0.3905 - regression_loss: 0.3482 - classification_loss: 0.0423
 3038/10000 [========>.....................] - ETA: 1:43:00 - loss: 0.3904 - regression_loss: 0.3481 - classification_loss: 0.0423
 3039/10000 [========>.....................] - ETA: 1:42:59 - loss: 0.3903 - regression_loss: 0.3480 - classification_loss: 0.0423
 3040/10000 [========>.....................] - ETA: 1:42:58 - loss: 0.3904 - regression_loss: 0.3480 - classification_loss: 0.0423
 3041/10000 [========>.....................] - ETA: 1:42:57 - loss: 0.3903 - regression_loss: 0.3480 - classification_loss: 0.0423
 3042/10000 [========>.....................] - ETA: 1:42:57 - loss: 0.3904 - regression_loss: 0.3480 - classification_loss: 0.0424
 3043/10000 [========>.....................] - ETA: 1:42:56 - loss: 0.3904 - regression_loss: 0.3480 - classification_loss: 0.0424
 3044/10000 [========>.....................] - ETA: 1:42:55 - loss: 0.3903 - regression_loss: 0.3479 - classification_loss: 0.0424
 3045/10000 [========>.....................] - ETA: 1:42:54 - loss: 0.3903 - regression_loss: 0.3479 - classification_loss: 0.0424
 3046/10000 [========>.....................] - ETA: 1:42:53 - loss: 0.3903 - regression_loss: 0.3479 - classification_loss: 0.0424
 3047/10000 [========>.....................] - ETA: 1:42:52 - loss: 0.3903 - regression_loss: 0.3479 - classification_loss: 0.0424
 3048/10000 [========>.....................] - ETA: 1:42:51 - loss: 0.3904 - regression_loss: 0.3480 - classification_loss: 0.0424
 3049/10000 [========>.....................] - ETA: 1:42:50 - loss: 0.3904 - regression_loss: 0.3480 - classification_loss: 0.0424
 3050/10000 [========>.....................] - ETA: 1:42:49 - loss: 0.3904 - regression_loss: 0.3479 - classification_loss: 0.0424
 3051/10000 [========>.....................] - ETA: 1:42:49 - loss: 0.3904 - regression_loss: 0.3479 - classification_loss: 0.0424
 3052/10000 [========>.....................] - ETA: 1:42:48 - loss: 0.3903 - regression_loss: 0.3479 - classification_loss: 0.0424
 3053/10000 [========>.....................] - ETA: 1:42:47 - loss: 0.3903 - regression_loss: 0.3479 - classification_loss: 0.0424
 3054/10000 [========>.....................] - ETA: 1:42:46 - loss: 0.3903 - regression_loss: 0.3479 - classification_loss: 0.0424
 3055/10000 [========>.....................] - ETA: 1:42:45 - loss: 0.3903 - regression_loss: 0.3479 - classification_loss: 0.0424
 3056/10000 [========>.....................] - ETA: 1:42:44 - loss: 0.3902 - regression_loss: 0.3478 - classification_loss: 0.0424
 3057/10000 [========>.....................] - ETA: 1:42:43 - loss: 0.3902 - regression_loss: 0.3478 - classification_loss: 0.0424
 3058/10000 [========>.....................] - ETA: 1:42:42 - loss: 0.3902 - regression_loss: 0.3478 - classification_loss: 0.0424
 3059/10000 [========>.....................] - ETA: 1:42:41 - loss: 0.3903 - regression_loss: 0.3479 - classification_loss: 0.0424
 3060/10000 [========>.....................] - ETA: 1:42:41 - loss: 0.3903 - regression_loss: 0.3479 - classification_loss: 0.0424
 3061/10000 [========>.....................] - ETA: 1:42:40 - loss: 0.3902 - regression_loss: 0.3479 - classification_loss: 0.0424
 3062/10000 [========>.....................] - ETA: 1:42:39 - loss: 0.3902 - regression_loss: 0.3478 - classification_loss: 0.0424
 3063/10000 [========>.....................] - ETA: 1:42:38 - loss: 0.3902 - regression_loss: 0.3478 - classification_loss: 0.0424
 3064/10000 [========>.....................] - ETA: 1:42:37 - loss: 0.3902 - regression_loss: 0.3478 - classification_loss: 0.0424
 3065/10000 [========>.....................] - ETA: 1:42:36 - loss: 0.3900 - regression_loss: 0.3477 - classification_loss: 0.0424
 3066/10000 [========>.....................] - ETA: 1:42:35 - loss: 0.3900 - regression_loss: 0.3476 - classification_loss: 0.0424
 3067/10000 [========>.....................] - ETA: 1:42:34 - loss: 0.3899 - regression_loss: 0.3475 - classification_loss: 0.0424
 3068/10000 [========>.....................] - ETA: 1:42:33 - loss: 0.3899 - regression_loss: 0.3475 - classification_loss: 0.0424
 3069/10000 [========>.....................] - ETA: 1:42:33 - loss: 0.3899 - regression_loss: 0.3475 - classification_loss: 0.0424
 3070/10000 [========>.....................] - ETA: 1:42:32 - loss: 0.3898 - regression_loss: 0.3475 - classification_loss: 0.0424
 3071/10000 [========>.....................] - ETA: 1:42:31 - loss: 0.3898 - regression_loss: 0.3474 - classification_loss: 0.0423
 3072/10000 [========>.....................] - ETA: 1:42:30 - loss: 0.3897 - regression_loss: 0.3474 - classification_loss: 0.0423
 3073/10000 [========>.....................] - ETA: 1:42:29 - loss: 0.3899 - regression_loss: 0.3476 - classification_loss: 0.0423
 3074/10000 [========>.....................] - ETA: 1:42:28 - loss: 0.3898 - regression_loss: 0.3475 - classification_loss: 0.0423
 3075/10000 [========>.....................] - ETA: 1:42:27 - loss: 0.3900 - regression_loss: 0.3476 - classification_loss: 0.0423
 3076/10000 [========>.....................] - ETA: 1:42:26 - loss: 0.3900 - regression_loss: 0.3476 - classification_loss: 0.0423
 3077/10000 [========>.....................] - ETA: 1:42:25 - loss: 0.3900 - regression_loss: 0.3477 - classification_loss: 0.0423
 3078/10000 [========>.....................] - ETA: 1:42:25 - loss: 0.3900 - regression_loss: 0.3477 - classification_loss: 0.0423
 3079/10000 [========>.....................] - ETA: 1:42:24 - loss: 0.3900 - regression_loss: 0.3477 - classification_loss: 0.0423
 3080/10000 [========>.....................] - ETA: 1:42:23 - loss: 0.3900 - regression_loss: 0.3476 - classification_loss: 0.0423
 3081/10000 [========>.....................] - ETA: 1:42:22 - loss: 0.3899 - regression_loss: 0.3476 - classification_loss: 0.0423
 3082/10000 [========>.....................] - ETA: 1:42:21 - loss: 0.3899 - regression_loss: 0.3476 - classification_loss: 0.0423
 3083/10000 [========>.....................] - ETA: 1:42:20 - loss: 0.3899 - regression_loss: 0.3475 - classification_loss: 0.0423
 3084/10000 [========>.....................] - ETA: 1:42:19 - loss: 0.3899 - regression_loss: 0.3476 - classification_loss: 0.0423
 3085/10000 [========>.....................] - ETA: 1:42:18 - loss: 0.3899 - regression_loss: 0.3476 - classification_loss: 0.0423
 3086/10000 [========>.....................] - ETA: 1:42:17 - loss: 0.3900 - regression_loss: 0.3477 - classification_loss: 0.0423
 3087/10000 [========>.....................] - ETA: 1:42:17 - loss: 0.3899 - regression_loss: 0.3477 - classification_loss: 0.0423
 3088/10000 [========>.....................] - ETA: 1:42:16 - loss: 0.3899 - regression_loss: 0.3476 - classification_loss: 0.0423
 3089/10000 [========>.....................] - ETA: 1:42:15 - loss: 0.3898 - regression_loss: 0.3475 - classification_loss: 0.0423
 3090/10000 [========>.....................] - ETA: 1:42:14 - loss: 0.3897 - regression_loss: 0.3475 - classification_loss: 0.0423
 3091/10000 [========>.....................] - ETA: 1:42:13 - loss: 0.3897 - regression_loss: 0.3474 - classification_loss: 0.0423
 3092/10000 [========>.....................] - ETA: 1:42:12 - loss: 0.3897 - regression_loss: 0.3475 - classification_loss: 0.0423
 3093/10000 [========>.....................] - ETA: 1:42:11 - loss: 0.3897 - regression_loss: 0.3474 - classification_loss: 0.0423
 3094/10000 [========>.....................] - ETA: 1:42:10 - loss: 0.3898 - regression_loss: 0.3475 - classification_loss: 0.0423
 3095/10000 [========>.....................] - ETA: 1:42:09 - loss: 0.3898 - regression_loss: 0.3475 - classification_loss: 0.0423
 3096/10000 [========>.....................] - ETA: 1:42:09 - loss: 0.3898 - regression_loss: 0.3475 - classification_loss: 0.0423
 3097/10000 [========>.....................] - ETA: 1:42:08 - loss: 0.3898 - regression_loss: 0.3475 - classification_loss: 0.0423
 3098/10000 [========>.....................] - ETA: 1:42:07 - loss: 0.3898 - regression_loss: 0.3475 - classification_loss: 0.0423
 3099/10000 [========>.....................] - ETA: 1:42:06 - loss: 0.3899 - regression_loss: 0.3477 - classification_loss: 0.0423
 3100/10000 [========>.....................] - ETA: 1:42:05 - loss: 0.3900 - regression_loss: 0.3477 - classification_loss: 0.0423
 3101/10000 [========>.....................] - ETA: 1:42:04 - loss: 0.3900 - regression_loss: 0.3477 - classification_loss: 0.0423
 3102/10000 [========>.....................] - ETA: 1:42:03 - loss: 0.3900 - regression_loss: 0.3477 - classification_loss: 0.0423
 3103/10000 [========>.....................] - ETA: 1:42:02 - loss: 0.3899 - regression_loss: 0.3476 - classification_loss: 0.0423
 3104/10000 [========>.....................] - ETA: 1:42:01 - loss: 0.3899 - regression_loss: 0.3476 - classification_loss: 0.0423
 3105/10000 [========>.....................] - ETA: 1:42:01 - loss: 0.3898 - regression_loss: 0.3476 - classification_loss: 0.0423
 3106/10000 [========>.....................] - ETA: 1:42:00 - loss: 0.3899 - regression_loss: 0.3476 - classification_loss: 0.0423
 3107/10000 [========>.....................] - ETA: 1:41:59 - loss: 0.3899 - regression_loss: 0.3476 - classification_loss: 0.0423
 3108/10000 [========>.....................] - ETA: 1:41:58 - loss: 0.3899 - regression_loss: 0.3476 - classification_loss: 0.0423
 3109/10000 [========>.....................] - ETA: 1:41:57 - loss: 0.3899 - regression_loss: 0.3477 - classification_loss: 0.0423
 3110/10000 [========>.....................] - ETA: 1:41:56 - loss: 0.3900 - regression_loss: 0.3478 - classification_loss: 0.0423
 3111/10000 [========>.....................] - ETA: 1:41:55 - loss: 0.3900 - regression_loss: 0.3477 - classification_loss: 0.0423
 3112/10000 [========>.....................] - ETA: 1:41:54 - loss: 0.3899 - regression_loss: 0.3477 - classification_loss: 0.0423
 3113/10000 [========>.....................] - ETA: 1:41:53 - loss: 0.3899 - regression_loss: 0.3476 - classification_loss: 0.0423
 3114/10000 [========>.....................] - ETA: 1:41:53 - loss: 0.3898 - regression_loss: 0.3476 - classification_loss: 0.0423
 3115/10000 [========>.....................] - ETA: 1:41:52 - loss: 0.3898 - regression_loss: 0.3476 - classification_loss: 0.0423
 3116/10000 [========>.....................] - ETA: 1:41:51 - loss: 0.3898 - regression_loss: 0.3475 - classification_loss: 0.0423
 3117/10000 [========>.....................] - ETA: 1:41:50 - loss: 0.3898 - regression_loss: 0.3476 - classification_loss: 0.0422
 3118/10000 [========>.....................] - ETA: 1:41:49 - loss: 0.3902 - regression_loss: 0.3479 - classification_loss: 0.0423
 3119/10000 [========>.....................] - ETA: 1:41:48 - loss: 0.3901 - regression_loss: 0.3478 - classification_loss: 0.0423
 3120/10000 [========>.....................] - ETA: 1:41:47 - loss: 0.3901 - regression_loss: 0.3478 - classification_loss: 0.0423
 3121/10000 [========>.....................] - ETA: 1:41:46 - loss: 0.3901 - regression_loss: 0.3479 - classification_loss: 0.0423
 3122/10000 [========>.....................] - ETA: 1:41:45 - loss: 0.3901 - regression_loss: 0.3478 - classification_loss: 0.0423
 3123/10000 [========>.....................] - ETA: 1:41:45 - loss: 0.3901 - regression_loss: 0.3478 - classification_loss: 0.0423
 3124/10000 [========>.....................] - ETA: 1:41:44 - loss: 0.3900 - regression_loss: 0.3478 - classification_loss: 0.0423
 3125/10000 [========>.....................] - ETA: 1:41:43 - loss: 0.3901 - regression_loss: 0.3479 - classification_loss: 0.0423
 3126/10000 [========>.....................] - ETA: 1:41:42 - loss: 0.3901 - regression_loss: 0.3478 - classification_loss: 0.0423
 3127/10000 [========>.....................] - ETA: 1:41:41 - loss: 0.3902 - regression_loss: 0.3479 - classification_loss: 0.0423
 3128/10000 [========>.....................] - ETA: 1:41:40 - loss: 0.3903 - regression_loss: 0.3480 - classification_loss: 0.0423
 3129/10000 [========>.....................] - ETA: 1:41:39 - loss: 0.3901 - regression_loss: 0.3478 - classification_loss: 0.0423
 3130/10000 [========>.....................] - ETA: 1:41:38 - loss: 0.3901 - regression_loss: 0.3478 - classification_loss: 0.0423
 3131/10000 [========>.....................] - ETA: 1:41:37 - loss: 0.3901 - regression_loss: 0.3478 - classification_loss: 0.0423
 3132/10000 [========>.....................] - ETA: 1:41:37 - loss: 0.3901 - regression_loss: 0.3478 - classification_loss: 0.0423
 3133/10000 [========>.....................] - ETA: 1:41:36 - loss: 0.3901 - regression_loss: 0.3478 - classification_loss: 0.0423
 3134/10000 [========>.....................] - ETA: 1:41:35 - loss: 0.3901 - regression_loss: 0.3478 - classification_loss: 0.0423
 3135/10000 [========>.....................] - ETA: 1:41:34 - loss: 0.3900 - regression_loss: 0.3478 - classification_loss: 0.0423
 3136/10000 [========>.....................] - ETA: 1:41:33 - loss: 0.3899 - regression_loss: 0.3477 - classification_loss: 0.0423
 3137/10000 [========>.....................] - ETA: 1:41:32 - loss: 0.3898 - regression_loss: 0.3476 - classification_loss: 0.0423
 3138/10000 [========>.....................] - ETA: 1:41:31 - loss: 0.3898 - regression_loss: 0.3476 - classification_loss: 0.0423
 3139/10000 [========>.....................] - ETA: 1:41:30 - loss: 0.3898 - regression_loss: 0.3476 - classification_loss: 0.0423
 3140/10000 [========>.....................] - ETA: 1:41:29 - loss: 0.3898 - regression_loss: 0.3476 - classification_loss: 0.0423
 3141/10000 [========>.....................] - ETA: 1:41:29 - loss: 0.3899 - regression_loss: 0.3477 - classification_loss: 0.0423
 3142/10000 [========>.....................] - ETA: 1:41:28 - loss: 0.3899 - regression_loss: 0.3476 - classification_loss: 0.0423
 3143/10000 [========>.....................] - ETA: 1:41:27 - loss: 0.3899 - regression_loss: 0.3476 - classification_loss: 0.0423
 3144/10000 [========>.....................] - ETA: 1:41:26 - loss: 0.3903 - regression_loss: 0.3479 - classification_loss: 0.0423
 3145/10000 [========>.....................] - ETA: 1:41:25 - loss: 0.3902 - regression_loss: 0.3479 - classification_loss: 0.0423
 3146/10000 [========>.....................] - ETA: 1:41:24 - loss: 0.3902 - regression_loss: 0.3478 - classification_loss: 0.0423
 3147/10000 [========>.....................] - ETA: 1:41:23 - loss: 0.3905 - regression_loss: 0.3481 - classification_loss: 0.0424
 3148/10000 [========>.....................] - ETA: 1:41:22 - loss: 0.3904 - regression_loss: 0.3481 - classification_loss: 0.0424
 3149/10000 [========>.....................] - ETA: 1:41:21 - loss: 0.3903 - regression_loss: 0.3480 - classification_loss: 0.0423
 3150/10000 [========>.....................] - ETA: 1:41:21 - loss: 0.3905 - regression_loss: 0.3481 - classification_loss: 0.0423
 3151/10000 [========>.....................] - ETA: 1:41:20 - loss: 0.3905 - regression_loss: 0.3482 - classification_loss: 0.0423
 3152/10000 [========>.....................] - ETA: 1:41:19 - loss: 0.3905 - regression_loss: 0.3482 - classification_loss: 0.0423
 3153/10000 [========>.....................] - ETA: 1:41:18 - loss: 0.3905 - regression_loss: 0.3482 - classification_loss: 0.0423
 3154/10000 [========>.....................] - ETA: 1:41:17 - loss: 0.3905 - regression_loss: 0.3482 - classification_loss: 0.0423
 3155/10000 [========>.....................] - ETA: 1:41:16 - loss: 0.3907 - regression_loss: 0.3484 - classification_loss: 0.0423
 3156/10000 [========>.....................] - ETA: 1:41:15 - loss: 0.3906 - regression_loss: 0.3483 - classification_loss: 0.0423
 3157/10000 [========>.....................] - ETA: 1:41:14 - loss: 0.3910 - regression_loss: 0.3486 - classification_loss: 0.0424
 3158/10000 [========>.....................] - ETA: 1:41:13 - loss: 0.3910 - regression_loss: 0.3486 - classification_loss: 0.0424
 3159/10000 [========>.....................] - ETA: 1:41:13 - loss: 0.3910 - regression_loss: 0.3486 - classification_loss: 0.0424
 3160/10000 [========>.....................] - ETA: 1:41:12 - loss: 0.3909 - regression_loss: 0.3485 - classification_loss: 0.0424
 3161/10000 [========>.....................] - ETA: 1:41:11 - loss: 0.3909 - regression_loss: 0.3485 - classification_loss: 0.0424
 3162/10000 [========>.....................] - ETA: 1:41:10 - loss: 0.3909 - regression_loss: 0.3485 - classification_loss: 0.0424
 3163/10000 [========>.....................] - ETA: 1:41:09 - loss: 0.3910 - regression_loss: 0.3486 - classification_loss: 0.0424
 3164/10000 [========>.....................] - ETA: 1:41:08 - loss: 0.3909 - regression_loss: 0.3485 - classification_loss: 0.0424
 3165/10000 [========>.....................] - ETA: 1:41:07 - loss: 0.3909 - regression_loss: 0.3485 - classification_loss: 0.0424
 3166/10000 [========>.....................] - ETA: 1:41:06 - loss: 0.3910 - regression_loss: 0.3486 - classification_loss: 0.0424
 3167/10000 [========>.....................] - ETA: 1:41:05 - loss: 0.3911 - regression_loss: 0.3487 - classification_loss: 0.0424
 3168/10000 [========>.....................] - ETA: 1:41:05 - loss: 0.3910 - regression_loss: 0.3486 - classification_loss: 0.0424
 3169/10000 [========>.....................] - ETA: 1:41:04 - loss: 0.3910 - regression_loss: 0.3487 - classification_loss: 0.0424
 3170/10000 [========>.....................] - ETA: 1:41:03 - loss: 0.3909 - regression_loss: 0.3486 - classification_loss: 0.0424
 3171/10000 [========>.....................] - ETA: 1:41:02 - loss: 0.3909 - regression_loss: 0.3485 - classification_loss: 0.0423
 3172/10000 [========>.....................] - ETA: 1:41:01 - loss: 0.3909 - regression_loss: 0.3486 - classification_loss: 0.0423
 3173/10000 [========>.....................] - ETA: 1:41:00 - loss: 0.3910 - regression_loss: 0.3487 - classification_loss: 0.0423
 3174/10000 [========>.....................] - ETA: 1:40:59 - loss: 0.3910 - regression_loss: 0.3487 - classification_loss: 0.0423
 3175/10000 [========>.....................] - ETA: 1:40:58 - loss: 0.3910 - regression_loss: 0.3487 - classification_loss: 0.0423
 3176/10000 [========>.....................] - ETA: 1:40:57 - loss: 0.3910 - regression_loss: 0.3487 - classification_loss: 0.0423
 3177/10000 [========>.....................] - ETA: 1:40:57 - loss: 0.3910 - regression_loss: 0.3487 - classification_loss: 0.0423
 3178/10000 [========>.....................] - ETA: 1:40:56 - loss: 0.3910 - regression_loss: 0.3487 - classification_loss: 0.0423
 3179/10000 [========>.....................] - ETA: 1:40:55 - loss: 0.3910 - regression_loss: 0.3487 - classification_loss: 0.0423
 3180/10000 [========>.....................] - ETA: 1:40:54 - loss: 0.3910 - regression_loss: 0.3487 - classification_loss: 0.0423
 3181/10000 [========>.....................] - ETA: 1:40:53 - loss: 0.3911 - regression_loss: 0.3488 - classification_loss: 0.0423
 3182/10000 [========>.....................] - ETA: 1:40:52 - loss: 0.3912 - regression_loss: 0.3489 - classification_loss: 0.0423
 3183/10000 [========>.....................] - ETA: 1:40:51 - loss: 0.3911 - regression_loss: 0.3488 - classification_loss: 0.0423
 3184/10000 [========>.....................] - ETA: 1:40:50 - loss: 0.3913 - regression_loss: 0.3490 - classification_loss: 0.0423
 3185/10000 [========>.....................] - ETA: 1:40:49 - loss: 0.3914 - regression_loss: 0.3492 - classification_loss: 0.0423
 3186/10000 [========>.....................] - ETA: 1:40:49 - loss: 0.3915 - regression_loss: 0.3492 - classification_loss: 0.0423
 3187/10000 [========>.....................] - ETA: 1:40:48 - loss: 0.3915 - regression_loss: 0.3492 - classification_loss: 0.0423
 3188/10000 [========>.....................] - ETA: 1:40:47 - loss: 0.3915 - regression_loss: 0.3491 - classification_loss: 0.0423
 3189/10000 [========>.....................] - ETA: 1:40:46 - loss: 0.3916 - regression_loss: 0.3493 - classification_loss: 0.0423
 3190/10000 [========>.....................] - ETA: 1:40:45 - loss: 0.3915 - regression_loss: 0.3492 - classification_loss: 0.0423
 3191/10000 [========>.....................] - ETA: 1:40:44 - loss: 0.3916 - regression_loss: 0.3493 - classification_loss: 0.0423
 3192/10000 [========>.....................] - ETA: 1:40:43 - loss: 0.3916 - regression_loss: 0.3493 - classification_loss: 0.0423
 3193/10000 [========>.....................] - ETA: 1:40:42 - loss: 0.3916 - regression_loss: 0.3492 - classification_loss: 0.0423
 3194/10000 [========>.....................] - ETA: 1:40:41 - loss: 0.3915 - regression_loss: 0.3492 - classification_loss: 0.0423
 3195/10000 [========>.....................] - ETA: 1:40:41 - loss: 0.3915 - regression_loss: 0.3492 - classification_loss: 0.0423
 3196/10000 [========>.....................] - ETA: 1:40:40 - loss: 0.3914 - regression_loss: 0.3491 - classification_loss: 0.0423
 3197/10000 [========>.....................] - ETA: 1:40:39 - loss: 0.3914 - regression_loss: 0.3491 - classification_loss: 0.0423
 3198/10000 [========>.....................] - ETA: 1:40:38 - loss: 0.3914 - regression_loss: 0.3491 - classification_loss: 0.0423
 3199/10000 [========>.....................] - ETA: 1:40:37 - loss: 0.3915 - regression_loss: 0.3492 - classification_loss: 0.0423
 3200/10000 [========>.....................] - ETA: 1:40:36 - loss: 0.3914 - regression_loss: 0.3491 - classification_loss: 0.0423
 3201/10000 [========>.....................] - ETA: 1:40:35 - loss: 0.3914 - regression_loss: 0.3491 - classification_loss: 0.0423
 3202/10000 [========>.....................] - ETA: 1:40:34 - loss: 0.3914 - regression_loss: 0.3491 - classification_loss: 0.0423
 3203/10000 [========>.....................] - ETA: 1:40:33 - loss: 0.3914 - regression_loss: 0.3491 - classification_loss: 0.0423
 3204/10000 [========>.....................] - ETA: 1:40:33 - loss: 0.3912 - regression_loss: 0.3490 - classification_loss: 0.0423
 3205/10000 [========>.....................] - ETA: 1:40:32 - loss: 0.3913 - regression_loss: 0.3490 - classification_loss: 0.0423
 3206/10000 [========>.....................] - ETA: 1:40:31 - loss: 0.3913 - regression_loss: 0.3490 - classification_loss: 0.0423
 3207/10000 [========>.....................] - ETA: 1:40:30 - loss: 0.3913 - regression_loss: 0.3490 - classification_loss: 0.0423
 3208/10000 [========>.....................] - ETA: 1:40:29 - loss: 0.3911 - regression_loss: 0.3489 - classification_loss: 0.0423
 3209/10000 [========>.....................] - ETA: 1:40:28 - loss: 0.3911 - regression_loss: 0.3488 - classification_loss: 0.0423
 3210/10000 [========>.....................] - ETA: 1:40:27 - loss: 0.3911 - regression_loss: 0.3488 - classification_loss: 0.0423
 3211/10000 [========>.....................] - ETA: 1:40:26 - loss: 0.3911 - regression_loss: 0.3488 - classification_loss: 0.0422
 3212/10000 [========>.....................] - ETA: 1:40:25 - loss: 0.3910 - regression_loss: 0.3488 - classification_loss: 0.0422
 3213/10000 [========>.....................] - ETA: 1:40:25 - loss: 0.3910 - regression_loss: 0.3487 - classification_loss: 0.0422
 3214/10000 [========>.....................] - ETA: 1:40:24 - loss: 0.3910 - regression_loss: 0.3487 - classification_loss: 0.0422
 3215/10000 [========>.....................] - ETA: 1:40:23 - loss: 0.3912 - regression_loss: 0.3489 - classification_loss: 0.0423
 3216/10000 [========>.....................] - ETA: 1:40:22 - loss: 0.3914 - regression_loss: 0.3489 - classification_loss: 0.0425
 3217/10000 [========>.....................] - ETA: 1:40:21 - loss: 0.3913 - regression_loss: 0.3489 - classification_loss: 0.0424
 3218/10000 [========>.....................] - ETA: 1:40:20 - loss: 0.3913 - regression_loss: 0.3489 - classification_loss: 0.0424
 3219/10000 [========>.....................] - ETA: 1:40:19 - loss: 0.3915 - regression_loss: 0.3490 - classification_loss: 0.0424
 3220/10000 [========>.....................] - ETA: 1:40:18 - loss: 0.3915 - regression_loss: 0.3490 - classification_loss: 0.0424
 3221/10000 [========>.....................] - ETA: 1:40:17 - loss: 0.3914 - regression_loss: 0.3489 - classification_loss: 0.0424
 3222/10000 [========>.....................] - ETA: 1:40:17 - loss: 0.3914 - regression_loss: 0.3490 - classification_loss: 0.0424
 3223/10000 [========>.....................] - ETA: 1:40:16 - loss: 0.3913 - regression_loss: 0.3489 - classification_loss: 0.0424
 3224/10000 [========>.....................] - ETA: 1:40:15 - loss: 0.3912 - regression_loss: 0.3488 - classification_loss: 0.0424
 3225/10000 [========>.....................] - ETA: 1:40:14 - loss: 0.3912 - regression_loss: 0.3488 - classification_loss: 0.0424
 3226/10000 [========>.....................] - ETA: 1:40:13 - loss: 0.3913 - regression_loss: 0.3489 - classification_loss: 0.0424
 3227/10000 [========>.....................] - ETA: 1:40:12 - loss: 0.3913 - regression_loss: 0.3489 - classification_loss: 0.0424
 3228/10000 [========>.....................] - ETA: 1:40:11 - loss: 0.3912 - regression_loss: 0.3488 - classification_loss: 0.0424
 3229/10000 [========>.....................] - ETA: 1:40:10 - loss: 0.3912 - regression_loss: 0.3488 - classification_loss: 0.0424
 3230/10000 [========>.....................] - ETA: 1:40:10 - loss: 0.3912 - regression_loss: 0.3488 - classification_loss: 0.0424
 3231/10000 [========>.....................] - ETA: 1:40:09 - loss: 0.3911 - regression_loss: 0.3488 - classification_loss: 0.0424
 3232/10000 [========>.....................] - ETA: 1:40:08 - loss: 0.3911 - regression_loss: 0.3488 - classification_loss: 0.0424
 3233/10000 [========>.....................] - ETA: 1:40:07 - loss: 0.3910 - regression_loss: 0.3486 - classification_loss: 0.0424
 3234/10000 [========>.....................] - ETA: 1:40:06 - loss: 0.3910 - regression_loss: 0.3486 - classification_loss: 0.0424
 3235/10000 [========>.....................] - ETA: 1:40:05 - loss: 0.3909 - regression_loss: 0.3486 - classification_loss: 0.0424
 3236/10000 [========>.....................] - ETA: 1:40:04 - loss: 0.3908 - regression_loss: 0.3485 - classification_loss: 0.0424
 3237/10000 [========>.....................] - ETA: 1:40:03 - loss: 0.3908 - regression_loss: 0.3485 - classification_loss: 0.0423
 3238/10000 [========>.....................] - ETA: 1:40:02 - loss: 0.3908 - regression_loss: 0.3484 - classification_loss: 0.0423
 3239/10000 [========>.....................] - ETA: 1:40:02 - loss: 0.3908 - regression_loss: 0.3485 - classification_loss: 0.0423
 3240/10000 [========>.....................] - ETA: 1:40:01 - loss: 0.3908 - regression_loss: 0.3484 - classification_loss: 0.0423
 3241/10000 [========>.....................] - ETA: 1:40:00 - loss: 0.3908 - regression_loss: 0.3484 - classification_loss: 0.0423
 3242/10000 [========>.....................] - ETA: 1:39:59 - loss: 0.3907 - regression_loss: 0.3484 - classification_loss: 0.0423
 3243/10000 [========>.....................] - ETA: 1:39:58 - loss: 0.3907 - regression_loss: 0.3484 - classification_loss: 0.0423
 3244/10000 [========>.....................] - ETA: 1:39:57 - loss: 0.3907 - regression_loss: 0.3484 - classification_loss: 0.0423
 3245/10000 [========>.....................] - ETA: 1:39:56 - loss: 0.3906 - regression_loss: 0.3483 - classification_loss: 0.0423
 3246/10000 [========>.....................] - ETA: 1:39:55 - loss: 0.3906 - regression_loss: 0.3484 - classification_loss: 0.0423
 3247/10000 [========>.....................] - ETA: 1:39:54 - loss: 0.3906 - regression_loss: 0.3483 - classification_loss: 0.0423
 3248/10000 [========>.....................] - ETA: 1:39:54 - loss: 0.3906 - regression_loss: 0.3483 - classification_loss: 0.0423
 3249/10000 [========>.....................] - ETA: 1:39:53 - loss: 0.3906 - regression_loss: 0.3483 - classification_loss: 0.0423
 3250/10000 [========>.....................] - ETA: 1:39:52 - loss: 0.3905 - regression_loss: 0.3483 - classification_loss: 0.0423
 3251/10000 [========>.....................] - ETA: 1:39:51 - loss: 0.3905 - regression_loss: 0.3482 - classification_loss: 0.0422
 3252/10000 [========>.....................] - ETA: 1:39:50 - loss: 0.3904 - regression_loss: 0.3482 - classification_loss: 0.0422
 3253/10000 [========>.....................] - ETA: 1:39:49 - loss: 0.3904 - regression_loss: 0.3481 - classification_loss: 0.0422
 3254/10000 [========>.....................] - ETA: 1:39:48 - loss: 0.3905 - regression_loss: 0.3482 - classification_loss: 0.0422
 3255/10000 [========>.....................] - ETA: 1:39:47 - loss: 0.3904 - regression_loss: 0.3482 - classification_loss: 0.0422
 3256/10000 [========>.....................] - ETA: 1:39:46 - loss: 0.3904 - regression_loss: 0.3482 - classification_loss: 0.0422
 3257/10000 [========>.....................] - ETA: 1:39:46 - loss: 0.3903 - regression_loss: 0.3481 - classification_loss: 0.0422
 3258/10000 [========>.....................] - ETA: 1:39:45 - loss: 0.3903 - regression_loss: 0.3481 - classification_loss: 0.0422
 3259/10000 [========>.....................] - ETA: 1:39:44 - loss: 0.3903 - regression_loss: 0.3481 - classification_loss: 0.0422
 3260/10000 [========>.....................] - ETA: 1:39:43 - loss: 0.3903 - regression_loss: 0.3481 - classification_loss: 0.0422
 3261/10000 [========>.....................] - ETA: 1:39:42 - loss: 0.3903 - regression_loss: 0.3481 - classification_loss: 0.0422
 3262/10000 [========>.....................] - ETA: 1:39:41 - loss: 0.3902 - regression_loss: 0.3480 - classification_loss: 0.0422
 3263/10000 [========>.....................] - ETA: 1:39:40 - loss: 0.3903 - regression_loss: 0.3481 - classification_loss: 0.0422
 3264/10000 [========>.....................] - ETA: 1:39:39 - loss: 0.3903 - regression_loss: 0.3481 - classification_loss: 0.0422
 3265/10000 [========>.....................] - ETA: 1:39:38 - loss: 0.3903 - regression_loss: 0.3481 - classification_loss: 0.0422
 3266/10000 [========>.....................] - ETA: 1:39:38 - loss: 0.3902 - regression_loss: 0.3481 - classification_loss: 0.0422
 3267/10000 [========>.....................] - ETA: 1:39:37 - loss: 0.3903 - regression_loss: 0.3481 - classification_loss: 0.0422
 3268/10000 [========>.....................] - ETA: 1:39:36 - loss: 0.3903 - regression_loss: 0.3481 - classification_loss: 0.0422
 3269/10000 [========>.....................] - ETA: 1:39:35 - loss: 0.3904 - regression_loss: 0.3482 - classification_loss: 0.0422
 3270/10000 [========>.....................] - ETA: 1:39:34 - loss: 0.3904 - regression_loss: 0.3482 - classification_loss: 0.0422
 3271/10000 [========>.....................] - ETA: 1:39:33 - loss: 0.3903 - regression_loss: 0.3481 - classification_loss: 0.0422
 3272/10000 [========>.....................] - ETA: 1:39:32 - loss: 0.3902 - regression_loss: 0.3480 - classification_loss: 0.0422
 3273/10000 [========>.....................] - ETA: 1:39:31 - loss: 0.3902 - regression_loss: 0.3480 - classification_loss: 0.0422
 3274/10000 [========>.....................] - ETA: 1:39:30 - loss: 0.3901 - regression_loss: 0.3479 - classification_loss: 0.0422
 3275/10000 [========>.....................] - ETA: 1:39:30 - loss: 0.3901 - regression_loss: 0.3480 - classification_loss: 0.0422
 3276/10000 [========>.....................] - ETA: 1:39:29 - loss: 0.3901 - regression_loss: 0.3479 - classification_loss: 0.0422
 3277/10000 [========>.....................] - ETA: 1:39:28 - loss: 0.3901 - regression_loss: 0.3479 - classification_loss: 0.0422
 3278/10000 [========>.....................] - ETA: 1:39:27 - loss: 0.3901 - regression_loss: 0.3479 - classification_loss: 0.0421
 3279/10000 [========>.....................] - ETA: 1:39:26 - loss: 0.3900 - regression_loss: 0.3479 - classification_loss: 0.0421
 3280/10000 [========>.....................] - ETA: 1:39:25 - loss: 0.3900 - regression_loss: 0.3479 - classification_loss: 0.0421
 3281/10000 [========>.....................] - ETA: 1:39:24 - loss: 0.3899 - regression_loss: 0.3478 - classification_loss: 0.0421
 3282/10000 [========>.....................] - ETA: 1:39:23 - loss: 0.3900 - regression_loss: 0.3479 - classification_loss: 0.0421
 3283/10000 [========>.....................] - ETA: 1:39:22 - loss: 0.3899 - regression_loss: 0.3478 - classification_loss: 0.0421
 3284/10000 [========>.....................] - ETA: 1:39:22 - loss: 0.3899 - regression_loss: 0.3478 - classification_loss: 0.0421
 3285/10000 [========>.....................] - ETA: 1:39:21 - loss: 0.3899 - regression_loss: 0.3478 - classification_loss: 0.0421
 3286/10000 [========>.....................] - ETA: 1:39:20 - loss: 0.3899 - regression_loss: 0.3478 - classification_loss: 0.0421
 3287/10000 [========>.....................] - ETA: 1:39:19 - loss: 0.3899 - regression_loss: 0.3478 - classification_loss: 0.0421
 3288/10000 [========>.....................] - ETA: 1:39:18 - loss: 0.3898 - regression_loss: 0.3477 - classification_loss: 0.0421
 3289/10000 [========>.....................] - ETA: 1:39:17 - loss: 0.3898 - regression_loss: 0.3477 - classification_loss: 0.0421
 3290/10000 [========>.....................] - ETA: 1:39:16 - loss: 0.3898 - regression_loss: 0.3478 - classification_loss: 0.0421
 3291/10000 [========>.....................] - ETA: 1:39:15 - loss: 0.3898 - regression_loss: 0.3477 - classification_loss: 0.0421
 3292/10000 [========>.....................] - ETA: 1:39:14 - loss: 0.3898 - regression_loss: 0.3477 - classification_loss: 0.0421
 3293/10000 [========>.....................] - ETA: 1:39:14 - loss: 0.3898 - regression_loss: 0.3477 - classification_loss: 0.0420
 3294/10000 [========>.....................] - ETA: 1:39:13 - loss: 0.3897 - regression_loss: 0.3477 - classification_loss: 0.0420
 3295/10000 [========>.....................] - ETA: 1:39:12 - loss: 0.3896 - regression_loss: 0.3476 - classification_loss: 0.0420
 3296/10000 [========>.....................] - ETA: 1:39:11 - loss: 0.3896 - regression_loss: 0.3476 - classification_loss: 0.0420
 3297/10000 [========>.....................] - ETA: 1:39:10 - loss: 0.3896 - regression_loss: 0.3475 - classification_loss: 0.0420
 3298/10000 [========>.....................] - ETA: 1:39:09 - loss: 0.3895 - regression_loss: 0.3475 - classification_loss: 0.0420
 3299/10000 [========>.....................] - ETA: 1:39:08 - loss: 0.3895 - regression_loss: 0.3475 - classification_loss: 0.0420
 3300/10000 [========>.....................] - ETA: 1:39:07 - loss: 0.3895 - regression_loss: 0.3475 - classification_loss: 0.0420
 3301/10000 [========>.....................] - ETA: 1:39:06 - loss: 0.3894 - regression_loss: 0.3474 - classification_loss: 0.0420
 3302/10000 [========>.....................] - ETA: 1:39:06 - loss: 0.3895 - regression_loss: 0.3475 - classification_loss: 0.0420
 3303/10000 [========>.....................] - ETA: 1:39:05 - loss: 0.3895 - regression_loss: 0.3475 - classification_loss: 0.0420
 3304/10000 [========>.....................] - ETA: 1:39:04 - loss: 0.3896 - regression_loss: 0.3476 - classification_loss: 0.0420
 3305/10000 [========>.....................] - ETA: 1:39:03 - loss: 0.3896 - regression_loss: 0.3476 - classification_loss: 0.0420
 3306/10000 [========>.....................] - ETA: 1:39:02 - loss: 0.3896 - regression_loss: 0.3476 - classification_loss: 0.0420
 3307/10000 [========>.....................] - ETA: 1:39:01 - loss: 0.3896 - regression_loss: 0.3476 - classification_loss: 0.0420
 3308/10000 [========>.....................] - ETA: 1:39:00 - loss: 0.3898 - regression_loss: 0.3477 - classification_loss: 0.0420
 3309/10000 [========>.....................] - ETA: 1:38:59 - loss: 0.3897 - regression_loss: 0.3477 - classification_loss: 0.0420
 3310/10000 [========>.....................] - ETA: 1:38:59 - loss: 0.3900 - regression_loss: 0.3479 - classification_loss: 0.0422
 3311/10000 [========>.....................] - ETA: 1:38:58 - loss: 0.3899 - regression_loss: 0.3478 - classification_loss: 0.0421
 3312/10000 [========>.....................] - ETA: 1:38:57 - loss: 0.3900 - regression_loss: 0.3478 - classification_loss: 0.0421
 3313/10000 [========>.....................] - ETA: 1:38:56 - loss: 0.3900 - regression_loss: 0.3478 - classification_loss: 0.0422
 3314/10000 [========>.....................] - ETA: 1:38:55 - loss: 0.3900 - regression_loss: 0.3478 - classification_loss: 0.0421
 3315/10000 [========>.....................] - ETA: 1:38:54 - loss: 0.3900 - regression_loss: 0.3479 - classification_loss: 0.0422
 3316/10000 [========>.....................] - ETA: 1:38:53 - loss: 0.3900 - regression_loss: 0.3478 - classification_loss: 0.0422
 3317/10000 [========>.....................] - ETA: 1:38:52 - loss: 0.3900 - regression_loss: 0.3479 - classification_loss: 0.0422
 3318/10000 [========>.....................] - ETA: 1:38:51 - loss: 0.3900 - regression_loss: 0.3479 - classification_loss: 0.0422
 3319/10000 [========>.....................] - ETA: 1:38:51 - loss: 0.3900 - regression_loss: 0.3478 - classification_loss: 0.0422
 3320/10000 [========>.....................] - ETA: 1:38:50 - loss: 0.3899 - regression_loss: 0.3478 - classification_loss: 0.0421
 3321/10000 [========>.....................] - ETA: 1:38:49 - loss: 0.3899 - regression_loss: 0.3477 - classification_loss: 0.0421
 3322/10000 [========>.....................] - ETA: 1:38:48 - loss: 0.3899 - regression_loss: 0.3477 - classification_loss: 0.0421
 3323/10000 [========>.....................] - ETA: 1:38:47 - loss: 0.3897 - regression_loss: 0.3476 - classification_loss: 0.0421
 3324/10000 [========>.....................] - ETA: 1:38:46 - loss: 0.3897 - regression_loss: 0.3476 - classification_loss: 0.0421
 3325/10000 [========>.....................] - ETA: 1:38:45 - loss: 0.3897 - regression_loss: 0.3476 - classification_loss: 0.0421
 3326/10000 [========>.....................] - ETA: 1:38:44 - loss: 0.3896 - regression_loss: 0.3475 - classification_loss: 0.0421
 3327/10000 [========>.....................] - ETA: 1:38:43 - loss: 0.3897 - regression_loss: 0.3476 - classification_loss: 0.0421
 3328/10000 [========>.....................] - ETA: 1:38:43 - loss: 0.3898 - regression_loss: 0.3477 - classification_loss: 0.0421
 3329/10000 [========>.....................] - ETA: 1:38:42 - loss: 0.3898 - regression_loss: 0.3477 - classification_loss: 0.0421
 3330/10000 [========>.....................] - ETA: 1:38:41 - loss: 0.3898 - regression_loss: 0.3477 - classification_loss: 0.0421
 3331/10000 [========>.....................] - ETA: 1:38:40 - loss: 0.3899 - regression_loss: 0.3478 - classification_loss: 0.0421
 3332/10000 [========>.....................] - ETA: 1:38:39 - loss: 0.3899 - regression_loss: 0.3478 - classification_loss: 0.0421
 3333/10000 [========>.....................] - ETA: 1:38:38 - loss: 0.3900 - regression_loss: 0.3478 - classification_loss: 0.0421
 3334/10000 [=========>....................] - ETA: 1:38:37 - loss: 0.3900 - regression_loss: 0.3479 - classification_loss: 0.0421
 3335/10000 [=========>....................] - ETA: 1:38:36 - loss: 0.3900 - regression_loss: 0.3478 - classification_loss: 0.0421
 3336/10000 [=========>....................] - ETA: 1:38:35 - loss: 0.3899 - regression_loss: 0.3478 - classification_loss: 0.0421
 3337/10000 [=========>....................] - ETA: 1:38:35 - loss: 0.3899 - regression_loss: 0.3478 - classification_loss: 0.0421
 3338/10000 [=========>....................] - ETA: 1:38:34 - loss: 0.3899 - regression_loss: 0.3478 - classification_loss: 0.0421
 3339/10000 [=========>....................] - ETA: 1:38:33 - loss: 0.3899 - regression_loss: 0.3478 - classification_loss: 0.0421
 3340/10000 [=========>....................] - ETA: 1:38:32 - loss: 0.3898 - regression_loss: 0.3477 - classification_loss: 0.0421
 3341/10000 [=========>....................] - ETA: 1:38:31 - loss: 0.3898 - regression_loss: 0.3477 - classification_loss: 0.0421
 3342/10000 [=========>....................] - ETA: 1:38:30 - loss: 0.3899 - regression_loss: 0.3478 - classification_loss: 0.0421
 3343/10000 [=========>....................] - ETA: 1:38:29 - loss: 0.3898 - regression_loss: 0.3477 - classification_loss: 0.0421
 3344/10000 [=========>....................] - ETA: 1:38:28 - loss: 0.3898 - regression_loss: 0.3477 - classification_loss: 0.0421
 3345/10000 [=========>....................] - ETA: 1:38:27 - loss: 0.3900 - regression_loss: 0.3479 - classification_loss: 0.0421
 3346/10000 [=========>....................] - ETA: 1:38:27 - loss: 0.3900 - regression_loss: 0.3479 - classification_loss: 0.0421
 3347/10000 [=========>....................] - ETA: 1:38:26 - loss: 0.3900 - regression_loss: 0.3479 - classification_loss: 0.0421
 3348/10000 [=========>....................] - ETA: 1:38:25 - loss: 0.3900 - regression_loss: 0.3479 - classification_loss: 0.0421
 3349/10000 [=========>....................] - ETA: 1:38:24 - loss: 0.3899 - regression_loss: 0.3479 - classification_loss: 0.0421
 3350/10000 [=========>....................] - ETA: 1:38:23 - loss: 0.3899 - regression_loss: 0.3478 - classification_loss: 0.0421
 3351/10000 [=========>....................] - ETA: 1:38:22 - loss: 0.3898 - regression_loss: 0.3478 - classification_loss: 0.0421
 3352/10000 [=========>....................] - ETA: 1:38:21 - loss: 0.3899 - regression_loss: 0.3478 - classification_loss: 0.0421
 3353/10000 [=========>....................] - ETA: 1:38:20 - loss: 0.3899 - regression_loss: 0.3478 - classification_loss: 0.0421
 3354/10000 [=========>....................] - ETA: 1:38:19 - loss: 0.3899 - regression_loss: 0.3478 - classification_loss: 0.0421
 3355/10000 [=========>....................] - ETA: 1:38:19 - loss: 0.3899 - regression_loss: 0.3479 - classification_loss: 0.0421
 3356/10000 [=========>....................] - ETA: 1:38:18 - loss: 0.3899 - regression_loss: 0.3479 - classification_loss: 0.0420
 3357/10000 [=========>....................] - ETA: 1:38:17 - loss: 0.3899 - regression_loss: 0.3478 - classification_loss: 0.0420
 3358/10000 [=========>....................] - ETA: 1:38:16 - loss: 0.3898 - regression_loss: 0.3478 - classification_loss: 0.0420
 3359/10000 [=========>....................] - ETA: 1:38:15 - loss: 0.3899 - regression_loss: 0.3479 - classification_loss: 0.0420
 3360/10000 [=========>....................] - ETA: 1:38:14 - loss: 0.3899 - regression_loss: 0.3478 - classification_loss: 0.0420
 3361/10000 [=========>....................] - ETA: 1:38:13 - loss: 0.3899 - regression_loss: 0.3479 - classification_loss: 0.0420
 3362/10000 [=========>....................] - ETA: 1:38:12 - loss: 0.3900 - regression_loss: 0.3479 - classification_loss: 0.0420
 3363/10000 [=========>....................] - ETA: 1:38:11 - loss: 0.3900 - regression_loss: 0.3479 - classification_loss: 0.0420
 3364/10000 [=========>....................] - ETA: 1:38:11 - loss: 0.3900 - regression_loss: 0.3480 - classification_loss: 0.0420
 3365/10000 [=========>....................] - ETA: 1:38:10 - loss: 0.3900 - regression_loss: 0.3480 - classification_loss: 0.0420
 3366/10000 [=========>....................] - ETA: 1:38:09 - loss: 0.3900 - regression_loss: 0.3480 - classification_loss: 0.0420
 3367/10000 [=========>....................] - ETA: 1:38:08 - loss: 0.3900 - regression_loss: 0.3480 - classification_loss: 0.0420
 3368/10000 [=========>....................] - ETA: 1:38:07 - loss: 0.3899 - regression_loss: 0.3479 - classification_loss: 0.0420
 3369/10000 [=========>....................] - ETA: 1:38:06 - loss: 0.3899 - regression_loss: 0.3479 - classification_loss: 0.0420
 3370/10000 [=========>....................] - ETA: 1:38:05 - loss: 0.3901 - regression_loss: 0.3481 - classification_loss: 0.0420
 3371/10000 [=========>....................] - ETA: 1:38:04 - loss: 0.3900 - regression_loss: 0.3480 - classification_loss: 0.0420
 3372/10000 [=========>....................] - ETA: 1:38:03 - loss: 0.3901 - regression_loss: 0.3480 - classification_loss: 0.0420
 3373/10000 [=========>....................] - ETA: 1:38:03 - loss: 0.3901 - regression_loss: 0.3481 - classification_loss: 0.0420
 3374/10000 [=========>....................] - ETA: 1:38:02 - loss: 0.3903 - regression_loss: 0.3482 - classification_loss: 0.0420
 3375/10000 [=========>....................] - ETA: 1:38:01 - loss: 0.3902 - regression_loss: 0.3482 - classification_loss: 0.0420
 3376/10000 [=========>....................] - ETA: 1:38:00 - loss: 0.3902 - regression_loss: 0.3482 - classification_loss: 0.0420
 3377/10000 [=========>....................] - ETA: 1:37:59 - loss: 0.3902 - regression_loss: 0.3481 - classification_loss: 0.0420
 3378/10000 [=========>....................] - ETA: 1:37:58 - loss: 0.3902 - regression_loss: 0.3482 - classification_loss: 0.0420
 3379/10000 [=========>....................] - ETA: 1:37:57 - loss: 0.3903 - regression_loss: 0.3482 - classification_loss: 0.0421
 3380/10000 [=========>....................] - ETA: 1:37:56 - loss: 0.3904 - regression_loss: 0.3483 - classification_loss: 0.0421
 3381/10000 [=========>....................] - ETA: 1:37:55 - loss: 0.3903 - regression_loss: 0.3482 - classification_loss: 0.0421
 3382/10000 [=========>....................] - ETA: 1:37:55 - loss: 0.3903 - regression_loss: 0.3482 - classification_loss: 0.0420
 3383/10000 [=========>....................] - ETA: 1:37:54 - loss: 0.3902 - regression_loss: 0.3482 - classification_loss: 0.0420
 3384/10000 [=========>....................] - ETA: 1:37:53 - loss: 0.3903 - regression_loss: 0.3482 - classification_loss: 0.0420
 3385/10000 [=========>....................] - ETA: 1:37:52 - loss: 0.3903 - regression_loss: 0.3483 - classification_loss: 0.0420
 3386/10000 [=========>....................] - ETA: 1:37:51 - loss: 0.3904 - regression_loss: 0.3484 - classification_loss: 0.0420
 3387/10000 [=========>....................] - ETA: 1:37:50 - loss: 0.3904 - regression_loss: 0.3484 - classification_loss: 0.0420
 3388/10000 [=========>....................] - ETA: 1:37:49 - loss: 0.3903 - regression_loss: 0.3483 - classification_loss: 0.0420
 3389/10000 [=========>....................] - ETA: 1:37:48 - loss: 0.3902 - regression_loss: 0.3482 - classification_loss: 0.0420
 3390/10000 [=========>....................] - ETA: 1:37:47 - loss: 0.3903 - regression_loss: 0.3483 - classification_loss: 0.0420
 3391/10000 [=========>....................] - ETA: 1:37:47 - loss: 0.3902 - regression_loss: 0.3483 - classification_loss: 0.0420
 3392/10000 [=========>....................] - ETA: 1:37:46 - loss: 0.3902 - regression_loss: 0.3483 - classification_loss: 0.0420
 3393/10000 [=========>....................] - ETA: 1:37:45 - loss: 0.3902 - regression_loss: 0.3482 - classification_loss: 0.0419
 3394/10000 [=========>....................] - ETA: 1:37:44 - loss: 0.3904 - regression_loss: 0.3484 - classification_loss: 0.0420
 3395/10000 [=========>....................] - ETA: 1:37:43 - loss: 0.3904 - regression_loss: 0.3484 - classification_loss: 0.0420
 3396/10000 [=========>....................] - ETA: 1:37:42 - loss: 0.3905 - regression_loss: 0.3485 - classification_loss: 0.0420
 3397/10000 [=========>....................] - ETA: 1:37:41 - loss: 0.3903 - regression_loss: 0.3484 - classification_loss: 0.0419
 3398/10000 [=========>....................] - ETA: 1:37:40 - loss: 0.3903 - regression_loss: 0.3484 - classification_loss: 0.0419
 3399/10000 [=========>....................] - ETA: 1:37:39 - loss: 0.3904 - regression_loss: 0.3485 - classification_loss: 0.0419
 3400/10000 [=========>....................] - ETA: 1:37:39 - loss: 0.3905 - regression_loss: 0.3485 - classification_loss: 0.0420
 3401/10000 [=========>....................] - ETA: 1:37:38 - loss: 0.3905 - regression_loss: 0.3485 - classification_loss: 0.0420
 3402/10000 [=========>....................] - ETA: 1:37:37 - loss: 0.3905 - regression_loss: 0.3486 - classification_loss: 0.0420
 3403/10000 [=========>....................] - ETA: 1:37:36 - loss: 0.3904 - regression_loss: 0.3485 - classification_loss: 0.0420
 3404/10000 [=========>....................] - ETA: 1:37:35 - loss: 0.3904 - regression_loss: 0.3485 - classification_loss: 0.0420
 3405/10000 [=========>....................] - ETA: 1:37:34 - loss: 0.3904 - regression_loss: 0.3484 - classification_loss: 0.0420
 3406/10000 [=========>....................] - ETA: 1:37:33 - loss: 0.3905 - regression_loss: 0.3486 - classification_loss: 0.0420
 3407/10000 [=========>....................] - ETA: 1:37:32 - loss: 0.3905 - regression_loss: 0.3485 - classification_loss: 0.0420
 3408/10000 [=========>....................] - ETA: 1:37:31 - loss: 0.3905 - regression_loss: 0.3486 - classification_loss: 0.0420
 3409/10000 [=========>....................] - ETA: 1:37:31 - loss: 0.3905 - regression_loss: 0.3486 - classification_loss: 0.0420
 3410/10000 [=========>....................] - ETA: 1:37:30 - loss: 0.3905 - regression_loss: 0.3486 - classification_loss: 0.0420
 3411/10000 [=========>....................] - ETA: 1:37:29 - loss: 0.3905 - regression_loss: 0.3486 - classification_loss: 0.0420
 3412/10000 [=========>....................] - ETA: 1:37:28 - loss: 0.3906 - regression_loss: 0.3487 - classification_loss: 0.0420
 3413/10000 [=========>....................] - ETA: 1:37:27 - loss: 0.3906 - regression_loss: 0.3487 - classification_loss: 0.0419
 3414/10000 [=========>....................] - ETA: 1:37:26 - loss: 0.3907 - regression_loss: 0.3487 - classification_loss: 0.0420
 3415/10000 [=========>....................] - ETA: 1:37:25 - loss: 0.3907 - regression_loss: 0.3487 - classification_loss: 0.0420
 3416/10000 [=========>....................] - ETA: 1:37:24 - loss: 0.3908 - regression_loss: 0.3488 - classification_loss: 0.0420
 3417/10000 [=========>....................] - ETA: 1:37:23 - loss: 0.3909 - regression_loss: 0.3489 - classification_loss: 0.0420
 3418/10000 [=========>....................] - ETA: 1:37:23 - loss: 0.3910 - regression_loss: 0.3490 - classification_loss: 0.0420
 3419/10000 [=========>....................] - ETA: 1:37:22 - loss: 0.3911 - regression_loss: 0.3491 - classification_loss: 0.0420
 3420/10000 [=========>....................] - ETA: 1:37:21 - loss: 0.3911 - regression_loss: 0.3491 - classification_loss: 0.0420
 3421/10000 [=========>....................] - ETA: 1:37:20 - loss: 0.3911 - regression_loss: 0.3491 - classification_loss: 0.0420
 3422/10000 [=========>....................] - ETA: 1:37:19 - loss: 0.3911 - regression_loss: 0.3491 - classification_loss: 0.0420
 3423/10000 [=========>....................] - ETA: 1:37:18 - loss: 0.3911 - regression_loss: 0.3491 - classification_loss: 0.0420
 3424/10000 [=========>....................] - ETA: 1:37:17 - loss: 0.3911 - regression_loss: 0.3492 - classification_loss: 0.0420
 3425/10000 [=========>....................] - ETA: 1:37:16 - loss: 0.3911 - regression_loss: 0.3491 - classification_loss: 0.0420
 3426/10000 [=========>....................] - ETA: 1:37:15 - loss: 0.3910 - regression_loss: 0.3490 - classification_loss: 0.0420
 3427/10000 [=========>....................] - ETA: 1:37:15 - loss: 0.3910 - regression_loss: 0.3490 - classification_loss: 0.0420
 3428/10000 [=========>....................] - ETA: 1:37:14 - loss: 0.3909 - regression_loss: 0.3489 - classification_loss: 0.0420
 3429/10000 [=========>....................] - ETA: 1:37:13 - loss: 0.3908 - regression_loss: 0.3489 - classification_loss: 0.0420
 3430/10000 [=========>....................] - ETA: 1:37:12 - loss: 0.3907 - regression_loss: 0.3488 - classification_loss: 0.0419
 3431/10000 [=========>....................] - ETA: 1:37:11 - loss: 0.3906 - regression_loss: 0.3487 - classification_loss: 0.0419
 3432/10000 [=========>....................] - ETA: 1:37:10 - loss: 0.3906 - regression_loss: 0.3487 - classification_loss: 0.0419
 3433/10000 [=========>....................] - ETA: 1:37:09 - loss: 0.3906 - regression_loss: 0.3487 - classification_loss: 0.0419
 3434/10000 [=========>....................] - ETA: 1:37:08 - loss: 0.3906 - regression_loss: 0.3487 - classification_loss: 0.0419
 3435/10000 [=========>....................] - ETA: 1:37:08 - loss: 0.3908 - regression_loss: 0.3488 - classification_loss: 0.0419
 3436/10000 [=========>....................] - ETA: 1:37:07 - loss: 0.3907 - regression_loss: 0.3488 - classification_loss: 0.0419
 3437/10000 [=========>....................] - ETA: 1:37:06 - loss: 0.3907 - regression_loss: 0.3487 - classification_loss: 0.0419
 3438/10000 [=========>....................] - ETA: 1:37:05 - loss: 0.3906 - regression_loss: 0.3487 - classification_loss: 0.0419
 3439/10000 [=========>....................] - ETA: 1:37:04 - loss: 0.3907 - regression_loss: 0.3488 - classification_loss: 0.0419
 3440/10000 [=========>....................] - ETA: 1:37:03 - loss: 0.3906 - regression_loss: 0.3487 - classification_loss: 0.0419
 3441/10000 [=========>....................] - ETA: 1:37:02 - loss: 0.3906 - regression_loss: 0.3487 - classification_loss: 0.0419
 3442/10000 [=========>....................] - ETA: 1:37:01 - loss: 0.3905 - regression_loss: 0.3486 - classification_loss: 0.0419
 3443/10000 [=========>....................] - ETA: 1:37:00 - loss: 0.3905 - regression_loss: 0.3486 - classification_loss: 0.0419
 3444/10000 [=========>....................] - ETA: 1:37:00 - loss: 0.3904 - regression_loss: 0.3486 - classification_loss: 0.0419
 3445/10000 [=========>....................] - ETA: 1:36:59 - loss: 0.3904 - regression_loss: 0.3485 - classification_loss: 0.0418
 3446/10000 [=========>....................] - ETA: 1:36:58 - loss: 0.3904 - regression_loss: 0.3485 - classification_loss: 0.0418
 3447/10000 [=========>....................] - ETA: 1:36:57 - loss: 0.3904 - regression_loss: 0.3485 - classification_loss: 0.0418
 3448/10000 [=========>....................] - ETA: 1:36:56 - loss: 0.3904 - regression_loss: 0.3486 - classification_loss: 0.0418
 3449/10000 [=========>....................] - ETA: 1:36:55 - loss: 0.3904 - regression_loss: 0.3486 - classification_loss: 0.0418
 3450/10000 [=========>....................] - ETA: 1:36:54 - loss: 0.3904 - regression_loss: 0.3486 - classification_loss: 0.0418
 3451/10000 [=========>....................] - ETA: 1:36:53 - loss: 0.3904 - regression_loss: 0.3486 - classification_loss: 0.0418
 3452/10000 [=========>....................] - ETA: 1:36:52 - loss: 0.3904 - regression_loss: 0.3486 - classification_loss: 0.0418
 3453/10000 [=========>....................] - ETA: 1:36:52 - loss: 0.3904 - regression_loss: 0.3486 - classification_loss: 0.0418
 3454/10000 [=========>....................] - ETA: 1:36:51 - loss: 0.3903 - regression_loss: 0.3485 - classification_loss: 0.0418
 3455/10000 [=========>....................] - ETA: 1:36:50 - loss: 0.3903 - regression_loss: 0.3485 - classification_loss: 0.0418
 3456/10000 [=========>....................] - ETA: 1:36:49 - loss: 0.3903 - regression_loss: 0.3485 - classification_loss: 0.0418
 3457/10000 [=========>....................] - ETA: 1:36:48 - loss: 0.3903 - regression_loss: 0.3485 - classification_loss: 0.0418
 3458/10000 [=========>....................] - ETA: 1:36:47 - loss: 0.3902 - regression_loss: 0.3484 - classification_loss: 0.0418
 3459/10000 [=========>....................] - ETA: 1:36:46 - loss: 0.3901 - regression_loss: 0.3483 - classification_loss: 0.0418
 3460/10000 [=========>....................] - ETA: 1:36:45 - loss: 0.3901 - regression_loss: 0.3483 - classification_loss: 0.0418
 3461/10000 [=========>....................] - ETA: 1:36:44 - loss: 0.3900 - regression_loss: 0.3482 - classification_loss: 0.0418
 3462/10000 [=========>....................] - ETA: 1:36:44 - loss: 0.3899 - regression_loss: 0.3481 - classification_loss: 0.0418
 3463/10000 [=========>....................] - ETA: 1:36:43 - loss: 0.3901 - regression_loss: 0.3483 - classification_loss: 0.0418
 3464/10000 [=========>....................] - ETA: 1:36:42 - loss: 0.3900 - regression_loss: 0.3483 - classification_loss: 0.0418
 3465/10000 [=========>....................] - ETA: 1:36:41 - loss: 0.3901 - regression_loss: 0.3483 - classification_loss: 0.0418
 3466/10000 [=========>....................] - ETA: 1:36:40 - loss: 0.3900 - regression_loss: 0.3482 - classification_loss: 0.0418
 3467/10000 [=========>....................] - ETA: 1:36:39 - loss: 0.3901 - regression_loss: 0.3483 - classification_loss: 0.0418
 3468/10000 [=========>....................] - ETA: 1:36:38 - loss: 0.3900 - regression_loss: 0.3482 - classification_loss: 0.0418
 3469/10000 [=========>....................] - ETA: 1:36:37 - loss: 0.3899 - regression_loss: 0.3481 - classification_loss: 0.0418
 3470/10000 [=========>....................] - ETA: 1:36:37 - loss: 0.3898 - regression_loss: 0.3481 - classification_loss: 0.0418
 3471/10000 [=========>....................] - ETA: 1:36:36 - loss: 0.3899 - regression_loss: 0.3482 - classification_loss: 0.0418
 3472/10000 [=========>....................] - ETA: 1:36:35 - loss: 0.3900 - regression_loss: 0.3482 - classification_loss: 0.0417
 3473/10000 [=========>....................] - ETA: 1:36:34 - loss: 0.3899 - regression_loss: 0.3482 - classification_loss: 0.0417
 3474/10000 [=========>....................] - ETA: 1:36:33 - loss: 0.3900 - regression_loss: 0.3482 - classification_loss: 0.0417
 3475/10000 [=========>....................] - ETA: 1:36:32 - loss: 0.3903 - regression_loss: 0.3486 - classification_loss: 0.0417
 3476/10000 [=========>....................] - ETA: 1:36:31 - loss: 0.3903 - regression_loss: 0.3485 - classification_loss: 0.0417
 3477/10000 [=========>....................] - ETA: 1:36:30 - loss: 0.3902 - regression_loss: 0.3485 - classification_loss: 0.0417
 3478/10000 [=========>....................] - ETA: 1:36:29 - loss: 0.3903 - regression_loss: 0.3486 - classification_loss: 0.0417
 3479/10000 [=========>....................] - ETA: 1:36:29 - loss: 0.3902 - regression_loss: 0.3485 - classification_loss: 0.0417
 3480/10000 [=========>....................] - ETA: 1:36:28 - loss: 0.3907 - regression_loss: 0.3489 - classification_loss: 0.0417
 3481/10000 [=========>....................] - ETA: 1:36:27 - loss: 0.3906 - regression_loss: 0.3489 - classification_loss: 0.0417
 3482/10000 [=========>....................] - ETA: 1:36:26 - loss: 0.3907 - regression_loss: 0.3490 - classification_loss: 0.0417
 3483/10000 [=========>....................] - ETA: 1:36:25 - loss: 0.3907 - regression_loss: 0.3489 - classification_loss: 0.0417
 3484/10000 [=========>....................] - ETA: 1:36:24 - loss: 0.3906 - regression_loss: 0.3489 - classification_loss: 0.0417
 3485/10000 [=========>....................] - ETA: 1:36:23 - loss: 0.3905 - regression_loss: 0.3488 - classification_loss: 0.0417
 3486/10000 [=========>....................] - ETA: 1:36:22 - loss: 0.3904 - regression_loss: 0.3487 - classification_loss: 0.0417
 3487/10000 [=========>....................] - ETA: 1:36:21 - loss: 0.3904 - regression_loss: 0.3487 - classification_loss: 0.0417
 3488/10000 [=========>....................] - ETA: 1:36:21 - loss: 0.3904 - regression_loss: 0.3487 - classification_loss: 0.0417
 3489/10000 [=========>....................] - ETA: 1:36:20 - loss: 0.3905 - regression_loss: 0.3488 - classification_loss: 0.0417
 3490/10000 [=========>....................] - ETA: 1:36:19 - loss: 0.3905 - regression_loss: 0.3488 - classification_loss: 0.0417
 3491/10000 [=========>....................] - ETA: 1:36:18 - loss: 0.3905 - regression_loss: 0.3488 - classification_loss: 0.0417
 3492/10000 [=========>....................] - ETA: 1:36:17 - loss: 0.3904 - regression_loss: 0.3487 - classification_loss: 0.0417
 3493/10000 [=========>....................] - ETA: 1:36:16 - loss: 0.3904 - regression_loss: 0.3487 - classification_loss: 0.0417
 3494/10000 [=========>....................] - ETA: 1:36:15 - loss: 0.3903 - regression_loss: 0.3486 - classification_loss: 0.0417
 3495/10000 [=========>....................] - ETA: 1:36:14 - loss: 0.3903 - regression_loss: 0.3486 - classification_loss: 0.0417
 3496/10000 [=========>....................] - ETA: 1:36:13 - loss: 0.3905 - regression_loss: 0.3486 - classification_loss: 0.0419
 3497/10000 [=========>....................] - ETA: 1:36:13 - loss: 0.3904 - regression_loss: 0.3486 - classification_loss: 0.0419
 3498/10000 [=========>....................] - ETA: 1:36:12 - loss: 0.3904 - regression_loss: 0.3485 - classification_loss: 0.0418
 3499/10000 [=========>....................] - ETA: 1:36:11 - loss: 0.3903 - regression_loss: 0.3485 - classification_loss: 0.0418
 3500/10000 [=========>....................] - ETA: 1:36:10 - loss: 0.3902 - regression_loss: 0.3484 - classification_loss: 0.0418
 3501/10000 [=========>....................] - ETA: 1:36:09 - loss: 0.3901 - regression_loss: 0.3483 - classification_loss: 0.0418
 3502/10000 [=========>....................] - ETA: 1:36:08 - loss: 0.3902 - regression_loss: 0.3483 - classification_loss: 0.0418
 3503/10000 [=========>....................] - ETA: 1:36:07 - loss: 0.3902 - regression_loss: 0.3484 - classification_loss: 0.0418
 3504/10000 [=========>....................] - ETA: 1:36:06 - loss: 0.3902 - regression_loss: 0.3484 - classification_loss: 0.0418
 3505/10000 [=========>....................] - ETA: 1:36:05 - loss: 0.3902 - regression_loss: 0.3484 - classification_loss: 0.0418
 3506/10000 [=========>....................] - ETA: 1:36:05 - loss: 0.3903 - regression_loss: 0.3484 - classification_loss: 0.0418
 3507/10000 [=========>....................] - ETA: 1:36:04 - loss: 0.3902 - regression_loss: 0.3484 - classification_loss: 0.0418
 3508/10000 [=========>....................] - ETA: 1:36:03 - loss: 0.3902 - regression_loss: 0.3484 - classification_loss: 0.0418
 3509/10000 [=========>....................] - ETA: 1:36:02 - loss: 0.3902 - regression_loss: 0.3484 - classification_loss: 0.0418
 3510/10000 [=========>....................] - ETA: 1:36:01 - loss: 0.3903 - regression_loss: 0.3484 - classification_loss: 0.0418
 3511/10000 [=========>....................] - ETA: 1:36:00 - loss: 0.3903 - regression_loss: 0.3484 - classification_loss: 0.0418
 3512/10000 [=========>....................] - ETA: 1:35:59 - loss: 0.3903 - regression_loss: 0.3485 - classification_loss: 0.0418
 3513/10000 [=========>....................] - ETA: 1:35:58 - loss: 0.3903 - regression_loss: 0.3485 - classification_loss: 0.0418
 3514/10000 [=========>....................] - ETA: 1:35:57 - loss: 0.3907 - regression_loss: 0.3489 - classification_loss: 0.0418
 3515/10000 [=========>....................] - ETA: 1:35:57 - loss: 0.3907 - regression_loss: 0.3488 - classification_loss: 0.0418
 3516/10000 [=========>....................] - ETA: 1:35:56 - loss: 0.3907 - regression_loss: 0.3488 - classification_loss: 0.0418
 3517/10000 [=========>....................] - ETA: 1:35:55 - loss: 0.3908 - regression_loss: 0.3489 - classification_loss: 0.0420
 3518/10000 [=========>....................] - ETA: 1:35:54 - loss: 0.3909 - regression_loss: 0.3490 - classification_loss: 0.0420
 3519/10000 [=========>....................] - ETA: 1:35:53 - loss: 0.3909 - regression_loss: 0.3489 - classification_loss: 0.0420
 3520/10000 [=========>....................] - ETA: 1:35:52 - loss: 0.3909 - regression_loss: 0.3489 - classification_loss: 0.0420
 3521/10000 [=========>....................] - ETA: 1:35:51 - loss: 0.3910 - regression_loss: 0.3489 - classification_loss: 0.0420
 3522/10000 [=========>....................] - ETA: 1:35:50 - loss: 0.3910 - regression_loss: 0.3489 - classification_loss: 0.0420
 3523/10000 [=========>....................] - ETA: 1:35:49 - loss: 0.3910 - regression_loss: 0.3490 - classification_loss: 0.0420
 3524/10000 [=========>....................] - ETA: 1:35:49 - loss: 0.3909 - regression_loss: 0.3489 - classification_loss: 0.0420
 3525/10000 [=========>....................] - ETA: 1:35:48 - loss: 0.3908 - regression_loss: 0.3489 - classification_loss: 0.0420
 3526/10000 [=========>....................] - ETA: 1:35:47 - loss: 0.3907 - regression_loss: 0.3488 - classification_loss: 0.0420
 3527/10000 [=========>....................] - ETA: 1:35:46 - loss: 0.3907 - regression_loss: 0.3487 - classification_loss: 0.0420
 3528/10000 [=========>....................] - ETA: 1:35:45 - loss: 0.3906 - regression_loss: 0.3486 - classification_loss: 0.0420
 3529/10000 [=========>....................] - ETA: 1:35:44 - loss: 0.3905 - regression_loss: 0.3486 - classification_loss: 0.0420
 3530/10000 [=========>....................] - ETA: 1:35:43 - loss: 0.3904 - regression_loss: 0.3485 - classification_loss: 0.0419
 3531/10000 [=========>....................] - ETA: 1:35:42 - loss: 0.3904 - regression_loss: 0.3485 - classification_loss: 0.0419
 3532/10000 [=========>....................] - ETA: 1:35:41 - loss: 0.3903 - regression_loss: 0.3484 - classification_loss: 0.0419
 3533/10000 [=========>....................] - ETA: 1:35:41 - loss: 0.3904 - regression_loss: 0.3485 - classification_loss: 0.0419
 3534/10000 [=========>....................] - ETA: 1:35:40 - loss: 0.3903 - regression_loss: 0.3484 - classification_loss: 0.0419
 3535/10000 [=========>....................] - ETA: 1:35:39 - loss: 0.3903 - regression_loss: 0.3484 - classification_loss: 0.0419
 3536/10000 [=========>....................] - ETA: 1:35:38 - loss: 0.3902 - regression_loss: 0.3483 - classification_loss: 0.0419
 3537/10000 [=========>....................] - ETA: 1:35:37 - loss: 0.3902 - regression_loss: 0.3483 - classification_loss: 0.0419
 3538/10000 [=========>....................] - ETA: 1:35:36 - loss: 0.3902 - regression_loss: 0.3483 - classification_loss: 0.0419
 3539/10000 [=========>....................] - ETA: 1:35:35 - loss: 0.3901 - regression_loss: 0.3482 - classification_loss: 0.0419
 3540/10000 [=========>....................] - ETA: 1:35:34 - loss: 0.3901 - regression_loss: 0.3482 - classification_loss: 0.0419
 3541/10000 [=========>....................] - ETA: 1:35:33 - loss: 0.3902 - regression_loss: 0.3482 - classification_loss: 0.0420
 3542/10000 [=========>....................] - ETA: 1:35:33 - loss: 0.3903 - regression_loss: 0.3483 - classification_loss: 0.0420
 3543/10000 [=========>....................] - ETA: 1:35:32 - loss: 0.3903 - regression_loss: 0.3483 - classification_loss: 0.0420
 3544/10000 [=========>....................] - ETA: 1:35:31 - loss: 0.3903 - regression_loss: 0.3483 - classification_loss: 0.0420
 3545/10000 [=========>....................] - ETA: 1:35:30 - loss: 0.3902 - regression_loss: 0.3482 - classification_loss: 0.0420
 3546/10000 [=========>....................] - ETA: 1:35:29 - loss: 0.3903 - regression_loss: 0.3482 - classification_loss: 0.0420
 3547/10000 [=========>....................] - ETA: 1:35:28 - loss: 0.3903 - regression_loss: 0.3483 - classification_loss: 0.0420
 3548/10000 [=========>....................] - ETA: 1:35:27 - loss: 0.3903 - regression_loss: 0.3483 - classification_loss: 0.0420
 3549/10000 [=========>....................] - ETA: 1:35:26 - loss: 0.3903 - regression_loss: 0.3483 - classification_loss: 0.0420
 3550/10000 [=========>....................] - ETA: 1:35:25 - loss: 0.3903 - regression_loss: 0.3483 - classification_loss: 0.0420
 3551/10000 [=========>....................] - ETA: 1:35:25 - loss: 0.3903 - regression_loss: 0.3483 - classification_loss: 0.0420
 3552/10000 [=========>....................] - ETA: 1:35:24 - loss: 0.3903 - regression_loss: 0.3483 - classification_loss: 0.0420
 3553/10000 [=========>....................] - ETA: 1:35:23 - loss: 0.3904 - regression_loss: 0.3483 - classification_loss: 0.0421
 3554/10000 [=========>....................] - ETA: 1:35:22 - loss: 0.3903 - regression_loss: 0.3483 - classification_loss: 0.0420
 3555/10000 [=========>....................] - ETA: 1:35:21 - loss: 0.3902 - regression_loss: 0.3482 - classification_loss: 0.0420
 3556/10000 [=========>....................] - ETA: 1:35:20 - loss: 0.3902 - regression_loss: 0.3481 - classification_loss: 0.0420
 3557/10000 [=========>....................] - ETA: 1:35:19 - loss: 0.3901 - regression_loss: 0.3481 - classification_loss: 0.0420
 3558/10000 [=========>....................] - ETA: 1:35:18 - loss: 0.3900 - regression_loss: 0.3480 - classification_loss: 0.0420
 3559/10000 [=========>....................] - ETA: 1:35:17 - loss: 0.3900 - regression_loss: 0.3480 - classification_loss: 0.0420
 3560/10000 [=========>....................] - ETA: 1:35:17 - loss: 0.3901 - regression_loss: 0.3480 - classification_loss: 0.0420
 3561/10000 [=========>....................] - ETA: 1:35:16 - loss: 0.3900 - regression_loss: 0.3480 - classification_loss: 0.0420
 3562/10000 [=========>....................] - ETA: 1:35:15 - loss: 0.3900 - regression_loss: 0.3480 - classification_loss: 0.0420
 3563/10000 [=========>....................] - ETA: 1:35:14 - loss: 0.3900 - regression_loss: 0.3480 - classification_loss: 0.0420
 3564/10000 [=========>....................] - ETA: 1:35:13 - loss: 0.3899 - regression_loss: 0.3479 - classification_loss: 0.0420
 3565/10000 [=========>....................] - ETA: 1:35:12 - loss: 0.3899 - regression_loss: 0.3479 - classification_loss: 0.0420
 3566/10000 [=========>....................] - ETA: 1:35:11 - loss: 0.3899 - regression_loss: 0.3479 - classification_loss: 0.0420
 3567/10000 [=========>....................] - ETA: 1:35:10 - loss: 0.3899 - regression_loss: 0.3479 - classification_loss: 0.0420
 3568/10000 [=========>....................] - ETA: 1:35:10 - loss: 0.3899 - regression_loss: 0.3479 - classification_loss: 0.0420
 3569/10000 [=========>....................] - ETA: 1:35:09 - loss: 0.3898 - regression_loss: 0.3478 - classification_loss: 0.0420
 3570/10000 [=========>....................] - ETA: 1:35:08 - loss: 0.3898 - regression_loss: 0.3478 - classification_loss: 0.0420
 3571/10000 [=========>....................] - ETA: 1:35:07 - loss: 0.3898 - regression_loss: 0.3478 - classification_loss: 0.0420
 3572/10000 [=========>....................] - ETA: 1:35:06 - loss: 0.3898 - regression_loss: 0.3478 - classification_loss: 0.0420
 3573/10000 [=========>....................] - ETA: 1:35:05 - loss: 0.3897 - regression_loss: 0.3477 - classification_loss: 0.0420
 3574/10000 [=========>....................] - ETA: 1:35:04 - loss: 0.3897 - regression_loss: 0.3477 - classification_loss: 0.0420
 3575/10000 [=========>....................] - ETA: 1:35:03 - loss: 0.3897 - regression_loss: 0.3477 - classification_loss: 0.0420
 3576/10000 [=========>....................] - ETA: 1:35:02 - loss: 0.3897 - regression_loss: 0.3477 - classification_loss: 0.0420
 3577/10000 [=========>....................] - ETA: 1:35:02 - loss: 0.3896 - regression_loss: 0.3477 - classification_loss: 0.0420
 3578/10000 [=========>....................] - ETA: 1:35:01 - loss: 0.3897 - regression_loss: 0.3477 - classification_loss: 0.0420
 3579/10000 [=========>....................] - ETA: 1:35:00 - loss: 0.3897 - regression_loss: 0.3477 - classification_loss: 0.0420
 3580/10000 [=========>....................] - ETA: 1:34:59 - loss: 0.3898 - regression_loss: 0.3478 - classification_loss: 0.0420
 3581/10000 [=========>....................] - ETA: 1:34:58 - loss: 0.3898 - regression_loss: 0.3478 - classification_loss: 0.0420
 3582/10000 [=========>....................] - ETA: 1:34:57 - loss: 0.3898 - regression_loss: 0.3478 - classification_loss: 0.0420
 3583/10000 [=========>....................] - ETA: 1:34:56 - loss: 0.3897 - regression_loss: 0.3478 - classification_loss: 0.0420
 3584/10000 [=========>....................] - ETA: 1:34:55 - loss: 0.3897 - regression_loss: 0.3477 - classification_loss: 0.0420
 3585/10000 [=========>....................] - ETA: 1:34:54 - loss: 0.3896 - regression_loss: 0.3477 - classification_loss: 0.0420
 3586/10000 [=========>....................] - ETA: 1:34:54 - loss: 0.3896 - regression_loss: 0.3476 - classification_loss: 0.0420
 3587/10000 [=========>....................] - ETA: 1:34:53 - loss: 0.3897 - regression_loss: 0.3477 - classification_loss: 0.0420
 3588/10000 [=========>....................] - ETA: 1:34:52 - loss: 0.3896 - regression_loss: 0.3476 - classification_loss: 0.0420
 3589/10000 [=========>....................] - ETA: 1:34:51 - loss: 0.3896 - regression_loss: 0.3476 - classification_loss: 0.0419
 3590/10000 [=========>....................] - ETA: 1:34:50 - loss: 0.3895 - regression_loss: 0.3476 - classification_loss: 0.0419
 3591/10000 [=========>....................] - ETA: 1:34:49 - loss: 0.3896 - regression_loss: 0.3477 - classification_loss: 0.0419
 3592/10000 [=========>....................] - ETA: 1:34:48 - loss: 0.3897 - regression_loss: 0.3478 - classification_loss: 0.0419
 3593/10000 [=========>....................] - ETA: 1:34:47 - loss: 0.3896 - regression_loss: 0.3477 - classification_loss: 0.0419
 3594/10000 [=========>....................] - ETA: 1:34:46 - loss: 0.3897 - regression_loss: 0.3478 - classification_loss: 0.0420
 3595/10000 [=========>....................] - ETA: 1:34:46 - loss: 0.3899 - regression_loss: 0.3479 - classification_loss: 0.0420
 3596/10000 [=========>....................] - ETA: 1:34:45 - loss: 0.3897 - regression_loss: 0.3478 - classification_loss: 0.0419
 3597/10000 [=========>....................] - ETA: 1:34:44 - loss: 0.3902 - regression_loss: 0.3481 - classification_loss: 0.0421
 3598/10000 [=========>....................] - ETA: 1:34:43 - loss: 0.3905 - regression_loss: 0.3484 - classification_loss: 0.0421
 3599/10000 [=========>....................] - ETA: 1:34:42 - loss: 0.3905 - regression_loss: 0.3484 - classification_loss: 0.0421
 3600/10000 [=========>....................] - ETA: 1:34:41 - loss: 0.3904 - regression_loss: 0.3483 - classification_loss: 0.0421
 3601/10000 [=========>....................] - ETA: 1:34:40 - loss: 0.3903 - regression_loss: 0.3482 - classification_loss: 0.0421
 3602/10000 [=========>....................] - ETA: 1:34:39 - loss: 0.3903 - regression_loss: 0.3482 - classification_loss: 0.0421
 3603/10000 [=========>....................] - ETA: 1:34:38 - loss: 0.3903 - regression_loss: 0.3482 - classification_loss: 0.0421
 3604/10000 [=========>....................] - ETA: 1:34:38 - loss: 0.3902 - regression_loss: 0.3482 - classification_loss: 0.0421
 3605/10000 [=========>....................] - ETA: 1:34:37 - loss: 0.3902 - regression_loss: 0.3481 - classification_loss: 0.0421
 3606/10000 [=========>....................] - ETA: 1:34:36 - loss: 0.3903 - regression_loss: 0.3482 - classification_loss: 0.0421
 3607/10000 [=========>....................] - ETA: 1:34:35 - loss: 0.3902 - regression_loss: 0.3481 - classification_loss: 0.0421
 3608/10000 [=========>....................] - ETA: 1:34:34 - loss: 0.3902 - regression_loss: 0.3481 - classification_loss: 0.0421
 3609/10000 [=========>....................] - ETA: 1:34:33 - loss: 0.3902 - regression_loss: 0.3481 - classification_loss: 0.0421
 3610/10000 [=========>....................] - ETA: 1:34:32 - loss: 0.3901 - regression_loss: 0.3481 - classification_loss: 0.0421
 3611/10000 [=========>....................] - ETA: 1:34:31 - loss: 0.3900 - regression_loss: 0.3480 - classification_loss: 0.0421
 3612/10000 [=========>....................] - ETA: 1:34:30 - loss: 0.3899 - regression_loss: 0.3479 - classification_loss: 0.0421
 3613/10000 [=========>....................] - ETA: 1:34:30 - loss: 0.3898 - regression_loss: 0.3478 - classification_loss: 0.0420
 3614/10000 [=========>....................] - ETA: 1:34:29 - loss: 0.3897 - regression_loss: 0.3477 - classification_loss: 0.0420
 3615/10000 [=========>....................] - ETA: 1:34:28 - loss: 0.3898 - regression_loss: 0.3477 - classification_loss: 0.0420
 3616/10000 [=========>....................] - ETA: 1:34:27 - loss: 0.3898 - regression_loss: 0.3477 - classification_loss: 0.0420
 3617/10000 [=========>....................] - ETA: 1:34:26 - loss: 0.3898 - regression_loss: 0.3477 - classification_loss: 0.0420
 3618/10000 [=========>....................] - ETA: 1:34:25 - loss: 0.3897 - regression_loss: 0.3477 - classification_loss: 0.0420
 3619/10000 [=========>....................] - ETA: 1:34:24 - loss: 0.3897 - regression_loss: 0.3477 - classification_loss: 0.0420
 3620/10000 [=========>....................] - ETA: 1:34:23 - loss: 0.3898 - regression_loss: 0.3477 - classification_loss: 0.0420
 3621/10000 [=========>....................] - ETA: 1:34:22 - loss: 0.3897 - regression_loss: 0.3477 - classification_loss: 0.0420
 3622/10000 [=========>....................] - ETA: 1:34:22 - loss: 0.3897 - regression_loss: 0.3477 - classification_loss: 0.0420
 3623/10000 [=========>....................] - ETA: 1:34:21 - loss: 0.3897 - regression_loss: 0.3477 - classification_loss: 0.0420
 3624/10000 [=========>....................] - ETA: 1:34:20 - loss: 0.3896 - regression_loss: 0.3476 - classification_loss: 0.0420
 3625/10000 [=========>....................] - ETA: 1:34:19 - loss: 0.3896 - regression_loss: 0.3476 - classification_loss: 0.0420
 3626/10000 [=========>....................] - ETA: 1:34:18 - loss: 0.3896 - regression_loss: 0.3476 - classification_loss: 0.0420
 3627/10000 [=========>....................] - ETA: 1:34:17 - loss: 0.3896 - regression_loss: 0.3476 - classification_loss: 0.0420
 3628/10000 [=========>....................] - ETA: 1:34:16 - loss: 0.3896 - regression_loss: 0.3476 - classification_loss: 0.0420
 3629/10000 [=========>....................] - ETA: 1:34:15 - loss: 0.3895 - regression_loss: 0.3476 - classification_loss: 0.0420
 3630/10000 [=========>....................] - ETA: 1:34:14 - loss: 0.3895 - regression_loss: 0.3475 - classification_loss: 0.0420
 3631/10000 [=========>....................] - ETA: 1:34:14 - loss: 0.3895 - regression_loss: 0.3475 - classification_loss: 0.0420
 3632/10000 [=========>....................] - ETA: 1:34:13 - loss: 0.3894 - regression_loss: 0.3475 - classification_loss: 0.0420
 3633/10000 [=========>....................] - ETA: 1:34:12 - loss: 0.3895 - regression_loss: 0.3475 - classification_loss: 0.0420
 3634/10000 [=========>....................] - ETA: 1:34:11 - loss: 0.3895 - regression_loss: 0.3475 - classification_loss: 0.0420
 3635/10000 [=========>....................] - ETA: 1:34:10 - loss: 0.3896 - regression_loss: 0.3476 - classification_loss: 0.0420
 3636/10000 [=========>....................] - ETA: 1:34:09 - loss: 0.3896 - regression_loss: 0.3476 - classification_loss: 0.0420
 3637/10000 [=========>....................] - ETA: 1:34:08 - loss: 0.3895 - regression_loss: 0.3475 - classification_loss: 0.0420
 3638/10000 [=========>....................] - ETA: 1:34:07 - loss: 0.3896 - regression_loss: 0.3476 - classification_loss: 0.0420
 3639/10000 [=========>....................] - ETA: 1:34:06 - loss: 0.3897 - regression_loss: 0.3477 - classification_loss: 0.0420
 3640/10000 [=========>....................] - ETA: 1:34:06 - loss: 0.3896 - regression_loss: 0.3476 - classification_loss: 0.0420
 3641/10000 [=========>....................] - ETA: 1:34:05 - loss: 0.3896 - regression_loss: 0.3476 - classification_loss: 0.0420
 3642/10000 [=========>....................] - ETA: 1:34:04 - loss: 0.3896 - regression_loss: 0.3476 - classification_loss: 0.0420
 3643/10000 [=========>....................] - ETA: 1:34:03 - loss: 0.3896 - regression_loss: 0.3476 - classification_loss: 0.0419
 3644/10000 [=========>....................] - ETA: 1:34:02 - loss: 0.3895 - regression_loss: 0.3475 - classification_loss: 0.0419
 3645/10000 [=========>....................] - ETA: 1:34:01 - loss: 0.3895 - regression_loss: 0.3475 - classification_loss: 0.0419
 3646/10000 [=========>....................] - ETA: 1:34:00 - loss: 0.3894 - regression_loss: 0.3475 - classification_loss: 0.0419
 3647/10000 [=========>....................] - ETA: 1:33:59 - loss: 0.3894 - regression_loss: 0.3475 - classification_loss: 0.0419
 3648/10000 [=========>....................] - ETA: 1:33:59 - loss: 0.3895 - regression_loss: 0.3476 - classification_loss: 0.0419
 3649/10000 [=========>....................] - ETA: 1:33:58 - loss: 0.3896 - regression_loss: 0.3477 - classification_loss: 0.0419
 3650/10000 [=========>....................] - ETA: 1:33:57 - loss: 0.3896 - regression_loss: 0.3476 - classification_loss: 0.0419
 3651/10000 [=========>....................] - ETA: 1:33:56 - loss: 0.3896 - regression_loss: 0.3477 - classification_loss: 0.0420
 3652/10000 [=========>....................] - ETA: 1:33:55 - loss: 0.3895 - regression_loss: 0.3476 - classification_loss: 0.0419
 3653/10000 [=========>....................] - ETA: 1:33:54 - loss: 0.3895 - regression_loss: 0.3476 - classification_loss: 0.0419
 3654/10000 [=========>....................] - ETA: 1:33:53 - loss: 0.3895 - regression_loss: 0.3475 - classification_loss: 0.0419
 3655/10000 [=========>....................] - ETA: 1:33:52 - loss: 0.3895 - regression_loss: 0.3475 - classification_loss: 0.0419
 3656/10000 [=========>....................] - ETA: 1:33:51 - loss: 0.3894 - regression_loss: 0.3475 - classification_loss: 0.0419
 3657/10000 [=========>....................] - ETA: 1:33:51 - loss: 0.3893 - regression_loss: 0.3474 - classification_loss: 0.0419
 3658/10000 [=========>....................] - ETA: 1:33:50 - loss: 0.3895 - regression_loss: 0.3476 - classification_loss: 0.0419
 3659/10000 [=========>....................] - ETA: 1:33:49 - loss: 0.3896 - regression_loss: 0.3477 - classification_loss: 0.0419
 3660/10000 [=========>....................] - ETA: 1:33:48 - loss: 0.3896 - regression_loss: 0.3477 - classification_loss: 0.0419
 3661/10000 [=========>....................] - ETA: 1:33:47 - loss: 0.3896 - regression_loss: 0.3477 - classification_loss: 0.0419
 3662/10000 [=========>....................] - ETA: 1:33:46 - loss: 0.3897 - regression_loss: 0.3477 - classification_loss: 0.0420
 3663/10000 [=========>....................] - ETA: 1:33:45 - loss: 0.3897 - regression_loss: 0.3477 - classification_loss: 0.0420
 3664/10000 [=========>....................] - ETA: 1:33:44 - loss: 0.3897 - regression_loss: 0.3477 - classification_loss: 0.0420
 3665/10000 [=========>....................] - ETA: 1:33:43 - loss: 0.3898 - regression_loss: 0.3478 - classification_loss: 0.0420
 3666/10000 [=========>....................] - ETA: 1:33:43 - loss: 0.3899 - regression_loss: 0.3479 - classification_loss: 0.0420
 3667/10000 [==========>...................] - ETA: 1:33:42 - loss: 0.3898 - regression_loss: 0.3478 - classification_loss: 0.0420
 3668/10000 [==========>...................] - ETA: 1:33:41 - loss: 0.3900 - regression_loss: 0.3479 - classification_loss: 0.0420
 3669/10000 [==========>...................] - ETA: 1:33:40 - loss: 0.3898 - regression_loss: 0.3478 - classification_loss: 0.0420
 3670/10000 [==========>...................] - ETA: 1:33:39 - loss: 0.3899 - regression_loss: 0.3479 - classification_loss: 0.0420
 3671/10000 [==========>...................] - ETA: 1:33:38 - loss: 0.3900 - regression_loss: 0.3479 - classification_loss: 0.0421
 3672/10000 [==========>...................] - ETA: 1:33:37 - loss: 0.3900 - regression_loss: 0.3479 - classification_loss: 0.0420
 3673/10000 [==========>...................] - ETA: 1:33:36 - loss: 0.3900 - regression_loss: 0.3480 - classification_loss: 0.0420
 3674/10000 [==========>...................] - ETA: 1:33:35 - loss: 0.3901 - regression_loss: 0.3480 - classification_loss: 0.0421
 3675/10000 [==========>...................] - ETA: 1:33:35 - loss: 0.3901 - regression_loss: 0.3480 - classification_loss: 0.0421
 3676/10000 [==========>...................] - ETA: 1:33:34 - loss: 0.3900 - regression_loss: 0.3480 - classification_loss: 0.0421
 3677/10000 [==========>...................] - ETA: 1:33:33 - loss: 0.3901 - regression_loss: 0.3480 - classification_loss: 0.0421
 3678/10000 [==========>...................] - ETA: 1:33:32 - loss: 0.3900 - regression_loss: 0.3479 - classification_loss: 0.0420
 3679/10000 [==========>...................] - ETA: 1:33:31 - loss: 0.3901 - regression_loss: 0.3480 - classification_loss: 0.0421
 3680/10000 [==========>...................] - ETA: 1:33:30 - loss: 0.3901 - regression_loss: 0.3480 - classification_loss: 0.0421
 3681/10000 [==========>...................] - ETA: 1:33:29 - loss: 0.3900 - regression_loss: 0.3480 - classification_loss: 0.0421
 3682/10000 [==========>...................] - ETA: 1:33:28 - loss: 0.3900 - regression_loss: 0.3479 - classification_loss: 0.0421
 3683/10000 [==========>...................] - ETA: 1:33:27 - loss: 0.3899 - regression_loss: 0.3479 - classification_loss: 0.0421
 3684/10000 [==========>...................] - ETA: 1:33:27 - loss: 0.3900 - regression_loss: 0.3479 - classification_loss: 0.0421
 3685/10000 [==========>...................] - ETA: 1:33:26 - loss: 0.3899 - regression_loss: 0.3479 - classification_loss: 0.0421
 3686/10000 [==========>...................] - ETA: 1:33:25 - loss: 0.3900 - regression_loss: 0.3479 - classification_loss: 0.0421
 3687/10000 [==========>...................] - ETA: 1:33:24 - loss: 0.3900 - regression_loss: 0.3479 - classification_loss: 0.0421
 3688/10000 [==========>...................] - ETA: 1:33:23 - loss: 0.3899 - regression_loss: 0.3478 - classification_loss: 0.0421
 3689/10000 [==========>...................] - ETA: 1:33:22 - loss: 0.3899 - regression_loss: 0.3478 - classification_loss: 0.0421
 3690/10000 [==========>...................] - ETA: 1:33:21 - loss: 0.3900 - regression_loss: 0.3479 - classification_loss: 0.0421
 3691/10000 [==========>...................] - ETA: 1:33:20 - loss: 0.3899 - regression_loss: 0.3478 - classification_loss: 0.0421
 3692/10000 [==========>...................] - ETA: 1:33:19 - loss: 0.3898 - regression_loss: 0.3477 - classification_loss: 0.0421
 3693/10000 [==========>...................] - ETA: 1:33:18 - loss: 0.3897 - regression_loss: 0.3476 - classification_loss: 0.0421
 3694/10000 [==========>...................] - ETA: 1:33:18 - loss: 0.3897 - regression_loss: 0.3476 - classification_loss: 0.0421
 3695/10000 [==========>...................] - ETA: 1:33:17 - loss: 0.3897 - regression_loss: 0.3476 - classification_loss: 0.0421
 3696/10000 [==========>...................] - ETA: 1:33:16 - loss: 0.3896 - regression_loss: 0.3475 - classification_loss: 0.0421
 3697/10000 [==========>...................] - ETA: 1:33:15 - loss: 0.3897 - regression_loss: 0.3476 - classification_loss: 0.0421
 3698/10000 [==========>...................] - ETA: 1:33:14 - loss: 0.3897 - regression_loss: 0.3476 - classification_loss: 0.0421
 3699/10000 [==========>...................] - ETA: 1:33:13 - loss: 0.3897 - regression_loss: 0.3476 - classification_loss: 0.0421
 3700/10000 [==========>...................] - ETA: 1:33:12 - loss: 0.3897 - regression_loss: 0.3476 - classification_loss: 0.0421
 3701/10000 [==========>...................] - ETA: 1:33:11 - loss: 0.3897 - regression_loss: 0.3475 - classification_loss: 0.0421
 3702/10000 [==========>...................] - ETA: 1:33:11 - loss: 0.3899 - regression_loss: 0.3477 - classification_loss: 0.0422
 3703/10000 [==========>...................] - ETA: 1:33:10 - loss: 0.3902 - regression_loss: 0.3477 - classification_loss: 0.0425
 3704/10000 [==========>...................] - ETA: 1:33:09 - loss: 0.3902 - regression_loss: 0.3477 - classification_loss: 0.0425
 3705/10000 [==========>...................] - ETA: 1:33:08 - loss: 0.3902 - regression_loss: 0.3477 - classification_loss: 0.0425
 3706/10000 [==========>...................] - ETA: 1:33:07 - loss: 0.3903 - regression_loss: 0.3478 - classification_loss: 0.0425
 3707/10000 [==========>...................] - ETA: 1:33:06 - loss: 0.3902 - regression_loss: 0.3477 - classification_loss: 0.0425
 3708/10000 [==========>...................] - ETA: 1:33:05 - loss: 0.3902 - regression_loss: 0.3477 - classification_loss: 0.0425
 3709/10000 [==========>...................] - ETA: 1:33:04 - loss: 0.3902 - regression_loss: 0.3477 - classification_loss: 0.0425
 3710/10000 [==========>...................] - ETA: 1:33:03 - loss: 0.3903 - regression_loss: 0.3478 - classification_loss: 0.0425
 3711/10000 [==========>...................] - ETA: 1:33:03 - loss: 0.3902 - regression_loss: 0.3477 - classification_loss: 0.0425
 3712/10000 [==========>...................] - ETA: 1:33:02 - loss: 0.3902 - regression_loss: 0.3478 - classification_loss: 0.0425
 3713/10000 [==========>...................] - ETA: 1:33:01 - loss: 0.3902 - regression_loss: 0.3477 - classification_loss: 0.0425
 3714/10000 [==========>...................] - ETA: 1:33:00 - loss: 0.3902 - regression_loss: 0.3477 - classification_loss: 0.0425
 3715/10000 [==========>...................] - ETA: 1:32:59 - loss: 0.3902 - regression_loss: 0.3477 - classification_loss: 0.0425
 3716/10000 [==========>...................] - ETA: 1:32:58 - loss: 0.3904 - regression_loss: 0.3479 - classification_loss: 0.0425
 3717/10000 [==========>...................] - ETA: 1:32:57 - loss: 0.3904 - regression_loss: 0.3479 - classification_loss: 0.0425
 3718/10000 [==========>...................] - ETA: 1:32:56 - loss: 0.3905 - regression_loss: 0.3480 - classification_loss: 0.0425
 3719/10000 [==========>...................] - ETA: 1:32:55 - loss: 0.3905 - regression_loss: 0.3479 - classification_loss: 0.0425
 3720/10000 [==========>...................] - ETA: 1:32:55 - loss: 0.3905 - regression_loss: 0.3479 - classification_loss: 0.0425
 3721/10000 [==========>...................] - ETA: 1:32:54 - loss: 0.3906 - regression_loss: 0.3481 - classification_loss: 0.0425
 3722/10000 [==========>...................] - ETA: 1:32:53 - loss: 0.3907 - regression_loss: 0.3481 - classification_loss: 0.0425
 3723/10000 [==========>...................] - ETA: 1:32:52 - loss: 0.3907 - regression_loss: 0.3482 - classification_loss: 0.0425
 3724/10000 [==========>...................] - ETA: 1:32:51 - loss: 0.3907 - regression_loss: 0.3482 - classification_loss: 0.0425
 3725/10000 [==========>...................] - ETA: 1:32:50 - loss: 0.3908 - regression_loss: 0.3483 - classification_loss: 0.0425
 3726/10000 [==========>...................] - ETA: 1:32:49 - loss: 0.3908 - regression_loss: 0.3483 - classification_loss: 0.0425
 3727/10000 [==========>...................] - ETA: 1:32:48 - loss: 0.3907 - regression_loss: 0.3482 - classification_loss: 0.0425
 3728/10000 [==========>...................] - ETA: 1:32:47 - loss: 0.3906 - regression_loss: 0.3481 - classification_loss: 0.0425
 3729/10000 [==========>...................] - ETA: 1:32:47 - loss: 0.3906 - regression_loss: 0.3481 - classification_loss: 0.0425
 3730/10000 [==========>...................] - ETA: 1:32:46 - loss: 0.3906 - regression_loss: 0.3481 - classification_loss: 0.0425
 3731/10000 [==========>...................] - ETA: 1:32:45 - loss: 0.3906 - regression_loss: 0.3481 - classification_loss: 0.0425
 3732/10000 [==========>...................] - ETA: 1:32:44 - loss: 0.3906 - regression_loss: 0.3481 - classification_loss: 0.0425
 3733/10000 [==========>...................] - ETA: 1:32:43 - loss: 0.3906 - regression_loss: 0.3481 - classification_loss: 0.0425
 3734/10000 [==========>...................] - ETA: 1:32:42 - loss: 0.3905 - regression_loss: 0.3481 - classification_loss: 0.0425
 3735/10000 [==========>...................] - ETA: 1:32:41 - loss: 0.3906 - regression_loss: 0.3481 - classification_loss: 0.0425
 3736/10000 [==========>...................] - ETA: 1:32:40 - loss: 0.3905 - regression_loss: 0.3481 - classification_loss: 0.0425
 3737/10000 [==========>...................] - ETA: 1:32:39 - loss: 0.3905 - regression_loss: 0.3481 - classification_loss: 0.0425
 3738/10000 [==========>...................] - ETA: 1:32:39 - loss: 0.3904 - regression_loss: 0.3480 - classification_loss: 0.0425
 3739/10000 [==========>...................] - ETA: 1:32:38 - loss: 0.3905 - regression_loss: 0.3480 - classification_loss: 0.0425
 3740/10000 [==========>...................] - ETA: 1:32:37 - loss: 0.3906 - regression_loss: 0.3481 - classification_loss: 0.0425
 3741/10000 [==========>...................] - ETA: 1:32:36 - loss: 0.3905 - regression_loss: 0.3481 - classification_loss: 0.0425
 3742/10000 [==========>...................] - ETA: 1:32:35 - loss: 0.3905 - regression_loss: 0.3480 - classification_loss: 0.0425
 3743/10000 [==========>...................] - ETA: 1:32:34 - loss: 0.3905 - regression_loss: 0.3480 - classification_loss: 0.0425
 3744/10000 [==========>...................] - ETA: 1:32:33 - loss: 0.3905 - regression_loss: 0.3480 - classification_loss: 0.0425
 3745/10000 [==========>...................] - ETA: 1:32:32 - loss: 0.3905 - regression_loss: 0.3480 - classification_loss: 0.0425
 3746/10000 [==========>...................] - ETA: 1:32:31 - loss: 0.3905 - regression_loss: 0.3480 - classification_loss: 0.0425
 3747/10000 [==========>...................] - ETA: 1:32:31 - loss: 0.3904 - regression_loss: 0.3480 - classification_loss: 0.0425
 3748/10000 [==========>...................] - ETA: 1:32:30 - loss: 0.3904 - regression_loss: 0.3479 - classification_loss: 0.0425
 3749/10000 [==========>...................] - ETA: 1:32:29 - loss: 0.3904 - regression_loss: 0.3479 - classification_loss: 0.0425
 3750/10000 [==========>...................] - ETA: 1:32:28 - loss: 0.3906 - regression_loss: 0.3481 - classification_loss: 0.0425
 3751/10000 [==========>...................] - ETA: 1:32:27 - loss: 0.3905 - regression_loss: 0.3480 - classification_loss: 0.0425
 3752/10000 [==========>...................] - ETA: 1:32:26 - loss: 0.3904 - regression_loss: 0.3480 - classification_loss: 0.0425
 3753/10000 [==========>...................] - ETA: 1:32:25 - loss: 0.3904 - regression_loss: 0.3480 - classification_loss: 0.0425
 3754/10000 [==========>...................] - ETA: 1:32:24 - loss: 0.3905 - regression_loss: 0.3481 - classification_loss: 0.0425
 3755/10000 [==========>...................] - ETA: 1:32:23 - loss: 0.3906 - regression_loss: 0.3481 - classification_loss: 0.0424
 3756/10000 [==========>...................] - ETA: 1:32:23 - loss: 0.3906 - regression_loss: 0.3482 - classification_loss: 0.0424
 3757/10000 [==========>...................] - ETA: 1:32:22 - loss: 0.3906 - regression_loss: 0.3482 - classification_loss: 0.0424
 3758/10000 [==========>...................] - ETA: 1:32:21 - loss: 0.3906 - regression_loss: 0.3482 - classification_loss: 0.0424
 3759/10000 [==========>...................] - ETA: 1:32:20 - loss: 0.3907 - regression_loss: 0.3482 - classification_loss: 0.0425
 3760/10000 [==========>...................] - ETA: 1:32:19 - loss: 0.3907 - regression_loss: 0.3482 - classification_loss: 0.0424
 3761/10000 [==========>...................] - ETA: 1:32:18 - loss: 0.3907 - regression_loss: 0.3483 - classification_loss: 0.0424
 3762/10000 [==========>...................] - ETA: 1:32:17 - loss: 0.3907 - regression_loss: 0.3483 - classification_loss: 0.0424
 3763/10000 [==========>...................] - ETA: 1:32:16 - loss: 0.3907 - regression_loss: 0.3483 - classification_loss: 0.0424
 3764/10000 [==========>...................] - ETA: 1:32:15 - loss: 0.3906 - regression_loss: 0.3482 - classification_loss: 0.0424
 3765/10000 [==========>...................] - ETA: 1:32:15 - loss: 0.3906 - regression_loss: 0.3482 - classification_loss: 0.0424
 3766/10000 [==========>...................] - ETA: 1:32:14 - loss: 0.3906 - regression_loss: 0.3482 - classification_loss: 0.0424
 3767/10000 [==========>...................] - ETA: 1:32:13 - loss: 0.3906 - regression_loss: 0.3482 - classification_loss: 0.0424
 3768/10000 [==========>...................] - ETA: 1:32:12 - loss: 0.3906 - regression_loss: 0.3481 - classification_loss: 0.0424
 3769/10000 [==========>...................] - ETA: 1:32:11 - loss: 0.3905 - regression_loss: 0.3481 - classification_loss: 0.0424
 3770/10000 [==========>...................] - ETA: 1:32:10 - loss: 0.3906 - regression_loss: 0.3481 - classification_loss: 0.0424
 3771/10000 [==========>...................] - ETA: 1:32:09 - loss: 0.3906 - regression_loss: 0.3482 - classification_loss: 0.0424
 3772/10000 [==========>...................] - ETA: 1:32:08 - loss: 0.3908 - regression_loss: 0.3484 - classification_loss: 0.0425
 3773/10000 [==========>...................] - ETA: 1:32:08 - loss: 0.3908 - regression_loss: 0.3484 - classification_loss: 0.0425
 3774/10000 [==========>...................] - ETA: 1:32:07 - loss: 0.3908 - regression_loss: 0.3484 - classification_loss: 0.0425
 3775/10000 [==========>...................] - ETA: 1:32:06 - loss: 0.3909 - regression_loss: 0.3484 - classification_loss: 0.0425
 3776/10000 [==========>...................] - ETA: 1:32:05 - loss: 0.3909 - regression_loss: 0.3485 - classification_loss: 0.0425
 3777/10000 [==========>...................] - ETA: 1:32:04 - loss: 0.3910 - regression_loss: 0.3485 - classification_loss: 0.0425
 3778/10000 [==========>...................] - ETA: 1:32:03 - loss: 0.3909 - regression_loss: 0.3485 - classification_loss: 0.0425
 3779/10000 [==========>...................] - ETA: 1:32:02 - loss: 0.3909 - regression_loss: 0.3485 - classification_loss: 0.0425
 3780/10000 [==========>...................] - ETA: 1:32:01 - loss: 0.3908 - regression_loss: 0.3484 - classification_loss: 0.0424
 3781/10000 [==========>...................] - ETA: 1:32:00 - loss: 0.3908 - regression_loss: 0.3483 - classification_loss: 0.0424
 3782/10000 [==========>...................] - ETA: 1:32:00 - loss: 0.3911 - regression_loss: 0.3486 - classification_loss: 0.0425
 3783/10000 [==========>...................] - ETA: 1:31:59 - loss: 0.3911 - regression_loss: 0.3486 - classification_loss: 0.0425
 3784/10000 [==========>...................] - ETA: 1:31:58 - loss: 0.3911 - regression_loss: 0.3486 - classification_loss: 0.0425
 3785/10000 [==========>...................] - ETA: 1:31:57 - loss: 0.3911 - regression_loss: 0.3486 - classification_loss: 0.0425
 3786/10000 [==========>...................] - ETA: 1:31:56 - loss: 0.3911 - regression_loss: 0.3487 - classification_loss: 0.0425
 3787/10000 [==========>...................] - ETA: 1:31:55 - loss: 0.3910 - regression_loss: 0.3486 - classification_loss: 0.0425
 3788/10000 [==========>...................] - ETA: 1:31:54 - loss: 0.3911 - regression_loss: 0.3486 - classification_loss: 0.0425
 3789/10000 [==========>...................] - ETA: 1:31:53 - loss: 0.3910 - regression_loss: 0.3485 - classification_loss: 0.0425
 3790/10000 [==========>...................] - ETA: 1:31:52 - loss: 0.3910 - regression_loss: 0.3485 - classification_loss: 0.0425
 3791/10000 [==========>...................] - ETA: 1:31:52 - loss: 0.3910 - regression_loss: 0.3485 - classification_loss: 0.0425
 3792/10000 [==========>...................] - ETA: 1:31:51 - loss: 0.3910 - regression_loss: 0.3485 - classification_loss: 0.0425
 3793/10000 [==========>...................] - ETA: 1:31:50 - loss: 0.3910 - regression_loss: 0.3485 - classification_loss: 0.0425
 3794/10000 [==========>...................] - ETA: 1:31:49 - loss: 0.3910 - regression_loss: 0.3485 - classification_loss: 0.0425
 3795/10000 [==========>...................] - ETA: 1:31:48 - loss: 0.3910 - regression_loss: 0.3485 - classification_loss: 0.0425
 3796/10000 [==========>...................] - ETA: 1:31:47 - loss: 0.3910 - regression_loss: 0.3485 - classification_loss: 0.0425
 3797/10000 [==========>...................] - ETA: 1:31:46 - loss: 0.3910 - regression_loss: 0.3485 - classification_loss: 0.0425
 3798/10000 [==========>...................] - ETA: 1:31:45 - loss: 0.3916 - regression_loss: 0.3488 - classification_loss: 0.0427
 3799/10000 [==========>...................] - ETA: 1:31:44 - loss: 0.3915 - regression_loss: 0.3488 - classification_loss: 0.0427
 3800/10000 [==========>...................] - ETA: 1:31:44 - loss: 0.3915 - regression_loss: 0.3488 - classification_loss: 0.0427
 3801/10000 [==========>...................] - ETA: 1:31:43 - loss: 0.3916 - regression_loss: 0.3488 - classification_loss: 0.0427
 3802/10000 [==========>...................] - ETA: 1:31:42 - loss: 0.3916 - regression_loss: 0.3488 - classification_loss: 0.0427
 3803/10000 [==========>...................] - ETA: 1:31:41 - loss: 0.3915 - regression_loss: 0.3488 - classification_loss: 0.0427
 3804/10000 [==========>...................] - ETA: 1:31:40 - loss: 0.3915 - regression_loss: 0.3488 - classification_loss: 0.0427
 3805/10000 [==========>...................] - ETA: 1:31:39 - loss: 0.3915 - regression_loss: 0.3487 - classification_loss: 0.0427
 3806/10000 [==========>...................] - ETA: 1:31:38 - loss: 0.3914 - regression_loss: 0.3487 - classification_loss: 0.0427
 3807/10000 [==========>...................] - ETA: 1:31:37 - loss: 0.3914 - regression_loss: 0.3487 - classification_loss: 0.0427
 3808/10000 [==========>...................] - ETA: 1:31:36 - loss: 0.3914 - regression_loss: 0.3487 - classification_loss: 0.0427
 3809/10000 [==========>...................] - ETA: 1:31:36 - loss: 0.3914 - regression_loss: 0.3487 - classification_loss: 0.0427
 3810/10000 [==========>...................] - ETA: 1:31:35 - loss: 0.3913 - regression_loss: 0.3486 - classification_loss: 0.0427
 3811/10000 [==========>...................] - ETA: 1:31:34 - loss: 0.3914 - regression_loss: 0.3486 - classification_loss: 0.0427
 3812/10000 [==========>...................] - ETA: 1:31:33 - loss: 0.3913 - regression_loss: 0.3486 - classification_loss: 0.0427
 3813/10000 [==========>...................] - ETA: 1:31:32 - loss: 0.3913 - regression_loss: 0.3486 - classification_loss: 0.0427
 3814/10000 [==========>...................] - ETA: 1:31:31 - loss: 0.3913 - regression_loss: 0.3486 - classification_loss: 0.0427
 3815/10000 [==========>...................] - ETA: 1:31:30 - loss: 0.3914 - regression_loss: 0.3486 - classification_loss: 0.0427
 3816/10000 [==========>...................] - ETA: 1:31:29 - loss: 0.3913 - regression_loss: 0.3486 - classification_loss: 0.0427
 3817/10000 [==========>...................] - ETA: 1:31:28 - loss: 0.3913 - regression_loss: 0.3486 - classification_loss: 0.0427
 3818/10000 [==========>...................] - ETA: 1:31:28 - loss: 0.3915 - regression_loss: 0.3488 - classification_loss: 0.0427
 3819/10000 [==========>...................] - ETA: 1:31:27 - loss: 0.3914 - regression_loss: 0.3488 - classification_loss: 0.0427
 3820/10000 [==========>...................] - ETA: 1:31:26 - loss: 0.3914 - regression_loss: 0.3488 - classification_loss: 0.0427
 3821/10000 [==========>...................] - ETA: 1:31:25 - loss: 0.3914 - regression_loss: 0.3487 - classification_loss: 0.0427
 3822/10000 [==========>...................] - ETA: 1:31:24 - loss: 0.3913 - regression_loss: 0.3487 - classification_loss: 0.0427
 3823/10000 [==========>...................] - ETA: 1:31:23 - loss: 0.3913 - regression_loss: 0.3486 - classification_loss: 0.0427
 3824/10000 [==========>...................] - ETA: 1:31:22 - loss: 0.3913 - regression_loss: 0.3486 - classification_loss: 0.0427
 3825/10000 [==========>...................] - ETA: 1:31:21 - loss: 0.3912 - regression_loss: 0.3486 - classification_loss: 0.0427
 3826/10000 [==========>...................] - ETA: 1:31:20 - loss: 0.3912 - regression_loss: 0.3485 - classification_loss: 0.0427
 3827/10000 [==========>...................] - ETA: 1:31:20 - loss: 0.3912 - regression_loss: 0.3485 - classification_loss: 0.0427
 3828/10000 [==========>...................] - ETA: 1:31:19 - loss: 0.3911 - regression_loss: 0.3484 - classification_loss: 0.0427
 3829/10000 [==========>...................] - ETA: 1:31:18 - loss: 0.3912 - regression_loss: 0.3485 - classification_loss: 0.0427
 3830/10000 [==========>...................] - ETA: 1:31:17 - loss: 0.3911 - regression_loss: 0.3485 - classification_loss: 0.0427
 3831/10000 [==========>...................] - ETA: 1:31:16 - loss: 0.3911 - regression_loss: 0.3485 - classification_loss: 0.0427
 3832/10000 [==========>...................] - ETA: 1:31:15 - loss: 0.3912 - regression_loss: 0.3485 - classification_loss: 0.0427
 3833/10000 [==========>...................] - ETA: 1:31:14 - loss: 0.3911 - regression_loss: 0.3484 - classification_loss: 0.0426
 3834/10000 [==========>...................] - ETA: 1:31:13 - loss: 0.3912 - regression_loss: 0.3485 - classification_loss: 0.0427
 3835/10000 [==========>...................] - ETA: 1:31:12 - loss: 0.3912 - regression_loss: 0.3485 - classification_loss: 0.0427
 3836/10000 [==========>...................] - ETA: 1:31:12 - loss: 0.3918 - regression_loss: 0.3488 - classification_loss: 0.0430
 3837/10000 [==========>...................] - ETA: 1:31:11 - loss: 0.3918 - regression_loss: 0.3488 - classification_loss: 0.0430
 3838/10000 [==========>...................] - ETA: 1:31:10 - loss: 0.3918 - regression_loss: 0.3488 - classification_loss: 0.0430
 3839/10000 [==========>...................] - ETA: 1:31:09 - loss: 0.3918 - regression_loss: 0.3488 - classification_loss: 0.0430
 3840/10000 [==========>...................] - ETA: 1:31:08 - loss: 0.3917 - regression_loss: 0.3488 - classification_loss: 0.0430
 3841/10000 [==========>...................] - ETA: 1:31:07 - loss: 0.3917 - regression_loss: 0.3487 - classification_loss: 0.0430
 3842/10000 [==========>...................] - ETA: 1:31:06 - loss: 0.3917 - regression_loss: 0.3488 - classification_loss: 0.0429
 3843/10000 [==========>...................] - ETA: 1:31:05 - loss: 0.3917 - regression_loss: 0.3487 - classification_loss: 0.0430
 3844/10000 [==========>...................] - ETA: 1:31:04 - loss: 0.3918 - regression_loss: 0.3488 - classification_loss: 0.0430
 3845/10000 [==========>...................] - ETA: 1:31:04 - loss: 0.3917 - regression_loss: 0.3488 - classification_loss: 0.0430
 3846/10000 [==========>...................] - ETA: 1:31:03 - loss: 0.3918 - regression_loss: 0.3488 - classification_loss: 0.0430
 3847/10000 [==========>...................] - ETA: 1:31:02 - loss: 0.3917 - regression_loss: 0.3488 - classification_loss: 0.0429
 3848/10000 [==========>...................] - ETA: 1:31:01 - loss: 0.3917 - regression_loss: 0.3488 - classification_loss: 0.0429
 3849/10000 [==========>...................] - ETA: 1:31:00 - loss: 0.3916 - regression_loss: 0.3487 - classification_loss: 0.0429
 3850/10000 [==========>...................] - ETA: 1:30:59 - loss: 0.3915 - regression_loss: 0.3486 - classification_loss: 0.0429
 3851/10000 [==========>...................] - ETA: 1:30:58 - loss: 0.3915 - regression_loss: 0.3486 - classification_loss: 0.0429
 3852/10000 [==========>...................] - ETA: 1:30:57 - loss: 0.3915 - regression_loss: 0.3486 - classification_loss: 0.0429
 3853/10000 [==========>...................] - ETA: 1:30:57 - loss: 0.3921 - regression_loss: 0.3490 - classification_loss: 0.0431
 3854/10000 [==========>...................] - ETA: 1:30:56 - loss: 0.3921 - regression_loss: 0.3490 - classification_loss: 0.0431
 3855/10000 [==========>...................] - ETA: 1:30:55 - loss: 0.3921 - regression_loss: 0.3490 - classification_loss: 0.0431
 3856/10000 [==========>...................] - ETA: 1:30:54 - loss: 0.3921 - regression_loss: 0.3490 - classification_loss: 0.0431
 3857/10000 [==========>...................] - ETA: 1:30:53 - loss: 0.3921 - regression_loss: 0.3490 - classification_loss: 0.0431
 3858/10000 [==========>...................] - ETA: 1:30:52 - loss: 0.3922 - regression_loss: 0.3490 - classification_loss: 0.0431
 3859/10000 [==========>...................] - ETA: 1:30:51 - loss: 0.3922 - regression_loss: 0.3491 - classification_loss: 0.0431
 3860/10000 [==========>...................] - ETA: 1:30:50 - loss: 0.3922 - regression_loss: 0.3490 - classification_loss: 0.0431
 3861/10000 [==========>...................] - ETA: 1:30:49 - loss: 0.3922 - regression_loss: 0.3491 - classification_loss: 0.0431
 3862/10000 [==========>...................] - ETA: 1:30:49 - loss: 0.3923 - regression_loss: 0.3492 - classification_loss: 0.0431
 3863/10000 [==========>...................] - ETA: 1:30:48 - loss: 0.3922 - regression_loss: 0.3491 - classification_loss: 0.0431
 3864/10000 [==========>...................] - ETA: 1:30:47 - loss: 0.3923 - regression_loss: 0.3491 - classification_loss: 0.0431
 3865/10000 [==========>...................] - ETA: 1:30:46 - loss: 0.3923 - regression_loss: 0.3492 - classification_loss: 0.0431
 3866/10000 [==========>...................] - ETA: 1:30:45 - loss: 0.3923 - regression_loss: 0.3491 - classification_loss: 0.0431
 3867/10000 [==========>...................] - ETA: 1:30:44 - loss: 0.3922 - regression_loss: 0.3491 - classification_loss: 0.0431
 3868/10000 [==========>...................] - ETA: 1:30:43 - loss: 0.3922 - regression_loss: 0.3491 - classification_loss: 0.0431
 3869/10000 [==========>...................] - ETA: 1:30:42 - loss: 0.3922 - regression_loss: 0.3491 - classification_loss: 0.0431
 3870/10000 [==========>...................] - ETA: 1:30:41 - loss: 0.3922 - regression_loss: 0.3490 - classification_loss: 0.0431
 3871/10000 [==========>...................] - ETA: 1:30:41 - loss: 0.3922 - regression_loss: 0.3491 - classification_loss: 0.0431
 3872/10000 [==========>...................] - ETA: 1:30:40 - loss: 0.3922 - regression_loss: 0.3491 - classification_loss: 0.0431
 3873/10000 [==========>...................] - ETA: 1:30:39 - loss: 0.3922 - regression_loss: 0.3491 - classification_loss: 0.0431
 3874/10000 [==========>...................] - ETA: 1:30:38 - loss: 0.3921 - regression_loss: 0.3490 - classification_loss: 0.0431
 3875/10000 [==========>...................] - ETA: 1:30:37 - loss: 0.3920 - regression_loss: 0.3489 - classification_loss: 0.0431
 3876/10000 [==========>...................] - ETA: 1:30:36 - loss: 0.3919 - regression_loss: 0.3488 - classification_loss: 0.0431
 3877/10000 [==========>...................] - ETA: 1:30:35 - loss: 0.3919 - regression_loss: 0.3488 - classification_loss: 0.0431
 3878/10000 [==========>...................] - ETA: 1:30:34 - loss: 0.3920 - regression_loss: 0.3489 - classification_loss: 0.0431
 3879/10000 [==========>...................] - ETA: 1:30:33 - loss: 0.3920 - regression_loss: 0.3489 - classification_loss: 0.0431
 3880/10000 [==========>...................] - ETA: 1:30:33 - loss: 0.3919 - regression_loss: 0.3489 - classification_loss: 0.0431
 3881/10000 [==========>...................] - ETA: 1:30:32 - loss: 0.3919 - regression_loss: 0.3488 - classification_loss: 0.0431
 3882/10000 [==========>...................] - ETA: 1:30:31 - loss: 0.3919 - regression_loss: 0.3488 - classification_loss: 0.0430
 3883/10000 [==========>...................] - ETA: 1:30:30 - loss: 0.3920 - regression_loss: 0.3489 - classification_loss: 0.0430
 3884/10000 [==========>...................] - ETA: 1:30:29 - loss: 0.3922 - regression_loss: 0.3492 - classification_loss: 0.0430
 3885/10000 [==========>...................] - ETA: 1:30:28 - loss: 0.3923 - regression_loss: 0.3493 - classification_loss: 0.0431
 3886/10000 [==========>...................] - ETA: 1:30:27 - loss: 0.3923 - regression_loss: 0.3492 - classification_loss: 0.0431
 3887/10000 [==========>...................] - ETA: 1:30:26 - loss: 0.3923 - regression_loss: 0.3493 - classification_loss: 0.0431
 3888/10000 [==========>...................] - ETA: 1:30:25 - loss: 0.3922 - regression_loss: 0.3492 - classification_loss: 0.0431
 3889/10000 [==========>...................] - ETA: 1:30:25 - loss: 0.3922 - regression_loss: 0.3492 - classification_loss: 0.0431
 3890/10000 [==========>...................] - ETA: 1:30:24 - loss: 0.3922 - regression_loss: 0.3491 - classification_loss: 0.0431
 3891/10000 [==========>...................] - ETA: 1:30:23 - loss: 0.3922 - regression_loss: 0.3491 - classification_loss: 0.0431
 3892/10000 [==========>...................] - ETA: 1:30:22 - loss: 0.3922 - regression_loss: 0.3492 - classification_loss: 0.0431
 3893/10000 [==========>...................] - ETA: 1:30:21 - loss: 0.3922 - regression_loss: 0.3492 - classification_loss: 0.0430
 3894/10000 [==========>...................] - ETA: 1:30:20 - loss: 0.3922 - regression_loss: 0.3492 - classification_loss: 0.0431
 3895/10000 [==========>...................] - ETA: 1:30:19 - loss: 0.3922 - regression_loss: 0.3492 - classification_loss: 0.0431
 3896/10000 [==========>...................] - ETA: 1:30:18 - loss: 0.3923 - regression_loss: 0.3492 - classification_loss: 0.0431
 3897/10000 [==========>...................] - ETA: 1:30:17 - loss: 0.3922 - regression_loss: 0.3492 - classification_loss: 0.0430
 3898/10000 [==========>...................] - ETA: 1:30:17 - loss: 0.3921 - regression_loss: 0.3491 - classification_loss: 0.0430
 3899/10000 [==========>...................] - ETA: 1:30:16 - loss: 0.3921 - regression_loss: 0.3491 - classification_loss: 0.0430
 3900/10000 [==========>...................] - ETA: 1:30:15 - loss: 0.3921 - regression_loss: 0.3491 - classification_loss: 0.0430
 3901/10000 [==========>...................] - ETA: 1:30:14 - loss: 0.3922 - regression_loss: 0.3491 - classification_loss: 0.0431
 3902/10000 [==========>...................] - ETA: 1:30:13 - loss: 0.3924 - regression_loss: 0.3493 - classification_loss: 0.0431
 3903/10000 [==========>...................] - ETA: 1:30:13 - loss: 0.3924 - regression_loss: 0.3493 - classification_loss: 0.0431
 3904/10000 [==========>...................] - ETA: 1:30:12 - loss: 0.3924 - regression_loss: 0.3493 - classification_loss: 0.0431
 3905/10000 [==========>...................] - ETA: 1:30:11 - loss: 0.3924 - regression_loss: 0.3493 - classification_loss: 0.0431
 3906/10000 [==========>...................] - ETA: 1:30:10 - loss: 0.3923 - regression_loss: 0.3493 - classification_loss: 0.0431
 3907/10000 [==========>...................] - ETA: 1:30:09 - loss: 0.3922 - regression_loss: 0.3492 - classification_loss: 0.0431
 3908/10000 [==========>...................] - ETA: 1:30:08 - loss: 0.3922 - regression_loss: 0.3492 - classification_loss: 0.0431
 3909/10000 [==========>...................] - ETA: 1:30:07 - loss: 0.3922 - regression_loss: 0.3491 - classification_loss: 0.0431
 3910/10000 [==========>...................] - ETA: 1:30:06 - loss: 0.3922 - regression_loss: 0.3491 - classification_loss: 0.0431
 3911/10000 [==========>...................] - ETA: 1:30:05 - loss: 0.3921 - regression_loss: 0.3490 - classification_loss: 0.0430
 3912/10000 [==========>...................] - ETA: 1:30:05 - loss: 0.3920 - regression_loss: 0.3490 - classification_loss: 0.0430
 3913/10000 [==========>...................] - ETA: 1:30:04 - loss: 0.3920 - regression_loss: 0.3489 - classification_loss: 0.0430
 3914/10000 [==========>...................] - ETA: 1:30:03 - loss: 0.3919 - regression_loss: 0.3489 - classification_loss: 0.0430
 3915/10000 [==========>...................] - ETA: 1:30:02 - loss: 0.3918 - regression_loss: 0.3488 - classification_loss: 0.0430
 3916/10000 [==========>...................] - ETA: 1:30:01 - loss: 0.3918 - regression_loss: 0.3488 - classification_loss: 0.0430
 3917/10000 [==========>...................] - ETA: 1:30:00 - loss: 0.3919 - regression_loss: 0.3488 - classification_loss: 0.0430
 3918/10000 [==========>...................] - ETA: 1:29:59 - loss: 0.3919 - regression_loss: 0.3489 - classification_loss: 0.0430
 3919/10000 [==========>...................] - ETA: 1:29:58 - loss: 0.3919 - regression_loss: 0.3488 - classification_loss: 0.0430
 3920/10000 [==========>...................] - ETA: 1:29:57 - loss: 0.3919 - regression_loss: 0.3489 - classification_loss: 0.0430
 3921/10000 [==========>...................] - ETA: 1:29:57 - loss: 0.3919 - regression_loss: 0.3489 - classification_loss: 0.0430
 3922/10000 [==========>...................] - ETA: 1:29:56 - loss: 0.3919 - regression_loss: 0.3489 - classification_loss: 0.0430
 3923/10000 [==========>...................] - ETA: 1:29:55 - loss: 0.3919 - regression_loss: 0.3489 - classification_loss: 0.0430
 3924/10000 [==========>...................] - ETA: 1:29:54 - loss: 0.3918 - regression_loss: 0.3488 - classification_loss: 0.0430
 3925/10000 [==========>...................] - ETA: 1:29:53 - loss: 0.3919 - regression_loss: 0.3488 - classification_loss: 0.0430
 3926/10000 [==========>...................] - ETA: 1:29:52 - loss: 0.3919 - regression_loss: 0.3488 - classification_loss: 0.0430
 3927/10000 [==========>...................] - ETA: 1:29:51 - loss: 0.3919 - regression_loss: 0.3488 - classification_loss: 0.0430
 3928/10000 [==========>...................] - ETA: 1:29:50 - loss: 0.3919 - regression_loss: 0.3489 - classification_loss: 0.0430
 3929/10000 [==========>...................] - ETA: 1:29:49 - loss: 0.3920 - regression_loss: 0.3489 - classification_loss: 0.0430
 3930/10000 [==========>...................] - ETA: 1:29:49 - loss: 0.3919 - regression_loss: 0.3489 - classification_loss: 0.0430
 3931/10000 [==========>...................] - ETA: 1:29:48 - loss: 0.3919 - regression_loss: 0.3488 - classification_loss: 0.0430
 3932/10000 [==========>...................] - ETA: 1:29:47 - loss: 0.3919 - regression_loss: 0.3489 - classification_loss: 0.0430
 3933/10000 [==========>...................] - ETA: 1:29:46 - loss: 0.3919 - regression_loss: 0.3488 - classification_loss: 0.0430
 3934/10000 [==========>...................] - ETA: 1:29:45 - loss: 0.3919 - regression_loss: 0.3488 - classification_loss: 0.0430
 3935/10000 [==========>...................] - ETA: 1:29:44 - loss: 0.3918 - regression_loss: 0.3488 - classification_loss: 0.0430
 3936/10000 [==========>...................] - ETA: 1:29:43 - loss: 0.3918 - regression_loss: 0.3487 - classification_loss: 0.0430
 3937/10000 [==========>...................] - ETA: 1:29:42 - loss: 0.3918 - regression_loss: 0.3488 - classification_loss: 0.0430
 3938/10000 [==========>...................] - ETA: 1:29:41 - loss: 0.3917 - regression_loss: 0.3487 - classification_loss: 0.0430
 3939/10000 [==========>...................] - ETA: 1:29:41 - loss: 0.3918 - regression_loss: 0.3488 - classification_loss: 0.0430
 3940/10000 [==========>...................] - ETA: 1:29:40 - loss: 0.3918 - regression_loss: 0.3488 - classification_loss: 0.0430
 3941/10000 [==========>...................] - ETA: 1:29:39 - loss: 0.3918 - regression_loss: 0.3488 - classification_loss: 0.0430
 3942/10000 [==========>...................] - ETA: 1:29:38 - loss: 0.3919 - regression_loss: 0.3489 - classification_loss: 0.0430
 3943/10000 [==========>...................] - ETA: 1:29:37 - loss: 0.3921 - regression_loss: 0.3490 - classification_loss: 0.0430
 3944/10000 [==========>...................] - ETA: 1:29:36 - loss: 0.3921 - regression_loss: 0.3491 - classification_loss: 0.0430
 3945/10000 [==========>...................] - ETA: 1:29:35 - loss: 0.3923 - regression_loss: 0.3493 - classification_loss: 0.0431
 3946/10000 [==========>...................] - ETA: 1:29:34 - loss: 0.3923 - regression_loss: 0.3492 - classification_loss: 0.0430
 3947/10000 [==========>...................] - ETA: 1:29:34 - loss: 0.3925 - regression_loss: 0.3494 - classification_loss: 0.0431
 3948/10000 [==========>...................] - ETA: 1:29:33 - loss: 0.3926 - regression_loss: 0.3496 - classification_loss: 0.0431
 3949/10000 [==========>...................] - ETA: 1:29:32 - loss: 0.3926 - regression_loss: 0.3495 - classification_loss: 0.0431
 3950/10000 [==========>...................] - ETA: 1:29:31 - loss: 0.3928 - regression_loss: 0.3497 - classification_loss: 0.0431
 3951/10000 [==========>...................] - ETA: 1:29:30 - loss: 0.3930 - regression_loss: 0.3499 - classification_loss: 0.0431
 3952/10000 [==========>...................] - ETA: 1:29:29 - loss: 0.3930 - regression_loss: 0.3499 - classification_loss: 0.0431
 3953/10000 [==========>...................] - ETA: 1:29:28 - loss: 0.3930 - regression_loss: 0.3499 - classification_loss: 0.0431
 3954/10000 [==========>...................] - ETA: 1:29:27 - loss: 0.3930 - regression_loss: 0.3499 - classification_loss: 0.0431
 3955/10000 [==========>...................] - ETA: 1:29:26 - loss: 0.3931 - regression_loss: 0.3500 - classification_loss: 0.0431
 3956/10000 [==========>...................] - ETA: 1:29:26 - loss: 0.3931 - regression_loss: 0.3500 - classification_loss: 0.0431
 3957/10000 [==========>...................] - ETA: 1:29:25 - loss: 0.3934 - regression_loss: 0.3502 - classification_loss: 0.0432
 3958/10000 [==========>...................] - ETA: 1:29:24 - loss: 0.3934 - regression_loss: 0.3502 - classification_loss: 0.0432
 3959/10000 [==========>...................] - ETA: 1:29:23 - loss: 0.3934 - regression_loss: 0.3502 - classification_loss: 0.0432
 3960/10000 [==========>...................] - ETA: 1:29:22 - loss: 0.3936 - regression_loss: 0.3503 - classification_loss: 0.0432
 3961/10000 [==========>...................] - ETA: 1:29:21 - loss: 0.3935 - regression_loss: 0.3503 - classification_loss: 0.0432
 3962/10000 [==========>...................] - ETA: 1:29:20 - loss: 0.3935 - regression_loss: 0.3502 - classification_loss: 0.0432
 3963/10000 [==========>...................] - ETA: 1:29:19 - loss: 0.3934 - regression_loss: 0.3502 - classification_loss: 0.0432
 3964/10000 [==========>...................] - ETA: 1:29:18 - loss: 0.3934 - regression_loss: 0.3502 - classification_loss: 0.0432
 3965/10000 [==========>...................] - ETA: 1:29:18 - loss: 0.3933 - regression_loss: 0.3501 - classification_loss: 0.0432
 3966/10000 [==========>...................] - ETA: 1:29:17 - loss: 0.3934 - regression_loss: 0.3501 - classification_loss: 0.0432
 3967/10000 [==========>...................] - ETA: 1:29:16 - loss: 0.3933 - regression_loss: 0.3501 - classification_loss: 0.0432
 3968/10000 [==========>...................] - ETA: 1:29:15 - loss: 0.3932 - regression_loss: 0.3500 - classification_loss: 0.0432
 3969/10000 [==========>...................] - ETA: 1:29:14 - loss: 0.3932 - regression_loss: 0.3500 - classification_loss: 0.0432
 3970/10000 [==========>...................] - ETA: 1:29:13 - loss: 0.3932 - regression_loss: 0.3500 - classification_loss: 0.0432
 3971/10000 [==========>...................] - ETA: 1:29:12 - loss: 0.3932 - regression_loss: 0.3500 - classification_loss: 0.0432
 3972/10000 [==========>...................] - ETA: 1:29:11 - loss: 0.3932 - regression_loss: 0.3500 - classification_loss: 0.0432
 3973/10000 [==========>...................] - ETA: 1:29:10 - loss: 0.3931 - regression_loss: 0.3499 - classification_loss: 0.0432
 3974/10000 [==========>...................] - ETA: 1:29:10 - loss: 0.3931 - regression_loss: 0.3499 - classification_loss: 0.0432
 3975/10000 [==========>...................] - ETA: 1:29:09 - loss: 0.3931 - regression_loss: 0.3499 - classification_loss: 0.0432
 3976/10000 [==========>...................] - ETA: 1:29:08 - loss: 0.3932 - regression_loss: 0.3500 - classification_loss: 0.0432slurmstepd: error: *** JOB 3477062 ON pg-gpu01 CANCELLED AT 2019-01-19T20:25:40 ***


###############################################################################
Peregrine Cluster
Job 3477062 for user 's3801128'
Finished at: Sat Jan 19 20:25:43 CET 2019

Job details:
============

Name                : 2nd Time,19-> Train retinanet 1 gpu vgg16 backbone
User                : s3801128
Partition           : gpu
Nodes               : pg-gpu01
Cores               : 12
State               : CANCELLED,CANCELLED by 33801128
Submit              : 2019-01-19T14:56:54
Start               : 2019-01-19T16:55:28
End                 : 2019-01-19T20:25:43
Reserved walltime   : 2-00:00:00
Used walltime       :   03:30:15
Used CPU time       : 5-18:45:30 (efficiency: 329.98%)
% User (Computation): 81.65%
% System (I/O)      : 18.35%
Mem reserved        : 8G/node
Max Mem used        : 1.76G (pg-gpu01)
Max Disk Write      : 342.95M (pg-gpu01)
Max Disk Read       : 277.37M (pg-gpu01)


Acknowledgements:
=================

Please see this page if you want to acknowledge Peregrine in your publications:

https://redmine.hpc.rug.nl/redmine/projects/peregrine/wiki/ScientificOutput

################################################################################
